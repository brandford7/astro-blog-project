const _astro_dataLayerContent = [["Map",1,2,9,10,2017,2018,2264,2265,2302,2303],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.4.1","content-config-digest","4bb9a2089dcc32e5","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://www.codetidehub.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"always\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":true,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image/\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"one-dark-pro\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"serializeConfig\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,33,34,51,52,68,69,89,90,108,109,125,126,143,144,161,162,178,179,196,197,214,215,232,233,250,251,267,268,284,285,302,303,320,321,337,338,354,355,371,372,388,389,405,406,422,423,439,440,457,458,474,475,491,492,509,510,527,528,544,545,562,563,582,583,599,600,635,636,652,653,670,671,687,688,704,705,721,722,738,739,755,756,772,773,789,790,806,807,823,824,840,841,857,858,874,875,891,892,909,910,926,927,943,944,960,961,978,979,995,996,1012,1013,1029,1030,1047,1048,1064,1065,1081,1082,1098,1099,1115,1116,1132,1133,1152,1153,1169,1170,1188,1189,1206,1207,1224,1225,1241,1242,1258,1259,1275,1276,1292,1293,1309,1310,1327,1328,1345,1346,1362,1363,1379,1380,1396,1397,449,1413,1429,1430,1447,1448,1464,1465,1481,1482,1498,1499,1515,1516,1533,1534,1550,1551,1568,1569,1585,1586,1602,1603,1620,1621,1637,1638,1654,1655,1671,1672,1688,1689,1705,1706,1723,1724,1740,1741,1757,1758,1775,1776,1792,1793,1809,1810,1826,1827,1844,1845,1861,1862,1878,1879,1896,1897,1914,1915,1931,1932,1948,1949,1965,1966,1982,1983,2000,2001],"add-contact-form-astro",{id:11,data:13,body:26,filePath:27,assetImports:28,digest:30,legacyId:31,deferredRender:32},{title:14,description:15,date:16,image:17,authors:18,categories:20,tags:22,canonical:25},"How To Add A Contact Form To Astro","See how you can easily add a contact form to an astro website or any static website and get notified via email with no spam.",["Date","2023-08-18T06:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/08/astro-add-contact-form.jpeg",[19],"Dragos",[21],"cms",[23,24],"astro","tutorials","https://www.bitdoze.com/add-contact-form-astro/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/08/opnform-settings.jpeg\";\r\n\r\nIn this tutorial I will show you how you can easily add a contact form to your Astro website, you can use the tutorial to add a contact form to any static website like NextJS or Nuxt,etc, but in this tutorial, we are going to use Astro.\r\n\r\n> You can check the tutorial: [Add A Contact Form To Any Static Website](https://www.bitdoze.com/add-contact-form-static-websites/) OpnForms is not free any more to send emails.\r\n\r\nWith static websites is more difficult to integrate a contact form that is sending emails and most of the time you need something external. We are going to use [OpnForms](https://www.bitdoze.com/opnform-open-source/) to create our form and add it to Astro contact page.\r\n\r\nYou don't need any special skils to do this, get notified via email and not receive spam.\r\n\r\n## How To Add A Contact Form To Astro\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/EhE6H9nCm8U\"\r\n  label=\"OpnForm - Add A Contact Form To Astro Free\"\r\n/>\r\n\r\n### 1. Create a form in OpnForm\r\n\r\nIf you don't have an account you just need to go to [OpnForm](https://opnform.com/) and create a free account. After you should go and create a new form.\r\nOpnForm allows you to add a basic form easily and change the structure and make files as required, in the video you will have all the details.\r\nYou can change the colours for your button and add a logo if you want, change the message and enable the notification via EMAIL or slack/discord.\r\nIn Security and privacy activate the Captcha.\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"OpnForm settings\"\r\n/>\r\n\r\n### 2. Get The enbed Code and Insert it to Astro\r\n\r\nI am using a [free Astro theme](https://astro.build/themes/details/bookworm-light/) for this demonstration as a contact page with a form that does not send any email. I will go into form share and grab the embed code and just add it there, it will be under _src/\\_pages/contact.astro_ :\r\n\r\n```html\r\n<iframe\r\n  style=\"border:none;width:100%;\"\r\n  height=\"410px\"\r\n  src=\"https://opnform.com/forms/contact-form-test-vmak0b\"\r\n></iframe>\r\n```\r\n\r\nYou can alter the height to look nicer in the function of the theme. In the video, you have the details.\r\n\r\n## Conclusions\r\n\r\nAnd that's about it with these simple steps you have a contact form added to your Astro static website in 2 minutes and ready to receive notifications. Hope you enjoy the article, if you have something to say just go to the video and drop a comment.","src/content/posts/add-contact-form-astro.mdx",[29],"../../assets/images/23/08/astro-add-contact-form.jpeg","fbbe83c1f25e8f8b","add-contact-form-astro.mdx",true,"add-cookie-notice-carrd",{id:33,data:35,body:45,filePath:46,assetImports:47,digest:49,legacyId:50,deferredRender:32},{title:36,description:37,date:38,image:39,authors:40,categories:41,tags:42,canonical:44},"Upgrade Your Carrd.co Website With A Cookie Notice in Minutes","Enhance your website's functionality and legal compliance in just a few minutes with our easy-to-implement cookie notice code for Carrd.co.",["Date","2023-05-30T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/05/add_cookie_carrd.jpeg",[19],[21],[43],"carrd","https://www.bitdoze.com/add-cookie-notice-carrd/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/05/carrd-embed.png\";\r\n\r\nA cookie notice is an important aspect of any website because it informs visitors about the use of cookies on the site. Cookies are small text files that are stored on a user's device when they visit a website. They can be used to track user behavior, personalize content, and remember user preferences. However, because cookies collect personal data, their use is regulated by data protection laws such as GDPR and CCPA.\r\n\r\nA cookie notice informs visitors that cookies are being used on the website and provides information on what data is being collected, why it is being collected, and how it will be used. This allows users to make an informed decision about whether or not they want to consent to the use of cookies.\r\n\r\nImplementing a cookie notice is not only a legal requirement in many jurisdictions, but it also helps to build trust with website visitors. It shows that the website owner respects their privacy and is transparent about how their data is being used. Without a cookie notice, website owners risk facing legal penalties and losing the trust of their visitors. Therefore, it is important for website owners to implement a cookie notice to ensure compliance with data protection laws and to build trust with their users.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nIn this article and video we are going to see how you can add a cookie notice on your [carrd.co](https://go.bitdoze.com/carrd) website, you will need to have the pro options with the embed functionality to add the cookey notice.\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\nThe code that we are going to use is below and can be changed to add other text or links as you like, you can change the colors as you like.\r\nIn case you want more details and tutorials on Carrd.co you can check the bellow articles:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [How To Add Custom Domain to Carrd.co](https://www.bitdoze.com/carrd-add-domain/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n\r\n  <YouTubeEmbed\r\n    url=\"https://www.youtube.com/embed/u3F19tG0hnE\"\r\n    label=\"Upgrade Your Carrd.co Website with A Cookie Notice in Minutes\"\r\n  />\r\n\r\n<Button link=\"https:/go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\n## Cookie Notice Code For Carrd\r\n\r\n```html\r\n<p id=\"cookie-notice\">\r\n  This website uses cookies to ensure you get the best experience on our\r\n  website<br /><button onclick=\"acceptCookie();\">Got it!</button>\r\n</p>\r\n\r\n<style>\r\n  #cookie-notice {\r\n    color: #fff;\r\n    font-family: inherit;\r\n    background: #596cd5;\r\n    padding: 20px;\r\n    position: fixed;\r\n    bottom: 10px;\r\n    left: 10px;\r\n    width: 100%;\r\n    max-width: 300px;\r\n    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\r\n    border-radius: 5px;\r\n    margin: 0px;\r\n    visibility: hidden;\r\n    z-index: 1000000;\r\n    box-sizing: border-box;\r\n  }\r\n  #cookie-notice button {\r\n    color: inherit;\r\n    background: #3842c7;\r\n    border: 0;\r\n    padding: 10px;\r\n    margin-top: 10px;\r\n    width: 100%;\r\n    cursor: pointer;\r\n  }\r\n  @media only screen and (max-width: 600px) {\r\n    #cookie-notice {\r\n      max-width: 100%;\r\n      bottom: 0;\r\n      left: 0;\r\n      border-radius: 0;\r\n    }\r\n  }\r\n</style>\r\n\r\n<script>\r\n  function acceptCookie() {\r\n    (document.cookie =\r\n      \"cookieaccepted=1; expires=\" +\r\n      new Date(Date.now() + 86400000).toUTCString() +\r\n      \"; path=/\"),\r\n      (document.getElementById(\"cookie-notice\").style.visibility = \"hidden\");\r\n  }\r\n  document.cookie.indexOf(\"cookieaccepted\") < 0 &&\r\n    (document.getElementById(\"cookie-notice\").style.visibility = \"visible\");\r\n</script>\r\n```\r\n\r\nThis code is used to display a cookie notice on a website and to set a cookie when the user accepts the notice.\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\nThe code first creates a paragraph element with an ID of \"cookie-notice\" that contains a message informing the user that the website uses cookies and a button labeled \"Got it!\" that the user can click to accept the notice.\r\n\r\nThe code then defines a style for the \"cookie-notice\" element, which sets the background color, font color, padding, position, width, and other properties of the notice. The style also sets the visibility of the notice to \"hidden\" by default.\r\n\r\nThe code then defines a JavaScript function called \"acceptCookie\" that is called when the user clicks the \"Got it!\" button. The function sets a cookie named \"cookieaccepted\" with a value of \"1\" and an expiration date of 1 day. The function also sets the visibility of the \"cookie-notice\" element to \"hidden\" so that it disappears from the webpage.\r\n\r\nFinally, the code checks if the \"cookieaccepted\" cookie has already been set by checking if the string \"cookieaccepted\" appears in the \"document.cookie\" string. If the cookie has not been set, the code sets the visibility of the \"cookie-notice\" element to \"visible\" so that it appears on the webpage.\r\n\r\n## How To Add The Cookie on Carrd\r\n\r\n### 1. Add the Embed Widget To The Bottom of the Website\r\n\r\nYou just need to add the embed widget on the bottom of the website, in here you can choose the **Type** as code and you add a **Label** name so you can understand what it is. as in bellow picture\"\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"Carrd embed\"\r\n/>\r\n\r\n### 2. Add the Cookie Code to the Carrd Website\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nIn the code area, you just copy the code above and you can modify the text by adding html into the first **p** block, you can change the color of the background 596cd5 with color you like in case you want something else, to increase the cookie duration you can modify the 86400000 which is for 1 day.\r\n\r\n### 3. Save The changes\r\n\r\nThe only thing remaining now is to save your changes and to check that everything is as you like, in carrd.co you can't see the changes live so you need to save them.\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />","src/content/posts/add-cookie-notice-carrd.mdx",[48],"../../assets/images/23/05/add_cookie_carrd.jpeg","e61c08b96c3f471f","add-cookie-notice-carrd.mdx","add-contact-form-static-websites",{id:51,data:53,body:62,filePath:63,assetImports:64,digest:66,legacyId:67,deferredRender:32},{title:54,description:55,date:56,image:57,authors:58,categories:59,tags:60,canonical:61},"How To Add A Contact Form To Any Static Website","Add a contact for to any static website free and easy in 2024 with formsubmit.co.",["Date","2024-01-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/contact-form-static-websites.jpeg",[19],[21],[23,24],"https://www.bitdoze.com/add-contact-form-static-websites/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nIn the past, I have created a tutorial with [OpnForms to add a contact form to Astro](https://www.bitdoze.com/add-contact-form-astro/) back then OpnForms had a free option that was sending also an email when the form was submitted but since then things changed and the email send feature was added behind a pay option.\r\n\r\nSince then I have searched to see the best free contact option to use for a static website and I have come across [formsubmit.co](https://formsubmit.co/) a web service that allows you to connect your HTML forms to an email endpoint and receive email notifications of form submissions. You do not need any coding or backend skills to use it. You just need to point your form’s action attribute to the Formsubmit.co URL and confirm your email address. You can also customize your form features, such as reply, subject, copy, and reCAPTCHA.\r\n\r\nWhat I like about formsubmit.co besides the fact that is easy to use is the reCAPTCHA that will block spam. I have tested a few similar services in the past and I was receiving a lot of spam.\r\n\r\n## How You Add The Contact Form to Any Static Website\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/vSABo52iPAs\"\r\n  label=\"Add Contact Form To Static Websites\"\r\n/>\r\n\r\n### 1. Create your Form\r\n\r\nDesign a form for your website using HTML. You can use any elements you want, such as `<input>`, `<select>`, and `<textarea>`, but make sure to include a name attribute for each element that you want to receive the submission data. For example, you can create a simple contact form like this:\r\n\r\n```html\r\n<form id=\"contact-form\">\r\n  <label for=\"name\">Name:</label>\r\n  <input type=\"text\" id=\"name\" name=\"name\" required />\r\n  <label for=\"email\">Email:</label>\r\n  <input type=\"email\" id=\"email\" name=\"email\" required />\r\n  <label for=\"message\">Message:</label>\r\n  <textarea id=\"message\" name=\"message\" required></textarea>\r\n  <button type=\"submit\">Send</button>\r\n</form>\r\n```\r\n\r\n### 2. Point the action Attribute\r\n\r\nPoint the action attribute of your form to the formsubmit.co URL with your email address as the parameter. This will enable submissions to be sent to your email address. For example, if your email address is your@email.com, you can add this attribute to your form:\r\n\r\n```html\r\n<form\r\n  id=\"contact-form\"\r\n  action=\"https://formsubmit.co/your@email.com\"\r\n  method=\"POST\"\r\n></form>\r\n```\r\n\r\nThe form will look like this:\r\n\r\n```html\r\n<form\r\n  id=\"contact-form\"\r\n  action=\"https://formsubmit.co/your@email.com\"\r\n  method=\"POST\"\r\n>\r\n  <label for=\"name\">Name:</label>\r\n  <input type=\"text\" id=\"name\" name=\"name\" required />\r\n  <label for=\"email\">Email:</label>\r\n  <input type=\"email\" id=\"email\" name=\"email\" required />\r\n  <label for=\"message\">Message:</label>\r\n  <textarea id=\"message\" name=\"message\" required></textarea>\r\n  <button type=\"submit\">Send</button>\r\n</form>\r\n```\r\n\r\n### 3. Submit the form once\r\n\r\nSubmit the form once on your website or visit the URL in your browser. A confirmation email will be sent to you with a link, which you will have to click to confirm your email address. This is a one-time step to verify that you own the email address.\r\n\r\n### 4. Customize the form with formsubmit.co options\r\n\r\nYou are all set to go! Now, whenever someone submits the form on your website, you will receive an email notification with the form data. You can also customize your form features, such as reply, subject, copy, and reCAPTCHA, by using special name attributes prefixed with an underscore.\r\n\r\n#### Page Redirect After Form Is Submitted\r\n\r\nYou can redirect it to a page `Thank You` after the form is submitted or any other page you just need to add _`_next`_ :\r\n\r\n```html\r\n<input type=\"hidden\" name=\"_next\" value=\"https://yourdomain.co/thanks.html\" />\r\n```\r\n\r\n#### Add CC to emails\r\n\r\nYou can send email to others with the _`cc`_ attribute as below:\r\n\r\n```html\r\n<input\r\n  type=\"hidden\"\r\n  name=\"_cc\"\r\n  value=\"another@email.com,yetanother@email.com\"\r\n/>\r\n```\r\n\r\nYou can check all the attributes in the [formsubmit.co documentation](https://formsubmit.co/documentation)\r\n\r\n## How You Add The Contact Form to Astro\r\n\r\nI am having my blog built in Astro and I need this to work there. Initially, when I only added the above I got an error:\r\n\r\n> Make sure your form has the method=\"POST\" attribute\r\n\r\nI am using ViewTransition in Astro and I think that caused the problem, to fix that I just added the `data-astro-reload` to the form and everything worked, just as below:\r\n\r\n```html\r\n<form\r\n  id=\"contact-form\"\r\n  action=\"https://formsubmit.co/your@email.com\"\r\n  method=\"POST\"\r\n  data-astro-reload\r\n></form>\r\n```\r\n\r\nThis made the contact form to work on Astro.\r\n\r\n## Conclusion\r\n\r\n[formsubmit.co](https://formsubmit.co/) provides an easy way to add a contact form to any static website, you don't need to be an expert you just add the email endpoint and you are ready to go.\r\nI hope you enjoyed this tutorial and found it useful. If you have any feedback or questions, please let us know by using the contact form.","src/content/posts/add-contact-form-static-websites.mdx",[65],"../../assets/images/24/01/contact-form-static-websites.jpeg","3625ed93868b953b","add-contact-form-static-websites.mdx","ai-coading-tools",{id:68,data:70,body:83,filePath:84,assetImports:85,digest:87,legacyId:88,deferredRender:32},{title:71,description:72,date:73,image:74,authors:75,categories:76,tags:78,canonical:82},"Best AI Coding Tools and Assistants in 2025","A comprehensive guide to the most effective AI coding tools and assistants in 2025, including web tools, IDE integrations, and CLI solutions.",["Date","2025-02-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/02/best-ai-code-tool.jpg",[19],[77],"tools",[79,80,81],"ai","development","ai-tools","https://www.bitdoze.com/ai-coading-tools/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\n\r\nThe landscape of software development has been dramatically transformed by artificial intelligence, with 2025 bringing an unprecedented array of sophisticated AI coding tools. These tools aren't just about code completion anymore – they're full-fledged coding partners that can understand context, manage entire codebases, and even help architect complex systems.\r\n\r\nFrom web-based platforms that can transform ideas into working applications within minutes, to IDE integrations that function as AI pair programmers, to CLI tools that can handle complex software engineering tasks – developers now have access to tools that significantly enhance their productivity and capabilities.\r\n\r\nWhat makes 2025's AI coding tools particularly exciting is their ability to:\r\n- Generate and edit code across multiple files while understanding project context\r\n- Debug and review code with human-like reasoning\r\n- Convert natural language descriptions or visual mockups into functional code\r\n- Assist with complex refactoring and architecture decisions\r\n- Automate routine tasks while maintaining high code quality\r\n\r\nIn this article, we'll explore the best AI coding tools across different categories, helping you understand:\r\n- What these tools can do and how they differ\r\n- Which tools might best suit your workflow\r\n- How to make the most of their features\r\n- Whether they offer free options for testing\r\n\r\nWe'll examine tools across four main categories: Web Tools, VS Code Extensions, Dedicated IDEs, and CLI Tools, providing detailed comparisons and practical insights for each.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/NC5u6Lce65w\"\r\n  label=\"Best AI Coding Tools and Assistants in 2025\"\r\n/>\r\n\r\n## What are AI Coding Tools and Assistants?\r\n\r\nAI coding tools and assistants are software applications that use artificial intelligence, particularly large language models (LLMs), to help developers write, review, debug, and maintain code. These tools have evolved beyond simple code completion to become sophisticated development partners that can understand complex requirements and context.\r\n\r\nLet's break down the main categories:\r\n\r\n### 1. Web Tools\r\nWeb-based platforms that allow you to:\r\n- Create applications directly in your browser\r\n- Transform design mockups into working code\r\n- Build and deploy projects without local setup\r\n- Collaborate in real-time with team members\r\n\r\n**Examples**: v0.dev, bolt.new, lovable.dev\r\n\r\n### 2. VS Code Extensions\r\nExtensions that integrate AI capabilities directly into Visual Studio Code:\r\n- Real-time code suggestions and completions\r\n- In-editor chat interfaces for code discussion\r\n- Multi-file editing and refactoring\r\n- Context-aware code generation\r\n\r\n**Examples**: GitHub Copilot, Cline, Roo Code\r\n\r\n### 3. Dedicated AI-First IDEs\r\nPurpose-built code editors with deep AI integration:\r\n- Advanced code understanding and generation\r\n- Built-in AI pair programming capabilities\r\n- Optimized performance for AI interactions\r\n- Specialized features for AI-assisted development\r\n\r\n**Examples**: Cursor, Windsurf Editor, Zed\r\n\r\n### 4. CLI Tools\r\nCommand-line interfaces that provide:\r\n- AI pair programming through terminal\r\n- Git-aware code modifications\r\n- Voice coding capabilities\r\n- Multi-file project understanding and editing\r\n\r\n**Examples**: Aider, Goose\r\n\r\nEach category serves different needs and workflows:\r\n\r\n| Category | Best For | Typical Users |\r\n|----------|----------|---------------|\r\n| Web Tools | Quick prototypes, MVP development | Founders, Product teams |\r\n| VS Code Extensions | Daily development tasks, existing workflows | Professional developers |\r\n| Dedicated IDEs | Full-time AI-assisted development | Teams focused on AI integration |\r\n| CLI Tools | Terminal-focused developers, git-based workflows | System administrators, Backend developers |\r\n\r\n\r\n## What Should You Look for in AI Code Tools?\r\n\r\nWhen evaluating AI coding tools, several key factors should influence your decision. Here's what to consider:\r\n\r\n### 1. Code Generation Capabilities\r\n- **Quality of Output**: How accurate and maintainable is the generated code?\r\n- **Language Support**: Which programming languages and frameworks are supported?\r\n- **Context Understanding**: Can it understand your existing codebase?\r\n\r\n### 2. Integration & Workflow\r\n| Feature | Why It Matters |\r\n|---------|---------------|\r\n| IDE Integration | Seamless work within your existing environment |\r\n| Git Support | Version control and collaboration capabilities |\r\n| Multi-file Support | Ability to handle complex projects |\r\n| Real-time Collaboration | Team coordination and pair programming |\r\n\r\n### 3. AI Model & Performance\r\n- **Model Quality**: Which LLMs are available (GPT-4, Claude, etc.)?\r\n- **Response Time**: How quickly does it generate and implement code?\r\n- **Customization**: Can you use custom models or fine-tune existing ones?\r\n\r\n### 4. Security & Privacy\r\n- **Code Privacy**: How is your code handled and stored?\r\n- **Data Protection**: What security measures are in place?\r\n- **Compliance**: Does it meet your organization's security requirements?\r\n\r\n### 5. Cost & Scaling\r\n\r\nConsider:\r\n- Free tier availability\r\n- Per-user pricing\r\n- Token/usage limits\r\n- Enterprise features\r\n\r\n\r\n### 6. Additional Features to Consider\r\n- **Debugging Capabilities**: Can it help identify and fix issues?\r\n- **Documentation**: Does it help with code documentation?\r\n- **Testing**: Can it generate and run tests?\r\n- **Code Review**: Does it offer code review features?\r\n\r\n### 7. Enterprise Requirements\r\n- **Team Management**: User roles and permissions\r\n- **Audit Trails**: Activity logging and tracking\r\n- **Custom Configurations**: Environment-specific settings\r\n- **Support**: Available technical support levels\r\n\r\n### Key Questions to Ask\r\n1. Does it fit your current development workflow?\r\n2. How steep is the learning curve?\r\n3. What's the total cost of ownership?\r\n4. How reliable is the service?\r\n5. What kind of support is available?\r\n\r\n\r\n## Best AI Coding Tools and Assistants\r\n\r\nLet's start with a comprehensive comparison table for all tools, then dive into detailed reviews by category.\r\n\r\n### Comparison Table\r\n\r\n| Tool Name       | Category | Rating   | Free Option | Price Range      | Best For                     |\r\n|------------------|----------|----------|-------------|------------------|------------------------------|\r\n| v0.dev           | Web      | ⭐⭐⭐⭐½    | Yes         | Free/$20+        | UI/Component Development     |\r\n| bolt.new         | Web      | ⭐⭐⭐⭐     | Yes         | Free/$20+        | Rapid Prototyping            |\r\n| lovable.dev      | Web      | ⭐⭐⭐⭐     | Yes         | Free/$20+        | Full-stack Development       |\r\n| replit.com/ai    | Web      | ⭐⭐⭐½     | Yes         | Free/$15+        | Learning/Small Projects      |\r\n| tempolabs.ai     | Web      | ⭐⭐⭐⭐     | Limited     | Free/$30+        | React Development            |\r\n| databutton.com   | Web      | ⭐⭐⭐⭐½    | No         | $20+        | Data Apps                    |\r\n| idx.dev          | Web      | ⭐⭐⭐      | Yes         | Free             | Google Cloud Development     |\r\n| GitHub Copilot   | VS Code  | ⭐⭐⭐      | Limited     | $10/month        | General Coding               |\r\n| Cline            | VS Code  | ⭐⭐⭐⭐½    | Yes         | Free/$10+        | Advanced Code Generation     |\r\n| Roo Code         | VS Code  | ⭐⭐⭐⭐½    | Yes         | Free/$15+        | Multi-file Development       |\r\n| Augment Code     | VS Code  | ⭐⭐⭐⭐½    | Yes          | Free/$30+             | Enterprise Development       |\r\n| Cursor           | IDE      | ⭐⭐⭐      | Yes         | Free/$20+        | Individual Development       |\r\n| Trae             | IDE      | ⭐⭐⭐⭐½    | Limited     | Free/$25+        | Team Development             |\r\n| Windsurf         | IDE      | ⭐⭐⭐⭐½    | Yes         | Free/$30+        | Full-stack Development       |\r\n| Zed              | IDE      | ⭐⭐⭐⭐     | Yes         | Free             | Performance-focused Development |\r\n| Aider            | CLI      | ⭐⭐⭐⭐½    | Yes         | Free             | Git-based Development        |\r\n| Goose            | CLI      | ⭐⭐⭐⭐     | Yes         | Free             | Local Development            |\r\n\r\n\r\n### Web Tools\r\n\r\n#### 1. [v0.dev]((https://v0.dev)) (⭐⭐⭐⭐½)\r\n\r\n**[v0.dev](https://v0.dev)**, developed by Vercel, is an AI-powered tool that transforms natural language and design inputs into production-ready UI components. It supports frameworks like React, Tailwind CSS, and Shadcn UI, making it a game-changer for developers and designers aiming to streamline web development.\r\n\r\n**Key Features:**\r\n- **Generative UI Creation**: Create components using text prompts or uploaded mockups.\r\n- **Framework Support**: Compatible with React, Vue, Svelte, HTML with CSS, and Tailwind CSS.\r\n- **Figma Integration**: Import designs directly from Figma for seamless collaboration.\r\n- **Code Export**: Export components to Next.js or scaffold entire projects via CLI.\r\n- **Responsive Design Previews**: Ensure designs work across devices.\r\n- **Customizable Components**: Modify generated UI elements in real-time.\r\n- **Accessibility Built-In**: ARIA attributes and best practices included by default.\r\n- **Project Management**: Organize work into projects with custom instructions and data sources.\r\n\r\n**Pricing:**\r\n- **Free Plan**: $0/month – Up to 5 projects and basic features.\r\n- **Premium Plan**: $20/month – Unlimited projects and Figma integration.\r\n- **Team Plan**: $30/user/month – Collaboration tools and centralized billing.\r\n- **Enterprise Plan**: Custom pricing – Advanced features like SSO and priority support.\r\n\r\n**Best For:**\r\n- Frontend developers working with React or Next.js.\r\n- UI/UX designers seeking rapid prototyping tools.\r\n- Teams requiring collaborative workflows.\r\n- Organizations prioritizing accessibility and responsive design.\r\n\r\n#### [Bolt.new](https://bolt.new) (⭐⭐⭐⭐)\r\n\r\nBolt.new, developed by StackBlitz, is an AI-powered full-stack application generator that simplifies web development. It enables developers to prompt, run, edit, and deploy applications directly from the browser, eliminating the need for local setups. With its robust features, Bolt.new is ideal for building MVPs, prototypes, and scalable web applications.\r\n\r\n**Key Features:**\r\n- Full-stack application generation from natural language descriptions\r\n- Framework support for React, Vue, Svelte, Next.js, Vite, and more\r\n- Browser-based IDE powered by WebContainers\r\n- Database integration with automatic schema generation (e.g., Supabase)\r\n- API endpoint creation with documentation\r\n- One-click deployment to platforms like Netlify\r\n- Authentication integration\r\n- Real-time collaboration for teams\r\n- Built-in error detection and AI code suggestions\r\n- Version control system\r\n\r\n**Pricing:**\r\n- **Free Plan**: Includes 200,000 tokens/day (up to 2 million tokens/month).\r\n- **Pro Plan ($20/month)**: 10 million tokens for light use.\r\n- **Pro 50 Plan ($50/month)**: 26 million tokens for moderate usage.\r\n- **Pro 100 Plan ($100/month)**: 55 million tokens for daily use.\r\n- **Pro 200 Plan ($200/month)**: 120 million tokens for heavy usage.\r\n- **Enterprise Plan**: Custom pricing with advanced features like SSO and priority support.\r\n\r\n**Best For:**\r\n- Startups building MVPs or prototypes\r\n- Solo developers seeking an all-in-one development environment\r\n- Small development teams requiring collaborative tools\r\n- Proof-of-concept projects or scalable web applications\r\n\r\n#### [lovable.dev](https://lovable.dev) (⭐⭐⭐⭐)\r\n\r\nLovable.dev has revolutionized the way developers approach full-stack development by providing an intuitive platform that combines AI-powered code generation with practical development workflows. It's especially effective at handling complex business logic while maintaining clean code architecture.\r\n\r\n**Key Features:**\r\n- AI-powered full-stack code generation from natural language prompts\r\n- Visual development environment with live previews and instant edits\r\n- Custom component creation and integration with modern UI libraries\r\n- Database management interface with seamless backend integration (e.g., Supabase)\r\n- API development tools with automatic documentation generation\r\n- Integrated testing framework for efficient debugging\r\n- Automated deployment and continuous integration capabilities\r\n- Real-time collaboration for distributed teams\r\n- Version control integration with GitHub\r\n- Performance monitoring and optimization insights\r\n\r\n**Benefits:**\r\n- Accelerates the development lifecycle while reducing time-to-market\r\n- Maintains consistent, high-quality, and maintainable code\r\n- Simplifies testing, debugging, and iterative improvements\r\n- Scales efficiently for growing applications and teams\r\n- Enhances team collaboration with built-in workflow tools\r\n- Automatically integrates documentation and security best practices\r\n\r\n**Pricing:**\r\n- **Free Plan**: Basic features with 1 project\r\n- **Pro Plan**: $20/month for unlimited projects\r\n- **Team Plan**: $50/month per user\r\n- **Enterprise Plan**: Custom solutions tailored to organizational needs\r\n\r\n**Best For:**\r\n- Full-stack developers seeking efficient code generation\r\n- Development agencies handling complex projects\r\n- Enterprise teams with demanding application requirements\r\n- Projects involving sophisticated full-stack development\r\n\r\n\r\n#### [replit.com/ai](https://replit.com/ai) (⭐⭐⭐½)\r\nReplit's AI integration brings a unique approach to cloud-based development by combining their popular online IDE with powerful AI capabilities. It excels in educational environments and rapid prototyping, offering a complete development environment accessible through a browser.\r\n\r\n**Key Features:**\r\n- AI-powered code generation and completion\r\n- Integrated development environment (IDE)\r\n- Multi-language support\r\n- Real-time collaboration\r\n- Built-in hosting and deployment\r\n- Git integration for version control\r\n- Package management system\r\n- Interactive learning features\r\n- Multiplayer coding environment\r\n- Integrated database solutions\r\n\r\n**Benefits:**\r\n- No local setup required\r\n- Perfect for learning and teaching\r\n- Instant project sharing\r\n- Community-driven development\r\n- Cross-platform compatibility\r\n- Built-in version control\r\n- Immediate deployment capabilities\r\n\r\n**Pricing:**\r\n- **Free Plan**: Basic features and public repos\r\n- **Pro Plan**: $10/month with private repos\r\n- **Teams Plan**: $20/user/month\r\n- **Enterprise Plan**: Custom pricing\r\n\r\n**Best For:**\r\n- Students and educators\r\n- Beginners learning to code\r\n- Quick prototyping\r\n- Collaborative coding projects\r\n\r\n#### [tempolabs.ai](https://www.tempolabs.ai) (⭐⭐⭐⭐)\r\nTempolabs.ai focuses on React development with an AI-first approach, offering sophisticated tools for building modern web applications. The platform specializes in advanced component generation and application architecture, ensuring high code quality and adherence to best practices.\r\n\r\n**Key Features:**\r\n- Advanced React component generation\r\n- AI-powered code refactoring\r\n- Component library management\r\n- State management solutions\r\n- Performance optimization tools\r\n- Testing suite integration\r\n- Design system implementation\r\n- API integration tools\r\n- Documentation generation\r\n\r\n**Benefits:**\r\n- Streamlined React development\r\n- Consistent code patterns and automated best practices\r\n- Reduced technical debt and improved development speed\r\n- Better code maintainability\r\n- Enhanced team collaboration\r\n\r\n**Pricing:**\r\n- **FREE**:: 500k daily tokens (10M monthly tokens) with basic features\r\n- **Basic**: $30/month\r\n- **Professional**: $50/month\r\n- **Team**: $100/user/month\r\n- **Enterprise**: Custom solutions\r\n\r\n**Best For:**\r\n- React developers\r\n- Frontend teams\r\n- Enterprise applications\r\n- Design system implementation\r\n\r\n#### [Databutton.com](https://databutton.com) (⭐⭐⭐⭐½)\r\nDatabutton combines AI capabilities with powerful data processing tools to simplify the creation of data-driven applications. It excels in building analytics dashboards, data visualization apps, and full-stack SaaS solutions.\r\n\r\n**Key Features:**\r\n- AI-assisted app creation using natural language prompts\r\n- Python-based backend development with FastAPI\r\n- Interactive dashboard and multi-page UI building\r\n- Advanced data visualization tools\r\n- Machine learning integration for analytics\r\n- Real-time data processing and API connectivity\r\n- One-click deployment to custom domains or cloud platforms\r\n- Collaborative workspaces with live editing\r\n- Version control for seamless development cycles\r\n- Built-in security features\r\n\r\n**Benefits:**\r\n- Rapid development of data-driven applications\r\n- Streamlined deployment process with minimal setup\r\n- Scalable architecture for handling growing data demands\r\n- Enhanced collaboration for analytics teams\r\n- Interactive prototyping and production-ready solutions\r\n\r\n**Pricing:**\r\n- **Micro Plan**: $20/month – 75 credits/month for small projects\r\n- **Individual Plan**: $50/month – 200 credits/month for moderate usage\r\n- **Growth Plan**: $200/month – 1,000 credits/month for scaling apps\r\n- **Pro Plan**: $800/month – 4,300 credits/month for enterprise needs\r\n\r\n**Best For:**\r\n- Data scientists building analytics tools\r\n- Business intelligence teams creating dashboards\r\n- Startups developing scalable SaaS applications\r\n- Developers seeking AI-powered app creation tools\r\n\r\n\r\n####  idx.dev (⭐⭐⭐)\r\nGoogle's IDX.dev provides a cloud-based development environment specifically optimized for Google Cloud Platform integration. It offers AI-assisted development with deep integration into Google's ecosystem.\r\n\r\n**Key Features:**\r\n- Google Cloud Platform integration\r\n- AI-powered code assistance\r\n- Cloud-native development tools\r\n- Multi-language support\r\n- Containerization tools\r\n- Cloud deployment\r\n- Collaborative features\r\n- Built-in security features\r\n- Performance monitoring\r\n- Resource management\r\n\r\n**Benefits:**\r\n- Seamless GCP integration\r\n- Zero setup required\r\n- Cloud-native development\r\n- Automated resource management\r\n- Team collaboration\r\n- Security compliance\r\n- Scalable infrastructure\r\n\r\n**Pricing:**\r\n- Free during preview\r\n- Pricing structure to be announced\r\n- Expected enterprise options\r\n\r\n**Best For:**\r\n- Google Cloud developers\r\n- Cloud-native applications\r\n- Enterprise teams\r\n- Startups on GCP\r\n\r\n\r\n### VS Code Extensions\r\n\r\n#### [GitHub Copilot](https://github.com/features/copilot) (⭐⭐⭐)\r\nGitHub Copilot, developed by GitHub and OpenAI, is a widely used AI-powered coding assistant. It offers intelligent code suggestions and seamless integration with popular IDEs, making it a valuable tool for developers.\r\n\r\n**Key Features]:**\r\n- Real-time code suggestions and completions\r\n- Multi-language support\r\n- Context-aware completions\r\n- Unit test generation\r\n- Documentation assistance and code explanation\r\n- GitHub integration for pull request summaries and workflows\r\n- Command line suggestions\r\n- Error detection and fixes\r\n\r\n**Pricing:**\r\n- **Free Plan**: Limited to 2,000 completions/month and 50 chat requests for students and OSS maintainers\r\n- **Pro Plan**: $10/month or $100/year for individuals\r\n- **Business Plan**: $19/user/month with organizational management features\r\n- **Enterprise Plan**: $39/user/month with advanced privacy, security, and model customization\r\n\r\n**Best For:**\r\n- Individual developers\r\n- Enterprise teams requiring organizational controls\r\n- Students and open-source contributors\r\n\r\n\r\n\r\n#### [Cline](https://cline.bot) (⭐⭐⭐⭐½)\r\nCline is an advanced AI-powered coding assistant designed for professional developers. It excels in handling complex tasks with customizable workflows and multi-model support.\r\n\r\n**Key Features:**\r\n- Autonomous code generation with human-in-the-loop control\r\n- Multi-file editing and project-wide refactoring\r\n- Terminal command execution and browser automation\r\n- Git integration for version control\r\n- Context-aware suggestions across large codebases\r\n- Custom tool creation using the Model Context Protocol (MCP)\r\n- Real-time error fixing and debugging assistance\r\n\r\n**Pricing:**\r\n- **Free**: Free to use with paying for models.\r\n\r\n\r\n**Best For:**\r\n- Professional developers managing large projects\r\n- Teams requiring deep codebase understanding\r\n- Developers seeking customizable AI workflows\r\n\r\n\r\n\r\n#### [Roo Code](https://github.com/RooVetGit/Roo-Code) (⭐⭐⭐⭐½)\r\nRoo Code, formerly known as Roo Cline, is a versatile AI coding assistant tailored for specialized roles. It integrates seamlessly with VS Code to enhance productivity through customizable AI personas.\r\n\r\n**Key Features:**\r\n- Custom AI modes/personas for different development roles\r\n- Multi-file development and Git-aware modifications\r\n- Real-time collaboration tools\r\n- Markdown editing support and terminal integration\r\n- Intelligent mode switching based on task context\r\n\r\n**Pricing:**\r\n- **Free**: Free to use with paying for models.\r\n\r\n\r\n\r\n**Best For:**\r\n- Development teams requiring role-specific assistance\r\n- Technical writers and product teams needing flexible workflows\r\n\r\n\r\n#### [Augment Code](https://augmentcode.com) (⭐⭐⭐⭐½)\r\nAugment Code is an enterprise-grade AI coding assistant designed to handle large codebases. It focuses on team productivity, security compliance, and quality assurance.\r\n\r\n**Key Features:**\r\n- Context-aware code generation across large repositories\r\n- Team collaboration tools with Slack integration\r\n- Security compliance (SOC 2 Type II certified)\r\n- Version control integration and performance analytics dashboards\r\n- Real-time code reviews and architecture suggestions\r\n\r\n**Pricing:**\r\n- **Community Plan**: Free with unlimited chats and completions (limited to 3,000 instructions/month)\r\n- **Professional Plan**: $30/user/month with email support and unlimited usage\r\n- **Enterprise Plan**: $60/user/month with advanced features like SSO, analytics dashboards, and dedicated support\r\n\r\n**Best For:**\r\n- Enterprise development teams managing complex projects\r\n- Organizations prioritizing security compliance and scalability\r\n\r\n\r\n\r\n\r\n### Dedicated AI-First IDEs\r\n\r\n#### [Cursor](https://www.cursor.com) (⭐⭐⭐)\r\nCursor is a standalone IDE built from the ground up with AI integration in mind. It offers a familiar VS Code–like experience while differentiating itself with native AI features and optimized performance.\r\n\r\n**Key Features:**\r\n- Built-in AI chat interface\r\n- Code generation and editing\r\n- Multi-language support\r\n- VS Code extension compatibility\r\n- Shadow workspaces for AI iteration\r\n- Real-time pair programming\r\n- File search and navigation\r\n- Terminal integration\r\n- Git integration\r\n- Performance optimization\r\n\r\n**Benefits:**\r\n- Familiar VS Code experience enhanced by native AI capabilities\r\n- Fast, optimized performance with seamless transition from traditional editors\r\n- Collaborative features and privacy options for secure development\r\n- Extension support to tailor the IDE for specific workflows\r\n\r\n**Pricing:**\r\n- Free Tier: Basic features\r\n- Pro: $20/month\r\n- Team: Custom pricing\r\n- Enterprise: Contact sales\r\n\r\n**Best For:**\r\n- Individual developers\r\n- Small teams\r\n- VS Code users\r\n- Startups\r\n\r\n\r\n#### [Trae](https://trae.ai) (⭐⭐⭐⭐½)\r\nTrae is a next-generation AI-powered IDE that offers a complete development environment with advanced AI capabilities and robust team collaboration tools. Its agent-based workflow supports seamless code generation and real-time assistance.\r\n\r\n**Key Features:**\r\n- AI-powered coding assistance and intelligent code generation\r\n- Integrated team collaboration tools\r\n- Support for multiple AI models (e.g., GPT‑4, Claude 3.5)\r\n- Custom extensions to adapt to different development workflows\r\n- Advanced debugging and integrated testing\r\n- Project management and code review functionalities\r\n- Performance monitoring and security scanning\r\n\r\n**Benefits:**\r\n- Enhances team productivity through integrated collaboration\r\n- Streamlines coding with advanced AI assistance and comprehensive toolsets\r\n- Improves quality assurance and security compliance\r\n- Simplifies project oversight with built-in management tools\r\n\r\n**Pricing:**\r\n- Currently Free (all features are available at no cost during the beta)\r\n\r\n**Best For:**\r\n- Development teams exploring innovative tools\r\n- Enterprise organizations looking to pilot cutting‑edge IDE features\r\n- Project managers and quality assurance teams\r\n\r\n\r\n#### [Windsurf](https://codeium.com/windsurf) (⭐⭐⭐⭐½)\r\nWindsurf Editor creates a seamless flow between developers and AI by leveraging AI Flows and Cascade technology. It enhances productivity and code quality through context-aware suggestions and robust multi-file editing capabilities.\r\n\r\n**Key Features:**\r\n- Integration of AI Flows and Cascade technology for context-aware suggestions\r\n- Multi-file editing with strong context awareness\r\n- Command suggestions and browser automation\r\n- Custom modes for personalized workflows\r\n- Real-time collaboration and team synchronization\r\n- Rich extension ecosystem for added functionality\r\n- Performance tools for rapid code editing\r\n\r\n**Benefits:**\r\n- Natural AI interaction that streamlines the development workflow\r\n- Improved collaboration and synchronization among team members\r\n- Enhanced code quality and accelerated development cycles\r\n- Customizable solutions designed for diverse project needs\r\n\r\n**Pricing:**\r\n- Free Tier: Unlimited access to basic features\r\n- Pro: $15/month\r\n- Team: Custom pricing\r\n- Enterprise: Contact sales\r\n\r\n**Best For:**\r\n- Professional developers and full‑stack projects\r\n- Teams seeking efficient, collaborative coding environments\r\n- Enterprises requiring robust AI integration for scalable solutions\r\n\r\n\r\n\r\n#### [Zed](https://zed.dev) (⭐⭐⭐⭐)\r\nZed is a high‑performance code editor built in Rust, focusing on speed, collaboration, and modern development practices. It integrates AI capabilities to boost code generation and transformation while maintaining an efficient, resource‑light experience.\r\n\r\n**Key Features:**\r\n- Lightning‑fast performance powered by a high‑performance engine\r\n- Real‑time collaboration with built‑in team features\r\n- Integrated AI code assistance for generating and transforming code\r\n- Multi‑language support and terminal integration\r\n- Git support paired with an extensive extensions system\r\n- Powerful search and native debugging support\r\n\r\n**Benefits:**\r\n- Exceptional speed and resource efficiency for a smooth coding experience\r\n- Seamless collaboration for distributed teams and open‑source contributors\r\n- Modern, intuitive interface with an easy learning curve\r\n- Open source foundation with ongoing community enhancements\r\n\r\n**Pricing:**\r\n- Free and open source 10$ a month\r\n- Premium features are pay as you go.\r\n\r\n\r\n**Best For:**\r\n- Developers focused on performance and speed\r\n- Collaborative teams and open‑source contributors\r\n- Users seeking a modern, resource‑efficient code editor\r\n\r\n\r\n\r\n### CLI Tools\r\n\r\n#### [Aider](https://aider.chat/) (⭐⭐⭐⭐½)\r\nAider stands out as a powerful command-line AI programming assistant, particularly notable for its deep git integration and ability to work with multiple files in existing codebases. It has gained significant popularity among developers who prefer terminal-based workflows.\r\n\r\n**Key Features:**\r\n- Git-aware code modifications\r\n- Multiple LLM support (Claude 3.5 Sonnet, GPT-4, etc.)\r\n- Multi-file editing\r\n- Voice coding capabilities\r\n- Context-aware suggestions\r\n- Automatic git commits\r\n- Code review functionality\r\n- Terminal-based UI\r\n- Project-wide search\r\n- Direct integration with popular editors\r\n\r\n**Benefits:**\r\n- Seamless git workflow\r\n- Terminal efficiency\r\n- Model flexibility\r\n- Privacy-focused\r\n- Fast implementation\r\n- Easy version control\r\n- Natural interaction\r\n\r\n**Pricing:**\r\n- Free and open source\r\n- No subscription required\r\n- Pay only for API usage\r\n- Enterprise support available\r\n\r\n**Best For:**\r\n- Terminal power users\r\n- Git-focused developers\r\n- Open source contributors\r\n- Privacy-conscious developers\r\n\r\n#### [Goose](https://block.github.io/goose/) (⭐⭐⭐⭐)\r\nBlock's Goose is an on-machine AI agent designed to automate engineering tasks while maintaining security and privacy. It excels at handling complex development tasks directly from the command line.\r\n\r\n**Key Features:**\r\n- Local-first architecture\r\n- Extensible MCP server support\r\n- Multiple LLM compatibility\r\n- Task automation\r\n- Code generation\r\n- Testing integration\r\n- Development workflow automation\r\n- Security-focused design\r\n- Custom tool creation\r\n- Project management\r\n\r\n**Benefits:**\r\n- Enhanced privacy\r\n- Local processing\r\n- Customizable workflow\r\n- Secure operations\r\n- Fast execution\r\n- Team integration\r\n- Resource efficiency\r\n\r\n**Pricing:**\r\n- Free and open source\r\n- Self-hosted option\r\n- Enterprise support available\r\n- Custom solutions possible\r\n\r\n**Best For:**\r\n- Security-conscious teams\r\n- Enterprise developers\r\n- DevOps engineers\r\n- System administrators\r\n\r\n\r\n\r\n## Why Should You Use AI Coding Assistant Tools?\r\n\r\nThe adoption of AI coding tools has become increasingly crucial for modern software development. Here are the key reasons why developers and teams should consider incorporating these tools into their workflow:\r\n\r\n### 1. Increased Productivity\r\n\r\n**Faster Development Cycles**\r\n  - Reduce boilerplate code writing\r\n  - Automate repetitive tasks\r\n  - Quick prototyping and MVP creation\r\n\r\n**Time Savings**\r\n  - Up to 55% faster code writing\r\n  - Reduced documentation time\r\n  - Faster bug fixing and debugging\r\n\r\n### 2. Code Quality Improvement\r\n\r\n**Consistency**\r\n  - Standardized coding patterns\r\n  - Better adherence to best practices\r\n  - Uniform documentation\r\n\r\n**Error Reduction**\r\n  - Early bug detection\r\n  - Security vulnerability checks\r\n  - Code optimization suggestions\r\n\r\n### 3. Learning and Skill Development\r\n\r\n**Knowledge Expansion**\r\n  - Learn new programming languages\r\n  - Understand different frameworks\r\n  - Access to best practices\r\n\r\n**Code Understanding**\r\n  - Better code comprehension\r\n  - Quick documentation lookup\r\n  - Pattern recognition\r\n\r\n### 4. Team Collaboration\r\n\r\n**Knowledge Sharing**\r\n  - Consistent coding standards\r\n  - Better code documentation\r\n  - Easier onboarding\r\n\r\n**Workflow Enhancement**\r\n  - Streamlined code reviews\r\n  - Improved pair programming\r\n  - Better project management\r\n\r\n### 5. Cost Effectiveness\r\n\r\n**Direct Benefits:**\r\n- Reduced development time\r\n- Fewer bugs in production\r\n- Lower technical debt\r\n- Faster onboarding\r\n\r\n**Indirect Benefits:**\r\n- Improved code maintainability\r\n- Better resource allocation\r\n- Reduced burnout\r\n\r\n### 6. Complex Problem Solving\r\n\r\n**Architecture Planning**\r\n  - Better system design\r\n  - Scalable solutions\r\n  - Pattern identification\r\n\r\n**Debug Assistance**\r\n  - Quick error resolution\r\n  - Root cause analysis\r\n  - Performance optimization\r\n\r\n### 7. Business Impact\r\n\r\n**Faster Time to Market**\r\n  - Quick feature implementation\r\n  - Rapid prototyping\r\n  - Efficient testing\r\n\r\n**Resource Optimization**\r\n  - Better resource allocation\r\n  - Reduced technical overhead\r\n  - Improved maintenance\r\n\r\n### 8. Future-Proofing Development\r\n\r\n**Adaptability**\r\n  - Easy adoption of new technologies\r\n  - Framework transitions\r\n  - Modern development practices\r\n\r\n**Scalability**\r\n  - Growing team support\r\n  - Codebase expansion\r\n  - Performance optimization\r\n\r\n### Real-World Benefits\r\n\r\n| Aspect | Traditional Development | With AI Tools |\r\n|--------|------------------------|---------------|\r\n| Code Writing Speed | Baseline | 55% faster |\r\n| Bug Detection | Manual review | Real-time detection |\r\n| Documentation | Time-consuming | Automated & consistent |\r\n| Learning Curve | Steep for new tech | Assisted & gradual |\r\n| Team Collaboration | Communication heavy | Streamlined & efficient |\r\n\r\n\r\n\r\n## Conclusions\r\n\r\n\r\n\r\nAI coding tools have become an essential part of modern software development. While each tool has its strengths, the key is finding the right combination that fits your specific needs, workflow, and budget. The investment in these tools can significantly improve productivity, code quality, and team collaboration, making them a valuable addition to any development workflow.\r\n\r\nRemember:\r\n- No single tool is perfect for everything\r\n- Combine tools for maximum benefit\r\n- Keep security and privacy in mind\r\n- Stay updated with new features\r\n- Measure impact on productivity\r\n\r\nThe future of development is increasingly AI-assisted, and staying ahead of the curve by adopting these tools now will provide a significant competitive advantage in the evolving landscape of software development.","src/content/posts/ai-coading-tools.mdx",[86],"../../assets/images/25/02/best-ai-code-tool.jpg","35a6cf9d1d3dd713","ai-coading-tools.mdx","add-users-to-docker-container",{id:89,data:91,body:102,filePath:103,assetImports:104,digest:106,legacyId:107,deferredRender:32},{title:92,description:93,date:94,image:95,authors:96,categories:97,tags:99,canonical:101},"How to Add Users to a Docker Container","Learn how to How to Add Users to a Docker Container to maker your work easier",["Date","2023-07-03T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/07/add-users-docker-container.jpeg",[19],[98],"vps",[100],"docker","https://www.bitdoze.com/add-users-to-docker-container/","Managing users in Docker containers is a crucial aspect of container security and access control. By assigning specific user privileges, you can ensure that only authorized individuals are able to interact with your containers. In this article, we will explore the steps on how to add users to a Docker container, allowing you to tightly control who can access and modify containerized environments.\r\n\r\nWhen it comes to adding users to a Docker container, there are a few different approaches you can take depending on your requirements. One approach involves creating a user in the Docker image itself, allowing for user-specific permissions and restrictions within the container. Another option is to map a host user to a user within the container, granting the same privileges as the host user. Both methods provide flexibility to tailor access levels based on your needs.\r\n\r\nIn this guide, we will delve into the intricacies of adding users to Docker containers using these different approaches. Whether you're looking to enhance security or streamline collaboration within your containerized environments, these techniques will empower you to manage user access effectively. So, let's dive in and explore the steps involved in adding users to Docker containers!\r\n\r\n## Why Add Users to Docker Container?\r\n\r\nWhen working with Docker containers, adding users to your containers can bring several benefits. Let's explore why it's important:\r\n\r\n### 1. Enhanced Security\r\n\r\nBy adding users to Docker containers, you can follow the principle of least privilege. This means giving each user only the permissions they need to perform their specific tasks within the container. By restricting access to sensitive resources, you can reduce the risk of unauthorized activities or potential security breaches.\r\n\r\n### 2. Isolation of Processes\r\n\r\nAdding users allows for better isolation of processes within a container. Each user can have their own set of permissions and can execute tasks independently. This separation helps prevent interference between different processes, ensuring a more stable and secure container environment.\r\n\r\n### 3. Better Resource Management\r\n\r\nWhen multiple users are added to a Docker container, it becomes easier to allocate and control resources efficiently. Each user can have their own resource limits, preventing a single user or process from monopolizing system resources. This ensures fair distribution and optimal utilization of available resources.\r\n\r\n### 4. Collaboration and Multi-tenancy\r\n\r\nAdding users to Docker containers facilitates collaboration among team members. Each user can have their own credentials to access the container, allowing them to work on shared projects without sharing login credentials. It enhances accountability and enables multiple users to work simultaneously on different tasks or applications within the same container.\r\n\r\n### 5. Auditing and Accountability\r\n\r\nBy assigning unique user identities to individuals or processes within a Docker container, it becomes easier to track and audit activities. Each action can be attributed to a specific user, enabling better accountability and troubleshooting in case of issues or errors.\r\n\r\nIn summary, adding users to Docker containers enhances security, enables process isolation, optimizes resource management, promotes collaboration, and facilitates auditing and accountability. By following these best practices, you can create a more controlled and secure container environment.\r\n\r\n## Creating Users in Docker\r\n\r\nWhen working with Docker containers, it can be useful to create multiple users with different privileges and access rights. In this section, we will explore how to add users to Docker containers effectively.\r\n\r\nSome other docker articles that can help you in your docker journey:\r\n\r\n- [Copy Multiple Files in One Layer Using a Dockerfile](https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/)\r\n- [Install Docker & Docker-compose for Ubuntu ARM](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n- [Redirect Docker Logs to a Single File](https://www.bitdoze.com/redirect-docker-logs-to-a-single-file/)\r\n- [Environment Variables ARG and ENV in Docker](https://www.bitdoze.com/docker-env-vars/)\r\n\r\n### Adding Users to Docker Containers\r\n\r\nTo add users to Docker containers, follow these steps:\r\n\r\n1. **Create a new user**: Inside the Docker container, use the `adduser` command to create a new user. Specify the username and any additional parameters as required.\r\n\r\n2. **Assign user permissions**: Depending on the requirements, you can assign different permissions to the user, such as granting administrative access or limiting them to read-only capabilities.\r\n\r\n3. **Set up user environment**: Customize the user's environment by installing necessary software, configuring settings, and ensuring required dependencies are met.\r\n\r\n4. **Switch to the new user**: Once the user is created, use the `su` command to switch to the newly created user account and test its functionalities. This step helps ensure that the user has the intended access and can work within the container.\r\n\r\n### Managing User Permissions\r\n\r\nBy default, Docker containers run with root privileges. However, it's recommended to create users with appropriate permissions to prevent potential security issues. To manage user permissions within Docker containers, consider the following:\r\n\r\n- **Use `USER` instruction**: In your Dockerfile, specify the default user for the container using the `USER` instruction. This ensures that the container runs using the specified user account instead of the root account.\r\n\r\n- **Grant sudo access selectively**: If a user requires administrative privileges within the container, you can selectively grant them with `sudo` access. Ensure to evaluate the need for such access to maintain security.\r\n\r\n- **Leverage user namespaces**: Docker allows us to leverage user namespaces to provide additional isolation for users within the container. This is particularly useful to restrict root-like privileges to a specific user without the actual root access.\r\n\r\nBy following these practices, you can create a secure and efficient environment by adding users to Docker containers with the appropriate permissions.\r\n\r\n## Granting Permissions to Docker Users\r\n\r\nWhen it comes to managing users within a Docker container, granting the appropriate permissions is crucial. In this section, we will explore the steps required to grant permissions to Docker users effectively.\r\n\r\n### Understanding User Roles\r\n\r\nBefore diving into the details, let's take a moment to understand the concept of user roles in a Docker container. Docker provides three distinct user roles: root, standard, and custom.\r\n\r\n- **Root**: The root user has unrestricted access to the Docker container and can perform any action without any limitations. However, it is important to note that granting root access to users should be done judiciously due to security implications.\r\n\r\n- **Standard**: The standard user role is more restricted compared to the root user. Standard users have limited permissions and cannot execute certain privileged operations within the Docker container. This is the recommended role for most users.\r\n\r\n- **Custom**: Docker also allows you to create custom user roles where you can define very granular permissions according to your specific requirements. However, creating custom roles often requires advanced knowledge and should be done cautiously.\r\n\r\n### Granting Permissions\r\n\r\nNow that we have a basic understanding of user roles, let's go through the process of granting permissions:\r\n\r\n1. **Create a Docker user**: Begin by creating a new user within the Docker container using the `useradd` command. For example:\r\n\r\n   ```\r\n   $ useradd -s /bin/bash -m dockeruser\r\n   ```\r\n\r\n2. **Assign desired permissions**: After creating the user, you can assign the appropriate permissions based on the user role. If you want to grant standard user permissions, it is recommended to add the user to the `docker` group using the `usermod` command. For instance:\r\n\r\n   ```\r\n   $ usermod -aG docker dockeruser\r\n   ```\r\n\r\n3. **Verify user permissions**: To ensure that the permissions are correctly assigned, you can use the `docker` command followed by the `run` option and any desired command. For example:\r\n\r\n   ```\r\n   $ docker run --rm -it --user dockeruser busybox id\r\n   ```\r\n\r\n   This command will start a container with the specified user (`dockeruser` in this case) and execute the `id` command, which will display the user's information if the permissions are correctly granted.\r\n\r\n4. **Test and refine permissions**: Finally, it is essential to thoroughly test the assigned permissions to ensure that users can perform their intended tasks without any hindrance. This may involve running various commands or performing typical operations within the Docker container to validate the permissions granted.\r\n\r\nBy following these steps, you can effectively grant permissions to Docker users while maintaining a secure environment within your container. Remember to strike the right balance between granting sufficient privileges and protecting the integrity and security of your containerized applications.\r\n\r\n| User Role | Description                                           |\r\n| --------- | ----------------------------------------------------- |\r\n| Root      | Unrestricted access with full privileges              |\r\n| Standard  | Restricted permissions for most operations            |\r\n| Custom    | Customized permissions based on specific requirements |\r\n\r\nRemember, it's crucial to assign permissions based on the principle of least privilege, granting only the necessary permissions to users to minimize the risk of unauthorized actions within the Docker container.\r\n\r\n## Best Practices for User Management in Docker Container\r\n\r\nWhen working with Docker containers, it's essential to implement proper user management practices to ensure security and maintain control over your systems. Here are some best practices to consider:\r\n\r\n### 1. Use Non-Root Users\r\n\r\nRunning containers as the root user is not recommended because it poses security risks. Instead, create and use non-root users within your containers. This helps minimize the potential impact of any security vulnerabilities and restricts the privileges of the container.\r\n\r\n### 2. Create a Dedicated User\r\n\r\nConsider creating a dedicated user specifically for running your containerized application. By isolating your application within its own user account, you can further enhance security. This user should only have the necessary privileges required for the application to function correctly.\r\n\r\n### 3. Implement User Namespaces\r\n\r\nUser namespaces provide an additional layer of isolation by mapping the non-root user inside the container to a different user outside the container. This allows the container to run as a non-root user while still functioning as intended. By enabling user namespaces, you reduce the risk of privilege escalation attacks.\r\n\r\n### 4. Limit Container Capabilities\r\n\r\nContainer capabilities define the operations a container can perform within the host system. Restricting unnecessary capabilities minimizes the potential attack surface. Docker provides the `--cap-drop` flag to drop specific capabilities and the `--cap-add` flag to add back specific capabilities, allowing fine-grained control over the privileges of your containers.\r\n\r\n### 5. Use Volume Permissions Wisely\r\n\r\nWhen mounting volumes into your containers, ensure that the permissions are properly set. Avoid giving overly permissive file permissions to mounted volumes to prevent unauthorized access. Follow the principle of least privilege and grant only the necessary permissions required by your application.\r\n\r\n### 6. Regularly Update Containers and Base Images\r\n\r\nKeeping your containers and base images up to date is crucial for maintaining security. Regularly check for updates and security patches and apply them promptly to ensure that your containers are protected against known vulnerabilities. Consider implementing an automated update process to streamline this task.\r\n\r\nBy following these best practices, you can enhance the security and manageability of your Docker containers. Combined with other containerization practices, such as image scanning and network segmentation, you can build a robust and secure container environment.\r\n\r\n## Conclusion\r\n\r\nIn this article, we explored the process of adding users to a Docker container. By following a few simple steps, you can enhance the security and manageability of your containers. Let's recap what we've learned:\r\n\r\n1. **Create a new user**: Use the `adduser` command to create a new user within the container. This user will have limited privileges and help isolate applications.\r\n\r\n2. **Assign proper permissions**: Ensure that the new user has the necessary permissions to perform required tasks. Use the `chown` and `chmod` commands to modify ownership and access rights.\r\n\r\n3. **Switch to the new user**: Once the new user is created, switch to their account using the `su` command. This allows you to operate within the container using reduced privileges.\r\n\r\n4. **Adjust user namespaces**: Docker provides the option to use user namespaces to further isolate user identities between the host and containers. This can be configured in the Docker daemon.\r\n\r\n5. **Test and verify**: After implementing the user modifications, test the container to ensure everything functions as expected. Verify that the new user can perform necessary tasks while maintaining proper security measures.\r\n\r\nAdding users to Docker containers brings an extra layer of protection and control. By limiting the privileges of the users operating within the container, you can mitigate potential security risks. Furthermore, user isolation aids in managing multiple applications within a single host environment.\r\n\r\nRemember that each container may have its own specific requirements when it comes to user management. It's essential to understand and evaluate these requirements before implementing any changes. Use the information provided in this article as a starting point, and adapt it to the specific needs of your containerized applications.\r\n\r\nBy following best practices and staying informed about Docker's ongoing developments, you can effectively manage users within containers and create a more secure environment for your applications.\r\n\r\nKeep exploring Docker's documentation and community resources for further insights and updates. Happy containerizing!","src/content/posts/add-users-to-docker-container.mdx",[105],"../../assets/images/23/07/add-users-docker-container.jpeg","641a8ba125dd70cf","add-users-to-docker-container.mdx","add-accordion-carrd",{id:108,data:110,body:119,filePath:120,assetImports:121,digest:123,legacyId:124,deferredRender:32},{title:111,description:112,date:113,image:114,authors:115,categories:116,tags:117,canonical:118},"How To Add Accordion FAQs Drop-Down to Carrd.co","Let's see how an accordion can be added easily and free to a carrd.co website.",["Date","2023-07-31T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/07/free-carrd-accorion.jpeg",[19],[21],[43],"https://www.bitdoze.com/add-accordion-carrd/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/05/carrd-embed.png\";\r\n\r\nIn this tutorial, we are going to see how you can add a nice accordion widget to your carrd.co website. With the widghet, you will be able to:\r\n\r\n- change the colours - you can change the colors of the tabs as you like, you can change the colors to everything.\r\n- add as many tab as you like - you can easily add as many tab as you link\r\n- mobile responsive - the tabs look good on mobile and on desktop\r\n\r\nFor using the code you will need to have a plan that allows embeds as this is a piece of code you are adding. The code can be downloaded from:\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n<Button link=\"https://go.carrdme.com/accordion\" text=\"Carrd.co accordion\" />\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Custom Domain to Carrd.co](https://www.bitdoze.com/carrd-add-domain/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n\r\n## Steps to add Accordion FAQs Drop-Down to Carrd.co\r\n\r\nYou can check the bellow video for the detailed steps:\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/c7WfK-LviA4\"\r\n  label=\"How To Add Accordion FAQs Drop-Down to Carrd.co\"\r\n/>\r\n\r\n### 1. Download the code\r\n\r\nThe code you need is free and you can download it from [here](https://go.carrdme.com/accordion)\r\n\r\n### 2. Add a container and embed widget\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nIn the section, you would like to add your accordion you just need to select a container and add an embed widget from carrd editor.\r\n\r\n### 3. Add your tabs\r\n\r\nIn the bellow code you can add or remove tabs, you can change the name as you like or you can add the text you want.\r\n\r\n```html\r\n<button class=\"accordion\">Tab 1</button>\r\n<div class=\"panel\">\r\n  <p>\r\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus.\r\n    Suspendisse lectus tortor, dignissim sit amet, adipiscing nec, ultricies\r\n    sed, dolor.\r\n  </p>\r\n</div>\r\n\r\n<button class=\"accordion\">Tab 2</button>\r\n<div class=\"panel\">\r\n  <p>\r\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus.\r\n    Suspendisse lectus tortor, dignissim sit amet, adipiscing nec, ultricies\r\n    sed, dolor.\r\n  </p>\r\n</div>\r\n\r\n<button class=\"accordion\">Tab 3</button>\r\n<div class=\"panel\">\r\n  <p>\r\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus.\r\n    Suspendisse lectus tortor, dignissim sit amet, adipiscing nec, ultricies\r\n    sed, dolor.\r\n  </p>\r\n</div>\r\n\r\n<button class=\"accordion\">Tab 4</button>\r\n<div class=\"panel\">\r\n  <p>\r\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus.\r\n    Suspendisse lectus tortor, dignissim sit amet, adipiscing nec, ultricies\r\n    sed, dolor.\r\n  </p>\r\n</div>\r\n```\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n### 4. Use the colours you want\r\n\r\nIn the beginning of the code you have the style that you can modify to have the design you like. All the colors are commented and you can add the code you want\r\n\r\n### 5. Save and publish\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nAfter you finish with the modifications you can save and see how it looks, you can only view it after you publish all the changes.\r\n\r\nYou can use the link below to try carrd if you don't know about it already.\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />","src/content/posts/add-accordion-carrd.mdx",[122],"../../assets/images/23/07/free-carrd-accorion.jpeg","b368d631144379e3","add-accordion-carrd.mdx","add-new-drive-lvm",{id:125,data:127,body:137,filePath:138,assetImports:139,digest:141,legacyId:142,deferredRender:32},{title:128,description:129,date:130,image:131,authors:132,categories:133,tags:134,canonical:136},"How To Install A New Drive to Ubuntu LVM and Mount It","Learn how to install a new drive on Ubuntu using LVM (Logical Volume Manager) and mount it for persistent use. ",["Date","2024-03-21T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/add-new-drive-lvm.jpeg",[19],[98],[135],"linux","https://www.bitdoze.com/add-new-drive-lvm/","This article will guide you through the process of installing a new drive to your Ubuntu system using Logical Volume Manager (LVM) and mounting it for persistent use. LVM is a flexible and powerful tool for managing disk storage, allowing you to easily resize and manage logical volumes.\r\n\r\n## Step 1: Check the Drives\r\n\r\nTo begin, let's check the available drives on your system using the `lshw` command:\r\n\r\n```sh\r\nsudo lshw -C disk\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n  *-disk:0\r\n      description: ATA Disk\r\n      product: Samsung SSD 870\r\n      physical id: 0\r\n      bus info: scsi@0:0.0.0\r\n      logical name: /dev/sda\r\n      version: 3B6Q\r\n      serial: S758NS0W807436T\r\n      size: 3726GiB (4TB)\r\n      configuration: ansiversion=5 logicalsectorsize=512 sectorsize=512\r\n *-disk:1\r\n      description: ATA Disk\r\n      product: RS512GSSD310\r\n      physical id: 1\r\n      bus info: scsi@1:0.0.0\r\n      logical name: /dev/sdb\r\n      version: 2A0\r\n      serial: EB091502A000561\r\n      size: 476GiB (512GB)\r\n      capabilities: gpt-1.00 partitioned partitioned:gpt\r\n      configuration: ansiversion=5 guid=0c679d6a-d4b6-40a1-962c-75145d093352 logicalsectorsize=512 sectorsize=512\r\n```\r\n\r\nThis command displays detailed information about the disks connected to your system. In this example, we have two disks: `/dev/sda` (4TB) and `/dev/sdb` (512GB).\r\n\r\n## Step 2: Display Physical Volumes (PV)\r\n\r\nTo display the existing physical volumes (PV) on your system, use the `pvs` command:\r\n\r\n```sh\r\nsudo pvs\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n PV         VG        Fmt  Attr PSize    PFree\r\n /dev/sdb3  ubuntu-vg lvm2 a--  <473.89g    0\r\n```\r\n\r\nThis output shows that there is currently one physical volume (`/dev/sdb3`) associated with the volume group `ubuntu-vg`.\r\n\r\n## Step 3: Create Physical Volumes (PV) on New Disk\r\n\r\nTo create a new physical volume on the new disk (`/dev/sda`), use the `pvcreate` command:\r\n\r\n```sh\r\nsudo pvcreate /dev/sda\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nPhysical volume \"/dev/sda\" successfully created.\r\n```\r\n\r\nVerify the creation of the new physical volume using `lvmdiskscan`:\r\n\r\n```sh\r\nsudo lvmdiskscan -l\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n WARNING: only considering LVM devices\r\n /dev/sda   [      <3.64 TiB] LVM physical volume\r\n /dev/sdb3  [    <473.89 GiB] LVM physical volume\r\n 1 LVM physical volume whole disk\r\n 1 LVM physical volume\r\n```\r\n\r\n## Step 4: Create a Volume Group (VG)\r\n\r\nCreate a new volume group named `mediavg` using the new physical volume `/dev/sda`:\r\n\r\n```sh\r\nsudo vgcreate mediavg /dev/sda\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nvolume group \"mediavg\" successfully created\r\n```\r\n\r\n## Step 5: Create a Logical Volume (LV)\r\n\r\nCreate a new logical volume named `medialv` within the `mediavg` volume group, using all available space:\r\n\r\n```sh\r\nsudo lvcreate -l +100%FREE -n medialv mediavg\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nLogical volume \"medialv\" created.\r\n```\r\n\r\n## Step 6: Create a Filesystem\r\n\r\nCreate an ext4 filesystem on the new logical volume:\r\n\r\n```sh\r\nsudo mkfs.ext4 /dev/mediavg/medialv\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nmke2fs 1.46.5 (30-Dec-2021)\r\nDiscarding device blocks: done\r\nCreating filesystem with 976753664 4k blocks and 244195328 inodes\r\nFilesystem UUID: 2d665675-4b2e-4a1f-9af6-4652e387d76e\r\nSuperblock backups stored on blocks:\r\n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,\r\n\t4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968,\r\n\t102400000, 214990848, 512000000, 550731776, 644972544\r\n\r\nAllocating group tables: done\r\nWriting inode tables: done\r\nCreating journal (262144 blocks): done\r\nWriting superblocks and filesystem accounting information: done\r\n```\r\n\r\n## Step 7: Mount the Filesystem\r\n\r\nCreate a mount point directory and mount the new filesystem:\r\n\r\n```sh\r\nsudo mkdir -p /media/storage\r\nsudo mount /dev/mediavg/medialv /media/storage\r\n```\r\n\r\nVerify the mount using the `df` command:\r\n\r\n```sh\r\ndf -h\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nFilesystem                         Size  Used Avail Use% Mounted on\r\ntmpfs                              1.6G  2.4M  1.6G   1% /run\r\n/dev/mapper/ubuntu--vg-ubuntu--lv  466G   23G  424G   6% /\r\ntmpfs                              7.8G     0  7.8G   0% /dev/shm\r\ntmpfs                              5.0M     0  5.0M   0% /run/lock\r\n/dev/sdb2                          2.0G  252M  1.6G  14% /boot\r\n/dev/sdb1                          1.1G  6.1M  1.1G   1% /boot/efi\r\ntmpfs                              1.6G  4.0K  1.6G   1% /run/user/1000\r\n/dev/mapper/mediavg-medialv        3.6T   28K  3.4T   1% /media/storage\r\n```\r\n\r\n## Step 8: Make the Mount Persistent\r\n\r\nTo ensure the filesystem is mounted automatically on system boot, add an entry to the `/etc/fstab` file.\r\n\r\nObtain the UUID of the new filesystem using `blkid`:\r\n\r\n```sh\r\nsudo blkid /dev/mediavg/medialv\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n/dev/mediavg/medialv: UUID=\"2d665675-4b2e-4a1f-9af6-4652e387d76e\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\"\r\n```\r\n\r\nEdit `/etc/fstab` and add a line like the following, replacing `UUID` with the actual UUID:\r\n\r\n```sh\r\nUUID=2d665675-4b2e-4a1f-9af6-4652e387d76e /media/storage ext4 defaults 0 2\r\n```\r\n\r\n## Step 9: Reboot and Verify\r\n\r\nReboot the server to ensure the new filesystem is mounted automatically:\r\n\r\n```sh\r\nsudo reboot\r\n```\r\n\r\nAfter the reboot, check the mounted filesystems using `df`:\r\n\r\n```sh\r\ndf -h\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\nFilesystem                         Size  Used Avail Use% Mounted on\r\ntmpfs                              1.6G  2.4M  1.6G   1% /run\r\n/dev/mapper/ubuntu--vg-ubuntu--lv  466G   23G  424G   6% /\r\ntmpfs                              7.8G     0  7.8G   0% /dev/shm\r\ntmpfs                              5.0M     0  5.0M   0% /run/lock\r\n/dev/sdb2                          2.0G  252M  1.6G  14% /boot\r\n/dev/sdb1                          1.1G  6.1M  1.1G   1% /boot/efi\r\n/dev/mapper/mediavg-medialv        3.6T   28K  3.4T   1% /media/storage\r\ntmpfs                              1.6G  4.0K  1.6G   1% /run/user/1000\r\n```\r\n\r\nThe output should show that `/dev/mapper/mediavg-medialv` is mounted on `/media/storage`.\r\n\r\n## Conclusion\r\n\r\nCongratulations! You have successfully installed a new drive to your Ubuntu system using LVM, created a logical volume, formatted it with the ext4 filesystem, and mounted it persistently. You can now use the `/media/storage` directory to store your data on the new drive.","src/content/posts/add-new-drive-lvm.mdx",[140],"../../assets/images/24/03/add-new-drive-lvm.jpeg","9ee9651d839a5a8e","add-new-drive-lvm.mdx","ai-images-mac",{id:143,data:145,body:155,filePath:156,assetImports:157,digest:159,legacyId:160,deferredRender:32},{title:146,description:147,date:148,image:149,authors:150,categories:151,tags:152,canonical:154},"How To Locally Generate AI Images (Flux) on Mac","Learn how you can use your own Mac to start and generate images with flux or stable difussion.",["Date","2025-01-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/01/easy-generate-local-images.jpg",[19],[98],[153],"mac","https://www.bitdoze.com/ai-images-mac/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nThe new Mac Mini M4 Pro has brought unprecedented power to desktop computing, making it an excellent choice for AI image generation. In this article, we'll explore how to use the FLUX AI model on your Mac Mini M4 using DiffusionBee, a user-friendly application that simplifies the process of creating AI-generated images.\r\n\r\nIf you are interested to see some free cool Mac Apps you can check [toolhunt.net mac apps section](https://toolhunt.net/mac/).\r\n\r\n## What is FLUX?\r\n\r\nFLUX is an advanced AI model developed by Black Forest Labs, known for producing high-quality images. It's particularly notable for its ability to generate photorealistic results and has quickly become a favorite among AI art enthusiasts.\r\n\r\n## Why Use DiffusionBee?\r\n\r\n[DiffusionBee](https://diffusionbee.com/) is often referred to as the fastest and easiest toolbox for running AI apps locally on Mac computers. It provides a straightforward interface for using various AI models, including FLUX, making it an ideal choice for both beginners and experienced users.\r\n\r\nDiffusionBee key Features:\r\n\r\n1. **Text-to-Image Generation:** DiffusionBee allows users to create stunning images from text prompts, transforming written descriptions into visual art using advanced AI technology.\r\n\r\n2. **Offline Functionality:** The application runs entirely on your local machine, ensuring privacy and security by processing all image generation without an internet connection.\r\n\r\n3. **Image Editing Tools:** DiffusionBee offers a comprehensive suite of image manipulation features, including image-to-image transformation, inpainting, outpainting, and generative fill.\r\n\r\n4. **High-Resolution Outputs:** Users can generate high-quality images with customizable dimensions, supporting resolutions up to 2K for clear, detailed visuals.\r\n\r\n5. **Custom Model Training:** DiffusionBee allows users to train their own image generation models using personal datasets, enabling the creation of highly personalized and unique artistic styles.\r\n\r\nThese features make DiffusionBee a powerful and versatile tool for AI art creation, catering to both beginners and experienced artists alike.\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/lD9EJGjTKj8\"\r\n  label=\"How To Locally Generate AI Images (Flux) on Mac\"\r\n/>\r\n\r\n\r\n## Step-by-Step Guide\r\n\r\n### 1. Download DiffusionBee\r\n\r\n- Visit the [DiffusionBee release page](https://github.com/divamgupta/diffusionbee-stable-diffusion-ui/releases/tag/2.5.3).\r\n- Download the latest version compatible with your Mac Mini M4 (arm64 architecture).\r\n- Look for a file named similar to \"DiffusionBee_MPS_arm64-2.5.3.dmg\" or newer.\r\n\r\n### 2. Install DiffusionBee\r\n\r\n- Locate the downloaded .dmg file in your Downloads folder.\r\n- Double-click the file to open it.\r\n- Drag the DiffusionBee icon into the Applications folder.\r\n\r\n### 3. Launch DiffusionBee\r\n\r\n- Open your Applications folder and double-click on the DiffusionBee app.\r\n- If you encounter any security warnings, right-click the app icon, select \"Open,\" and confirm that you want to open the app.\r\n\r\n### 4. Access FLUX Models\r\n\r\n- In DiffusionBee, scroll down to the bottom of the home screen.\r\n- Look for the FLUX section.\r\n- Choose the flux model that you want to use.\r\n\r\n\r\n### 5. Download FLUX Models\r\n\r\n- In the FLUX section, you'll see available FLUX models.\r\n- Click on the download button next to the model you want to use.\r\n- Wait for the download to complete. The FLUX dev model is approximately 13.6GB.\r\n\r\n\r\n![diffusionbee models](../../assets/images/25/01/diffusionbee-models.jpeg)\r\n\r\n### 6. Generate Images with FLUX\r\n\r\n- Once the model is downloaded, select it from the model dropdown menu.\r\n- Enter your prompt in the text field.\r\n- Adjust any settings as desired (image size, number of steps, etc.).\r\n- Click the \"Generate\" button.\r\n\r\n![diffusionbee generate image](../../assets/images/25/01/diffusionbee-gen-image.png)\r\n\r\n### 7. How Much It Takes\r\n\r\nI have the M4 Pro base model with 24GB of RAM. When I am generating one image with Flux Dev and 25 steps at 704x704 it takes about 6 minutes to have an image generated. When I use the default model 30 seconds.\r\n\r\nThe Flux generation on DiffusionBee is using 7 GB of RAM with 50% off GPU. THe standard one half of the RAM and GPU from Flux.\r\n\r\n## Tips for Optimal Performance\r\n\r\n- For better results, try using the \"FLUX.1-schnell\" model on Macs with 16GB RAM.\r\n- Close unnecessary applications to free up system resources.\r\n- Experiment with different prompts and settings to achieve the best results.\r\n- Use fewer steps for faster image generation.\r\n\r\n## Conclusion\r\n\r\nGenerating AI images with FLUX on your Mac Mini M4 is now easier than ever, thanks to DiffusionBee. With just one click, you can create stunning, high-quality images right on your desktop. As you explore this powerful tool, remember that the key to great AI-generated art lies in experimentation and creativity. Enjoy your journey into the world of AI art creation!","src/content/posts/ai-images-mac.mdx",[158],"../../assets/images/25/01/easy-generate-local-images.jpg","0e471987c9c456ff","ai-images-mac.mdx","add-stickey-header-carrd",{id:161,data:163,body:172,filePath:173,assetImports:174,digest:176,legacyId:177,deferredRender:32},{title:164,description:165,date:166,image:167,authors:168,categories:169,tags:170,canonical:171},"How To Add a Sticky Header to Carrd","Let's see how you can add a sticky header to a Carrd website for a better experience.",["Date","2023-05-30T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/05/stickey_header_card.jpeg",[19],[21],[43],"https://www.bitdoze.com/add-stickey-header-carrd/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/05/carrd-page-settings.png\";\r\n\r\nIn this tutorial, we are going to see how we can add a sticky header or navbar to a Carrd.co website to make it more interesting for your visitors. The code will need the Pro standard plan as we will need to use the embed widget, you don't need to be on the latest plan as we will take the container ID.\r\n\r\nThere will be 2 codes, one that will make everything sticky or fix how you want to call it on mobile and PC and the second code that will make everything sticky on screens bigger than 600px.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nIf you are interested in more carrd tutorials you can check:\r\n\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [How To Add Custom Domain to Carrd.co](https://www.bitdoze.com/carrd-add-domain/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## How To Add a Sticky Header to Carrd\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/hClMHp1CSec\"\r\n  label=\"How To Add a Sticky Header to Carrd.co\"\r\n/>\r\n\r\n### 1. Set Carrd.co Page Settings\r\n\r\nThe first thing you need to do is to set the vertical and horizontal padding to 0 in the page settings, in case they are not 0 the floating navbar will not have the correct sizes and will not look good.\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"Carrd page settings\"\r\n/>\r\n\r\n<Button link=\"https:/go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\n### 2. Get Container ID OR Add a Name to Container\r\n\r\nIn case you are not on the Pro Plus plan and you are on the standard plan you will need to get the ID of the container that is the header, to do that you need to use the inspect element in the browser as you see in the video. The ID is important as will be linked to the CSS that will make the header sticky. The video has everything you need,\r\n\r\nIn case you are on the Pro Plus plan you can add an ID to the container and use that in your CSS code, you just go to settings and add your ID there.\r\n\r\n<Button link=\"https:/go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\n### 3. CSS Code\r\n\r\nThe next thing that you need to do is to add an embed widget and add the CSS code below, as mentioned there will be 2 codes, one that is making the header sticky everywhere and the other that is not making the code sticky on mobile\r\n\r\n#### Stickey On Mobile Also\r\n\r\n```html\r\n<style>\r\n  #container01 {\r\n    position: fixed !important;\r\n    z-index: 99;\r\n    top: 0;\r\n  }\r\n</style>\r\n```\r\n\r\n#### Stickey On Displays bigger Then 600px\r\n\r\n```html\r\n<style>\r\n  @media screen and (min-width: 600px) {\r\n    #container01 {\r\n      position: fixed !important;\r\n      z-index: 99;\r\n      top: 0;\r\n    }\r\n  }\r\n</style>\r\n```\r\n\r\nIn the above codes, you need to replace **container01** with the ID of your container from second point\r\n\r\n### 4. Add a Space\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nThe next container will be pushed up and the website can look bad, to fix that you can add a divider and margins to the top of it, you can use the container below the paddings but is better to use the divider as you can control the mobile padding also, and in function off what you want you can play around. All the explanations are on the video\r\n\r\n### 5. Save the page\r\n\r\nAfter you are happy with the changes you can save it and see how it looks.\r\n\r\n<Button link=\"https:/go.bitdoze.com/carrd\" text=\"Carrd.co\" />","src/content/posts/add-stickey-header-carrd.mdx",[175],"../../assets/images/23/05/stickey_header_card.jpeg","6a8e5acdc354ab4c","add-stickey-header-carrd.mdx","best-astrojs-online-courses",{id:178,data:180,body:190,filePath:191,assetImports:192,digest:194,legacyId:195,deferredRender:32},{title:181,description:182,date:183,image:184,authors:185,categories:186,tags:187,canonical:189},"Best Astro.js Online Courses/Tutorials","If you want to learn Astro.js you should check these online courses that can help you get started.",["Date","2022-10-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/2210/astrojs_tutorials.jpeg",[19],[21],[23,188],"courses","https://www.bitdoze.com/best-astrojs-online-courses/","[Astro.js](https://astro.build/) is a new static website generator that can help you build any website fast. It is currently what is powering my blog www.bitdoze.com and I have just started working with it a couple of weeks.\r\n\r\nBeing at the beginning it doesn't have a lot of courses out here that can help you learn it. It can be difficult to learn especially for someone who is not into javascript and static website generators. That's why to learn it I will get whatever I can.\r\n\r\nIn the past, I have written an article with [Best Gatsby.Js Online Courses](https://www.bitdoze.com/gatsby-js-online-courses/) that can help you get started with Gatsby, and here I plan to do the same thing.\r\n\r\n## Best Astro.js Online Courses\r\n\r\nLet's see below what are some of the Best Astro.js courses that can be found online at this point. Most of them are on Youtube and have everything you need to get started with.\r\n\r\nAs a start to get you going with node.js and GitHub you can check: [Install Node.js using NVM](https://www.bitdoze.com/install-nodejs-using-nvm-macos-ubuntu/) and [Link GitHub with A SSH Key](https://www.bitdoze.com/link-github-with-ssh-maco-linux/)\r\n\r\n### [Astro Crash Course](https://www.youtube.com/watch?v=Oi9z5gfIHJs)\r\n\r\nThis is a course that will bring you thru all that Astro has to offer, you will build a project from scratch and at the end, you will deploy the project to Netlify. This has 1 hour and 35 minutes and will walk you through everything Astro has to offer.\r\n\r\n### [Building a Portfolio with Astro](https://www.youtube.com/watch?v=0kVmdaIquJc&t=2s)\r\n\r\nThis is an Astro video that will help you get started with Astro and create a portfolio website. The tutorial will start from scratch and show you everything you need to configure this. All of it has 1 hour and 47 minutes and will help you better understand Astro and create a portfolio website.\r\n\r\n### [Build A Website with Astro, TailwindCSS and React](https://www.youtube.com/watch?v=eVjk3RP8ElE)\r\n\r\nThis tutorial will show you how to create an Astro website with TailwindCSS and React. The video will bring you through all the configs you need to make to have a blog built on Astro with TailwindCSS.\r\n\r\n### [Jamstack Astro Training](https://courses.jamstack.training/p/ship-less-javascript-with-astro)\r\n\r\nThis is a full course that will present everything on Astro. You need to register for the course, it's free and will help you get started with Astro and understand what is about.\r\n\r\n### [I Try Astro For the First Time](https://www.youtube.com/watch?v=2H9T1-H5V3M)\r\n\r\nAnother youtube video that will get you into what Astro has to offer, this is more like intro things and is not building a website from scratch.\r\n\r\n### [Astro from the Ground Up](https://www.youtube.com/watch?v=9juD4JVGmfc)\r\n\r\nThis is going to teach you some Astro about idle, routing, and some live coding.\r\n\r\n### [Little Sticks](https://www.youtube.com/c/LittleSticks/videos)\r\n\r\nThis is a youtube channel that has various videos about Astro, on his channel you can find videos of how you can add categories and tags, how to use images, and how to use Astro with Sanity. He has also some nice themes that you can use for your website.\r\n\r\nIf you have an Astro course that needs to be added in here just drop me an email at: bitdoze1[@]gmail.com","src/content/posts/best-astrojs-online-courses.mdx",[193],"../../assets/images/2210/astrojs_tutorials.jpeg","74d6e79b5b8bf113","best-astrojs-online-courses.mdx","best-mini-pc-home-server",{id:196,data:198,body:208,filePath:209,assetImports:210,digest:212,legacyId:213,deferredRender:32},{title:199,description:200,date:201,image:202,authors:203,categories:204,tags:205,canonical:207},"Best Mini PC For Home Server: Build A Home Lab At Low Costs","Explore top mini PCs for home servers. Compare Intel, AMD, and ARM options for streaming, NAS, and more. Find your ideal home lab setup.",["Date","2024-09-13T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/09/best-mini-pc-home-server.jpeg",[19],[98],[206],"homelab","https://www.bitdoze.com/best-mini-pc-home-server/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/09/beelink-s12.jpg\";\r\nimport img2 from \"../../assets/images/24/09/TRIGKEY_Micro.jpg\";\r\nimport img3 from \"../../assets/images/24/09/ZOTAC.jpg\";\r\nimport img4 from \"../../assets/images/24/09/intelnuc13.jpg\";\r\nimport img5 from \"../../assets/images/24/09/ASUS_NUC_14.jpg\";\r\nimport img6 from \"../../assets/images/24/09/ASUS_NUC_14_Proplus.jpg\";\r\nimport img7 from \"../../assets/images/24/09/ACEMAGICIAN.jpg\";\r\nimport img8 from \"../../assets/images/24/09/GEEKOM_AE7.jpg\";\r\nimport img9 from \"../../assets/images/24/09/Raspberry_Pi_5.jpg\";\r\nimport img10 from \"../../assets/images/24/09/mac-mini.jpg\";\r\nimport img11 from \"../../assets/images/24/09/gmktec-mini-pc.jpg\";\r\nimport img12 from \"../../assets/images/24/09/orange-pi-5-plus.jpg\";\r\n\r\n\r\nIn the era of digital transformation, home servers have become increasingly popular among tech enthusiasts and homeowners alike. A home server acts as a central hub for managing various digital tasks, from storing and streaming media to hosting personal websites and automating home devices. As the demand for compact, energy-efficient, and cost-effective solutions grows, mini PCs have emerged as an excellent choice for building home servers.\r\n\r\nI have an N100 Mini PC at my home where I host a couple of services like file sharing, or media streaming and is enough for the basic things. Other then this I am using it to host various docker apps that I present also on my website so I know this subject preaty well.\r\n\r\n## Purpose of a home server\r\n\r\nA home server serves multiple purposes, catering to the diverse needs of modern households:\r\n\r\n1. Centralized file storage and backup\r\n2. Media streaming and management\r\n3. Personal cloud services\r\n4. Home automation control\r\n5. Web hosting and development environment\r\n6. Network-attached storage (NAS)\r\n7. Virtual machine hosting for testing and learning\r\n\r\n> You can check [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/) to see what apps can be hosted with the help of docker.\r\n\r\n## Benefits of a mini PC for home server\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/0bLrnG0S4mg\"\r\n  label=\"Best Mini PC Home Server\"\r\n/>\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nMini PCs offer several advantages when used as home servers:\r\n\r\n1. **Compact size:** Takes up minimal space in your home\r\n2. **Energy efficiency:** Consumes less power compared to traditional servers\r\n3. **Quiet operation:** Ideal for 24/7 use in living spaces\r\n4. **Cost-effectiveness:** More affordable than full-sized servers or custom builds\r\n5. **Versatility:** Can be repurposed for other tasks if needed\r\n6. **Easy maintenance:** Simple to upgrade and service\r\n7. **Low heat output:** Reduces cooling requirements\r\n\r\nBy leveraging a mini PC for your home server, you can enjoy the benefits of a powerful, centralized system without the drawbacks of larger, more complex setups.\r\n\r\nMoreover, mini PCs can be easily categorized into two main types: ARM-based and X86-based systems. The choice between these depends on specific needs and the desired level of compatibility with certain applications.\r\n\r\nAdditionally, setting up a home lab using a mini PC as a server can be a valuable educational experience. It allows users to try new technologies, experiment with different operating systems, and gain hands-on experience in managing and configuring networks and servers. This practical knowledge can be beneficial for both personal and professional development, especially for those interested in IT and computing fields.\r\n\r\n\r\nIf you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## Factors to Consider\r\n\r\nWhen selecting a mini PC for your home server, several key factors should influence your decision:\r\n\r\n### RAM and CPU specification (ARM, X86)\r\n\r\n- **RAM:** Consider at least 8GB for basic tasks, 16GB or more for demanding applications, even more if you have the budget.\r\n- **CPU:** Choose between ARM and X86 architectures\r\n  - **ARM:** Generally more energy-efficient, but may have software compatibility limitations\r\n  - **X86:** Wider software compatibility, typically higher performance but more power-hungry\r\n- Look for multi-core processors to handle multiple tasks efficiently\r\n\r\n### Storage capacity\r\n\r\n- Consider SSDs for faster performance and reliability\r\n- Determine your storage needs based on intended use (e.g., media server, backups)\r\n- Look for models with expandable storage options or multiple drive bays\r\n\r\n### Connectivity options (e.g., USB ports, Ethernet, Wi-Fi)\r\n\r\n- **Ethernet:** Gigabit Ethernet is essential; consider models with dual NICs for improved network performance\r\n- **USB ports:** Ensure sufficient ports for external devices and expansion\r\n- **Wi-Fi:** Built-in Wi-Fi can be convenient, but wired connections are generally more reliable for servers\r\n- **HDMI or DisplayPort:** Useful for initial setup and troubleshooting\r\n\r\n### Power consumption and energy efficiency\r\n\r\n- Look for energy-efficient processors and components\r\n- Consider the long-term running costs, especially for 24/7 operation\r\n- Fanless designs can offer silent operation and lower power consumption\r\n\r\n### Cost\r\n\r\n- Balance initial purchase price with long-term value\r\n- Consider the total cost of ownership, including power consumption and potential upgrades\r\n- Evaluate the cost-effectiveness of building your own vs. buying a pre-built system\r\n\r\nBy carefully considering these factors, you can select a mini PC that best suits your home server needs while optimizing performance, efficiency, and cost.\r\n\r\n## Recommended Mini PCs for Home Server\r\n\r\n| Product | CPU | RAM | Storage | Connectivity | Ideal For | Power Consumption (Approx.) | Quick Link | Price Range |\r\n|---------------------|-----|-----|---------|--------------|-----------|---------------------------|------------|-------------|\r\n| [GMKtec Mini PC](https://amzn.to/3XqHzr4) | Intel N100 (4C/4T) | 16GB DDR4 | 1TB NVMe SSD | Wi-Fi 6, BT 5.2, Gigabit Ethernet | Basic home server, Office tasks | 10-25W | [Details](#gmktec-mini-pc-intel-n100-windows-11-pro) | $ |\r\n| [Beelink S12 Pro](https://amzn.to/3AYvfqi) | Intel N100 (4C/4T) | 16GB DDR4 | 512GB NVMe SSD | Dual Ethernet, Wi-Fi 6, BT 5.2 | Basic home server, Media streaming | 10-25W | [Details](#beelink-s12-pro-mini-pc-intel-12th-gen-alder-lake--n100) | $ |\r\n| [TRIGKEY Micro Computer](https://amzn.to/3To5MNC) | Intel N100 (4C/4T) | 16GB DDR4 | 500GB M.2 SSD | Dual Ethernet, Wi-Fi 6, BT 5.2 | File sharing, Basic NAS | 10-25W | [Details](#trigkey-micro-computer-12th-gen-intel-n100-4c4t-16g-ddr4-500g-m2) | $ |\r\n| [ZOTAC ZBOX CI337 nano](https://amzn.to/3TqugWp) | Intel N100 (4C/4T) | Up to 32GB DDR4 | Supports 2.5\" & M.2 | Dual Ethernet, Wi-Fi 6, BT 5.2 | Silent operation, Customizable storage | 10-25W | [Details](#zotac-zbox-ci337-nano-fanless-silent-mini-pc-intel-processor-n100) | $ |\r\n| [Intel NUC13 Pro](https://amzn.to/3B1Ci1v) | i5-1340P (12C/16T) | 16GB DDR4 | 512GB NVMe SSD | TB4, Wi-Fi 6E, BT 5.3 | Virtualization, Advanced media server | 20-60W | [Details](#intel-nuc13-pro-nuc13anhi5-16gb-ram-512gb-ssd-mini-pc) | $$$ |\r\n| [ASUS NUC Pro 14 Ultra 5](https://amzn.to/3ziR6Za) | Intel Ultra 5 125H (14C/18T) | 32GB DDR5 | 512GB NVMe SSD | TB4, Wi-Fi 6E, BT 5.3 | AI tasks, High-performance server | 25-65W | [Details](#asus-nuc-pro-14-ultra-5-ai-mini-desktop) | $$$$ |\r\n| [ASUS NUC 14 Pro+](https://amzn.to/3MI01q3) | Core Ultra 9 185H (16C/22T) | Up to 96GB DDR5 | Multiple M.2 slots | TB4, Wi-Fi 6E, BT 5.3 | Extreme performance, Multiple VMs | 30-75W | [Details](#asus-nuc-14-pro-barebone-with-intel-14thgen-core-ultra-9-185h) | $$$$ |\r\n| [ACEMAGICIAN Mini PC](https://amzn.to/3MQ9M5v) | Ryzen 7 5800U (8C/16T) | 16GB DDR4 | 512GB SSD | Dual Ethernet, Wi-Fi 6, BT 5.2 | Multi-tasking, Media transcoding | 15-45W | [Details](#acemagician-dual-lan-mini-gaming-pc-amd-ryzen-7-5800u) | $$ |\r\n| [GEEKOM AE7 Mini PC](https://amzn.to/47qJIaM) | Ryzen 9 7940HS (8C/16T) | Up to 64GB DDR5 | Multiple M.2 slots | TB4, Wi-Fi 6E, BT 5.2 | High-performance tasks, Development | 20-55W | [Details](#geekom-ae7-mini-pc-amd-ryzen-9-7940hs-processor-ai-mini-computer) | $$$ |\r\n| [Orange Pi 5 Plus](https://amzn.to/4d31fqR) | RK3588 (8C) | 32GB LPDDR4X | 256GB eMMC | Dual Gigabit Ethernet, Wi-Fi 6, BT | Low-power server, Development | 5-15W | [Details](#orange-pi-5-plus-32gb-with-256gb-emmc) | $ |\r\n| [CanaKit Raspberry Pi 5](https://amzn.to/4gprJ8B) | BCM2712 (4C) | 8GB LPDDR4X | 128GB microSD | Dual Gigabit Ethernet, Wi-Fi 6, BT 5.0 | Low-power services, Learning | 5-15W | [Details](#canakit-raspberry-pi-5-starter-kit-pro---turbine-black) | $ |\r\n| [Apple Mac Mini M2](https://amzn.to/4grPgWL) | M2 (8C CPU/10C GPU) | 8GB (up to 24GB) | 256GB-2TB SSD | TB4, Wi-Fi 6E, BT 5.3 | macOS server, Energy-efficient tasks | 10-35W | [Details](#apple-2023-mac-mini-desktop-computer-with-apple-m2-chip-with-8core-cpu-and-10core-gpu) | $$$ |\r\n\r\nPrice Range Key:\r\n$ = Budget-friendly\r\n$$ = Mid-range\r\n$$$ = High-end\r\n$$$$ = Premium\r\n\r\n### ARM vs X86\r\n\r\nWhen choosing a mini PC for your home server, one of the fundamental decisions is between ARM and X86 architectures:\r\n\r\n**ARM:**\r\n- **Pros:**\r\n  - Excellent energy efficiency\r\n  - Lower heat generation\r\n  - Often more affordable\r\n  - Ideal for always-on, low-power servers\r\n- **Cons:**\r\n  - Limited software compatibility compared to X86\r\n  - Generally lower performance for complex tasks\r\n  - Fewer high-performance options available\r\n\r\n**X86:**\r\n- **Pros:**\r\n  - Wide software compatibility\r\n  - Higher performance capabilities\r\n  - More options available across various price points\r\n  - Better suited for virtualization and demanding applications\r\n- **Cons:**\r\n  - Typically higher power consumption\r\n  - Can generate more heat, potentially requiring active cooling\r\n  - Often more expensive than comparable ARM options\r\n\r\nChoose ARM if you prioritize energy efficiency and have simple server needs. Opt for X86 if you require broader software compatibility or plan to run more demanding applications on your home server.\r\n\r\n### N100 CPU Mini PC (Cheaper)\r\n\r\n\r\n#### [GMKtec Mini PC, Intel N100 Windows 11 Pro](https://amzn.to/3XqHzr4)\r\n\r\n<Picture\r\n  src={img11}\r\n  alt=\"GMKtec Mini PC, Intel N100 Windows 11 Pro\"\r\n/>\r\n\r\nThe GMKtec Mini PC offers a compelling balance of performance and affordability, powered by the efficient Intel N100 processor. This compact device comes pre-installed with Windows 11 Pro, making it ready for immediate use in home server applications. With 16GB of DDR4 RAM and a spacious 1TB NVMe SSD, it provides ample resources for various tasks, from file sharing to light media streaming. The inclusion of Wi-Fi 6 and Bluetooth 5.2 ensures robust wireless connectivity, while the Gigabit Ethernet port caters to more demanding network applications.\r\n\r\n**Features:**\r\n- Intel N100 CPU (4 cores, 4 threads, up to 3.4GHz)\r\n- 16GB DDR4 RAM\r\n- 1TB PCIe M.2 NVMe SSD\r\n- Wi-Fi 6 and Bluetooth 5.2\r\n- Gigabit Ethernet\r\n- 4K Dual HDMI display support\r\n- Multiple USB 3.2 ports\r\n- Windows 11 Pro pre-installed\r\n\r\n**Why it's good for home server:**\r\n- Energy-efficient processor suitable for 24/7 operation\r\n- Large built-in storage capacity\r\n- Windows environment familiar to many users\r\n- Compact size ideal for small spaces\r\n- Capable of handling basic server tasks and office applications\r\n\r\n**Why it might not be for others:**\r\n- Limited upgradeability\r\n- May struggle with intensive tasks like multiple 4K transcodes\r\n- Not as customizable as some barebones options\r\n\r\n<Button link=\"https://amzn.to/3XqHzr4\" text=\"Check IT\" />\r\n\r\n#### [Beelink S12 Pro Mini PC, Intel 12th Gen Alder Lake- N100](https://amzn.to/3AYvfqi)\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"Beelink S12 Pro Mini PC\"\r\n/>\r\n\r\nThe Beelink S12 Pro is a compact and versatile mini PC that offers a strong balance of performance and energy efficiency, powered by Intel's 12th Gen Alder Lake-N100 processor. Its small form factor makes it an ideal choice for users looking to set up a home server without consuming too much space or budget. The S12 Pro features dual Ethernet ports, which provide network redundancy and the option for link aggregation, enhancing reliability for home server setups. With expandable RAM and storage options, this mini PC can adapt to various needs, starting as a basic file server and potentially evolving into a more complex home automation hub or media server.\r\n\r\n**Features:**\r\n- **Intel N100 CPU** (4 cores, 4 threads, up to 3.4GHz)\r\n- **16GB DDR4 RAM**\r\n- **512GB NVMe SSD**\r\n- **Dual Ethernet ports**\r\n- **Wi-Fi 6 and Bluetooth 5.2**\r\n- **Multiple USB ports, including USB 3.2**\r\n- **Dual HDMI outputs**\r\n- **Compact form factor** (124 x 113 x 41 mm)\r\n\r\n**Why it's good for home server:**\r\n- Energy-efficient processor suitable for 24/7 operation\r\n- Dual Ethernet for improved network performance or redundancy\r\n- Compact size ideal for small spaces or discreet placement\r\n- Silent operation due to low heat generation\r\n- Expandable storage options for growing data needs\r\n- Wi-Fi 6 support for high-speed wireless connectivity\r\n\r\n**Why it might not be for others:**\r\n- Limited CPU power for resource-intensive tasks like real-time video transcoding\r\n- May struggle with multiple simultaneous transcoding jobs for media servers\r\n- Not suitable for heavy virtualization workloads or running multiple VMs\r\n- Graphics performance may be insufficient for GPU-accelerated tasks\r\n\r\nOverall, the Beelink S12 Pro is a cost-effective solution for light computing tasks, home servers, and media playback, but users should be aware of its limitations in high-performance scenarios.\r\n\r\n\r\n<Button link=\"https://amzn.to/3AYvfqi\" text=\"Check IT\" />\r\n\r\n\r\n\r\n#### [TRIGKEY Micro Computer 12th Gen Intel N100 4C/4T 16G DDR4 500G M.2](https://amzn.to/3To5MNC)\r\n\r\n<Picture\r\n  src={img2}\r\n  alt=\"TRIGKEY Micro Computer 12th Gen Intel N100\"\r\n/>\r\n\r\nThe TRIGKEY Micro Computer is a robust mini PC solution that comes well-equipped out of the box. Featuring the efficient Intel N100 processor, this compact powerhouse offers a generous 16GB of RAM and a spacious 500GB M.2 SSD, providing ample resources for a variety of home server applications. Its metal chassis not only gives it a premium feel but also aids in heat dissipation, contributing to its stability during extended use. The dual Ethernet ports make it a strong contender for network-intensive tasks, while the inclusion of Wi-Fi 6 ensures fast wireless connectivity when needed. This mini PC strikes a balance between performance, storage, and connectivity, making it a versatile choice for users who want a ready-to-go solution with room for future expansion.\r\n\r\n**Features:**\r\n- Intel N100 CPU (4 cores, 4 threads)\r\n- 16GB DDR4 RAM\r\n- 500GB M.2 SSD\r\n- Dual Ethernet ports\r\n- Wi-Fi 6 and Bluetooth 5.2\r\n- Compact metal chassis for better heat dissipation\r\n- Multiple USB ports for peripheral connectivity\r\n- HDMI and DisplayPort for dual-monitor support\r\n\r\n**Why it's good for home server:**\r\n- Higher RAM and storage capacity for improved multitasking and data storage\r\n- Energy-efficient design for continuous operation\r\n- Dual LAN ports for network redundancy or link aggregation\r\n- Sufficient storage for basic media server needs or file sharing\r\n- Metal chassis aids in passive cooling for stable 24/7 operation\r\n- Pre-configured system reduces setup time\r\n\r\n**Why it might not be for others:**\r\n- CPU may be underpowered for heavy server applications or databases\r\n- Limited upgradeability compared to larger systems or custom builds\r\n- Not suitable for GPU-intensive tasks or gaming server applications\r\n- May be overkill for very basic NAS or file-sharing needs\r\n\r\nOverall, the TRIGKEY Micro Computer is a compelling option for users seeking a compact and efficient mini PC for home server tasks and general computing needs.\r\n\r\n<Button link=\"https://amzn.to/3To5MNC\" text=\"Check IT\" />\r\n\r\n\r\n#### [ZOTAC ZBOX CI337 nano Fanless Silent Mini PC Intel Processor N100](https://amzn.to/3TqugWp)\r\n\r\n<Picture\r\n  src={img3}\r\n  alt=\"ZOTAC ZBOX CI337 nano Fanless Silent Mini PC Intel Processor N100\"\r\n/>\r\n\r\nThe ZOTAC ZBOX CI337 nano stands out in the mini PC market with its completely fanless design, offering silent operation that's perfect for noise-sensitive environments. This compact powerhouse utilizes the efficient Intel N100 processor, balancing performance with energy efficiency. Its passively cooled system not only ensures quiet operation but also increases reliability by eliminating moving parts. The CI337 nano's flexibility shines through its support for both 2.5\" drives and M.2 SSDs, allowing users to customize their storage setup. While it comes as a barebones unit, this approach offers the advantage of personalized configuration, letting users choose their preferred RAM and storage options. The dual Ethernet ports and Wi-Fi 6 support make it a versatile networking device, suitable for various home server applications where silent operation is paramount.\r\n\r\n**Features:**\r\n- Intel N100 CPU (4 cores, 4 threads)\r\n- Supports up to 32GB DDR4 RAM\r\n- Supports 2.5\" SSD/HDD and M.2 SSD\r\n- Dual Ethernet ports\r\n- Wi-Fi 6 and Bluetooth 5.2\r\n- Completely fanless design for silent operation\r\n- Compact form factor (127.8 x 126.7 x 56.8mm)\r\n- HDMI and DisplayPort outputs\r\n\r\n**Why it's good for home server:**\r\n- Fanless design ensures silent 24/7 operation, ideal for living areas\r\n- Flexible storage options for customization of capacity and performance\r\n- Compact form factor for space-constrained setups or discreet placement\r\n- Low power consumption for energy efficiency and reduced operating costs\r\n- Dual Ethernet for improved network capabilities or redundancy\r\n- Barebones model allows for personalized configuration\r\n\r\n\r\n**Why it might not be for others:**\r\n- Barebones model requires separate purchase of RAM and storage, increasing initial setup effort\r\n- Limited CPU performance for demanding server applications or heavy multitasking\r\n- Passive cooling may limit sustained performance under heavy loads in warm environments\r\n- Lack of dedicated GPU limits uses for graphics-intensive applications\r\n- Initial cost may be higher when factoring in additional components for a complete system\r\n\r\n<Button link=\"https://amzn.to/3TqugWp\" text=\"Check IT\" />\r\n\r\n\r\n### Intel More Powerful Mini PC\r\n\r\n#### [Intel NUC13 Pro NUC13ANHi5 16GB RAM 512GB SSD Mini PC](https://amzn.to/3B1Ci1v)\r\n\r\n<Picture\r\n  src={img4}\r\n  alt=\"Intel NUC13 Pro NUC13ANHi5 16GB RAM 512GB SSD Mini PC\"\r\n/>\r\n\r\nThe Intel NUC13 Pro NUC13ANHi5 represents a significant step up in performance for mini PCs. Powered by the robust Intel Core i5-1340P processor, this compact powerhouse offers 12 cores and 16 threads, making it capable of handling demanding server tasks with ease. The included 16GB of RAM and 512GB SSD provide a solid foundation for various applications, from virtualization to media transcoding. Intel's vPro technology adds an extra layer of security and manageability, making this NUC an excellent choice for both home and small business environments. The inclusion of Thunderbolt 4 ports offers blazing-fast data transfer speeds and the ability to connect high-performance external devices, expanding its capabilities even further. Despite its powerful internals, the NUC13 Pro maintains a small footprint, making it suitable for space-constrained setups.\r\n\r\n**Features:**\r\n- Intel Core i5-1340P (12 cores, 16 threads)\r\n- 16GB DDR4 RAM (expandable)\r\n- 512GB NVMe SSD\r\n- Thunderbolt 4 ports\r\n- Intel vPro technology\r\n- Wi-Fi 6E and Bluetooth 5.3\r\n- Multiple USB ports including USB 3.2 Gen 2\r\n- HDMI 2.0b and DisplayPort 1.4a\r\n\r\n**Why it's good for home server:**\r\n- High-performance processor capable of handling multiple server roles simultaneously\r\n- Thunderbolt 4 support for ultra-fast external storage or device connectivity\r\n- Intel vPro for enhanced security and remote management\r\n- Compact size despite powerful internals\r\n- Suitable for virtualization and running multiple VMs\r\n- Can handle real-time media transcoding for home media servers\r\n\r\n**Why it might not be for others:**\r\n- Higher power consumption compared to N100-based systems\r\n- More expensive than entry-level mini PCs\r\n- May generate more heat, potentially requiring better ventilation\r\n- Overkill for basic file sharing or simple web hosting tasks\r\n\r\n<Button link=\"https://amzn.to/3B1Ci1v\" text=\"Check IT\" />\r\n\r\n\r\n\r\n#### [ASUS NUC Pro 14 Ultra 5 AI Mini Desktop](https://amzn.to/3ziR6Za)\r\n\r\n<Picture\r\n  src={img5}\r\n  alt=\"ASUS NUC Pro 14 Ultra 5 AI Mini Desktop\"\r\n/>\r\n\r\nThe ASUS NUC Pro 14 Ultra 5 AI Mini Desktop brings cutting-edge technology to the mini PC form factor. Featuring the Intel Ultra 5 125H processor, this compact machine is designed to handle AI-accelerated tasks alongside traditional computing workloads. The inclusion of 32GB DDR5 RAM and a 512GB SSD ensures snappy performance across a wide range of applications. This NUC stands out with its AI capabilities, making it future-proof for emerging technologies and applications in home server environments. The combination of Thunderbolt 4, Wi-Fi 6E, and Bluetooth 5.3 provides versatile connectivity options, allowing it to serve as a central hub for various home networking and automation tasks. Its compact yet powerful nature makes it an ideal choice for users looking to leverage AI capabilities in their home server setup without sacrificing space.\r\n\r\n**Features:**\r\n- Intel Ultra 5 125H processor with AI acceleration (14 cores, 18 threads)\r\n- 32GB DDR5 RAM\r\n- 512GB NVMe SSD\r\n- Thunderbolt 4 connectivity\r\n- Wi-Fi 6E and Bluetooth 5.3\r\n- Multiple USB ports and display outputs\r\n- Compact form factor\r\n- ASUS-specific cooling and power management features\r\n\r\n**Why it's good for home server:**\r\n- AI acceleration capabilities for future-proofing and advanced tasks\r\n- High-performance processor suitable for multitasking and demanding applications\r\n- DDR5 RAM for improved memory performance\r\n- Excellent connectivity options for versatile home server setups\r\n- Compact size allows for flexible placement options\r\n- Capable of handling complex home automation and media server tasks\r\n\r\n**Why it might not be for others:**\r\n- Higher price point compared to basic mini PCs\r\n- May be overkill for simple file sharing or basic web hosting needs\r\n- Potentially higher power consumption than more basic models\r\n- AI capabilities may not be utilized in all home server scenarios\r\n\r\n<Button link=\"https://amzn.to/3ziR6Za\" text=\"Check IT\" />\r\n\r\n\r\n\r\n#### [ASUS NUC 14 Pro+ Barebone with Intel 14thGen Core Ultra 9 185H](https://amzn.to/3MI01q3)\r\n\r\n<Picture\r\n  src={img6}\r\n  alt=\"ASUS NUC 14 Pro+ Barebone with Intel 14thGen Core Ultra 9 185H\"\r\n/>\r\n\r\nThe ASUS NUC 14 Pro+ Barebone represents the pinnacle of mini PC performance. Equipped with the top-tier Intel 14th Gen Core Ultra 9 185H processor, this powerhouse is designed for users who demand server-grade performance in a compact form factor. The barebone configuration allows for extensive customization, supporting up to 96GB of DDR5 RAM and multiple M.2 SSDs. This flexibility makes it ideal for power users who need to tailor their system to specific high-performance home server applications. The inclusion of NPU (Neural Processing Unit) support adds significant AI processing capabilities, opening up possibilities for advanced home automation, machine learning projects, and future AI-driven applications. Despite its small size, this NUC can rival the performance of much larger desktop systems, making it suitable for demanding tasks like running multiple virtual machines, real-time 4K video transcoding, or serving as a powerful development environment.\r\n\r\n**Features:**\r\n- Intel Core Ultra 9 185H processor (16 cores, 22 threads)\r\n- Supports up to 96GB DDR5 RAM\r\n- Multiple M.2 SSD slots for expansive storage\r\n- Thunderbolt 4 connectivity\r\n- Wi-Fi 6E and Bluetooth 5.3\r\n- NPU support for AI-accelerated tasks\r\n- Extensive I/O options including multiple USB and display ports\r\n- Barebone configuration for maximum customization\r\n\r\n**Why it's good for home server:**\r\n- Extreme performance capable of handling multiple demanding server roles\r\n- Extensive RAM support for memory-intensive applications or multiple VMs\r\n- Flexible storage options allow for high-capacity, high-speed configurations\r\n- NPU support enables advanced AI and machine learning capabilities\r\n- Thunderbolt 4 provides fast connectivity for external devices or networked storage\r\n- Barebone setup allows for precise customization to specific needs\r\n\r\n**Why it might not be for others:**\r\n- High cost, especially when factoring in RAM and storage\r\n- Overkill for basic home server needs like file sharing or simple web hosting\r\n- Higher power consumption and heat generation compared to lower-end models\r\n- Requires more technical knowledge to set up due to barebone configuration\r\n- May be unnecessarily powerful for users not leveraging its full capabilities\r\n\r\n<Button link=\"https://amzn.to/3MI01q3\" text=\"Check IT\" />\r\n\r\n\r\n### AMD Mini PC\r\n\r\n#### [ACEMAGICIAN Dual LAN Mini Gaming PC AMD Ryzen 7 5800U](https://amzn.to/3MQ9M5v)\r\n\r\n<Picture\r\n  src={img7}\r\n  alt=\"ACEMAGICIAN Dual LAN Mini Gaming PC AMD Ryzen 7 5800U\"\r\n/>\r\n\r\n\r\nThe ACEMAGICIAN Dual LAN Mini Gaming PC, powered by the AMD Ryzen 7 5800U processor, offers a compelling blend of performance and efficiency in a compact form factor. This 8-core, 16-thread processor provides robust multitasking capabilities, making it well-suited for various home server applications. The mini PC's dual LAN ports are a standout feature, offering enhanced network performance and redundancy options crucial for a reliable home server setup. With 16GB of DDR4 RAM and a 512GB SSD, it provides ample resources for running multiple server applications simultaneously. While marketed as a gaming PC, its specifications make it equally adept at serving as a powerful home server. The combination of AMD's energy-efficient architecture and high core count allows this mini PC to handle demanding tasks like media transcoding, virtualization, and running multiple docker containers with ease.\r\n\r\n**Features:**\r\n- AMD Ryzen 7 5800U (8 cores, 16 threads)\r\n- 16GB DDR4 RAM\r\n- 512GB SSD\r\n- Dual Ethernet ports\r\n- Wi-Fi 6 and Bluetooth 5.2\r\n- Multiple USB ports including USB 3.2\r\n- HDMI and DisplayPort outputs\r\n- Compact form factor with gaming-inspired design\r\n\r\n**Why it's good for home server:**\r\n- Powerful multi-core processor suitable for various server applications\r\n- Dual LAN for improved network performance or redundancy\r\n- Energy-efficient AMD architecture for 24/7 operation\r\n- Capable of handling media transcoding and virtualization tasks\r\n- Compact size allows for flexible placement options\r\n- Good balance of performance and power consumption\r\n\r\n**Why it might not be for others:**\r\n- May be overkill for basic file sharing or simple web hosting needs\r\n- Gaming-oriented design might not blend well in all home environments\r\n- Potentially higher initial cost compared to entry-level mini PCs\r\n- Lack of ECC RAM support (common in consumer-grade systems) may be a concern for critical data applications\r\n\r\n<Button link=\"https://amzn.to/3MQ9M5v\" text=\"Check IT\" />\r\n\r\n\r\n\r\n#### [GEEKOM AE7 Mini PC, AMD Ryzen 9 7940HS Processor AI Mini Computer](https://amzn.to/47qJIaM)\r\n\r\n<Picture\r\n  src={img8}\r\n  alt=\"GEEKOM AE7 Mini PC, AMD Ryzen 9 7940HS Processor AI Mini Computer\"\r\n/>\r\n\r\n The GEEKOM AE7 Mini PC represents a significant leap in mini PC performance, featuring the high-end AMD Ryzen 9 7940HS processor. This powerhouse combines exceptional multi-core performance with integrated RDNA 3 graphics, making it suitable for a wide range of demanding home server applications. The \"AI\" designation hints at its capabilities in machine learning and AI-assisted tasks, thanks to AMD's advancements in this area. Supporting up to 64GB of DDR5 RAM and multiple M.2 SSDs, this mini PC can be configured to handle intensive workloads such as running multiple virtual machines, real-time 4K video transcoding, or serving as a robust development environment. Its Thunderbolt 4 connectivity adds versatility for high-speed external devices or networking. Despite its powerful components, the AE7 maintains a compact form factor, making it an excellent choice for users who need server-grade performance in a small package.\r\n\r\n**Features:**\r\n- AMD Ryzen 9 7940HS Processor (8 cores, 16 threads)\r\n- Supports up to 64GB DDR5 RAM\r\n- Multiple M.2 SSD slots for expansive storage\r\n- Thunderbolt 4 connectivity\r\n- Wi-Fi 6E and Bluetooth 5.2\r\n- RDNA 3 integrated graphics\r\n- Extensive I/O options including multiple USB and display ports\r\n- Compact design with efficient cooling system\r\n\r\n**Why it's good for home server:**\r\n- High-performance processor capable of handling multiple demanding server roles\r\n- Support for large amounts of fast DDR5 RAM, ideal for memory-intensive applications\r\n- Flexible storage options allow for high-capacity, high-speed configurations\r\n- Integrated RDNA 3 graphics can assist with GPU-accelerated tasks\r\n- Thunderbolt 4 provides fast connectivity for external devices or networked storage\r\n- Compact size despite powerful internals, suitable for space-constrained setups\r\n\r\n**Why it might not be for others:**\r\n- Higher cost compared to entry-level or mid-range mini PCs\r\n- May be unnecessarily powerful for basic home server needs\r\n- Potentially higher power consumption and heat output than lower-end models\r\n- Overkill for simple file sharing, basic web hosting, or lightweight NAS applications\r\n- The advanced features and performance may not be fully utilized by all users\r\n\r\n<Button link=\"https://amzn.to/47qJIaM\" text=\"Check IT\" />\r\n\r\n\r\nBoth these AMD-based mini PCs offer strong alternatives to Intel-based systems, providing excellent multi-core performance and energy efficiency. They are particularly well-suited for users who need to run multiple demanding applications simultaneously or require strong computational power for tasks like virtualization, media processing, or running complex home automation systems.\r\n\r\n\r\n### Best ARM Mini PC\r\n\r\n#### [Orange Pi 5 Plus 32GB with 256GB eMMC](https://amzn.to/4d31fqR)\r\n\r\n<Picture\r\n  src={img12}\r\n  alt=\"Orange Pi 5 Plus 32GB with 256GB eMMC\"\r\n/>\r\n\r\nThe Orange Pi 5 Plus represents a significant leap in ARM-based single-board computer performance. Powered by the Rockchip RK3588 octa-core processor, this board offers server-grade capabilities in a compact form factor. With 32GB of LPDDR4X RAM and 256GB of eMMC storage, it provides ample resources for running multiple server applications simultaneously. The dual Gigabit Ethernet ports make it an excellent choice for network-intensive tasks, while Wi-Fi 6 and Bluetooth support add versatility for various connectivity needs.\r\n\r\n**Features:**\r\n- Rockchip RK3588 octa-core 64-bit processor\r\n- 32GB LPDDR4X RAM\r\n- 256GB eMMC storage\r\n- Dual Gigabit Ethernet ports\r\n- Wi-Fi 6 and Bluetooth\r\n- Multiple USB ports including USB 3.0\r\n- Supports Orange Pi OS, Android, Debian, Ubuntu\r\n- Comes with case, power supply, and remote control\r\n\r\n**Why it's good for home server:**\r\n- High performance-to-power ratio for ARM-based systems\r\n- Extensive RAM and storage for demanding applications\r\n- Dual Ethernet ports for improved network capabilities\r\n- Flexible OS options for various server configurations\r\n- Great for development and experimentation\r\n\r\n**Why it might not be for others:**\r\n- May require more technical knowledge to set up compared to consumer-oriented mini PCs\r\n- Some x86-specific applications may not be compatible\r\n- Cooling may be necessary for intensive, continuous operation\r\n\r\n<Button link=\"https://amzn.to/4d31fqR\" text=\"Check IT\" />\r\n\r\n\r\n#### [CanaKit Raspberry Pi 5 Starter Kit PRO - Turbine Black](https://amzn.to/4gprJ8B)\r\n\r\n<Picture\r\n  src={img9}\r\n  alt=\"CanaKit Raspberry Pi 5 Starter Kit PRO - Turbine Black\"\r\n/>\r\n\r\nThe Raspberry Pi 5, available in this comprehensive CanaKit Starter Kit PRO, represents a significant leap forward in ARM-based single-board computers. This latest iteration boasts impressive performance improvements over its predecessors, making it a compelling option for home server applications. The Raspberry Pi 5 is powered by a custom Broadcom BCM2712 quad-core 64-bit ARM Cortex-A76 processor, delivering up to 2-3 times the performance of the Raspberry Pi 4. With 8GB of LPDDR4X RAM and a 128GB high-speed microSD card, this kit provides ample resources for various server tasks. The inclusion of dual-band Wi-Fi 6 and Bluetooth 5.0 enhances its networking capabilities, while the dual micro-HDMI ports support dual 4K displays. The Turbine Black case with integrated fan ensures proper cooling for sustained performance. This Raspberry Pi 5 kit strikes an excellent balance between power efficiency, performance, and cost-effectiveness for home server enthusiasts.\r\n\r\n**Features:**\r\n- Broadcom BCM2712 quad-core ARM Cortex-A76 processor (up to 2.4GHz)\r\n- 8GB LPDDR4X RAM\r\n- 128GB microSD card included\r\n- Dual-band Wi-Fi 6 and Bluetooth 5.0\r\n- Dual Gigabit Ethernet ports\r\n- 4 USB 3.0 ports\r\n- Dual micro-HDMI supporting 4K60\r\n- Turbine Black case with integrated fan for cooling\r\n\r\n**Why it's good for home server:**\r\n- Excellent performance-to-power ratio, ideal for 24/7 operation\r\n- Dual Gigabit Ethernet ports for improved network capabilities\r\n- Extensive community support and wide range of ARM-compatible server software\r\n- Low cost compared to x86 mini PCs with similar capabilities\r\n- Compact size allows for flexible placement options\r\n- Great platform for learning and experimenting with server technologies\r\n\r\n**Why it might not be for others:**\r\n- Limited CPU power compared to high-end x86 processors\r\n- Some x86-specific server applications may not be compatible\r\n- Performance may be insufficient for heavy virtualization or simultaneous 4K transcoding\r\n- Limited storage expansion options compared to larger mini PCs\r\n\r\n<Button link=\"https://amzn.to/4gprJ8B\" text=\"Check IT\" />\r\n\r\n#### [Apple 2023 Mac Mini Desktop Computer with Apple M2 chip with 8‑core CPU and 10‑core GPU](https://amzn.to/4grPgWL)\r\n\r\n<Picture\r\n  src={img10}\r\n  alt=\"Apple 2023 Mac Mini Desktop Computer with Apple M2 chip with 8‑core CPU and 10‑core GPU\"\r\n/>\r\n\r\nDescription: The 2023 Apple Mac Mini with the M2 chip represents a powerful and energy-efficient option in the ARM-based mini PC market. Leveraging Apple's custom silicon, this compact powerhouse offers exceptional performance and efficiency. The M2 chip features an 8-core CPU and a 10-core GPU, providing robust computing capabilities for various server tasks. While primarily designed as a desktop computer, its performance, silent operation, and energy efficiency make it an excellent candidate for a home server. The Mac Mini's macOS environment offers a user-friendly interface and a wide range of server applications, making it accessible for both beginners and experienced users. With support for up to two displays, including one at 6K resolution, it also provides flexibility for setup and monitoring. The inclusion of Wi-Fi 6E and Bluetooth 5.3 ensures fast wireless connectivity options.\r\n\r\n**Features:**\r\n- Apple M2 chip with 8-core CPU and 10-core GPU\r\n- 8GB unified memory (configurable up to 24GB)\r\n- 256GB SSD (configurable up to 2TB)\r\n- Wi-Fi 6E (802.11ax) and Bluetooth 5.3\r\n- 2x Thunderbolt 4 ports, 2x USB-A ports, HDMI port\r\n- Gigabit Ethernet (configurable to 10Gb Ethernet)\r\n- Supports up to two displays (one up to 6K and one up to 4K)\r\n- Compact design with efficient cooling\r\n\r\n**Why it's good for home server:**\r\n- Exceptional performance-to-power ratio, ideal for energy-efficient 24/7 operation\r\n- Silent operation due to efficient cooling system\r\n- macOS provides a user-friendly environment for server setup and management\r\n- Thunderbolt 4 ports offer high-speed connectivity for external storage or devices\r\n- Ability to run iOS/iPadOS apps, expanding software options\r\n- Time Machine support for easy backup solutions\r\n\r\n**Why it might not be for others:**\r\n- Higher cost compared to many x86 and ARM alternatives\r\n- Limited upgradability (RAM and storage are not user-upgradeable)\r\n- Some traditional server software may not be compatible with macOS or ARM architecture\r\n- Less flexibility in terms of operating system choices compared to generic x86 hardware\r\n- May be overkill for basic file sharing or simple web hosting needs\r\n\r\n<Button link=\"https://amzn.to/4grPgWL\" text=\"Check IT\" />\r\n\r\n\r\nBoth these ARM-based mini PCs offer unique advantages for home server setups. The Raspberry Pi 5 provides an affordable, highly customizable platform with a vast community and software ecosystem, ideal for learning and experimenting. The Apple Mac Mini with M2 chip, while more expensive, offers a powerful, energy-efficient solution with a user-friendly interface, making it suitable for those who prioritize ease of use and performance in their home server setup.\r\n\r\n\r\n\r\n## Best Use Cases\r\n\r\nMini PCs offer versatile solutions for home server applications. Here are some of the best use cases, categorized by the different types of mini PCs we've explored:\r\n\r\n| Mini PC Type | Ideal Use Cases |\r\n|--------------|-----------------|\r\n| Entry-Level x86 (N100) | Media streaming, Basic NAS, Home automation hub |\r\n| Mid-range to High-end x86 | Virtualization, Advanced media server, Game server |\r\n| ARM-based | Low-power 24/7 services, Educational projects, Lightweight containerization |\r\n\r\n\r\n### Entry-Level x86 (e.g., N100-based systems)\r\n\r\n1. Media Streaming Server:\r\n   - Host Plex, Jellyfin, or Emby for personal media libraries\r\n   - Suitable for 1-2 simultaneous 1080p streams\r\n\r\n2. Basic Network-Attached Storage (NAS):\r\n   - File sharing and backup solution for home networks\r\n   - Personal cloud storage alternative\r\n\r\n3. Home Automation Hub:\r\n   - Run software like Home Assistant or OpenHAB\r\n   - Integrate and control smart home devices\r\n\r\n4. Web Hosting:\r\n   - Host personal websites or small business sites\r\n   - Run content management systems like WordPress\r\n\r\n5. VPN Server:\r\n   - Set up a personal VPN for secure remote access to your home network\r\n\r\n### Mid-range to High-end x86 (e.g., Intel NUC, AMD Ryzen Mini PCs)\r\n\r\n1. Virtualization Server:\r\n   - Run multiple virtual machines for testing or development\r\n   - Host separate environments for different applications\r\n\r\n2. Advanced Media Server:\r\n   - Handle multiple 4K transcoding streams\r\n   - Run more resource-intensive media management software\r\n\r\n3. Game Server:\r\n   - Host multiplayer game servers for popular titles\r\n   - Support game streaming services like Steam Link\r\n\r\n4. Development Environment:\r\n   - Run integrated development environments (IDEs) and compile code\r\n   - Host Git repositories and CI/CD pipelines\r\n\r\n5. Database Server:\r\n   - Run SQL or NoSQL databases for personal or small business use\r\n\r\n### ARM-based (e.g., Raspberry Pi, Apple Mac Mini)\r\n\r\n1. Low-Power 24/7 Services:\r\n   - DNS server (e.g., Pi-hole for network-wide ad blocking)\r\n   - MQTT broker for IoT devices\r\n\r\n2. Educational Projects:\r\n   - Learn Linux system administration\r\n   - Experiment with server setup and management\r\n\r\n3. Retro Gaming Emulation:\r\n   - Host retro gaming platforms and stream to other devices\r\n\r\n4. Lightweight Containerization:\r\n   - Run Docker containers for various services\r\n   - Experiment with Kubernetes for container orchestration\r\n\r\n5. Media Center:\r\n   - Use as a Kodi or OSMC box connected directly to a TV\r\n\r\n\r\nYour N100-based mini PC with a 4TB SSD is particularly well-suited for media streaming and app hosting, as you've experienced. This setup can easily handle tasks like running a Plex server, hosting a few web applications, and serving as a file sharing hub for your home network. The large SSD provides ample space for media libraries and application data, making it an excellent all-in-one solution for many home server needs.\r\n\r\nRemember that while these use cases are categorized by hardware type, there's often overlap. The key is to match the hardware capabilities with your specific requirements and expected workload.\r\n\r\n## Conclusion\r\n\r\nThe world of mini PCs for home servers offers a diverse range of options, from budget-friendly ARM-based solutions like the Raspberry Pi to high-performance x86 powerhouses and energy-efficient ARM chips like Apple's M2. Each category caters to different needs, budgets, and use cases.\r\n\r\nAs someone who owns and operates an N100-based mini PC for streaming and app hosting with a 4TB SSD, you've experienced firsthand the capabilities of these affordable yet capable devices. Your setup demonstrates that even entry-level x86 mini PCs can handle significant home server tasks effectively. The N100's efficiency allows for 24/7 operation without excessive power consumption, while still providing enough performance for media streaming and hosting multiple applications.\r\n\r\nYour choice of a large 4TB SSD also highlights an important aspect of home server setups: storage capacity is often as crucial as processing power. This configuration allows for extensive media libraries and ample space for various applications and their data.\r\n\r\n**Key takeaways:**\r\n1. Entry-level mini PCs like those with the N100 can be sufficient for many home server needs, including media streaming and app hosting.\r\n2. Storage capacity is a critical factor in home server setups, often more so than raw processing power for many use cases.\r\n3. Energy efficiency is a significant advantage of newer mini PCs, allowing for cost-effective 24/7 operation.\r\n4. The right choice depends on specific needs: while an N100 works well for your streaming and app hosting, users with more demanding requirements might need to look at more powerful options like the Intel NUC or AMD-based systems.\r\n5. ARM-based solutions like the Raspberry Pi or Mac Mini offer unique advantages in terms of power efficiency and, in Apple's case, integration with their ecosystem.\r\n\r\nUltimately, your experience validates that one doesn't necessarily need high-end hardware to create a functional and efficient home server. The key is to match the hardware capabilities with your specific requirements. For many users, a setup similar to yours – an efficient x86 mini PC with ample storage – will provide an excellent balance of performance, capacity, and energy efficiency for their home server needs.\r\n\r\nAs technology evolves, we can expect even more powerful and efficient options in the mini PC space, further expanding the possibilities for home servers. However, your current setup demonstrates that today's entry-level hardware is already capable of meeting many home server demands effectively.\r\n\r\n\r\n## FAQs\r\n\r\n1. **Q: Can I use a mini PC as a 24/7 home server?**\r\n   A: Yes, many mini PCs are designed for continuous operation and are energy-efficient, making them suitable for 24/7 use.\r\n\r\n2. **Q: How much RAM do I need for a home server?**\r\n   A: For basic tasks, 8GB is sufficient. For more demanding applications or virtualization, consider 16GB or more.\r\n\r\n3. **Q: Are ARM-based mini PCs good for home servers?**\r\n   A: ARM-based systems like the Raspberry Pi are excellent for low-power, always-on servers and are particularly good for learning and experimentation.\r\n\r\n4. **Q: Can I upgrade the components in a mini PC?**\r\n   A: It depends on the model. Some mini PCs allow RAM and storage upgrades, while others have fixed configurations.\r\n\r\n5. **Q: Is it better to build a custom server or use a mini PC?**\r\n   A: Mini PCs offer a compact, energy-efficient, and often more cost-effective solution for home servers compared to custom builds, especially for basic to moderate server needs.","src/content/posts/best-mini-pc-home-server.mdx",[211],"../../assets/images/24/09/best-mini-pc-home-server.jpeg","e3ef39d84c622433","best-mini-pc-home-server.mdx","best-oh-my-zsh-plugins",{id:214,data:216,body:226,filePath:227,assetImports:228,digest:230,legacyId:231,deferredRender:32},{title:217,description:218,date:219,image:220,authors:221,categories:222,tags:223,canonical:225},"Top 15 Oh My ZSH Plugins You Must Try","Enhance your Oh My Zsh with this list of top 15 plugins.",["Date","2023-11-14T01:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/best-oh-my-zsh-plugins.jpeg",[19],[98],[224],"zsh","https://www.bitdoze.com/best-oh-my-zsh-plugins/","The best Oh My ZSH plugins are essential tools that enhance the functionality and productivity of your command-line interface. These plugins offer a wide range of features and customizations to make your terminal experience more efficient and enjoyable. Whether you're looking for auto-completion, syntax highlighting, or git integration, this article will guide you through some of the top-notch Oh My ZSH plugins available today. Discover how these powerful additions can streamline your workflow and take your command-line skills to the next level!\r\n\r\n## Oh My ZSH Installation and Setup\r\n\r\nTo install Oh My Zsh and start using its awesome plugins, follow these simple steps:\r\n\r\n1. **Install Oh My Zsh**:\r\n\r\n```shell\r\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\r\n```\r\n\r\n2. **Enable Oh My Zsh Plugins**:\r\n\r\nPlugins need to be installed so they can be enabled. Each plugin can be installed from the git with an easy command in the plugin directory off Oh My Zsh.\r\n\r\n- Oh My Zsh offers numerous plugins that enhance productivity and provide additional functionality.\r\n- Open your `.zshrc` file again and locate the line starting with plugins=().\r\n- Add plugin names within parentheses separated by spaces to enable them. For example:\r\n\r\n```shell\r\nplugins=(git npm node)\r\n```\r\n\r\n3. **Activate Changes**:\r\n\r\nAfter making any changes to your `.zshrc` file, save it, and then run this command for changes to take effect:\r\n\r\n```shell\r\nsource ~/.zshrc\r\n```\r\n\r\n## Essential Oh My Zsh Plugins\r\n\r\nWhen it comes to maximizing the power and functionality of your Oh My Zsh terminal, having the right plugins can make a world of difference. Here are some essential plugins that every user should consider:\r\n\r\n- **[zsh-autosuggestions](https://github.com/zsh-users/zsh-autosuggestions)**: This plugin suggests commands as you type based on your command history, making it easier and faster to complete common tasks. There is a detailed tutorial of how you can use it here: [How to Enable Command Autocomplete in ZSH](https://www.bitdoze.com/enable-command-autocomplete-in-zsh/)\r\n- **[zsh-syntax-highlighting](https://github.com/zsh-users/zsh-syntax-highlighting)**: With this plugin, syntax errors in your commands are highlighted in real-time, helping you catch and fix mistakes before running them. There is a detailed tutorial of how you can use it here: [How to Enable Syntax Highlighting in Zsh](https://www.bitdoze.com/enable-syntax-highlighting-zsh//)\r\n- **[history-substring-search](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/history-substring-search)**: This handy plugin allows you to search through your command history by typing partial strings from previous commands directly into the prompt. It saves time scrolling through long histories manually.\r\n- **git**: The git plugin provides a set of useful aliases and functions for working with Git repositories. It simplifies common Git operations and enhances productivity when managing version control.\r\n- **[docker](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/docker)**: This plugin enables auto-completion and aliases for docker commands, allowing you to manage docker containers, images, networks, and volumes with ease.\r\n- **[virtualenv](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/virtualenv)**: This plugin helps you manage Python virtual environments, which are isolated environments that contain specific versions of Python and other packages. It automatically activates and deactivates virtual environments as you navigate through different directories.\r\n- **[web-search](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/web-search)**: This plugin allows you to search the web from the terminal using various search engines, such as Google, Bing, DuckDuckGo, and Wikipedia. You can use the web-search command or the ws alias to launch a web search in your default browser.\r\n- **[extract](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/extract)**: Tired of manually extracting archives? The extract plugin allows you to extract compressed files easily with just one command—no need for remembering complex extraction commands!\r\n- **[docker-compose](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/docker-compose)**: If you work with Docker Compose regularly, this handy plugin provides autocompletion for Docker Compose commands, saving time and reducing errors when managing containers.\r\n- **[z](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/z)**: The z plugin allows you to quickly navigate between directories without specifying their full paths. It learns your most frequently used directories and intelligently guesses where you want to go.\r\n- **[npm](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/npm)**: This plugin provides auto-completion and aliases for npm commands, which are used to manage Node.js packages and projects. It also shows the current npm version and the name of the package in the prompt.\r\n- **[kubectx](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/kubectx)**: This plugin helps you switch between different Kubernetes contexts and namespaces, which are used to access different clusters and resources. It also provides auto-completion and aliases for kubectl commands, which are used to interact with Kubernetes.\r\n- **[fzf](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/fzf)**: This plugin integrates the fzf command-line fuzzy finder with Zsh, allowing you to search and select files, directories, commands, processes, and more using fuzzy matching. It also adds key bindings for various fzf features, such as Ctrl+T to insert files, Ctrl+R to search history, and Alt+C to change directories.\r\n- **[thefuck](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/thefuck)**: This plugin corrects your previous command when you mistype it. It uses the thefuck command-line tool, which suggests possible corrections based on the error message. You can use the fuck command or the Ctrl+G key binding to apply the correction.\r\n- **[colored-man-pages](https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/colored-man-pages)**: This plugin adds colors to the man pages, which are the documentation for various commands and programs. It makes the man pages more readable and appealing.\r\n\r\n> Remember that you can always explore more plugins from [Oh My ZSH GitHub repository](https://github.com/ohmyzsh/ohmyzsh/wiki/Plugins) or create custom ones tailored to your specific needs. Happy Zsh-ing!\r\n\r\n## Conclusion\r\n\r\nIn conclusion, Oh My Zsh plugins are an essential element of maximizing the functionality and customization options of the popular command-line framework. With a wide range of plugins available, users can enhance their productivity, streamline their workflow, and personalize their experience to suit their specific needs.\r\n\r\nBy leveraging the power of Oh My Zsh plugins, users can extend the capabilities of the default shell and unlock advanced features that boost efficiency. Whether it's auto-completion with zsh-autosuggestions, syntax highlighting with zsh-syntax-highlighting, or version control integration with git plugin - there is something for everyone. These plugins not only simplify complex tasks but also provide a seamless user experience.\r\n\r\nIn summary, by exploring and incorporating various Oh My Zsh plugins into your workflow, you have the opportunity to transform your terminal into a versatile tool that caters to your unique requirements. So go ahead, dive in, experiment with different combinations of plugins, and take full advantage of what Oh My Zsh has to offer!","src/content/posts/best-oh-my-zsh-plugins.mdx",[229],"../../assets/images/23/11/best-oh-my-zsh-plugins.jpeg","bc655550b5989668","best-oh-my-zsh-plugins.mdx","best-self-hosted-panels",{id:232,data:234,body:244,filePath:245,assetImports:246,digest:248,legacyId:249,deferredRender:32},{title:235,description:236,date:237,image:238,authors:239,categories:240,tags:241,canonical:243},"Self-Hosted Server Panels: A Comparison of the Best Options Available","Check out this best self-hosted server panel alternatives to host your project and keep control. ",["Date","2023-11-29T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/best-self-hosted-panels.jpeg",[19],[98],[242],"self-hosted","https://www.bitdoze.com/best-self-hosted-panels/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/11/cloudpanel-interface.png\";\r\nimport imag2 from \"../../assets/images/23/11/cyberpanel-features-hero.png\";\r\nimport imag3 from \"../../assets/images/23/11/easypanel-interface.png\";\r\nimport imag4 from \"../../assets/images/23/11/coolify-v4-ui.png\";\r\nimport imag5 from \"../../assets/images/23/11/caprover-ui.png\";\r\n\r\nManaging server environments can be a complex task, but with the right self-hosted server panel, it's a breeze. I've explored a variety of panels that promise to simplify this process, and I'm excited to share the cream of the crop with you.\r\n\r\nWhether you're a developer, system admin, or a tech enthusiast, you'll want a server panel that's robust, user-friendly, and reliable. In this article, I'll walk you through the best self-hosted server panels that'll help you host your projects efficiently and with greater control.\r\n\r\n## Self-Hosted Panels and How They Can Help You\r\n\r\nDiscovering the right tools for managing server environments can be akin to finding a needle in a haystack. That's where **self-hosted server panels** come into play, and I've found they're an absolute game-changer for efficiency and control. These panels simplify the complex task of managing servers, and I'm here to share how they can revolutionize your workflow.\r\n\r\n**What's a Self-Hosted Server Panel?** In essence, a self-hosted server panel is a dashboard that lets you oversee all components of your server without the need for command-line expertise. Imagine having the power to control databases, domains, email accounts, and more, all from one central user interface.\r\n\r\n**Streamline Server Tasks** With a self-hosted server panel, you'll have the tools to:\r\n\r\n- Configure and manage websites effortlessly\r\n- Create databases with just a few clicks\r\n- Monitor server health and resources in real-time\r\n- Automate backups for peace of mind\r\n- Deploy applications with just one click\r\n- Host CI/CD projects for automatic deploy\r\n\r\nGone are the days of toggling between various systems to manage your servers. These panels consolidate the essential tools, making server management a breeze.\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n**Enhanced Security and Control** One standout advantage of self-hosted panels is the high level of security they offer. Hosting your own panel means you're not sharing resources with other users, which minimizes potential vulnerabilities. Moreover, you have the autonomy to implement your own security measures that align with your project's requirements.\r\n\r\nMy experience has shown me that self-hosted server panels are invaluable for anyone looking to steer their projects with precision and confidence. They're designed to cater to both novices and professionals, enabling you to focus on innovation rather than intricate server management details.\r\n\r\nAs we delve further, I'll unveil the top self-hosted server panels that have impressed me with their robust features, user-friendly interfaces, and steadfast reliability.\r\n\r\n## What features should be considered while choosing a self-hosted server panel?\r\n\r\n- When choosing a self-hosted server panel, there are several important features to consider. One of the key factors is the user interface. A user-friendly and intuitive interface can greatly simplify the management of your server and make it easier to navigate through various settings and options. Look for a panel that offers a clean and organized interface, with clear labels and easy-to-understand controls.\r\n\r\n- Security is also a top priority when it comes to self-hosted server panels. Make sure the panel you choose has robust security measures in place to protect your server and data. Look for features such as two-factor authentication, SSL/TLS support, and regular security updates. Additionally, consider whether the panel allows you to easily manage firewall rules and access controls to further enhance the security of your server.\r\n\r\n- Scalability is another important consideration. As your projects grow and your server requirements increase, you'll want a panel that can scale with your needs. Look for a server panel that supports easy scaling of resources, such as adding additional servers or increasing the capacity of existing ones. This will ensure that your projects can continue to run smoothly as they expand.\r\n\r\n- Another crucial feature to consider is the level of customization and flexibility offered by the server panel. Different projects may have unique requirements, so it's important to choose a panel that allows you to tailor the server settings to your specific needs. Look for options to configure security settings, manage user accounts, and customize server resources such as CPU, RAM, and storage.\r\n\r\n## Best Self Hosted Panels For PHP Applications (cPanel Alternatives)\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/MijpWPVRgqA\"\r\n  label=\"Self-Hosted Server Panels\"\r\n/>\r\n\r\nManaging PHP applications can be a breeze with the right server panel. I've put several tools through their paces and among them, two panels stood out not just for their PHP support but for their comprehensive functionality as well. Let's dive into the specifics of each.\r\n\r\n### CloudPanel\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"CloudPanel Interface\"\r\n/>\r\n\r\n[CloudPanel](https://www.cloudpanel.io/) is one of my top picks when it comes to self-hosted server panels specifically designed for PHP applications, as a web server it uses NGINX.\r\n\r\nCloudPanel it is using the below tech stack under the hood:\r\n\r\n- NGINX as the web server\r\n- MySQL, MariaDB as the relational database\r\n- Redis as the in-memory data store and cache\r\n- Varnish Cache as the HTTP accelerator\r\n\r\nIt's not just an ordinary control panel; it's a powerhouse that provides an incredible set of features tailored for a smooth PHP development experience.\r\n\r\n- **One-Click Installations**: CloudPanel offers one-click installations for popular PHP applications. This feature streamlines the deployment process and significantly reduces setup time.\r\n- **Free SSL Certificates**: Security is effortless with CloudPanel's built-in support for Let's Encrypt, providing free SSL certificates for your domains.\r\n- **PHP Management**: Fine-tune PHP settings for each site directly from the panel. The ability to adjust memory limits, upload sizes, and execution times on-the-fly is invaluable. It offers multiple PHP versions.\r\n- **Intuitive Interface**: The user interface is clean and efficient, making it easy to navigate and manage your servers even if you're not deeply technical.\r\n- **File Management**: It has a file manager that allows users to upload, edit, delete, and manage files and folders\r\n- It has advanced security features, such as site isolation, firewall, IP and bot blocking, basic authentication, and two-factor authentication\r\n- It supports multiple application types, such as PHP, Node.js, Python, and static websites or it can be used as a reverse proxy.\r\n- **Backups**: It has the options to backup your websites and databases to external locations with Rclone\r\n\r\nThe responsiveness of CloudPanel's interface and the minimal resource footprint make it an ideal choice for developers who want to maintain efficiency without sacrificing functionality. I am using it as a cPanel alternative to host all my\r\nWordPress Websites.\r\n\r\nBelow are some tutorials that can get you started:\r\n\r\n- [How To Install CloudPanel and Host Node.js Apps](https://www.bitdoze.com/install-cloudpanel-host-nodejs/)\r\n- [CloudPanel Install on Hetzner With WordPress Setup](https://www.wpdoze.com/cloudpanel-install-on-hetzner/)\r\n- [How To Secure CloudPanel And Have a Better Sleep](https://www.wpdoze.com/secure-cloudpanel/)\r\n- [How to Safely Update CloudPanel to The Last Version](https://www.wpdoze.com/safely-update-cloudpanel/)\r\n- [Setup CloudPanel with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n\r\n> I have created also a course that can get you started, this is how much I like CloudPanel: [Master CloudPanel Course](https://webdoze.net/courses/cloudpanel-setup/)\r\n\r\nCloudPanel does not come with an email system so you can create email accounts on it, just wanted to let you know.\r\n\r\n### CyberPanel\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"CyberPanel Interface\"\r\n/>\r\n\r\nOn the other hand, [CyberPanel](https://cyberpanel.net/) stands out with its integration of LiteSpeed technology. For developers and businesses looking for performance, CyberPanel delivers that in spades, thanks to its use of LiteSpeed Web Server.\r\n\r\nCyberPanel it is using the below tech stack under the hood:\r\n\r\n- LiteSpeed as the web server\r\n- MariaDB as the relational database\r\n- Redis as the in-memory data store and cache\r\n- Email Setup\r\n- Docker Manager\r\n\r\nTop Features of CyberPanel:\r\n\r\n- **LiteSpeed Server**: With CyberPanel's LiteSpeed support, PHP applications benefit from unparalleled performance and reduced server load, ensuring your applications run at their best.\r\n- **Auto Backups**: Your data's safety won't keep you up at night—CyberPanel features automatic backups to remote destinations, so you're always a step ahead of any potential data loss.\r\n- **Built-in Firewall**: Security is a breeze with the included firewall, which helps in thwarting potential security threats before they hit your PHP applications.\r\n- **File Manager**: File Manager to manage your files\r\n- **Docker Manager**: CyberPanel comes with a Docker Manager, scaling up your PHP applications becomes as simple as clicking a button.\r\n- **Email Server**: CyberPanel allows you to host a full email server\r\n- **DNS**: It comes with a DNS manager for your domain\r\n- **User Management**: It has advanced package manager and user managemt as you would see in Cpanel for instance.\r\n\r\n### Final Word on PHP Apps Panels\r\n\r\nIdentifying the ideal self-hosted server panel for PHP applications depends on your specific needs. However, with CloudPanel's comprehensive feature set and CyberPanel's LiteSpeed technology, both offer compelling reasons to consider them for hosting and managing your PHP projects. Each brings unique advantages to the table, ensuring your PHP applications run smoothly and securely.\r\n\r\nDeciding which panel fits your workflow best could depend on whether you value the one-click simplicity and clean interface of CloudPanel or the performance-optimized LiteSpeed integration that CyberPanel boasts. Consider the size of your projects, the scalability you need, and how critical performance is for your applications before making the final call.\r\n\r\n## Best Heroku & Netlify Alternatives for CI/CD and Docker Apps\r\n\r\nWhile Heroku and Netlify are popular choices for CI/CD and Docker applications deployment, they are not the only options out there. Numerous self-hosted platforms are providing similar functionalities, but with added benefits such as cost-effectiveness, customizability, and complete control over your data. In this section we are going to see some of the best panels that you can use for your apps. I have used the first 2 so I can say they are some of the best.\r\n\r\n### EasyPanel\r\n\r\n<Picture\r\n  src={imag3}\r\n  alt=\"EasyPanel Interface\"\r\n/>\r\n\r\n[EasyPanel](https://easypanel.io/) is a web-based server control panel that allows you to deploy and manage applications, databases, and SSL certificates with ease. It supports various programming languages and frameworks, and uses Docker and Cloud Native Buildpacks to create and run containers. It also provides web consoles, logs, templates, backups, and more features to help you manage your server without fighting the terminal. Easypanel.io is simple, powerful, and reliable. It has received positive feedback from many users who appreciate its flexibility and convenience.\r\n\r\nHere's why I find it worth considering:\r\n\r\n- **Effortless Setup**: EasyPanel is designed for a quick and hassle-free installation process. You won't waste time on tedious configurations.\r\n- **User-Friendly Interface**: The dashboard is straightforward, making it easy to manage deployments and visualize your projects.\r\n- **CI/CD Automation**: With EasyPanel, setting up automated pipelines for your applications is a breeze, which helps improve development efficiency.\r\n- **Docker Integration**: One of the strongest features is its native support for Docker, providing the flexibility to containerize your applications with ease.\r\n- **Integrates with Github:** You can push your code to Github and Easypanel.io will automatically build and deploy it for you.\r\n- **Offers easy templates:** You can use ready-made templates to deploy popular applications such as n8n, plausible anatitics, SuiteCRM, etc. in seconds.\r\n- **Easy Database Create**: You can create databases with ease like: MySQL, MariaDB, Postgres, MongoDB Redis\r\n- **Logs**: It's providing logs in the interface\r\n- **Backups**\r\n\r\nEasyPanel is a great panel that can help you host your project easily, below are some of the articles I have created:\r\n\r\n- [Easypanel.io: A Modern Hosting Panel for Applications and Databases](https://www.bitdoze.com/easypanel-modern-server-control-panel/)\r\n- [How to Deploy Astro on Your VPS with EasyPanel](https://www.bitdoze.com/deploy-astro-easypanel/)\r\n- [How to Launch Your Own Newsletter Platform with Keila and Docker](https://www.bitdoze.com/keila-setup/)\r\n\r\n### Coolify\r\n\r\n<Picture\r\n  src={imag4}\r\n  alt=\"Coolify Interface\"\r\n/>\r\n\r\n[Coolify](https://coolify.io/) is an open-source and self-hosted Heroku alternative that offers users the same features as Heroku, such as automated deployment, scalability, and easy configuration, but with the added benefit of being self-hosted.\r\n\r\nCoolify goes through a transformation now as version 4 is released where everything is redisigned.\r\n\r\n- It allows users to deploy their Node.js and static sites just by pushing code to git.\r\n- It supports one-click deployments of various databases, such as MongoDB, MySQL, PostgreSQL, and Redis.\r\n- It has a hassle-free installation process that installs the all-in-one infrastructure with one command.\r\n- It has a hassle-free upgrade process that updates the system with one click.\r\n- It automatically backs up the databases to any S3 compatible solution.\r\n- It automatically monitors the configured servers and deployed resources and notifies the users if something goes wrong.\r\n\r\nCoolify is offering similar functionalities like easypanel but it has fewer apps to choose from now(you can deploy easily your own apps with docker-compose) and no docker UI stats and command executions.\r\n\r\nBelow are some articles I have done in the past for Coolify v3:\r\n\r\n- [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/)\r\n- [How To Deploy Uptime Kuma With One Click](https://www.bitdoze.com/deploy-uptime-kuma/)\r\n- [How To Install Plausible With One Click](https://www.bitdoze.com/install-plausible-analytics/)\r\n\r\n### CapRover\r\n\r\n<Picture\r\n  src={imag5}\r\n  alt=\"CapRover Interface\"\r\n/>\r\n\r\n[CapRover](https://caprover.com/) is a platform that lets you deploy and manage your web applications and databases with ease and convenience. It is a free and open source PaaS (Platform as a Service) that uses Docker, nginx, Let’s Encrypt, and NetData under the hood to provide a fast and robust service.\r\nYou can use CapRover to deploy apps in any language, such as Node.js, Python, PHP, Ruby, and more, and also install databases like MySQL, MongoDB, Postgres, and more with just a few clicks.\r\nYou can also use CapRover to secure your apps with free SSL certificates, integrate with Cloudflare, customize your nginx settings, monitor your server performance, and scale your apps with Docker Swarm.\r\nYou can use CapRover’s web GUI, CLI, or webhooks to manage your apps and databases. CapRover is a great solution for developers who want to save time and money on deploying and hosting their web projects.\r\n\r\nCapRover Features:\r\n\r\n- It lets you deploy apps and databases of any language that can be packaged as a Docker image, such as Node.js, Python, PHP, Ruby, and more\r\n- It secures your services over HTTPS for free with Let’s Encrypt SSL certificates and automatically redirects HTTP to HTTPS\r\n- It offers one-click apps, such as WordPress, MongoDB, MySQL, Postgres, and more, that can be deployed in seconds\r\n- It supports multiple ways to deploy apps, such as uploading your source from the dashboard, using the command line caprover deploy, or using webhooks and building upon git push\r\n- It has a user-friendly interface with a dashboard that shows server information and resource usage, and a file manager that allows you to upload, edit, delete, and manage files and folders\r\n- It has advanced security features, such as firewall, IP and bot blocking, basic authentication, two-factor authentication\r\n\r\n## Conclusions\r\n\r\nTo sum up, choosing a server panel to host your programming projects doesn't have to be a daunting task, especially with the array of top-tier self-hosted server panels available today. Each panel has its distinct features and benefits, making it easier for you to find one tailored to meet your specific needs.\r\n\r\nQuality server panels like CloudPanel and CyberPanel have set a high bar with features like resource-efficient usage, auto SSL, and high-performance stacks. They are particularly great for running PHP applications, offering unmatched control over your hosting environment.\r\n\r\nIf you are on the lookout for Heroku or Netlify alternatives for CI/CD and Docker apps, then server panels like EasyPanel, Coolify, and CapRover have got you covered. Blocks or circular text indicating certain aspects like one-click apps installation, Docker swarm as a service, free SSL, and WebSocket support make these panels an excellent choice.","src/content/posts/best-self-hosted-panels.mdx",[247],"../../assets/images/23/11/best-self-hosted-panels.jpeg","e06d10b18b92588e","best-self-hosted-panels.mdx","block-ai-crawlers",{id:250,data:252,body:261,filePath:262,assetImports:263,digest:265,legacyId:266,deferredRender:32},{title:253,description:254,date:255,image:256,authors:257,categories:258,tags:259,canonical:260},"Stop AI Crawler Bots: How to Safeguard Your Website","Let's see the various alternatives that exist to block all AI Crawler bots that try to access your website content.",["Date","2024-02-09T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/block-ai-crawlers.jpeg",[19],[21],[79],"https://www.bitdoze.com/block-ai-crawlers/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/02/cf-proxy.jpeg\";\r\nimport imag2 from \"../../assets/images/24/02/cf-waf-bloc-ai.jpeg\";\r\nimport imag3 from \"../../assets/images/24/02/cf-waf-ip-block.png\";\r\n\r\nIf you are a content writer you may not be happy with the fact that AI is using your content and you may want to block them. Lately, AI has taken off and there are a lot of AI-powered options that will try to use your content.\r\n\r\nAlso, the development of AI-powered search responses started to kick off and you have tools like Perplexity, Microsoft Copilot, Gemini and ARC Browser in the future that will respond to your questions directly by crawling the best content and providing answers.\r\n\r\nThis creates a problem for blog owners as the traffic is not sent to your site anymore, in this way you lose audience and money.\r\n\r\n> I am not saying that what websites have become now is good as most of them have 1000 ads, but still, some respect the visitors and are not bombarding you with all sorts of ads. Plus you have an option to use an add blocker.\r\n\r\nBut with AI there is a risk that no one will write as they will not be incentivized in any way, the visitor will not subscribe to his social media, not join the newsletter or will not make the money anymore to pay his mortgage.\r\n\r\nIn this article, we are going to see the various ways that exist and will help you block AI Crawler but let Search Engine Crawlers still index your content.\r\n\r\n## How to Block AI Crawler Bots To Access Website Content\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/4s5I5Bz-IDE\"\r\n  label=\"Stop AI Crawler Bots: How to Safeguard Your Website\"\r\n/>\r\n\r\nThere are a few ways to protect your content from **AI Crawlers** and in this article, we will go through them and see how you can set them app.\r\n\r\n### 1. Block AI Crawler Bots via WAF\r\n\r\nWAF (Web Application Firewall) are gateways that stand between the visitor (AI Crawler) and your website, this is the best option that you can use to block AI Crawlers to reach your content.\r\n\r\nOne free option that can be used in CloudFlare Free WAF, they have introduced a **Verified Bot Category** that will help you block various things like:\r\n\r\n- Search Engine Crawler\r\n- Aggregator\r\n- AI Crawler\r\n- Page Preview\r\n- Advertising\r\n- Academic Research\r\n- Accessibility\r\n- Feed Fetcher\r\n- Security\r\n- Webhooks\r\n\r\n[Clauflare WAF Bot Category](https://blog.cloudflare.com/ai-bots) can be checked for more details.\r\n\r\nTo use CloudFlare Free WAF to block AI Crawlers you need to:\r\n\r\n**1. Website to use CloudFlare.**\r\n\r\nYou need to use CloudFlare as a DNS provider and have your website go through them.\r\n\r\n**2. CloudFlare Proxy needs to be activated**\r\n\r\nThe website needs to be proxied through CloudFlare otherwise will not work. So just as in below picture:\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"Cloudflare Proxy\"\r\n/>\r\n\r\n**3. Create the CloudFlare WAF Rule**\r\n\r\nIn the Free WAF you have 5 rules that can be used and you can create 1 just for this. To activate it go to **Security - WAF** and hit create a new Rule. In there you need to set:\r\n\r\n- **Rule name** - add a name so you know what is used for\r\n- **Field** - choose Verified Bot Category\r\n- **Operator** - equals\r\n- **Value** - AI Crawler\r\n- **Choose action** - Block\r\n- **Deploy** - hit deploy\r\n\r\nThe Expression Preview should be:\r\n\r\n```\r\n(cf.verified_bot_category eq \"AI Crawler\")\r\n```\r\n\r\nJust as in the image:\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"Cloudflare WAF Block AI Crawler\"\r\n/>\r\n\r\nThen let this for a couple of minutes to be active.\r\n\r\nNow you should be set to go, you can track the stats in the main WAF section and see how many were stopped after some time.\r\n\r\n### 2. Block AI Crawler Bots via robots.txt\r\n\r\n`robots.txt` is a file where you can specify what is allowed and what is not allowed to be fetched by crawlers. All the bots should check this file and see if they have access to fetch your content ( in theory). The major companies are doing this as otherwise they can have legal problems and they can be sued but don't expect everyone to do this.\r\n\r\nBecause of the above, this is not a bulletproof solution to block AI Crawler and should be used in combination with a WAF, plus you will need to add the AI agents as they appear to catch them all.\r\n\r\n**Block AI Crawlers with `robots.txt`**\r\n\r\nThe sintax for this is as bellow:\r\n\r\n```\r\nuser-agent: {AI-Ccrawlers-Bot-Name-Here}\r\ndisallow: /\r\n```\r\n\r\nYou just need to replace the `AI-Ccrawlers-Bot-Name-Here` with the AI bot that you want to be blocked.\r\n\r\nAn example for the OpenAI is:\r\n\r\n```\r\nUser-agent: GPTBot\r\nDisallow: /\r\nUser-agent: ChatGPT-User\r\nDisallow: /\r\n```\r\n\r\nThis will block all the pages on your website, you can alter the `Disallow` , you can add specific pages and so on. To allow someting you just need to use `Allow` instead of `Disallow`\r\n\r\n```\r\n\r\nUser-agent: GPTBot\r\nAllow: /\r\nUser-agent: ChatGPT-User\r\nAllow: /\r\n```\r\n\r\n### 3. Block AI Crawler Bots via IP\r\n\r\nAnother way to block AI Crawlers is to block the IP address of the bot in the firewall. When communication is happening between AI bot and your webserver it will use an IP, you have the option to block the IP address of the bots in the firewall.\r\n\r\nIf you are self-hosting your website on a VPS on linux you have a firewall utility that can be used to block the IP's.\r\n\r\n**ufw**\r\n\r\n`ufw` is a utility that can help you with this. For instance if we are going to check the [openAI IPs](https://platform.openai.com/docs/plugins/bot) and block then you should run:\r\n\r\n```sh\r\nsudo ufw deny proto tcp from 23.98.142.176/28 to any port 80\r\nsudo ufw deny proto tcp from 23.98.142.176/28 to any port 443\r\nsudo ufw deny proto tcp from 40.84.180.224/28 to any port 80\r\nsudo ufw deny proto tcp from 40.84.180.224/28 to any port 443\r\nsudo ufw deny proto tcp from 40.84.180.224/28 to any port 80\r\nsudo ufw deny proto tcp from 13.65.240.240/28 to any port 443\r\n```\r\n\r\n> This method implies that you should know the IP addresses of all the AI bots so you can add them here. Also, they can change over time but in case you are seeing a lot of traffic you can check the logs and see the exact IP.\r\n\r\n**cPanel Firewall**\r\n\r\nIf you are using cPanel or another hosting panel you should have a firewall option in there that can be used and block the IPs, so you are covered in case cPanel is used. Again you should know the IP's\r\n\r\n**CloudFlare WAF IP Access Rules**\r\n\r\nIf you are on CloudFlare they got you covered, in **Security - WAF** section under **Tools** you have the option to block or allow certan IP's or ranges, all for free.\r\n\r\n<Picture\r\n  src={imag3}\r\n  alt=\"Cloudflare WAF IP Block AI Crawler\"\r\n/>\r\n\r\n### 4. Block AI Crawler via Content Tags\r\n\r\nAnother option that can be used is to add a tag to your page directly that `should` prevent AI from crawling your pages. This is the same as for `robots.txt` the Bot should check it and not do anything if this is set.\r\n\r\nIt should be added in the head section of your website or page that you don't want to be Crawl by AI:\r\n\r\n```html\r\n<meta name=\"robots\" content=\"noai, noimageai\" />\r\n```\r\n\r\nThe `noai` directive is intended to prevent all content on the page from being used by AI, while the `noimageai` directive is specifically aimed at preventing images from being used for AI training purposes.\r\n\r\nIf you are on WordPress you can check your SEO plugin to see if this is enabled and if so you would be able to block the complete website or only pages. A WordPress plugin for WordPress that can do that in case your SEO plugin doesn't offer this is: [Simple NoAI and NoImageAI](https://wordpress.org/plugins/simple-noai-and-noimageai/)\r\n\r\n## Conclusions\r\n\r\nThese are some of the ways that you can block AI Crawler Bots from using your content if you have a serious issue with AI using your content then you should use all to be protected, other then this you can check your access logs and see there who slips thru the cracks.","src/content/posts/block-ai-crawlers.mdx",[264],"../../assets/images/24/02/block-ai-crawlers.jpeg","ad3955bb18608544","block-ai-crawlers.mdx","build-one-page-website-budget",{id:267,data:269,body:278,filePath:279,assetImports:280,digest:282,legacyId:283,deferredRender:32},{title:270,description:271,date:272,image:273,authors:274,categories:275,tags:276,canonical:277},"How to Build a One Page Website on a Budget","Learn how to create and host a one page website on a budget using Astro, static WordPress, or Carrd. These options are fast, simple, and cost-effective.",["Date","2023-11-22T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/build-one-page-budget.jpeg",[19],[21],[24],"https://www.bitdoze.com/build-one-page-website-budget/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nOne page websites are a great way to showcase your portfolio, business, or personal brand online. They are easy to navigate, mobile-friendly, and can capture the attention of your visitors with minimal scrolling.\r\n\r\nHowever, creating and hosting a one page website can be expensive and complicated if you don’t know where to start. You might think that you need to hire a web developer, buy a domain name, and pay for a hosting service to get your website online.\r\n\r\nBut what if we told you that there are 3 easy and affordable options to host a one page website on a budget? You don’t need any coding skills, and you can get your website up and running in minutes.\r\n\r\nIn this article, we will show you how to use Astro, static WordPress, or Carrd to create and host a one page website on a budget. These options are fast, simple, and cost-effective, and they offer various features and benefits to suit your needs.\r\n\r\nWhether you want to create a landing page, a portfolio, a resume, or a personal website, you can find the best option for you in this article. Let’s dive in!\r\n\r\n## How to Build a One Page Website on a Budget\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/HpGWpBefkOE\"\r\n  label=\"How to Build a One Page Website on a Budget\"\r\n/>\r\n\r\n### Astro js\r\n\r\nAstro is a web framework that lets you build fast and scalable websites with your favorite UI library. Astro can render your pages to static HTML for better performance, SEO, and accessibility. You can also use Astro to create dynamic web applications and APIs with any backend service. Astro supports multiple UI frameworks, such as React, Svelte, Vue, and more.\r\n\r\nAstro has a couple of templates that can help you build the website but you need to have some HTML or CSS knowledge to modify them. Astro is fast and can help you have optimized websites hosted for free on various services like CloudFlare Pages, Vercel or Netlify.\r\n\r\nBelow are some tutorials that will get you started:\r\n\r\n- [Best Astro.js Online Courses/Tutorials](https://www.bitdoze.com/best-astrojs-online-courses/)\r\n- [How To Deploy An Astro.JS Blog On Cloudflare](https://www.bitdoze.com/deploy-astrojs-cloudflare/)\r\n- [Link GitHub with A SSH Key to MacOS or Linux](https://www.bitdoze.com/link-github-with-ssh-maco-linux/)\r\n\r\n### Static WordPress Website\r\n\r\nWordPress is one of the most popular and versatile platforms for creating websites. However, WordPress websites can also be slow, insecure, and expensive to host, especially if you don’t need a lot of dynamic features or content.\r\n\r\nThat’s why some people prefer to use static WordPress websites, which are WordPress websites that are converted into static HTML files and hosted on a server that only serves static files. Static WordPress websites are faster, more secure, and cheaper to host than regular WordPress websites, and they are ideal for one-page websites that don’t require a lot of updates or interactions.\r\n\r\nYou can create and host a static WordPress website on a budget using Kinsta Static, a hosting service that specializes in static WordPress websites. You can use free themes or build your own layout with the help of free WordPress resources and host them on Static website platforms like Kinsta Static, CloudFlare Pages, Vercel or Netlify.\r\n\r\nBelow are some articles with videos that can help you get started having a static WordPress website:\r\n\r\n- [Free WordPress Static Site on Kinsta](https://www.wpdoze.com/deploy-wp-static-website-kinsta-static/)\r\n- [Breakdance + Kinsta = The Ultimate Combo for Creating Fast and Secure Static Sites in WordPress](https://www.wpdoze.com/breakdance-kinsta-static-site/)\r\n\r\n### Carrd.co\r\n\r\n[Carrd](https://try.carrd.co/bitdoze) is a website-building platform that specializes in creating and hosting one-page websites. You don’t need any coding skills, and you can get your website online in minutes. Carrd offers various templates, elements, and integrations that you can customize to suit your needs and style.\r\n\r\nWhether you want to create a landing page, a portfolio, a resume, or a personal website, you can find the best option for you on Carrd. If you want to use a custom domain you can get the Pro plan that start at $19 a year for 10 websites.\r\n\r\nBelow are some articles that can get you started with Carrd.co:\r\n\r\n- [Carrd.co Review: The Best Budget Landing Page Builder](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add a Sticky Header to Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Upgrade Your Carrd.co Website With A Cookie Notice in Minutes](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n\r\nOn [carrdme.com](https://carrdme.com/) you can find some free resources that will get you started.\r\n\r\n## Conclusions\r\n\r\nThis are the best 3 ways that I have used that will help you build a small website or an one page one on a budget side, if you don't know any HTML or CSS I recommend using the Static WordPress or Carrd, but if you have some dev basic knowladge Astro is for you.","src/content/posts/build-one-page-website-budget.mdx",[281],"../../assets/images/23/11/build-one-page-budget.jpeg","3a579a47ece3e728","build-one-page-website-budget.mdx","best-python-web-frameworks",{id:284,data:286,body:296,filePath:297,assetImports:298,digest:300,legacyId:301,deferredRender:32},{title:287,description:288,date:289,image:290,authors:291,categories:292,tags:293,canonical:295},"30+ Best Python Web Frameworks for 2024","A complete list with the best Python Web Frameworks for 2024 that you can use to add an interface to your Python applications.",["Date","2024-02-16T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/python-web-frameworks.jpeg",[19],[21],[294],"python","https://www.bitdoze.com/best-python-web-frameworks/","Python web frameworks are essential tools for developers looking to build web applications efficiently. These frameworks provide a structured and systematic approach to web development, offering pre-built components, best practices, and a way to automate many of the repetitive tasks involved in creating web applications. Python, being a popular programming language, has a wide range of frameworks catering to different needs, from simple applications to complex, high-traffic websites.\r\n\r\n## Types of Python Web Frameworks\r\n\r\nPython web frameworks can be categorized into three main types:\r\n\r\n- **Full-stack frameworks**: These provide complete support for developing applications, including both frontend and backend components. They are designed to be an all-in-one solution, offering a wide range of functionalities out of the box.\r\n\r\n- **Microframeworks**: Lightweight and flexible, microframeworks offer the basics to get a web application up and running. They do not include additional functionalities like database abstraction or form validation, which allows developers to choose their tools and extensions.\r\n\r\n- **Asynchronous frameworks**: These are designed to handle a large number of concurrent connections, making them ideal for real-time applications. Asynchronous frameworks utilize Python's asyncio library for managing asynchronous I/O operations.\r\n- **Specialized Frameworks**: These are designed for specific use cases such as RESTful API development, data analysis, or content management systems.\r\n\r\n## Advantages of Using Python Web Frameworks\r\n\r\nPython web frameworks offer several benefits:\r\n\r\n- **Efficiency and Speed**: By abstracting common patterns in web development, frameworks enable rapid development of applications.\r\n- **Security**: Many frameworks include built-in security features that help protect against common vulnerabilities.\r\n- **Scalability**: Frameworks often provide tools and features that support the growth of applications, handling increased traffic and data smoothly.\r\n- **Community and Support**: Popular frameworks have large communities, offering extensive documentation, tutorials, and forums for troubleshooting.\r\n\r\n## Best Python Web Frameworks for 2024\r\n\r\n### Full-Stack Frameworks\r\n\r\nFull-Stack Frameworks provide a one-stop solution for both front-end and back-end development needs. They include built-in libraries for form generation, template layouts, database interaction, and more.\r\n\r\n- **[Django](https://www.djangoproject.com/)**: A high-level framework that promotes rapid development and clean, pragmatic design. It's known for being secure and scalable, making it suitable for both small projects and large-scale applications **[Django Repo](https://github.com/django/django)**.\r\n\r\n- **[Pyramid](https://trypyramid.com)**: Known for starting simple and being able to scale up to complex applications. It allows developers to choose components for templating, databases, and security, providing flexibility and control **[Pyramid Repo](https://github.com/Pylons/pyramid)**.\r\n\r\n- **[TurboGears](https://turbogears.org)**: A framework that can start as a single-file app and scale up to a full-stack solution. It's built on top of several other frameworks and offers a mix of full-stack and microframework capabilities **[TurboGears Repo](https://github.com/TurboGears/tg2)**.\r\n\r\n- **[Web2py](http://www.web2py.com)**: A full-stack framework that requires no installation or configuration. It's designed for rapid development of secure database-driven web applications **[Web2py Repo](https://github.com/web2py/web2py)**.\r\n\r\n- **[CubicWeb](https://www.cubicweb.org)**: An open-source framework built with Semantic Web principles. It emphasizes reusability through components and an explicit data model **[CubicWeb Repo](https://forge.extranet.logilab.fr/cubicweb/cubicweb)**.\r\n\r\n- **[Taipy](https://www.taipy.io)**: A framework that integrates with existing open-source libraries and provides a set of robust features and tools for web application development **[Taipy Repo](https://github.com/Avaiga/taipy)**. Read more on [Taipy vs Streamlit](https://www.bitdoze.com/streamlit-vs-taipy/)\r\n\r\n- **[Masonite](https://docs.masoniteproject.com/)**: A modern and developer-centric Python web framework that aims to be a tool for crafting powerful web applications quickly and easily **[Masonite Repo](http://github.com/MasoniteFramework/masonite)**.\r\n\r\n- **[Zope & BlueBream](https://www.zope.dev/)**: A framework that allows for the development of complex web applications with a strong emphasis on reusability and component-based development **[Zope Repo](https://github.com/zopefoundation/Zope)**.\r\n\r\n### Microframeworks\r\n\r\nMicroframeworks are lightweight and suitable for applications that do not require a lot of additional functionalities like database abstraction layers or form validation. They are ideal for small to medium-sized applications.\r\n\r\n- **[Flask](https://flask.palletsprojects.com/)**: A lightweight WSGI web application framework that is easy to get started with and can scale up to complex applications **[Flask Repo](https://github.com/pallets/flask)**.\r\n\r\n- **[Bottle](http://bottlepy.org/)**: A fast, simple, and lightweight WSGI micro web-framework for Python. It is distributed as a single file module and has no dependencies other than the Python Standard Library **[Bottle Repo](https://github.com/bottlepy/bottle)**.\r\n\r\n- **[CherryPy](https://cherrypy.dev/)**: A minimalist Python web framework, allowing developers to build web applications in much the same way they would build any other object-oriented Python program **[CherryPy Repo](https://github.com/cherrypy/cherrypy)**.\r\n\r\n- **[Dash](https://dash.plotly.com/)**: A productive Python framework for building web applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python **[Dash Repo](https://github.com/plotly/dash)**.\r\n\r\n- **[Falcon](https://falconframework.org/)**: A bare-metal Python web API framework for building very fast app backends and microservices **[Falcon Repo](https://github.com/falconry/falcon)**.\r\n\r\n- **[Hug](https://www.hug.rest/)**: A framework that aims to make developing Python-driven APIs as simple as possible, but no simpler **[Hug Repo](https://github.com/hugapi/hug)**.\r\n\r\n- **[web.py](http://webpy.org/)**: A web framework for Python that is as simple as it is powerful **[web.py Repo](https://github.com/webpy/webpy)**.\r\n\r\n### Asynchronous Frameworks\r\n\r\nAsynchronous Frameworks are designed to handle a large number of concurrent connections and are built on Python’s asyncio library. They are perfect for applications that require real-time features and high concurrency.\r\n\r\n- **[Sanic](https://sanic.dev//)**: A web server and web framework that's written to go fast. It allows the usage of the async/await syntax added in Python 3.5, which makes your code non-blocking and speedy **[Sanic Repo](https://github.com/sanic-org/sanic)**.\r\n\r\n- **[Tornado](https://www.tornadoweb.org)**: A Python web framework and asynchronous networking library, originally developed at FriendFeed **[Tornado Repo](https://github.com/tornadoweb/tornado)**.\r\n\r\n- **[AIOHTTP](https://docs.aiohttp.org/)**: An asynchronous HTTP client/server framework for asyncio and Python **[AIOHTTP Repo](https://github.com/aio-libs/aiohttp)**.\r\n\r\n- **[FastAPI](https://fastapi.tiangolo.com/)**: A modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints **[FastAPI Repo](https://github.com/tiangolo/fastapi)**.\r\n\r\n- **[UvLoop](https://github.com/MagicStack/uvloop)**: An ultra-fast, drop-in replacement for the asyncio event loop. UvLoop is implemented in Cython and uses libuv under the hood.\r\n\r\n- **[Starlette](https://www.starlette.io/)**: A lightweight ASGI framework/toolkit, which is ideal for building high-performance asyncio services **[Starlette Repo](https://github.com/encode/starlette)**.\r\n\r\n- **[Quart](https://quart.palletsprojects.com/)**: A Python ASGI web microframework with the same API as Flask, allowing for Flask extensions to run **[Quart Repo](https://github.com/pallets/quart)**.\r\n\r\n### Specialized Frameworks\r\n\r\nSpecialized Frameworks are designed for specific use cases such as RESTful API development, data analysis, or content management systems.\r\n\r\n- **[Streamlit](https://streamlit.io/)**: An open-source app framework for Machine Learning and Data Science teams. Create beautiful data apps in hours, not weeks. All in pure Python **[Streamlit Repo](https://github.com/streamlit/streamlit)**. **Check [Streamlit Deploy in VPS](https://www.bitdoze.com/streamlit-deploy-vps-cloudflare/)** to see how you can run your Streamlit apps.\r\n\r\n- **[Anvil](https://anvil.works/)**: A platform for building full-stack web apps with nothing but Python. It allows you to write your client-side code in Python and deploy it over the web **[Anvil Repo](https://github.com/anvil-works/anvil-runtime)**.\r\n\r\n- **[Plotly Dash](https://plotly.com/dash/)**: A productive Python framework for building web applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python **[Dash Repo](https://github.com/plotly/dash)**.\r\n\r\n- **[Panel](https://panel.holoviz.org/)**: A high-level app and dashboarding solution for Python. Panel works with visualizations from Bokeh, Matplotlib, HoloViews, and many other Python plotting libraries **[Panel Repo](https://github.com/holoviz/panel)**.\r\n\r\n- **[NiceGui](https://nicegui.io/)**: A user interface framework that allows you to create beautiful and responsive web applications with Python only **[NiceGui Repo](https://github.com/zauberzeug/nicegui)**. You can check also [Streamlit vs. NiceGUI](https://www.bitdoze.com/streamlit-vs-nicegui/) for an in-depth comparison. You can check [NiceGUI for Beginners](https://www.bitdoze.com/nicegui-get-started/) to find out more.\r\n\r\n- **[Reflex](https://reflex.dev/)**: A library for building React applications with Python **[Reflex Repo](https://github.com/reflex-dev/reflex)** .\r\n\r\n- **[Flet](https://flet.dev/)**: A framework that enables building interactive multi-user web, mobile, and desktop apps in Python without prior experience in frontend development **[Flet Repo](https://github.com/flet-dev/flet)**.\r\n\r\n- **[Justpy](https://justpy.io/)**: An object-oriented, component-based, high-level Python web framework that requires no front-end programming **[Justpy Repo](https://github.com/justpy-org/justpy)**.\r\n\r\n- **[Lona](https://lona-web.org/)**: A web application framework, designed to write responsive web apps in full Python **[Lona Repo](https://github.com/lona-web-org/lona)**.\r\n\r\n- **[ReactPy](https://reactpy.dev/docs/index.html)**: A package that allows you to create interactive web applications in Python using components **[ReactPy Repo](https://github.com/reactive-python/reactpy)**.\r\n\r\n- **[Reahl](https://www.reahl.org/)**: A web application framework that allows a Python programmer to work in familiar object-oriented paradigms and to use a single programming language to create all parts of a web application **[Reahl Repo](https://github.com/reahl/reahl)**.\r\n\r\nThis list includes some of the most popular and widely used Python web frameworks, each with its unique features and capabilities. Whether you're building a simple web application, a high-performance API, or a full-fledged web service, there's a Python web framework that fits your project's needs.\r\n\r\n## Choosing the Right Framework\r\n\r\nChoosing the right Python web framework for your web development project is a critical decision that can affect both the development process and the success of the application. Here are some key considerations to help you make an informed decision:\r\n\r\n**Project Size and Scope**\r\n\r\n- **Full-stack frameworks** are generally better for larger applications that require a comprehensive suite of features right from the start.\r\n- **Microframeworks** are more suitable for smaller, simpler applications where you want the flexibility to pick and choose the components you use.\r\n- **Asynchronous frameworks** are ideal for applications that need to handle many concurrent connections, such as chat apps or real-time data processing systems.\r\n\r\n**Learning Curve**\r\n\r\n- Evaluate the complexity of the framework and the time it will take for your team to become proficient in using it.\r\n- Consider the availability of documentation, tutorials, and community support, which can help speed up the learning process.\r\n\r\n**Community and Support**\r\n\r\n- A strong community can be a valuable resource for finding solutions to problems, discussing best practices, and staying updated with the latest developments.\r\n\r\n**Performance and Scalability**\r\n\r\n- Consider how well the framework can handle the growth of your application, both in terms of user base and functionality.\r\n- Some frameworks are optimized for performance and can handle high loads more efficiently.\r\n\r\n**Security**\r\n\r\n- Security is paramount, so it's important to choose a framework with robust security features or one that makes it easy to implement security best practices.\r\n\r\n**Long-Term Viability**\r\n\r\n- Look for a framework that is actively maintained and has a roadmap for future development to ensure it will continue to meet your needs as your project evolves.\r\n\r\n**Specific Project Requirements**\r\n\r\n- Think about the specific features you need for your project. If you need a powerful ORM, an admin interface, or built-in security features, a full-stack framework might be the best choice.\r\n- If you need a lightweight application with minimal dependencies, a microframework could be more appropriate.\r\n\r\n**Developer Preference and Project Philosophy**\r\n\r\n- The preferences of your development team and the overall philosophy of the project can also influence the choice of framework.\r\n\r\nIn summary, the right Python web framework for your project will depend on a variety of factors, including the size and complexity of the project, the team's expertise, the speed of development, community support, performance and scalability considerations, security needs, and the long-term viability of the framework. It's important to choose a framework that aligns with your project's specific requirements and your team's capabilities. Popular frameworks like Django, Flask, and FastAPI cater to different types of projects and offer their own sets of advantages.\r\n\r\n## Conclusion\r\n\r\nPython's web framework ecosystem is rich and varied, offering solutions for all kinds of web development needs. Whether you're building a simple application or a complex system, there's a Python web framework that can help you achieve your goals efficiently.","src/content/posts/best-python-web-frameworks.mdx",[299],"../../assets/images/24/02/python-web-frameworks.jpeg","0a95288b4d961d72","best-python-web-frameworks.mdx","bun-package-manager",{id:302,data:304,body:314,filePath:315,assetImports:316,digest:318,legacyId:319,deferredRender:32},{title:305,description:306,date:307,image:308,authors:309,categories:310,tags:311,canonical:313},"Bun vs NPM, Yarn, PNPM, and Others","Learn what Bun is, how it differs from other package managers like NPM, Yarn, and PNPM, and how to install and use it in your Node.js projects.",["Date","2024-02-07T01:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/bun-npm.jpeg",[19],[98],[312],"bun","https://www.bitdoze.com/bun-package-manager/","import { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport img1 from \"../../assets/images/24/02/bun-speed.webp\";\r\n\r\nBun is a fresh entrant in the JavaScript ecosystem, designed to be a comprehensive JavaScript runtime and package manager. It aims to provide a faster and more efficient alternative to existing tools like NPM, Yarn, and PNPM. Built from scratch, Bun leverages modern technologies such as Zig and JavaScriptCore to optimize performance and offer a seamless development experience.\r\n\r\nBun has several advantages over other package managers, such as:\r\n\r\n- **Speed:** Bun installs packages in parallel, using multiple threads and processes. It also caches the packages locally, so that subsequent installs are faster. Bun can install packages up to 10 times faster than NPM, Yarn, or PNPM, according to its benchmarks.\r\n- **Simplicity:** Bun has a simple and intuitive command-line interface, with only a few commands and options. It does not require any configuration files, lock files, or scripts. It also has a minimal and consistent output, with clear and helpful error messages.\r\n- **Reliability:** Bun uses a flat and deterministic dependency tree, which means that every package has only one version and location in the node_modules folder. This avoids the problems of duplicate, conflicting, or missing packages that can occur with nested and non-deterministic dependency trees. Bun also verifies the integrity and authenticity of the packages, using checksums and signatures.\r\n\r\nBun is compatible with most Node.js projects that use NPM, Yarn, or pnpm. It can read and write the package.json file, and install the packages from the same sources. It can also work alongside other package managers, as long as they use the same dependency resolution strategy.\r\n\r\n## Why Switch Away from NPM, PNPM, or Yarn to Bun?\r\n\r\nSwitching from NPM, PNPM, or Yarn to Bun can be motivated by several factors, primarily revolving around performance. If you will test test bun you will see that the performance when installing packages can be **up to 30x** faster\r\n\r\n- your node dependencies will install faster\r\n- assets will compile faster\r\n\r\n> However, it's important to note that some developers express skepticism about the long-term viability of Bun, drawing parallels with the rise and fall of Yarn and questioning the necessity of Bun's features.\r\n\r\nYou can check [How To Migrate Astro on Bun in CloudFlare](https://www.bitdoze.com/migrate-astro-bun/) to find out more.\r\n\r\n## How to Install Bun\r\n\r\nBun is easy to install and use, as it does not require any global installation or configuration. You can install Bun locally in your Node.js project.\r\n\r\n**Install Bun on MACOS**\r\n\r\nOn Mac devices can be installed with brew, you just add the source and the install it as below:\r\n\r\n```sh\r\nbrew tap oven-sh/bun\r\nbrew install bun\r\n```\r\n\r\n**Install Bun on Linux**\r\n\r\nLinux installation is easy you just run:\r\n\r\n```sh\r\ncurl -fsSL https://bun.sh/install | bash\r\n```\r\n\r\n> Linux users — The unzip package is required to install Bun. Use sudo apt install unzip to install unzip package. Kernel version 5.6 or higher is strongly recommended, but the minimum is 5.1. Use uname -r to check Kernel version.\r\n\r\n**Install Bun on Windows**\r\n\r\nBun provides a limited, experimental native build for Windows. So it is recommended to use WLS to have it running on Windows.\r\n\r\nFor the complete install doc check [bun.sh install](https://bun.sh/docs/installation)\r\n\r\n## Bun versus Node.js: Package Manager\r\n\r\nBun and Node.js differ significantly in their approach to package management. While Node.js relies on npm as its default package manager, Bun incorporates its own package manager. This integrated package manager is designed to be faster and more efficient than npm, leveraging a global module cache to eliminate redundant downloads\r\n\r\n| Bun Command               | npm Command                   | Purpose                                         |\r\n| ------------------------- | ----------------------------- | ----------------------------------------------- |\r\n| `bun install`             | `npm install`                 | Install all dependencies from package.json      |\r\n| `bun add <package>`       | `npm install <package>`       | Add a new package to the project                |\r\n| `bun add <package> --dev` | `npm install <package> --dev` | Add a new development-only package              |\r\n| `bun remove <package>`    | `npm uninstall <package>`     | Remove a package from the project               |\r\n| `bun update <package>`    | `npm update <package>`        | Update a specific package to its latest version |\r\n| `bun run <script>`        | `npm run <script>`            | Execute a specified script from package.json    |\r\n\r\n`bun` and `npm` share similar commands for package management in JavaScript projects. Both install commands install dependencies listed in package.json. add and install commands add new packages, while add `--dev` and install `--dev` add development-only packages. remove and uninstall commands remove packages, and update commands update specific packages. Finally, run commands execute scripts from `package.json`. The key difference lies in the choice of package manager, with bun and npm serving as alternatives, each with its syntax for accomplishing these common development tasks.\r\n\r\n<Picture src={img1}  alt=\"bun speed\" />\r\n\r\n## How to replace NPM, Yarn, or PNPM with Bun\r\n\r\nTo replace NPM, Yarn, or pnpm with Bun, you first need to remove the lock files of the existing package manager, as Bun uses its own lock file `bun.lockb`. Here are the commands to remove the lock files:\r\n\r\n```sh\r\n# If you were using NPM:\r\nrm package-lock.json\r\n\r\n# If you were using pnpm:\r\nrm pnpm-lock.yaml\r\n\r\n# If you were using Yarn:\r\nrm yarn.lock\r\n```\r\n\r\nAfter you just run `bun install` it will start installing everything.\r\n\r\n## Bun Install in Action\r\n\r\nI have done this exercise for my bitdoze.com blog that is using Astro, did the exact steps above and below are the results.\r\n\r\nI am using NPM which I think is the slowest and when :\r\n\r\n**Install the packages with dependencies**\r\n\r\nNPM:\r\n\r\n```sh\r\n➜  bitdoze-astro-bkw git:(main) time npm install\r\n\r\n\r\nnpm install  13.22s user 3.82s system 23% cpu 1:13.89 total\r\n\r\n\r\nnpm run build  55.96s user 3.41s system 126% cpu 46.776 total\r\n```\r\n\r\nBun:\r\n\r\n```sh\r\n➜  bitdoze-astro-bkw git:(main) time bun install\r\n\r\nbun install  10.83s user 2.14s system 121% cpu 10.694 total\r\n\r\nbun run build  55.30s user 3.34s system 126% cpu 46.181 total\r\n```\r\n\r\nAs can be seen above the install with bun took around **11 seconds** and npm install took 73 seconds which makes bun 7 times faster in this case. The build process was the same.\r\n\r\n## Bun Usage\r\n\r\n**Add a package using Bun**\r\n\r\nLike with another package manager you can add packages with `bun add`:\r\n\r\n```sh\r\nbun add tailwindcss autoprefixer postcss\r\n```\r\n\r\n**Remove a package using Bun**\r\n\r\n`bun remove` will do the trick also here:\r\n\r\n```sh\r\nbun remove tailwindcss\r\n```\r\n\r\n**Run your scripts using Bun**\r\n\r\n`bun run` will help you do this for instance for astro `bun run dev` will start astro and `bun run build` will build the app. Bun will check `package.json` to know what to do.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/BsnCpESUEqM\"\r\n  label=\"Bun Video Presentation\"\r\n/>","src/content/posts/bun-package-manager.mdx",[317],"../../assets/images/24/02/bun-npm.jpeg","5ad5e027fc0eae7b","bun-package-manager.mdx","bun-update-packages",{id:320,data:322,body:331,filePath:332,assetImports:333,digest:335,legacyId:336,deferredRender:32},{title:323,description:324,date:325,image:326,authors:327,categories:328,tags:329,canonical:330},"How To Update Node Packages to The Last Version in Bun.sh","Learn how you can update node packages to last version with Bun.sh including the package.json",["Date","2024-03-26T01:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/bun-update-packages.jpeg",[19],[98],[312],"https://www.bitdoze.com/bun-update-packages/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nIn the fast-paced world of web development, keeping your Node.js packages up to date is crucial for security, performance, and access to the latest features. Bun.sh, a JavaScript runtime like Node.js, provides a set of commands to streamline the update process. This article will guide you through the steps to upgrade your Node packages to the latest version using Bun.sh.\r\n\r\nIf you want to check more on Bun.sh you can check [Bun vs NPM, Yarn, PNPM, and Others](https://www.bitdoze.com/bun-package-manager/). In case you want to migrate your Astro project to Bun see: [How to Migrate Astro to Bun on CloudFlare](https://www.bitdoze.com/migrate-astro-bun/)\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/UFXpCsqnURY\"\r\n  label=\"How To Update Node Packages to Last Version in Bun.sh\"\r\n/>\r\n\r\n## Upgrade Bun to the Latest Version\r\n\r\nTo ensure you're using the latest features and improvements of Bun itself, you should start by upgrading Bun to the latest version. This can be done with a simple command:\r\n\r\n```sh\r\nbun upgrade\r\n```\r\n\r\nThis command checks for the latest version of Bun and upgrades it accordingly.\r\n\r\n## Update Bun Packages without package.json\r\n\r\nIf you want to update the packages in your current project and you don't have a `package.json` file, or you simply want to update globally installed packages, you can use the following command:\r\n\r\n```sh\r\nbun update\r\n```\r\n\r\nAfter running this command, you'll see an output similar to:\r\n\r\n```sh\r\nbun update v1.0.35 (940448d6)\r\n\r\n + @types/react@18.2.71\r\n + postcss@8.4.38\r\n + sharp@0.33.3\r\n + @astrojs/mdx@2.2.1\r\n + astro@4.5.9\r\n```\r\n\r\nThis output lists the packages that have been updated along with their new versions.\r\n\r\n## Force Update Bun packages\r\n\r\nSometimes, you may encounter situations where a simple update does not fetch the latest versions due to version locking or other constraints. In such cases, you can force update the packages:\r\n\r\n```sh\r\nbun update --force\r\n```\r\n\r\nThis command ignores the current versions and forcefully updates all packages to their latest versions. The output will list all the packages that were updated, including those that were not updated in a regular update process.\r\n\r\nThe output will indicate that the packages have been forcefully updated:\r\n\r\n```sh\r\n❯ bun update --force\r\nbun update v1.0.35 (940448d6)\r\n\r\n + @astrojs/react@3.1.0\r\n + @astrojs/sitemap@3.1.1\r\n + @astrojs/tailwind@5.1.0\r\n + @tailwindcss/forms@0.5.7\r\n + @tailwindcss/typography@0.5.10\r\n + @types/marked@6.0.0\r\n + @types/react@18.2.71\r\n + postcss@8.4.38\r\n + prettier@3.2.5\r\n + prettier-plugin-astro@0.13.0\r\n + prettier-plugin-tailwindcss@0.5.12\r\n + react@18.2.0\r\n + react-dom@18.2.0\r\n + sass@1.72.0\r\n + sharp@0.33.3\r\n + tailwind-bootstrap-grid@5.1.0\r\n + tailwindcss@3.4.1\r\n + @astrojs/mdx@2.2.1\r\n + @astrojs/partytown@2.0.4\r\n + @astrojs/rss@4.0.5\r\n + astro@4.5.9\r\n + astro-analytics@2.7.0\r\n + astro-seo@0.8.3\r\n + date-fns@3.6.0\r\n + fuse.js@7.0.0\r\n + github-slugger@2.0.0\r\n + marked@12.0.1\r\n + react-icons@4.12.0 (v5.0.1 available)\r\n```\r\n\r\nNote that using `--force` will not update the `package.json` file. It will remain as is:\r\n\r\n```json\r\n{\r\n  \"name\": \"bitdoze.com\",\r\n  \"version\": \"1\",\r\n  \"license\": \"MIT\",\r\n  \"scripts\": {\r\n    \"dev\": \"astro dev\",\r\n    \"start\": \"astro dev\",\r\n    \"build\": \"astro build\",\r\n    \"preview\": \"astro preview\",\r\n    \"astro\": \"astro\",\r\n    \"format\": \"prettier -w .\"\r\n  },\r\n  \"dependencies\": {\r\n    \"@astrojs/mdx\": \"^2.2.0\",\r\n    \"@astrojs/partytown\": \"^2.0.4\",\r\n    \"@astrojs/rss\": \"^4.0.5\",\r\n    \"astro\": \"^4.5.2\",\r\n    \"astro-analytics\": \"^2.7.0\",\r\n    \"astro-seo\": \"^0.8.3\",\r\n    \"date-fns\": \"^3.4.0\",\r\n    \"fuse.js\": \"^7.0.0\",\r\n    \"github-slugger\": \"^2.0.0\",\r\n    \"marked\": \"^12.0.1\",\r\n    \"react-icons\": \"^4.12.0\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@astrojs/react\": \"^3.1.0\",\r\n    \"@astrojs/sitemap\": \"^3.1.1\",\r\n    \"@astrojs/tailwind\": \"^5.1.0\",\r\n    \"@tailwindcss/forms\": \"^0.5.7\",\r\n    \"@tailwindcss/typography\": \"^0.5.10\",\r\n    \"@types/marked\": \"^6.0.0\",\r\n    \"@types/react\": \"^18.2.65\",\r\n    \"postcss\": \"^8.4.35\",\r\n    \"prettier\": \"^3.2.5\",\r\n    \"prettier-plugin-astro\": \"^0.13.0\",\r\n    \"prettier-plugin-tailwindcss\": \"^0.5.12\",\r\n    \"react\": \"^18.2.0\",\r\n    \"react-dom\": \"^18.2.0\",\r\n    \"sass\": \"^1.71.1\",\r\n    \"sharp\": \"^0.33.2\",\r\n    \"tailwind-bootstrap-grid\": \"^5.1.0\",\r\n    \"tailwindcss\": \"^3.4.1\"\r\n  }\r\n}\r\n```\r\n\r\n## Update package.json with Bun\r\n\r\nTo update the `package.json` file with the latest versions of your packages, you'll need to use a utility like `npm-check-updates`. First, install it globally if you haven't already:\r\n\r\n```sh\r\nnpm install -g npm-check-updates\r\n```\r\n\r\nThen, run `npm-check-updates` with Bun to update your `package.json`:\r\n\r\n```sh\r\nbunx npm-check-updates -ui\r\n```\r\n\r\nThis command will check for updates and interactively allow you to choose which packages to upgrade. After confirming the selections, npm-check-updates will update your package.json to reflect the latest versions.\r\n\r\n```sh\r\n❯ bunx npm-check-updates -ui\r\n\r\nUsing bun\r\nUpgrading /Users/dbalota/websites/bitdoze-astro-bkw_test/package.json\r\n[====================] 29/29 100%\r\n\r\n? Choose which packages to update ›\r\n  ↑/↓: Select a package\r\n  Space: Toggle selection\r\n  a: Toggle all\r\n  Enter: Upgrade\r\n\r\n  ◉ @astrojs/mdx    ^2.2.0  →    ^2.2.1\r\n  ◉ @types/react  ^18.2.65  →  ^18.2.71\r\n  ◉ astro           ^4.5.2  →    ^4.5.9\r\n  ◉ date-fns        ^3.4.0  →    ^3.6.0\r\n  ◉ postcss        ^8.4.35  →   ^8.4.38\r\n❯ ◯ react-icons    ^4.12.0  →    ^5.0.1\r\n  ◉ sass           ^1.71.1  →   ^1.72.0\r\n  ◉ sharp          ^0.33.2  →   ^0.33.3\r\n```\r\n\r\nAfter confirming the updates, check your `package.json` to see that it has been updated with the new versions:\r\n\r\n```json\r\n{\r\n  \"name\": \"bitdoze.com\",\r\n  \"version\": \"1\",\r\n  \"license\": \"MIT\",\r\n  \"scripts\": {\r\n    \"dev\": \"astro dev\",\r\n    \"start\": \"astro dev\",\r\n    \"build\": \"astro build\",\r\n    \"preview\": \"astro preview\",\r\n    \"astro\": \"astro\",\r\n    \"format\": \"prettier -w .\"\r\n  },\r\n  \"dependencies\": {\r\n    \"@astrojs/mdx\": \"^2.2.1\",\r\n    \"@astrojs/partytown\": \"^2.0.4\",\r\n    \"@astrojs/rss\": \"^4.0.5\",\r\n    \"astro\": \"^4.5.9\",\r\n    \"astro-analytics\": \"^2.7.0\",\r\n    \"astro-seo\": \"^0.8.3\",\r\n    \"date-fns\": \"^3.6.0\",\r\n    \"fuse.js\": \"^7.0.0\",\r\n    \"github-slugger\": \"^2.0.0\",\r\n    \"marked\": \"^12.0.1\",\r\n    \"npm-check-updates\": \"^16.14.18\",\r\n    \"react-icons\": \"^4.12.0\"\r\n  },\r\n  \"devDependencies\": {\r\n    \"@astrojs/react\": \"^3.1.0\",\r\n    \"@astrojs/sitemap\": \"^3.1.1\",\r\n    \"@astrojs/tailwind\": \"^5.1.0\",\r\n    \"@tailwindcss/forms\": \"^0.5.7\",\r\n    \"@tailwindcss/typography\": \"^0.5.10\",\r\n    \"@types/marked\": \"^6.0.0\",\r\n    \"@types/react\": \"^18.2.71\",\r\n    \"postcss\": \"^8.4.38\",\r\n    \"prettier\": \"^3.2.5\",\r\n    \"prettier-plugin-astro\": \"^0.13.0\",\r\n    \"prettier-plugin-tailwindcss\": \"^0.5.12\",\r\n    \"react\": \"^18.2.0\",\r\n    \"react-dom\": \"^18.2.0\",\r\n    \"sass\": \"^1.72.0\",\r\n    \"sharp\": \"^0.33.3\",\r\n    \"tailwind-bootstrap-grid\": \"^5.1.0\",\r\n    \"tailwindcss\": \"^3.4.1\"\r\n```\r\n\r\n## Check and see that your project is working\r\n\r\nAfter updating your packages, it's essential to verify that your project still works as expected. Run your project's build and test commands to ensure that the updates haven't introduced any breaking changes.\r\n\r\n## Conclusions\r\n\r\nKeeping your Bun environment and packages up to date is essential for the health and security of your projects. Regularly upgrading Bun, updating packages, and ensuring your package.json reflects the latest versions can help leverage the latest features and improvements while minimizing potential security vulnerabilities. Always test your project thoroughly after updates to ensure compatibility and stability.","src/content/posts/bun-update-packages.mdx",[334],"../../assets/images/24/03/bun-update-packages.jpeg","c260b7aad8d971a1","bun-update-packages.mdx","carrd-add-pricing-table",{id:337,data:339,body:348,filePath:349,assetImports:350,digest:352,legacyId:353,deferredRender:32},{title:340,description:341,date:342,image:343,authors:344,categories:345,tags:346,canonical:347},"How To Add Pricing Table to Carrd.co","Learn how to add a responsive monthly-year toggle pricing table to Carrd.co",["Date","2023-11-27T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/carrd-priging-table.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-add-pricing-table/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nIn this tutorial we are going to see how you can add a responsive pricing table to your Carrd.co website that has a monthly and yearly toggle and looks nice. The plugin costs $5 and you can use it on as many carrd.co websites as you want.\r\n\r\n## What is Carrd.co?\r\n\r\n[Carrd.co](https://try.carrd.co/bitdoze) is a platform that allows users to create single-page, responsive websites with ease. Perfect for personal profiles, business landing pages, and more, Carrd has become a go-to choice for those seeking a simple yet effective solution for their online presence.\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [How To Add Custom Domain to Carrd.co](https://www.bitdoze.com/carrd-add-domain/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n- [Back To Top Button on Carrd](https://www.bitdoze.com/carrd-back-to-top-button/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## How To Add The Pricing Table to Carrd\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/Q3UpuDDs0AY\"\r\n  label=\"How To Add Pricing Table to Carrd.co\"\r\n/>\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nAs carrd.co is not coming up with an easy way to add a pricing table in the widgets it has you will need to be on the Pro Plus plan that will allow you to embed code. In the below tutorial, we are going to see everything that needs to be done to have a responsive pricing table.\r\n\r\n### 1. Download the Pricing Table Code\r\n\r\nThe first thing to do is to download the code that you are going to use on your Carrd website. To do that just go to: [Carrd Pricing Table](https://webdoze.net/buy/carrd-co-pricing-table/) and get your pricing table. It's not free it costs $5 which is the price of a coffee.\r\n\r\n### 2. Customize the Code\r\n\r\nAfter you have your code you will be able to change colors, prices, and font and add the price elements for your business. The code that will help you do this is outlined below:\r\n\r\n**Change Colours**\r\n\r\n```css\r\n:root {\r\n  --priceb-col-gr: linear-gradient(\r\n    135deg,\r\n    rgba(163, 168, 240, 1) 0%,\r\n    rgba(105, 111, 221, 1) 100%\r\n  );\r\n}\r\n```\r\n\r\n**Change Font**\r\n\r\nYou can use your won font, you can alter the bellow line under style:\r\n\r\n```css\r\nfont-family: inherit;\r\n```\r\n\r\nThe above code holds the colors for buttons, active card, and toggle. You can change or add your color and will be used automatically as this is a css variable.\r\n\r\n**Change Prices**\r\n\r\nYou need to edit 2 elements for the prices, 1 is for html code:\r\n\r\n```html\r\n<li id=\"basic\" class=\"price bottom-bar\">&dollar;199.99</li>\r\n```\r\n\r\nThe other one is for the java script where it has the toggle:\r\n\r\n```js\r\nconst prices = {\r\n  basic: { monthly: \"$19.99\", annual: \"$199.99\" },\r\n  professional: { monthly: \"$24.99\", annual: \"$249.99\" },\r\n  master: { monthly: \"$39.99\", annual: \"$399.99\" },\r\n};\r\n```\r\n\r\nYou can add your own prices don't forget to change the plan name in case you use custom ones for your business.\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n**Change Elements**\r\n\r\nTo add your own price elements you just replace the `li` items in each card with your own, you can add more to suite your needs.\r\n\r\n```html\r\n<li class=\"bottom-bar\">1 TB Storage</li>\r\n<li class=\"bottom-bar\">5 Users Allowed</li>\r\n<li class=\"bottom-bar\">Send up to 10 GB</li>\r\n<li class=\"bottom-bar\">Unlimited</li>\r\n```\r\n\r\n**Change Buttons**\r\n\r\nYou can add your won text on the buttons and you can replace the link to your package with your details, to do so alter:\r\n\r\n```html\r\n<li>\r\n  <a href=\"https://carrdme.com\"><div class=\"btn\">Learn More</div></a>\r\n</li>\r\n```\r\n\r\n### 3. Add the code to Carrd.co\r\n\r\nAfter you customize the code you will need to go to the carrd website and add a container, in that container you can add the `embed` widget, give it a name and paste the code. You can align the text as you link and ad spacing for the container.\r\n\r\n### 4. Save and check the modifications\r\n\r\nWhat remains is to save your project and see how is looking, just publish the design and see how is looking, If there are issues you can alter the width of the container and make some customizations in carrd interface.\r\n\r\n## Conclusions\r\n\r\nIn this way, you can add a pricing table to your carrd.co website with the price of a coffee. This shouldn't be hard and you need only some basic knowlade to change the elements and customize the elements as you need. For additional carrd plugins, you can check [carrdme.com](https://carrdme.com/) website.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />","src/content/posts/carrd-add-pricing-table.mdx",[351],"../../assets/images/23/11/carrd-priging-table.jpeg","e95e0121b558bc9f","carrd-add-pricing-table.mdx","carrd-add-domain",{id:354,data:356,body:365,filePath:366,assetImports:367,digest:369,legacyId:370,deferredRender:32},{title:357,description:358,date:359,image:360,authors:361,categories:362,tags:363,canonical:364},"How To Add Custom Domain to Carrd.co Website","Let's see how a custom domain name can be added to your carrd.co website.",["Date","2024-01-16T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/carrd-domain-setup.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-add-domain/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/domain-settings-carrd.png\";\r\n\r\n[Carrd.co](https://go.bitdoze.com/carrd) is a great platform for creating simple and beautiful websites. But what if you want to use your own domain name instead of the default .carrd.co URL? In this article, you’ll learn how to set up a custom domain for your Carrd.co website in just a few steps.\r\nYou need the Carrd.co Pro Standard plan that allows 10 domains (websites) at $19/year to add a domain name.\r\n\r\n## Key Takeaways\r\n\r\n- You need a Pro Standard or higher plan to use a custom domain on Carrd.co\r\n- You need to register a domain name from a domain provider (such as Namecheap, Cloudflare, or GoDaddy)\r\n- You need to add two A records and one CNAME record to your domain’s DNS settings\r\n- You need to enter your custom domain name in Carrd.co’s publish settings and wait for it to initialize\r\n- You can use Cloudflare’s free SSL service to encrypt your site and improve its SEO ranking\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n- [Back To Top Button on Carrd](https://www.bitdoze.com/carrd-back-to-top-button/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## How to Register a Domain Name\r\n\r\nThe first step to using a custom domain on Carrd.co is to register a domain name. A domain name is the address of your website on the internet, such as example.com or mysite.net. You can choose any domain name that is available and suits your purpose.\r\n\r\nTo register a domain name, you need to use a domain provider. A domain provider is a company that sells and manages domain names. There are many domain providers to choose from, such as Namecheap, Cloudflare, or GoDaddy. The price and features of each domain provider may vary, so you should compare them before making a decision.\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\nHere are some [tips to help you choose a domain name](https://www.wpdoze.com/choose-domain-name/) and a domain provider:\r\n\r\n- Choose a domain name that is short, memorable, and relevant to your website’s topic or niche\r\n- Choose a domain extension that matches your website’s purpose or audience, such as .com for commercial, .org for non-profit, or .io for tech\r\n- Check the availability of your domain name using a domain search tool, such as Namecheap’s Domain Search or Cloudflare’s Domain Registration\r\n- Compare the prices and features of different domain providers, such as Namecheap’s Pricing or Cloudflare’s Pricing\r\n- Look for domain providers that offer free or discounted services\r\n  Once you have chosen a domain name and a domain provider, you can proceed to register your domain name.\r\n\r\n## How to Configure Your Domain’s DNS Settings\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nAll of this is covered in below video:\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/wwRzzLGSc2U\"\r\n  label=\"How To Add Custom Domain to Carrd.co Website\"\r\n/>\r\n\r\nThe second step to using a custom domain on Carrd.co is to configure your domain’s DNS settings. DNS stands for Domain Name System, and it is a system that translates domain names into IP addresses. IP addresses are numerical identifiers that locate devices on the internet, such as servers or computers.\r\n\r\nTo use a custom domain on Carrd.co, you need to point your domain name to Carrd.co’s IP addresses. This way, when someone types your domain name in their browser, they will be directed to your Carrd.co website. To do this, you need to add two A records and one CNAME record to your domain’s DNS settings.\r\n\r\nAn A record is a type of DNS record that maps a domain name to an IP address. A CNAME record is a type of DNS record that maps a domain name to another domain name. You need to add two A records to point your domain name and your www subdomain to Carrd.co’s IP addresses. You need to add one CNAME record to redirect your www subdomain to your domain name.\r\n\r\nTo configure your domain’s DNS settings, you need to use your domain provider’s DNS management tool. The DNS management tool is where you can add, edit, or delete DNS records for your domain. The location and interface of the DNS management tool may vary depending on the domain provider, but it usually can be accessed from your domain provider’s dashboard or control panel.\r\n\r\nHere are some examples of how to access and use the DNS management tool for some popular domain providers:\r\n\r\n- **Namecheap’s DNS Management**: Log in to your Namecheap account and go to Domain List. Click on Manage next to your domain name and select Advanced DNS. Click on Add New Record and select A Record from the drop-down menu. Enter @ in the Host field and one of Carrd.co’s IP addresses in the Value field. Repeat the same steps for the second A record. Click on Add New Record and select CNAME Record from the drop-down menu. Enter www in the Host field and your domain name in the Value field. Save the changes and wait for the DNS propagation to take effect.\r\n- **Cloudflare’s DNS Management**: Log in to your Cloudflare account and go to the dashboard. Click on the domain name that you want to manage and select DNS. Click on Add record and select A from the Type drop-down menu. Enter @ in the Name field and one of Carrd.co’s IP addresses in the IPv4 address field. Repeat the same steps for the second A record. Click on Add record and select CNAME from the Type drop-down menu. Enter www in the Name field and your domain name in the Target field. Save the changes and wait for the DNS propagation to take effect.\r\n\r\nThe IP addresses that you need to use for the A records are:\r\n\r\n151.101.1.195\r\n151.101.65.195\r\n\r\nYou can find these IP addresses in Carrd.co’s publish settings, which we will cover in the next step.\r\n\r\n## How to Publish Your Carrd.co Website with a Custom Domain\r\n\r\nThe third and final step to using a custom domain on Carrd.co is to publish your Carrd.co website with a custom domain. This is where you tell Carrd.co what domain name you want to use for your website and wait for it to initialize.\r\n\r\nTo publish your Carrd.co website with a custom domain, you need to use Carrd.co’s publish settings. The publish settings are where you can choose where and how to publish your Carrd.co website. You can access the publish settings from Carrd.co’s editor or dashboard.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nHere are the steps to publish your Carrd.co website with a custom domain:\r\n\r\n- Log in to your Carrd.co account and open the website that you want to publish\r\n- Click on the Publish icon (the disk icon) on the top right corner of the editor or dashboard\r\n- Set the Action to Publish to a custom domain\r\n- Enter your domain name exactly as you want it to appear in the address bar (either domain.ext or www.domain.ext)\r\n- Scroll down a bit to locate the site’s host records. These are the same IP addresses that you used for the A records in the previous step. You can use them to verify that you configured your domain’s DNS settings correctly.\r\n- Click on Publish Changes\r\n\r\n<Picture src={imag1} alt=\"Carrd domain add\" />\r\n\r\nWait for the domain to initialize. This is an automatic process that can take up to an hour to complete (though it’s usually a lot faster than that). Once initialized, you’ll be able to access your website using the custom domain that you just set up.\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />","src/content/posts/carrd-add-domain.mdx",[368],"../../assets/images/24/01/carrd-domain-setup.jpeg","aebe7ad45dd0d96b","carrd-add-domain.mdx","carrd-back-to-top-button",{id:371,data:373,body:382,filePath:383,assetImports:384,digest:386,legacyId:387,deferredRender:32},{title:374,description:375,date:376,image:377,authors:378,categories:379,tags:380,canonical:381},"How to Add Back To Top Button on Carrd Website","Learn how you can add a back to top on your carrd.co website.",["Date","2024-02-28T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/carrd-back-to-top-button.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-back-to-top-button/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/02/carrd-back-to-top-embed.png\";\r\n\r\n[Carrd.co](https://go.bitdoze.com/carrd) offers a good way to build one-page websites but one-page websites can have a lot of elements and in case the visitors need to go back to the top they will scroll a lot. That's an issue for most websites but this can be fixed easily with the help of a code and embed functionality in carrd.\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## How to Add Back To Top Button on Carrd Website\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/xnzKERC5SvY\"\r\n  label=\"How to Add Back To Top Button on Carrd Website\"\r\n/>\r\n\r\n### 1. Add an embed element anywhere on the website\r\n\r\nYou just need to go on the `+` sign and add an Embed element anywhere on the website. Below you will\r\n\r\n- Type: Code\r\n- Style: Hidden, Head\r\n\r\nJust as in below picture:\r\n\r\n<Picture src={imag1} alt=\"Carrd back to top\" />\r\n\r\n### 2. Use the HTML Code:\r\n\r\nBelow is the code you should use with the explination:\r\n\r\n```html\r\n<style>\r\n  html {\r\n    scroll-behavior: smooth;\r\n  }\r\n\r\n  /* CSS Variables for customization */\r\n  :root {\r\n    --button-color: #555;\r\n    --button-hover-color: #333;\r\n    --arrow-color: white;\r\n  }\r\n\r\n  #scroll-to-top {\r\n    display: none;\r\n    position: fixed;\r\n    bottom: 20px;\r\n    right: 20px;\r\n    z-index: 99;\r\n    width: 50px;\r\n    height: 50px;\r\n    background-color: var(--button-color);\r\n    color: var(--arrow-color);\r\n    border: none;\r\n    outline: none;\r\n    cursor: pointer;\r\n    border-radius: 50%;\r\n  }\r\n\r\n  #scroll-to-top::before {\r\n    content: \"\\25b2\";\r\n    font-size: 24px;\r\n    position: absolute;\r\n    top: 50%;\r\n    left: 50%;\r\n    transform: translate(-50%, -50%);\r\n  }\r\n\r\n  #scroll-to-top:hover {\r\n    background-color: var(--button-hover-color);\r\n  }\r\n</style>\r\n\r\n<button id=\"scroll-to-top\" onclick=\"scrollToTop()\"></button>\r\n\r\n<script>\r\n  window.onscroll = function () {\r\n    scrollFunction();\r\n  };\r\n\r\n  function scrollFunction() {\r\n    if (\r\n      document.body.scrollTop > 200 ||\r\n      document.documentElement.scrollTop > 200\r\n    ) {\r\n      document.getElementById(\"scroll-to-top\").style.display = \"block\";\r\n    } else {\r\n      document.getElementById(\"scroll-to-top\").style.display = \"none\";\r\n    }\r\n  }\r\n\r\n  function scrollToTop() {\r\n    document.body.scrollTop = 0;\r\n    document.documentElement.scrollTop = 0;\r\n  }\r\n</script>\r\n```\r\n\r\n**Colors**\r\n\r\n- **Background Colors:**\r\n\r\n  - Change the `--button-color` variable to modify the button's default background color.\r\n  - Change the `--button-hover-color` variable to modify the background color when you hover over the button.\r\n\r\n- **Arrow Color:**\r\n  - Change the `--arrow-color` variable to modify the color of the upward arrow within the button.\r\n\r\n**Size**\r\n\r\n- **Width & Height:**\r\n\r\n  - Find the CSS rule `#scroll-to-top` and adjust the `width` and `height` properties to control the button's size (they are currently set to 50px).\r\n\r\n- **Arrow Size:**\r\n  - Within the `#scroll-to-top::before` rule, modify the `font-size` property to adjust the size of the arrow symbol.\r\n\r\n**Position**\r\n\r\n- **Bottom & Right Offset:**\r\n\r\n  - In the `#scroll-to-top` rule, modify the `bottom` and `right` properties to control the button's distance from the bottom-right corner of the screen.\r\n\r\n  <Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\n## Conclusion\r\n\r\nThis is how easy it is to add a back to top button on your Carrd website, you can customize it easily to fit your designs.","src/content/posts/carrd-back-to-top-button.mdx",[385],"../../assets/images/24/02/carrd-back-to-top-button.jpeg","4a047d7ac421ba1a","carrd-back-to-top-button.mdx","carrd-floating-menu",{id:388,data:390,body:399,filePath:400,assetImports:401,digest:403,legacyId:404,deferredRender:32},{title:391,description:392,date:393,image:394,authors:395,categories:396,tags:397,canonical:398},"How to Add a Floating Menu to an Carrd Website","Learn how you can add an floating hamburger menu to an carrd.co website easy.",["Date","2024-08-28T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/08/carrd-floating-menu.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-floating-menu/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/02/carrd-back-to-top-embed.png\";\r\n\r\n[Carrd.co](https://go.bitdoze.com/carrd) offers a good way to build one-page websites but one-page websites can have a lot of elements and in case the visitors need to go back to the previous section can be hard without a floating hamburger menu. That's an issue for most websites but this can be fixed easily with the help of a code and embed functionality in carrd.\r\n\r\nA floating bottom right hamburger menu is a great addition to a Carrd site for several reasons:\r\n\r\n1. **It keeps the navigation easily accessible** without cluttering the main content area, especially on mobile devices with smaller screens.\r\n\r\n2. **The hamburger icon is a widely recognized symbol** for a menu, making it intuitive for users to click and access the navigation links.\r\n\r\n3. **Placing the menu at the bottom right** takes advantage of the natural thumb position for one-handed mobile use.\r\n\r\n4. **The floating behavior ensures the menu is always visible** to the user as they scroll through the page content.\r\n\r\nIn summary, a floating bottom right hamburger menu provides a clean, user-friendly way to incorporate navigation into a Carrd site while optimizing for mobile usability.\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## How to Add the Carrd Floating Hamburger Menu\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/0RPYtau6PxI\"\r\n  label=\"How to Add a Floating Menu to an Carrd Website\"\r\n/>\r\n\r\n### 1. Add an embed element anywhere on the website\r\n\r\nYou just need to go on the `+` sign and add an Embed element anywhere on the website. Below you will\r\n\r\n- Type: Code\r\n- Style: Hidden, Head\r\n\r\nJust as in below picture:\r\n\r\n<Picture src={imag1} alt=\"Carrd embed element\" />\r\n\r\n### 2. Use the HTML Code:\r\n\r\nBelow is the code you should use with the explination:\r\n\r\n```html\r\n<style>\r\n  :root {\r\n    --primary-color-ha: rgba(0, 112, 15, 0.58); /* Semi-transparent background */\r\n    --secondary-color-ha: #fff;\r\n    --font-size-base-ha: 16px;\r\n    --floating-hamburger-size: 50px;\r\n    --floating-font: inherit; /* Add your preferred font here */\r\n  }\r\n\r\n  * {\r\n    margin: 0;\r\n    padding: 0;\r\n    box-sizing: border-box;\r\n  }\r\n\r\n  body {\r\n    font-size: var(--font-size-base-ha);\r\n    font-family: var(--floating-font);\r\n  }\r\n\r\n  /* Floating Hamburger Menu */\r\n  .floating-hamburger {\r\n    position: fixed;\r\n    bottom: 20px;\r\n    right: 20px;\r\n    width: var(--floating-hamburger-size);\r\n    height: var(--floating-hamburger-size);\r\n    background-color: var(--primary-color-ha);\r\n    border-radius: 50%;\r\n    display: flex;\r\n    flex-direction: column;\r\n    justify-content: center;\r\n    align-items: center;\r\n    cursor: pointer;\r\n    z-index: 1000;\r\n  }\r\n\r\n  .floating-hamburger .floating-bar {\r\n    width: 30px;\r\n    height: 3px;\r\n    background-color: var(--secondary-color-ha);\r\n    margin: 3px 0;\r\n    transition: 0.4s;\r\n  }\r\n\r\n  /* Hide Checkbox */\r\n  #floating-menu-toggle {\r\n    display: none;\r\n  }\r\n\r\n  /* Menu Styling */\r\n  .floating-menu {\r\n    position: fixed;\r\n    bottom: calc(var(--floating-hamburger-size) + 30px);\r\n    right: 20px;\r\n    background-color: var(--primary-color-ha);\r\n    padding: 20px;\r\n    border-radius: 10px;\r\n    transform: scale(0);\r\n    transform-origin: bottom right;\r\n    transition: transform 0.3s ease-in-out;\r\n    z-index: 999;\r\n    font-family: var(--floating-font);\r\n  }\r\n\r\n  #floating-menu-toggle:checked ~ .floating-menu {\r\n    transform: scale(1);\r\n  }\r\n\r\n  .floating-menu ul {\r\n    list-style: none;\r\n  }\r\n\r\n  .floating-menu li {\r\n    margin: 15px 0;\r\n  }\r\n\r\n  .floating-menu li a {\r\n    color: var(--secondary-color-ha);\r\n    text-decoration: none;\r\n    font-size: 1em;\r\n    transition: font-size 0.3s ease;\r\n  }\r\n\r\n  .floating-menu li a:hover {\r\n    font-size: 1.1em;\r\n  }\r\n\r\n  .floating-close-button {\r\n    position: absolute;\r\n    top: 10px;\r\n    right: 10px;\r\n    background: none;\r\n    border: none;\r\n    color: var(--secondary-color-ha);\r\n    font-size: 1.2em;\r\n    cursor: pointer;\r\n    font-family: var(--floating-font);\r\n  }\r\n\r\n  /* Responsive Text Scaling */\r\n  @media (max-width: 768px) {\r\n    body {\r\n      font-size: calc(var(--font-size-base-ha) * 0.9);\r\n    }\r\n  }\r\n\r\n  @media (max-width: 480px) {\r\n    body {\r\n      font-size: calc(var(--font-size-base-ha) * 0.8);\r\n    }\r\n  }\r\n</style>\r\n\r\n<input type=\"checkbox\" id=\"floating-menu-toggle\" />\r\n<label for=\"floating-menu-toggle\" class=\"floating-hamburger\">\r\n  <span class=\"floating-bar\"></span>\r\n  <span class=\"floating-bar\"></span>\r\n  <span class=\"floating-bar\"></span>\r\n</label>\r\n<nav class=\"floating-menu\">\r\n  <button class=\"floating-close-button\">X</button>\r\n  <ul>\r\n    <li><a href=\"#\">Home</a></li>\r\n    <li><a href=\"#about\">About</a></li>\r\n    <li><a href=\"#testimonials\">Testimonials</a></li>\r\n    <li><a href=\"#contact\">Contact</a></li>\r\n  </ul>\r\n</nav>\r\n\r\n<script>\r\n  document\r\n    .querySelector(\".floating-close-button\")\r\n    .addEventListener(\"click\", function () {\r\n      document.getElementById(\"floating-menu-toggle\").checked = false;\r\n    });\r\n\r\n  document.querySelectorAll(\".floating-menu a\").forEach((link) => {\r\n    link.addEventListener(\"click\", function () {\r\n      document.getElementById(\"floating-menu-toggle\").checked = false;\r\n    });\r\n  });\r\n</script>\r\n```\r\n\r\nBelow are the details so you can understand what needs tp be changed:\r\n\r\n1. `--primary-color-ha: rgba(0, 112, 15, 0.58);`\r\n\r\n   - This sets the primary color to a semi-transparent green.\r\n   - The `rgba()` function allows for color definition with alpha transparency.\r\n   - The values are: Red (0), Green (112), Blue (15), and Alpha (0.58 or 58% opaque).\r\n   - you can use [https://rgbacolorpicker.com/](https://rgbacolorpicker.com/) to choose your color.\r\n\r\n2. `--secondary-color-ha: #fff;`\r\n\r\n   - This sets the secondary color to white.\r\n   - `#fff` is a shorthand hexadecimal color code for white.\r\n\r\n3. `--font-size-base-ha: 16px;`\r\n\r\n   - This establishes the base font size for the document as 16 pixels.\r\n\r\n4. `--floating-hamburger-size: 50px;`\r\n\r\n   - This defines the size of the floating hamburger menu button as 50 pixels.\r\n\r\n5. `--floating-font: inherit;`\r\n\r\n   - This sets the font for the floating elements to inherit from their parent.\r\n   - The comment suggests replacing `inherit` with a preferred font family.\r\n\r\n6. `.floating-hamburger .floating-bar -  width: 30px;`\r\n   - this is modifying the hamburger lines size in case you need a different size.\r\n\r\nThese variables can be easily modified to change the overall look of the menu without having to search through the entire stylesheet. For example, to change the primary color, you would only need to update the `--primary-color` value here, and it would affect all elements using this variable throughout the stylesheet.\r\n\r\n{\" \"}\r\n\r\n<Button link=\"https://go.bitdoze.com/carrd\" text=\"Carrd.co\" />\r\n\r\n### 3. Change the menu items\r\n\r\n```html\r\n<ul>\r\n  <li><a href=\"#\">Home</a></li>\r\n  <li><a href=\"#about\">About</a></li>\r\n  <li><a href=\"#testimonials\">Testimonials</a></li>\r\n  <li><a href=\"#contact\">Contact</a></li>\r\n</ul>\r\n```\r\n\r\nThis is the code responsible for adding the items in the menu. You can add the items you need to have a complete menu. just add an `<li><a href=\"#mything\">MyThing</a></li>` for every menu item you need in carrd.\r\n\r\n## Conclusion\r\n\r\nThis floating menu not only provides an elegant way to navigate your single-page website but also adds a touch of interactivity that can engage visitors and improve their overall experience. The customizable nature of the code allows you to tailor the menu's appearance to match your site's design, ensuring a cohesive look and feel.\r\n\r\nRemember, while Carrd.co excels in simplicity, it doesn't mean your site has to be basic. By incorporating custom elements like this floating menu, you're taking full advantage of Carrd's flexibility while maintaining its core benefits of speed and ease of use.","src/content/posts/carrd-floating-menu.mdx",[402],"../../assets/images/24/08/carrd-floating-menu.jpeg","3d8f9cd9be5f802b","carrd-floating-menu.mdx","carrd-mobile-navbar",{id:405,data:407,body:416,filePath:417,assetImports:418,digest:420,legacyId:421,deferredRender:32},{title:408,description:409,date:410,image:411,authors:412,categories:413,tags:414,canonical:415},"How to Create A Carrd.co Mobile Responsive Navbar","Learn how you can add a Responsive Navbar to your carrd website that has a mobile toggle.",["Date","2024-02-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/carrd-mobile-navbar.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-mobile-navbar/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[Carrd.co](https://try.carrd.co/bitdoze) is a platform that allows you to create simple, responsive, and beautiful websites with just a few clicks. You can choose from a variety of templates, customize them with your own content, and publish them online for free or with a premium plan.\r\n\r\nOne of the features that Carrd.co offers is the ability to create a header for your website, which is the top section that usually contains your logo, navigation menu, and other elements. A header can help your visitors navigate your website and access the most important information.\r\n\r\nHowever, creating a header that looks good on both desktop and mobile devices can be challenging, especially if you want to have a hamburger menu, which is a common design pattern for responsive websites. A hamburger menu is a button that consists of three horizontal lines, and when clicked, it reveals a hidden menu with more options.\r\n\r\nIn this article, we will show you how to create a responsive header with a mobile navbar in Carrd.co using an embed widget and custom code option.\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/PkZ7CYfKxSg\"\r\n  label=\"How to Create A Carrd.co Mobile Responsive Navbar\"\r\n/>\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [Carrd.co Review](https://www.bitdoze.com/carrd-review/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [Back To Top Button on Carrd](https://www.bitdoze.com/carrd-back-to-top-button/)\r\n\r\n> The responsive navigation menu will need **Pro Standard** plan as we are going to use a simple HTML and CSS code for this.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\n## 1. Create a Container\r\n\r\nThe header that will have the navigation menu will need to be inserted in a container, so you will need to add a container with 2 columns for this. In the first column, you will add you logo and in the second column, you will add the embed code.\r\n\r\nYou can split them as 25% - 75% and choose what background you want for your header, this will not impact the responsive nav bar. Check the video where I go in detail.\r\n\r\n## 2. Add an Embed Widget\r\n\r\nIn the second column of the container, you will need to add the embed widget that will hold the code. You just assign a name.\r\n\r\n## 3. Add the HTML/CSS code\r\n\r\nBelow is the code that we are going to use to create the responsive navbar and menu items:\r\n\r\n```html\r\n<style>\r\n  :root {\r\n    --mbm-main-font: inherit;\r\n    --mbm-font-size-base: 18px;\r\n    --primary-color: #200eed;\r\n    --secondary-color: #fff;\r\n  }\r\n\r\n  /* Basic Reset and Body Styling */\r\n  * {\r\n    margin: 0;\r\n    padding: 0;\r\n    box-sizing: border-box;\r\n  }\r\n\r\n  /* Navigation Styling */\r\n  nav {\r\n    color: var(--secondary-color);\r\n    padding: 15px;\r\n    display: flex;\r\n    align-items: center; /* Changed: Align items to the center vertically */\r\n    justify-content: flex-end; /* Changed: Align items to the start (left) */\r\n    align-content: flex-end; /* Not strictly necessary for this change */\r\n    font-family: var(--mbm-main-font);\r\n    font-size: var(--mbm-font-size-base);\r\n  }\r\n\r\n  .menu {\r\n    list-style: none;\r\n    display: flex;\r\n  }\r\n\r\n  .menu li a {\r\n    display: block;\r\n    color: var(--secondary-color);\r\n    text-decoration: none;\r\n    padding: 10px 15px;\r\n    position: relative;\r\n  }\r\n\r\n  .menu li a::after {\r\n    /* Target a pseudo-element for the underline */\r\n    content: \"\";\r\n    position: absolute;\r\n    bottom: 2px; /* Distance from the text */\r\n    left: 50%; /* Center the underline */\r\n    width: 0; /* Initial width is 0 */\r\n    height: 2px;\r\n    background-color: var(--secondary-color);\r\n    transition: width 0.3s ease-in-out; /* Control the transition effect */\r\n  }\r\n\r\n  .menu li a:hover::after {\r\n    width: 100%; /* Expand to full width on hover */\r\n    left: 0; /* Start the underline from the left */\r\n  }\r\n\r\n  /* Hover effect for submenu items */\r\n  /* Hamburger Styling */\r\n  .hamburger {\r\n    display: none;\r\n    cursor: pointer;\r\n  }\r\n\r\n  .hamburger .bar {\r\n    display: block;\r\n    width: 25px;\r\n    height: 3px;\r\n    background-color: var(--secondary-color);\r\n    margin: 5px 0;\r\n  }\r\n\r\n  /* Hide Checkbox */\r\n  #menu-toggle {\r\n    display: none;\r\n  }\r\n\r\n  .close-button {\r\n    display: none;\r\n  }\r\n\r\n  /* Responsive - Media Queries (For Mobile) */\r\n  @media (max-width: 768px) {\r\n    /* Show the Hamburger */\r\n    .hamburger {\r\n      display: block;\r\n    }\r\n\r\n    /* Hide and Modify Menu on Mobile */\r\n    .menu {\r\n      position: absolute;\r\n      width: 100%;\r\n      top: calc(15px + 6em);\r\n      left: 0;\r\n      background-color: var(--primary-color);\r\n      text-align: center;\r\n      padding: 40px 0 0 0;\r\n      display: none;\r\n      z-index: 999;\r\n    }\r\n\r\n    .menu li {\r\n      width: 100%;\r\n    }\r\n\r\n    .menu li a {\r\n      padding: 15px;\r\n      border-bottom: 1px solid rgba(255, 255, 255, 0.1);\r\n    }\r\n\r\n    #menu-toggle:checked ~ .menu {\r\n      display: block;\r\n    }\r\n    .close-button {\r\n      display: block;\r\n      position: absolute;\r\n      top: 20px;\r\n      right: 30px;\r\n      background: none;\r\n      border: none;\r\n      font-size: 18px;\r\n      color: var(--secondary-color);\r\n      cursor: pointer;\r\n    }\r\n  }\r\n</style>\r\n\r\n<nav>\r\n  <input type=\"checkbox\" id=\"menu-toggle\" />\r\n  <label for=\"menu-toggle\" class=\"hamburger\">\r\n    <span class=\"bar\"></span>\r\n    <span class=\"bar\"></span>\r\n    <span class=\"bar\"></span>\r\n  </label>\r\n  <ul class=\"menu\">\r\n    <button class=\"close-button\">X</button>\r\n    <li><a href=\"#\">Home</a></li>\r\n    <li><a href=\"#about\">About</a></li>\r\n    <li><a href=\"#testimonials\">Testimonials</a></li>\r\n    <li><a href=\"#contact\">Contact</a></li>\r\n  </ul>\r\n</nav>\r\n<script>\r\n  document\r\n    .querySelector(\".close-button\")\r\n    .addEventListener(\"click\", function () {\r\n      document.getElementById(\"menu-toggle\").checked = false;\r\n    });\r\n  document\r\n    .querySelector(\".close-button\")\r\n    .addEventListener(\"click\", function () {\r\n      document.getElementById(\"menu-toggle\").checked = false;\r\n    });\r\n\r\n  // Close menu on menu item click\r\n  document.querySelectorAll(\".menu a\").forEach((link) => {\r\n    link.addEventListener(\"click\", function () {\r\n      document.getElementById(\"menu-toggle\").checked = false;\r\n    });\r\n  });\r\n</script>\r\n```\r\n\r\n**Code Explanation**\r\n\r\n- The `:root` selector defines CSS variables for font, colors, and base font size. These variables make it easy to customize the appearance of the navbar.\r\n  - `--mbm-main-font`: Inherits the font from the main Carrd theme.\r\n  - `--mbm-font-size-base`: Sets the base font size for the menu items.\r\n  - `--primary-color`: Defines the color for the mobile menu background.\r\n  - `--secondary-color`: Sets the color for text and icons.\r\n\r\n- The `nav` element is styled to align items to the center vertically and to the right horizontally, creating a clean and modern look.\r\n\r\n- The `.menu` class styles the list of navigation items, displaying them in a row on desktop.\r\n\r\n- Each menu item (`<a>` inside `<li>`) has a hover effect that creates an underline animation when hovered over.\r\n\r\n- The hamburger menu (`.hamburger`) is hidden by default and only appears on mobile screens.\r\n\r\n- Media queries for screens smaller than 768px wide:\r\n  - The hamburger menu becomes visible.\r\n  - The menu items stack vertically and are hidden by default.\r\n  - A checkbox (`#menu-toggle`) is used to toggle the mobile menu's visibility.\r\n  - A close button is added to the mobile menu for better user experience.\r\n\r\n- The `<nav>` element contains the menu structure:\r\n  ```html\r\n  <ul class=\"menu\">\r\n    <button class=\"close-button\">X</button>\r\n    <li><a href=\"#\">Home</a></li>\r\n    <li><a href=\"#about\">About</a></li>\r\n    <li><a href=\"#testimonials\">Testimonials</a></li>\r\n    <li><a href=\"#contact\">Contact</a></li>\r\n  </ul>\r\n  ```\r\n  You can modify the `href` attributes and text content to match your Carrd sections.\r\n\r\n- The JavaScript at the end adds functionality to close the mobile menu when the close button is clicked or when a menu item is selected.\r\n\r\nTo customize the navbar:\r\n1. Adjust the color variables in the `:root` selector to match your Carrd theme.\r\n2. Modify the menu items in the `<ul class=\"menu\">` to reflect your site's structure.\r\n3. If needed, adjust the `--mbm-font-size-base` to change the text size of menu items.\r\n\r\nThis responsive navbar provides a sleek design for desktop views and a user-friendly mobile menu, enhancing navigation across all device sizes.\r\n\r\nThe tutorial made on how you can [add a sticky header to carrd](https://www.bitdoze.com/add-stickey-header-carrd/) is working perfectly with this tutorial so you can use both.\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />","src/content/posts/carrd-mobile-navbar.mdx",[419],"../../assets/images/24/02/carrd-mobile-navbar.jpeg","5d74c046ceb31d9e","carrd-mobile-navbar.mdx","carrd-review",{id:422,data:424,body:433,filePath:434,assetImports:435,digest:437,legacyId:438,deferredRender:32},{title:425,description:426,date:427,image:428,authors:429,categories:430,tags:431,canonical:432},"Carrd.co Review: The Best Budget Landing Page Builder","Carrd.co is the perfect tool for creating a simple yet effective landing page. Read our review to learn more about its features, pricing, and pros and cons.",["Date","2023-05-23T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/05/carrd_review.jpeg",[19],[21],[43],"https://www.bitdoze.com/carrd-review/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nAs businesses and individuals continue to shift towards online platforms, having a professional and user-friendly website is becoming increasingly important. However, not everyone can afford to pay for expensive web developers or designers. This is where Carrd.co comes in. In this review, we'll take a closer look at Carrd.co, its features, pricing, pros and cons, as well as its competitors.\r\n\r\n## What is Carrd.co?\r\n\r\n[Carrd.co](https://try.carrd.co/bitdoze) is a platform that allows users to create single-page, responsive websites with ease. Perfect for personal profiles, business landing pages, and more, Carrd has become a go-to choice for those seeking a simple yet effective solution for their online presence. In this section, we'll take a closer look at what sets Carrd apart from its competitors.\r\n\r\nSome Carrd Tutorials:\r\n\r\n- [Add Stickey Header Carrd](https://www.bitdoze.com/add-stickey-header-carrd/)\r\n- [Add Carrd Cookie Notice](https://www.bitdoze.com/add-cookie-notice-carrd/)\r\n- [How To Add Pricing Table to Carrd.co](https://www.bitdoze.com/carrd-add-pricing-table/)\r\n- [How To Add Accordion FAQs Drop-Down to Carrd.co](https://www.bitdoze.com/add-accordion-carrd/)\r\n- [How To Add Custom Domain to Carrd.co](https://www.bitdoze.com/carrd-add-domain/)\r\n- [Carrd.co Mobile Responsive Navbar](https://www.bitdoze.com/carrd-mobile-navbar/)\r\n- [Back To Top Button on Carrd](https://www.bitdoze.com/carrd-back-to-top-button/)\r\n\r\n> The complete list with Carrd plugins, themes and tutorials you can find on my **[carrdme.com](https://carrdme.com/)** website.\r\n\r\n## Carrd.co Video Review\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/nFnfaQI-nt0\"\r\n  label=\"Carrd.co Review: The Best Budget Landing Page Builder\"\r\n/>\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n## Carrd.co Features\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />\r\n\r\nCarrd offers a variety of features that make it an attractive choice for building a budget-friendly landing page. In this section, we'll discuss some of the standout features that make Carrd a compelling choice for your next landing page project.\r\n\r\n- A drag-and-drop interface that makes it easy to customize your website\r\n- 100+ templates to choose from, including options for bios, portfolios, and more\r\n- Integration with popular payment processors like PayPal and Stripe\r\n- Custom domains\r\n- SEO optimization tools\r\n- A built-in form builder that can be connected with (ActiveCampaign, Beehiiv, Buttondown, ConvertKit, EmailOctopus, GetResponse, Klaviyo, Mailchimp, MailerLite, SendFox, Sendinblue, and Sendy)\r\n- Responsive design for mobile devices\r\n- Google Analytics\r\n\r\n## Carrd.co Pricing\r\n\r\nCarrd.co offers a range of pricing options, including a free plan and three paid plans:\r\n\r\n- **Free**: Includes basic features and a Carrd.co subdomain\r\n- **Pro Lite**: $9/year for a premium domains and more features\r\n- **Pro Standard**: $19/year for more templates and advanced features\r\n- **Pro Plus**: $49/year for even more customization options and features\r\n\r\n## Carrd.co Pros and Cons\r\n\r\nLike any website builder, Carrd.co has its pros and cons. Here are a few to consider:\r\n\r\n### Pros:\r\n\r\n- Affordable pricing options, including a free plan\r\n- Easy-to-use drag-and-drop interface\r\n- Professional-looking templates\r\n- Responsive design for mobile devices\r\n- A built-in form builder and integration with popular payment processors\r\n\r\n### Cons:\r\n\r\n- Limited customization options compared to other website builders\r\n- Only supports one-page websites\r\n- No e-commerce features\r\n- Limited SEO optimization tools\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n## Carrd Alternatives and Competitors\r\n\r\nWhile Carrd is an excellent option for many users, it's essential to consider other alternatives in the market. In this section, we'll compare Carrd to some of its main competitors to help you decide which platform is the best fit for your needs.\r\n\r\n### Carrd vs. Webflow\r\n\r\nWebflow is a powerful web design tool that offers more advanced features and customization options than Carrd, but it also comes with a steeper learning curve and higher pricing.\r\n\r\n### Carrd vs. Wix\r\n\r\nWix is a popular website builder with a wide range of features, but it may not be the best choice for users seeking a simple and budget-friendly landing page solution like Carrd.\r\n\r\n### Carrd vs. WordPress\r\n\r\nWordPress is a versatile and widely-used platform, but it can be more complex than Carrd and may not be the best choice for users seeking a straightforward landing page builder. WordPress sites need constant maintenance and updates and even if WordPress is free you need to pay for hosting.\r\n\r\n### Carrd vs. Squarespace\r\n\r\nSquarespace is known for its beautiful templates and robust features, but its pricing structure can be less budget-friendly than Carrd's. For a basic site Squarespace can be quite expensive.\r\n\r\n### Carrd vs. Framer\r\n\r\nFramer is a powerful design and prototyping tool, but it may be overkill for users seeking a simple, budget-friendly landing page builder like Carrd. Framer can help you build very nice websites but the price is higher and is more suitable for complex things.\r\n\r\n## Conclusion\r\n\r\nIn conclusion, Carrd.co offers a budget-friendly, easy-to-use solution for creating landing pages and single-page websites. While it may not have all the features of its more expensive competitors, it's an excellent option for users who need a simple, reliable, and affordable platform to build their online presence.\r\n\r\n## FAQ\r\n\r\n### Is Carrd good for blogs?\r\n\r\nWhile Carrd is an excellent platform for creating landing pages and single-page websites, it may not be the best choice for a full-fledged blog due to its limited content management capabilities.\r\n\r\n### Is Carrd a good portfolio website builder?\r\n\r\nCarrd is a great choice for creating simple, visually appealing portfolio websites on a budget, especially for freelancers and creative professionals.\r\n\r\n### Does Carrd offer a free trial?\r\n\r\nYes, Carrd offers a free tier with limited features, allowing users to explore the platform and create a basic website without any financial commitment.\r\n\r\n### Is Carrd reliable?\r\n\r\nCarrd has a reputation for being reliable, with solid performance and uptime, making it a dependable choice for your landing page or single-page website.\r\n\r\n<Button link=\"https://try.carrd.co/bitdoze\" text=\"Carrd.co\" />\r\n\r\n<Button link=\"https://carrdme.com/\" text=\"Carrd Plugins and Themes\" />","src/content/posts/carrd-review.mdx",[436],"../../assets/images/23/05/carrd_review.jpeg","d8ac1827c2510d84","carrd-review.mdx","check-remote-port-in-linux-nc",{id:439,data:441,body:451,filePath:452,assetImports:453,digest:455,legacyId:456,deferredRender:32},{title:442,description:443,date:444,image:445,authors:446,categories:447,tags:448,canonical:450},"How to Check Remote Ports are Reachable Using ‘nc’ Command","Learn how to check if remote ports are opened in linux with nc (netcat) command.",["Date","2023-11-22T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/check-remote-port-in-linux-nc.jpeg",[19],[98],[449],"linux-commands","https://www.bitdoze.com/check-remote-port-in-linux-nc/","The `nc` command, also known as netcat, is a powerful tool that allows you to check if remote ports are reachable on a network. In this article, we will explore how to use the `nc` command to verify the accessibility of remote ports. By following these simple steps, you'll be able to quickly determine whether a specific port is open and ready for connections using the versatile `nc` command. So, let's dive in and discover how to check remote ports are reachable using the `nc` command.\r\n\r\nHave you ever wondered if a particular port on a remote server is accessible? The `nc` command comes to your rescue! With just a few easy commands, you can test whether or not certain ports are open and available for communication over the network. Whether you're troubleshooting connectivity issues or performing security audits, understanding how to utilize the `nc` command effectively can save you valuable time and effort. So without further ado, let's learn how to use the 'nc' command to check if remote ports are reachable and ensure smooth communication across your network infrastructure.\r\n\r\n## What is the 'nc' command?\r\n\r\nThe 'nc' command, short for netcat, is a powerful utility that allows you to establish network connections using TCP or UDP protocols. It acts as both a client and a server, making it an extremely versatile tool for network troubleshooting and testing.\r\n\r\nHere are some key features of the 'nc' command:\r\n\r\n1. **Port Scanning**: With 'nc', you can quickly scan remote hosts to check if specific ports are open or closed. This can be useful in identifying potential vulnerabilities in your network.\r\n\r\n2. **Listening Mode**: By running 'nc' in listening mode, you can create a simple server that waits for incoming connections on a specified port. This makes it easy to test how services respond when accessed from other devices.\r\n\r\n3. **Data Transfer**: The 'nc' command enables data transfer between two hosts over the network. You can send files, stream audio or video, or even use it as a basic chat tool by typing messages back and forth.\r\n\r\n4. **Proxying Connections**: Using 'nc', you can redirect traffic from one host to another by acting as an intermediary proxy server. This feature is handy when debugging complex networking setups or analyzing network traffic.\r\n\r\n5. **Banner Grabbing**: When connected to certain servers like HTTP web servers, 'nc' allows you to retrieve information about the service's version or configuration by reading the initial response sent by the server upon connection establishment.\r\n\r\nOverall, the versatility of the 'nc' command makes it an essential tool for any sysadmin or network engineer who needs fine-grained control over their networking tasks without relying on heavyweight software solutions.\r\n\r\n> Note: The availability and functionality of specific options may vary slightly depending on different operating systems and versions of netcat implementations used.\r\n\r\n## Checking remote ports with the 'nc' command\r\n\r\nThe `nc` (netcat) command is a useful tool for checking if remote ports are reachable. It allows you to establish TCP or UDP connections and send data over them. Here's how you can use it to check the status of remote ports:\r\n\r\nFirst, you should install netcat to be able to use it:\r\n\r\n```shell\r\nyum install nc                  [On CentOS/RHEL]\r\ndnf install nc                  [On Fedora 22+]\r\nsudo apt-get install netcat     [On Debian/Ubuntu]\r\n```\r\n\r\n1. Open your terminal or command prompt.\r\n2. Type the following command, replacing `<host>` with the IP address or hostname of the target system, and `<port>` with the port number you want to check:\r\n\r\n```shell\r\nnc -zv <host> <port>\r\n```\r\n\r\n3. Press Enter to execute the command.\r\n\r\nThe `-z` option tells `nc` not to send any data after establishing a connection, while `-v` enables verbose output so that you can see detailed information about the connection attempt.\r\n\r\nHere's what each part of the output means:\r\n\r\n- If you see `Connection to <host> <port> port [tcp/udp] succeeded!`, it means that the specified port on the target system is open and reachable.\r\n- If you see `Ncat: Connection refused`, it indicates that no service is listening on that particular port of the target system.\r\n- If you see `Ncat: Operation timed out`, it suggests that there may be network connectivity issues preventing access to that port.\r\n\r\n**Examples:**\r\n\r\n**Successful connection for SMTP check:**\r\n\r\n```shell\r\nnc -vz smtp.gmail.com 587\r\nConnection to smtp.gmail.com port 587 [tcp/submission] succeeded!\r\n```\r\n\r\n**Failed connection:**\r\n\r\n```shell\r\nnc -vz smtp.gmail.com 5555\r\nnc: connectx to smtp.gmail.com port 5555 (tcp) failed: Connection refused\r\n```\r\n\r\nYou can also specify a range of ports using hyphens (`<start-port>-<end-port>`), like this:\r\n\r\n```\r\nnc -zv <host> <start-port>-<end-port>\r\n```\r\n\r\n**Example:**\r\n\r\n```shell\r\nnc -vz smtp.gmail.com 585-590\r\nnc: connectx to smtp.gmail.com port 585 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 585 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 586 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 586 (tcp) failed: Connection refused\r\nConnection to smtp.gmail.com port 587 [tcp/submission] succeeded!\r\nnc: connectx to smtp.gmail.com port 588 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 588 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 589 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 589 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 590 (tcp) failed: Connection refused\r\nnc: connectx to smtp.gmail.com port 590 (tcp) failed: Connection refused\r\n```\r\n\r\nThis will check multiple ports within a specified range.\r\n\r\nUsing these simple steps, you can quickly determine whether specific remote ports are accessible from your local machine. This knowledge is valuable for troubleshooting network connectivity issues or verifying if certain services are running on a server without having direct access to it.\r\n\r\nRemember, always exercise caution when performing any kind of network scanning activity, as unauthorized port scanning may be against the policies of some networks and could potentially lead to legal consequences.\r\n\r\n## Troubleshooting common issues\r\n\r\nHere are some common issues you may encounter when using the 'nc' command to check remote ports and how to troubleshoot them:\r\n\r\n1. **Connection refused error**: If you receive a \"connection refused\" error, it means that the target port is closed or not listening for incoming connections. To resolve this issue, ensure that the remote host is running the service on the specified port and that any firewalls or security settings allow inbound connections.\r\n\r\n2. **Host unreachable**: When you get a \"host unreachable\" error, it indicates that there is no network route to reach the destination host. Double-check your network configuration, including IP addresses, subnet masks, and gateways. Make sure both your local machine and the remote host have proper connectivity.\r\n\r\n3. **Timeout while waiting for connection**: A timeout error suggests that either the target IP address or port number is incorrect or blocked by a firewall. Verify that you have entered the correct details for both IP address and port number in your 'nc' command. Additionally, confirm if any firewalls are blocking outgoing connections from your machine.\r\n\r\n4. **No response received**: If you do not receive any response after executing an 'nc' command, it could indicate various issues such as network congestion or a misconfiguration of services on either end (local/remote). Ensure both hosts are reachable from each other by pinging them first before attempting to connect with 'nc'.\r\n\r\n5. **Firewall restrictions**: Firewalls can often interfere with establishing connections using 'nc'. Check if there are any rules in place restricting outbound/inbound traffic on either end (local/remote). Adjust firewall settings accordingly to allow communication through desired ports.\r\n\r\nRemember to consider these troubleshooting steps whenever encountering problems while checking remote ports using the 'nc' command.\r\n\r\n| Error                                | Possible Cause                                     | Solution                                                             |\r\n| ------------------------------------ | -------------------------------------------------- | -------------------------------------------------------------------- |\r\n| Connection refused                   | Target port closed/not listening                   | Ensure service is running and no firewall blocks inbound connections |\r\n| Host unreachable                     | Network configuration issue                        | Verify IP addresses, subnet masks, and gateways                      |\r\n| Timeout while waiting for connection | Incorrect target details or blocked by a firewall  | Double-check IP address/port number and confirm firewall settings    |\r\n| No response received                 | Network congestion or misconfiguration of services | Ping both hosts to verify reachability before using 'nc' command     |\r\n| Firewall restrictions                | Rules blocking communication on either end         | Adjust firewall settings to allow desired port communication         |\r\n\r\nThese troubleshooting tips will help you diagnose and resolve common issues when using the 'nc' command for checking remote ports.\r\n\r\n## Conclusion\r\n\r\nIn conclusion, the `nc` command is a powerful tool that allows you to check if remote ports are reachable. By using this command, you can quickly and efficiently test network connectivity between different devices.\r\n\r\nThroughout this article, we have explored how to use the `nc` command in various scenarios. We have learned how to check if a specific port is open on a remote server, as well as how to scan multiple ports simultaneously. Additionally, we have discussed some common troubleshooting tips and potential issues that may arise when using the `nc` command.\r\n\r\nRemember that understanding the status of your network ports is crucial for maintaining smooth communication between devices. The `nc` command provides an easy-to-use solution for verifying port accessibility without the need for complex software or extensive configuration.\r\n\r\nBy utilizing the knowledge gained from this article, you can confidently diagnose connectivity problems and ensure efficient data transfer across your network infrastructure. So go ahead and start exploring the power of the `nc` command – it's time to take control of your remote port checking!","src/content/posts/check-remote-port-in-linux-nc.mdx",[454],"../../assets/images/23/11/check-remote-port-in-linux-nc.jpeg","6b60265e1898df3f","check-remote-port-in-linux-nc.mdx","clean-docker-overlay2-dir",{id:457,data:459,body:468,filePath:469,assetImports:470,digest:472,legacyId:473,deferredRender:32},{title:460,description:461,date:462,image:463,authors:464,categories:465,tags:466,canonical:467},"Reclaim Disk Space by Cleaning Up /var/lib/docker/overlay2","Learn how to free up disk space on your Docker host by cleaning up the /var/lib/docker/overlay2 directory.",["Date","2024-05-23T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/05/free-disk-space-docker-overlay2.jpeg",[19],[98],[100],"https://www.bitdoze.com/clean-docker-overlay2-dir/","The `/var/lib/docker/overlay2` directory is where Docker stores the layers and metadata for all the Docker images and containers on your system. Over time, this directory can grow significantly in size, especially if you have many Docker images and containers, or if you frequently build and rebuild images. A large `/var/lib/docker/overlay2` directory can consume a significant amount of disk space, potentially leading to performance issues or even system crashes if the disk becomes full.\r\n\r\n## What you have in `/var/lib/docker/overlay2`\r\n\r\nThe `/var/lib/docker/overlay2` directory contains several subdirectories, each representing a Docker image or container layer. These subdirectories are named using a unique identifier (a hash value) that corresponds to the specific layer. Inside each subdirectory, you'll find files and directories that make up the layer's contents, such as the filesystem, metadata, and configuration files.\r\n\r\n## Reasons for `/var/lib/docker/overlay2` using a lot of space\r\n\r\nThere are several reasons why the `/var/lib/docker/overlay2` directory can grow large:\r\n\r\n1. **Unused Images and Containers**: When you build or pull Docker images, or create and stop containers, their layers are stored in the `/var/lib/docker/overlay2` directory. If you don't remove these unused images and containers, they will continue to occupy disk space.\r\n\r\n2. **Cached Layers**: Docker caches image layers to speed up the build and deployment process. However, these cached layers can accumulate over time, consuming disk space.\r\n\r\n3. **Large Images and Containers**: Some Docker images and containers can be quite large, especially if they include large application binaries, libraries, or data files.\r\n\r\n4. **Inefficient Image Layers**: If Docker images are not built efficiently, with unnecessary files or layers, they can consume more disk space than necessary.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## Check the space used\r\n\r\n1. Check the space that `/var/lib/docker/overlay2` is using:\r\n\r\n```sh\r\ndu -sh /var/lib/docker/overlay2\r\n```\r\n\r\nThis command uses the `du` (disk usage) utility to display the total disk space used by the `/var/lib/docker/overlay2` directory and its contents. The `-s` option summarizes the total space used, and the `-h` option displays the size in human-readable format (e.g., KB, MB, GB).\r\n\r\n2. Check what is using the most space (top 5):\r\n\r\n```sh\r\nroot@python-server:~# du -sh /var/lib/docker/overlay2/* | sort -hr | head -5\r\n3.6G    /var/lib/docker/overlay2/b5b5b1b4abef270dbc91bdd6c385865d778b6bd8ee9568030635994a296533ab\r\n2.5G    /var/lib/docker/overlay2/89048c0675f730b36077f7b967754ec94efead1d7d45b35870131f72bf412ff3\r\n610M    /var/lib/docker/overlay2/dc123594104a9a124bfb4def0780c9e2e65205abf366ea37128663cc7020e444\r\n559M    /var/lib/docker/overlay2/ac6efc422ab6e481c58b22d13e6d636d0c649bf58b0ef1bd2d9e01a01c5cbd8c\r\n512M    /var/lib/docker/overlay2/4052cb983b6bb7bbe28a6dff67fd1833eb87d4ee9a606b26d925c50e7a823b31\r\n```\r\n\r\nThis command lists the disk space used by each subdirectory (layer) inside `/var/lib/docker/overlay2`. It uses the `du` command to get the size of each subdirectory, sorts the output in descending order (`sort -hr`), and displays the top 5 entries (`head -5`). This helps identify which specific layers or images are consuming the most disk space.\r\n\r\n## Cleanup `/var/lib/docker/overlay2`\r\n\r\n**1. Remove all unused containers, networks, images, and volumes:**\r\n\r\n```sh\r\ndocker system prune -a -f\r\n```\r\n\r\nThe `docker system prune` command removes unused Docker objects, such as containers, networks, images, and volumes. The `-a` option specifies that all unused objects should be removed, and the `-f` option forces the removal without prompting for confirmation.\r\n\r\nThis command is a good starting point for cleaning up the `/var/lib/docker/overlay2` directory, as it removes any unused Docker objects that are no longer needed, freeing up disk space.\r\n\r\n**2. Check `/var/lib/docker/overlay2` and see what image is using the space:**\r\n\r\nI have created an util script that can be used to check this you can do that with:\r\n\r\n```sh\r\ncurl -sSL https://utils.bitdoze.com/scripts/docker-overlay2-view.sh | bash\r\n```\r\n\r\nAll the scripts an be found under: [utils.bitdoze.com](https://utils.bitdoze.com/)\r\n\r\nThe script is below:\r\n\r\n```sh\r\n#!/bin/bash\r\n\r\n# Define the overlay2 directory\r\noverlay_dir=\"/var/lib/docker/overlay2\"\r\n\r\n# Create a temporary file to store the results\r\ntemp_file=$(mktemp)\r\n\r\n# Array to store used subfolders\r\nused_subfolders=()\r\n\r\n# Loop through each Docker image ID\r\nfor image_id in $(docker image ls -q); do\r\n  # Inspect the image to get detailed information\r\n  image_info=$(docker inspect \"$image_id\")\r\n\r\n  # Get the image name (repository and tag)\r\n  repo_tag=$(docker inspect --format '{{if .RepoTags}}{{index .RepoTags 0}}{{else}}<none>{{end}}' \"$image_id\")\r\n\r\n  # Check all subfolders in the overlay2 directory\r\n  while IFS= read -r -d '' subfolder; do\r\n    if echo \"$image_info\" | grep -q \"$subfolder\"; then\r\n      # Get the space used by the subfolder\r\n      space_used=$(du -sh \"$subfolder\" | cut -f1)\r\n\r\n      # Convert space used to bytes for sorting\r\n      space_used_bytes=$(du -sb \"$subfolder\" | cut -f1)\r\n\r\n      # Output the image ID, image name, subfolder, space used, and space used in bytes to the temporary file\r\n      echo \"$space_used_bytes $space_used Image ID: $image_id Image Name: $repo_tag Subfolder: $subfolder\" >> \"$temp_file\"\r\n\r\n      # Add subfolder to the used_subfolders array\r\n      used_subfolders+=(\"$subfolder\")\r\n    fi\r\n  done < <(find \"$overlay_dir\" -maxdepth 1 -mindepth 1 -type d -print0)\r\ndone\r\n\r\n# Loop through all subfolders in the overlay2 directory\r\nwhile IFS= read -r -d '' subfolder; do\r\n  # Check if the subfolder is not in the used_subfolders array\r\n  if ! [[ \" ${used_subfolders[@]} \" =~ \" $subfolder \" ]]; then\r\n    # Get the space used by the subfolder\r\n    space_used=$(du -sh \"$subfolder\" | cut -f1)\r\n\r\n    # Convert space used to bytes for sorting\r\n    space_used_bytes=$(du -sb \"$subfolder\" | cut -f1)\r\n\r\n    # Output the subfolder information to the temporary file\r\n    echo \"$space_used_bytes $space_used Image ID: N/A Image Name: N/A Subfolder: $subfolder\" >> \"$temp_file\"\r\n  fi\r\ndone < <(find \"$overlay_dir\" -maxdepth 1 -mindepth 1 -type d -print0)\r\n\r\n# Sort the results by space used in bytes (from largest to smallest) and display them\r\nsort -nr \"$temp_file\" | cut -d' ' -f2- | column -t\r\n\r\n# Remove the temporary file\r\nrm \"$temp_file\"\r\n```\r\n\r\nThis Bash script provides a detailed breakdown of the disk space used by each Docker image and its associated layers in the `/var/lib/docker/overlay2` directory. Here's what the script does:\r\n\r\n1. It defines the `overlay_dir` variable with the path to the `/var/lib/docker/overlay2` directory.\r\n2. It creates a temporary file to store the results using `mktemp`.\r\n3. It loops through each Docker image ID using `docker image ls -q`.\r\n4. For each image ID, it inspects the image to get detailed information using `docker inspect`.\r\n5. It gets the image name (repository and tag) using `docker inspect --format '{{index .RepoTags 0}}'`.\r\n6. It checks all subdirectories in the `overlay_dir` directory using `find`.\r\n7. If the subfolder is associated with the current image ID (based on the `image_info` output), it gets the space used by the subfolder using `du -sh` and `du -sb`.\r\n8. It outputs the image ID, image name, subfolder path, space used (human-readable and bytes) to the temporary file.\r\n9. It adds the used subfolder to the `used_subfolders` array.\r\n10. After processing all images, it loops through all subdirectories in the `overlay_dir` directory again.\r\n11. If a subfolder is not in the `used_subfolders` array, it outputs the subfolder path and space used to the temporary file with \"N/A\" for the image ID and name.\r\n12. It sorts the results in the temporary file by space used in bytes (from largest to smallest) using `sort -nr`.\r\n13. It displays the sorted results in a tabular format using `cut` and `column -t`.\r\n14. Finally, it removes the temporary file.\r\n\r\nThis script provides a comprehensive view of the disk space usage in the `/var/lib/docker/overlay2` directory, allowing you to identify which Docker images and their associated layers are consuming the most space. With this information, you can make informed decisions about which images or layers to remove or optimize to reclaim disk space.\r\n\r\nOutput for the script:\r\n\r\n```sh\r\n3.6G  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/b5b5b1b4abef270dbc91bdd6c385865d778b6bd8ee9568030635994a296533ab\r\n2.5G  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/89048c0675f730b36077f7b967754ec94efead1d7d45b35870131f72bf412ff3\r\n559M  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/ac6efc422ab6e481c58b22d13e6d636d0c649bf58b0ef1bd2d9e01a01c5cbd8c\r\n610M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/dc123594104a9a124bfb4def0780c9e2e65205abf366ea37128663cc7020e444\r\n512M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/4052cb983b6bb7bbe28a6dff67fd1833eb87d4ee9a606b26d925c50e7a823b31\r\n507M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/8cf368abcb835aff6cb6c19cc7b630a32fd5cf23698f9b0fc93a9ed738708a7d\r\n441M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/c6f5534c7692f17026ec630ed076f051914367334916e8bc1a7ee8b9ea3c8da6\r\n271M  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/a7384d24b3ef895b797fe5b65ae365b34e0aedff7544de5e90780ff202cddc03\r\n241M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/78226c49fc485223aa093120075cf760af3ff184952eb3f10819652981cf5124\r\n233M  Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/84ccf08125081b2cf089de802a25923b776ab2c4fbaa760c570223fb784b22ec\r\n196M  Image  ID:  81bab60b840e  Image  Name:  netdata/netdata:edge           Subfolder:  /var/lib/docker/overlay2/264e56fd84b5c0c899a502ad3e0e4944f041390b29af3cdb6396e9f4989d1b41\r\n180M  Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/a5f9492730e056c8b4e687dea92944d36d3b12f50a7112a51e1a9d804eb21345\r\n162M  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/2de288641807d7f4146a579b42d93c62dfa5b539a9a41628592aefbb16dffb20\r\n166M  Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/70732a3ddee8da21207391465d25e6d1980319dd594f4eac890c249a9ee5d271\r\n156M  Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/28de4368b82e98c7bef746a5a31fe4bfafad13580552840164e4f7f57f49f607\r\n212M  Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/7d2dab81fd1f423b10620e080d82c29f5ce1f8ddcef73f3c60ab70704af4c41b\r\n121M  Image  ID:  81bab60b840e  Image  Name:  netdata/netdata:edge           Subfolder:  /var/lib/docker/overlay2/54cae94407a6d824b0d6a5254e2c53a0e43dd824b7908cb2be30566dd0b2fd87\r\n127M  Image  ID:  81bab60b840e  Image  Name:  netdata/netdata:edge           Subfolder:  /var/lib/docker/overlay2/616d0c25253ba7eccbd94a0b34e0bb78d3f72c5149d35584aba21330f9fe8465\r\n114M  Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/eb7adfcd18fee985e866fb66c2ceda1a70dd5b5b923012c11d772dc5bf42b012\r\n118M  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/bcc513179de454299c4dc84463d3dec4dc70ddb1dee9bb179b39e76e7ad9070a\r\n116M  Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/4dcda8262d4dcddea3460b0cc8352136699b49fde2e43dcbfc888979f3cdb24d\r\n89M   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/c719f12c5c660788646618912cafe8f9511dff366e88571d7b39ce36e4249b2c\r\n91M   Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/a9a75fc152f24f5dcb849d735594a42c71c15a72250213cc04691cd6848bca42\r\n88M   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/836a250971cab36b72e36bcabc4f2b48e4c1054c26830d7306b795cac3e337fb\r\n108M  Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/24bf466ef126eabfdf0e67c297462938fe27dafdc89ea24aabeff793ef946dda\r\n82M   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/7a7a28d3b59658445437522404eee89ad0a709fda04b1a29be4726353c972dec\r\n76M   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/b2a9be0ddbf6cf6f43cc14bc22c3406f177a6cd600d8eb479228d6e21b02f93a\r\n68M   Image  ID:  81bab60b840e  Image  Name:  netdata/netdata:edge           Subfolder:  /var/lib/docker/overlay2/56c96ee45c9d6b69bea4befa4de6399f64e43cc59f1a098e91cbb4415eb3b995\r\n66M   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/c3a6596644eef9684bac2754082775f4a22ab092699e57f69a119f1f81db2089\r\n46M   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/a2b95f146415f89c5be49ae361164466e06c8f572a89fb1f3bd3ee8c14109f3c\r\n41M   Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/7ec53d7dcaa0bdef829064006eb7cd1707e5bc81329e2bcc4ca098435f7e35ea\r\n36M   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/169678de43422c93d84d7908bd8ffb7bc29f24c66a874a7d8dc8b6134b9b23a0\r\n34M   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/deb7a08befba79a1da771c1fe01b25769af5b726b119739daed14c0ab6e0443f\r\n15M   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/17584cae6292ae28147159436ba7d81ce25f633b51d1d1931c63f3ba4290a86a\r\n13M   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/f26a189443b56e2bd58d4b29127cf27e56ce40792786c3f96f59c7236762832b\r\n13M   Image  ID:  5e183bda51e6  Image  Name:  amir20/dozzle:latest           Subfolder:  /var/lib/docker/overlay2/19f6568f4e859b471da14ef3be4a22fd1a944d02ddf0c4e7e6e51f4841ea4696\r\n14M   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/09edad670708d284424a65b8ede770ede084ccb4e093ce5a68f34aeebc6806ef\r\n14M   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/f7071dc2fa0b9ceafccf8c7c4f2a649cf2afd45db3fad514390fcdd070f8de91\r\n15M   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/3d3109e52bf7eca7bee228781ddd25c1bbe5a7f71629b79e9c017718e8569490\r\n12M   Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/2c4b6fc84bd70116a94140c2244c48818b620a8169563c1063a55670fbab3f85\r\n9.3M  Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/e581648a5e94b4cbcc465c5bfa959297e28b3393134b43c90a55ab431804e5dd\r\n9.2M  Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/4a012de19c527f3e720e0865a78997e41e048a81aa2172ca580d5f72ec017451\r\n7.6M  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/088898d60221b6b9287044663d3eaad6c2e35e0a945ba7041cd197311bb134cf\r\n7.7M  Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/dee1cc2f27a0fc622000bc9dadfde261c597c738aee05c4bd5f420eb89bb5521\r\n7.7M  Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/dee1cc2f27a0fc622000bc9dadfde261c597c738aee05c4bd5f420eb89bb5521\r\n7.7M  Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/737eb00b7ac618fe27f498f4738402115d0f62c130a3e8e5673275bc03dda87a\r\n6.7M  Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/f66e314738c63153f7d5fe979454b07861f5fe49df645656079bf959d386e30d\r\n4.0M  Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/f4cf434f5995eae8c53d7745ddae7961411a462543af09d43891fb5d11447fb5\r\n2.8M  Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/8b150f3d7ebc35c34f8dcacf5cca156b121d966c76668b591834686b26397e31\r\n2.7M  Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/c179768305fafe34e3217c421e93606341f2af6b33d1bdb73bf7441da9210ccd\r\n1.6M  Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/a721bb296ea20c16f958c075c9b679308dbcc2654f4c50db03384e6d709942c0\r\n412K  Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/7508c8e0187a5e6dfb9de3a6af7f5bbe06e7ba5cf8e8d76d011569b17603b2f8\r\n244K  Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/b535a986525ba548407bf1018dfbdd5c4d9a9541a43bc8d5eaeee2186253d385\r\n244K  Image  ID:  5e183bda51e6  Image  Name:  amir20/dozzle:latest           Subfolder:  /var/lib/docker/overlay2/1fb78bfe25036ef4d4a3c4f12e53b403752ca85db3c47a5d729aed2cb3e4fa2b\r\n288K  Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/c527a8de51fb12b2097c6ef772447853efaa78c2b4407ac84bc675bc8e3c599b\r\n100K  Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/e4c8d32b00fafea8c6c4cd514fc3b85d4bb7243227e0bd3b63281995b1407423\r\n48K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/f26a189443b56e2bd58d4b29127cf27e56ce40792786c3f96f59c7236762832b-init\r\n52K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/f64973cf2ec7c3b32a85d96d797d7948dd1a41313bc4d28134b1b5c43511cd4e\r\n44K   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/947da6241e914cf8a2fdf3303cfb954a4baa4bf7eab56c1a81252ff44b877d21\r\n84K   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/22377e49347f85a438e9f49b410464809c62af69e7ce4ab93eea505f80f27fd9\r\n52K   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/ea1ac89b01a118204f867706c2f5b742c58f78baed0572baadb6f770aa0c4798\r\n60K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/886f61fcef063b23a687b4e75474e4fdce608262e7f4736f02b6346a6b6ce01d\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/4052cb983b6bb7bbe28a6dff67fd1833eb87d4ee9a606b26d925c50e7a823b31-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/8cf368abcb835aff6cb6c19cc7b630a32fd5cf23698f9b0fc93a9ed738708a7d-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/dc123594104a9a124bfb4def0780c9e2e65205abf366ea37128663cc7020e444-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/b5b5b1b4abef270dbc91bdd6c385865d778b6bd8ee9568030635994a296533ab-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/78226c49fc485223aa093120075cf760af3ff184952eb3f10819652981cf5124-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/a2b95f146415f89c5be49ae361164466e06c8f572a89fb1f3bd3ee8c14109f3c-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/2de288641807d7f4146a579b42d93c62dfa5b539a9a41628592aefbb16dffb20-init\r\n40K   Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/c6f5534c7692f17026ec630ed076f051914367334916e8bc1a7ee8b9ea3c8da6-init\r\n40K   Image  ID:  81bab60b840e  Image  Name:  netdata/netdata:edge           Subfolder:  /var/lib/docker/overlay2/557dc929b44dbf8f56c017d71648e3dcf5df42d4ae45522a1c8319316dab6efb\r\n40K   Image  ID:  53f011320bac  Image  Name:  filebrowser/filebrowser:v2-s6  Subfolder:  /var/lib/docker/overlay2/fbd2875f8195d4ebf67d9e43b12c2e6e8bbc3512d77d73de83897ebda68398bf\r\n36K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/90831482be916d167036989bbbc74fd8ef156ef44d0eaa10180587b45cada241\r\n36K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/9189514b80b1ac311df7fe79dba8c8d856f3f3c52fb3880692f69ed4112c094d\r\n396K  Image  ID:  N/A           Image  Name:  N/A                            Subfolder:  /var/lib/docker/overlay2/l\r\n40K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/b5a1a0f5473f942dd88c52f6e38689a6d3fcebf157640122ae3d54cdeec6e614\r\n36K   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/10da3c9a9d54a96145163ce70f680d2fee458acaf77d300c3fe33c01be4e11e1\r\n36K   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/c64fa7986a23b02391406d8b99040ea2d1261e70496d778eccaf91eb671d0072\r\n36K   Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/3a670fdb3c2ca2b72f1f03bdff00a762641546ff45dc7eeac76feeec25b7689b\r\n32K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/f463aafd74960608f1ffb2fba5de101dc10fecf4f52022e19e80a9a17c41706a\r\n28K   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/db62eb43b9a60a135d7119a46aaf84726ad7484c43ccd7542a73a32c2622926f\r\n28K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/1e31a6ff9eec4981357ae5b8aa9e51f9e99b0defcb9954238f09267d64e6c6a3\r\n28K   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/6d7e99168b19fb1de03a54e57b7dbd816bb8f6f72e0843e970795634b9e9439f\r\n28K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/07cd3509e14a75f406646c468c8ef96073b7ea0ab231cae87c328cabc61c5189\r\n28K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/d94c15106fa777ec5df2f18787b3bbafe13b54fc81bc3eaa4bc93a1def464a50\r\n28K   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/3e2ae7a5c55528b2708064323486d88b9beecb756ed556ab82f6e2b19b3b5882\r\n24K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/c1eca20bdd90d0d8b65dacfbbe64742697c548a205203fef7d1ae6bea17cdf8c\r\n24K   Image  ID:  2e7228c0315e  Image  Name:  flowiseai/flowise:latest       Subfolder:  /var/lib/docker/overlay2/f6e32501c061a5237d6eb0c6fd878c0b6b3a09c52105bbc8e26dbe47eede6274\r\n24K   Image  ID:  ffb6864bc6f8  Image  Name:  louislam/dockge:1              Subfolder:  /var/lib/docker/overlay2/7ec87e3da5abb0920146610dc5aaf3b1f257ea87e8d701e696f6fbdbe51d17af\r\n24K   Image  ID:  89c919b22c13  Image  Name:  zauberzeug/nicegui:latest      Subfolder:  /var/lib/docker/overlay2/b3d8dc9ee09fd3649da5370c3de00764309f6194a5d595a38a2a3dac8e9ac32f\r\n24K   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/95a75358634d89ff52294d2c59fe6cd7ea188f29e6445cbef88296a08cc2c6a6\r\n24K   Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/17145fbf43a0394832c88f2305ed5923601a4adbf86e40e3cfcf930f3d0593c7\r\n24K   Image  ID:  a7079537d060  Image  Name:  harness/gitness:3.0.0-beta.5   Subfolder:  /var/lib/docker/overlay2/ad3087421c452c131c34846406a74c551c4cd23595af7e18f67bb29df94c8399\r\n24K   Image  ID:  9a510ccf1de4  Image  Name:  postgres:16-alpine             Subfolder:  /var/lib/docker/overlay2/bd2aeafbfb363b9cd57102c8f4885bdfe8c4aa8339bcc281dd05d4789d99d705\r\n20K   Image  ID:  b23ac695b1b4  Image  Name:  louislam/uptime-kuma:1         Subfolder:  /var/lib/docker/overlay2/447033b241af6fd664626609f8b7bed74275f02407f0a12e1381a916033001e3\r\n16K   Image  ID:  5e183bda51e6  Image  Name:  amir20/dozzle:latest           Subfolder:  /var/lib/docker/overlay2/7464a99ae22b909b5657a85b7428d669ea743e5de39989e59767edbf4a9efa81\r\n```\r\n\r\n**Why there are subfolders in /`var/lib/docker/overlay2` not corresponding to an image even after I run `docker system prune -a -f `**\r\n\r\nThe presence of subfolders in `/var/lib/docker/overlay2` that do not correspond to Docker images, even after running `docker system prune -a -f`, can be attributed to several reasons:\r\n\r\n1. **Orphaned Layers:** When you pull or build Docker images, each layer is stored as a directory within `/var/lib/docker/overlay2`. Sometimes, due to interruptions during image pulls or builds, or issues with Docker's internal processes, these layers can become orphaned, meaning they are not associated with any active Docker image. Orphaned layers are not automatically removed by `docker system prune`.\r\n\r\n2. **Shared Layers:** Docker images can share common layers. If an image is deleted but its layers are shared with other images, those layers will remain in `/var/lib/docker/overlay2` as they are still being used by other images. `docker system prune` only removes layers that are not referenced by any existing images.\r\n\r\n3. **Dangling Volumes:** Docker volumes created by containers are stored in `/var/lib/docker/volumes`. If a volume is not associated with any active container, it becomes a dangling volume. While `docker system prune` removes dangling volumes, it may not remove all orphaned layers in `/var/lib/docker/overlay2`.\r\n\r\n4. **Incomplete Cleanup:** In rare cases, Docker's cleanup mechanisms may not remove all unused or orphaned resources, leaving behind residual data in `/var/lib/docker/overlay2`.\r\n\r\nTo further investigate and clean up these leftover subfolders, you can:\r\n\r\n- Manually inspect the contents of the subfolders within `/var/lib/docker/overlay2` to identify any leftover layers or orphaned files.\r\n- Use tools like `docker image prune`, `docker container prune`, and `docker volume prune` to remove additional unused resources.\r\n- If necessary, you can delete the leftover subfolders manually, but exercise caution as deleting essential Docker files can result in data loss or system instability.\r\n\r\nRegular maintenance and cleanup of Docker's storage directories are essential for efficient resource utilization and preventing disk space issues.\r\n\r\n**If you want to start fresh while keeping some of your existing data, you can create a new Docker environment by following these steps:**\r\n\r\n1. Stop and remove all running containers.\r\n2. Save any images you want to keep by creating a new image from a container or exporting the image to a tar file.\r\n3. Remove all unused images with `docker image prune -a`.\r\n4. Stop the Docker daemon.\r\n5. Rename or move the `/var/lib/docker` directory to a backup location.\r\n6. Start the Docker daemon again, which will create a new, empty `/var/lib/docker` directory.\r\n7. Import or load any images you want to keep in the new environment.\r\n\r\nThis process will effectively reset your Docker environment while allowing you to selectively keep or discard existing images and data.\r\n\r\n## Conclusions\r\n\r\nMaintaining the `/var/lib/docker/overlay2` directory is crucial for efficient Docker usage and preventing disk space issues. By regularly removing unused Docker objects, monitoring disk space usage, and optimizing Docker images, you can keep the `/var/lib/docker/overlay2` directory at a manageable size and ensure smooth operation of your Docker environment.","src/content/posts/clean-docker-overlay2-dir.mdx",[471],"../../assets/images/24/05/free-disk-space-docker-overlay2.jpeg","b3d248dd21c1f543","clean-docker-overlay2-dir.mdx","cleanup-all-docker-things",{id:474,data:476,body:485,filePath:486,assetImports:487,digest:489,legacyId:490,deferredRender:32},{title:477,description:478,date:479,image:480,authors:481,categories:482,tags:483,canonical:484},"How To Clean All Docker Images With Disks and Everything","Learn how to declutter your Docker environment. Our guide shows you how to remove images, containers, volumes, and networks, freeing up valuable disk space. Follow step-by-step instructions to start fresh with Docker.",["Date","2023-08-16T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/08/docker-cleanup.jpeg",[19],[98],[100],"https://www.bitdoze.com/cleanup-all-docker-things/","I was responsible for maintaining the analytics platform for several websites using plausible.io, which I had set up using Docker and docker-compose. The other day, I noticed there was an update available for the plausible.io Docker image. Eager to benefit from the latest features, I updated the docker-compose image configuration and tried to apply the update.\r\n\r\nTo my surprise, the update failed. After checking the logs, I realized the problem was with the PostgreSQL database version. I had version 12, but the new plausible.io image required version 14. I thought the quickest solution would be to revert the PostgreSQL database to its previous state using the old image.\r\n\r\nAfter restoring the database and trying to restart [plausible.io](https://www.bitdoze.com/install-plausible-analytics/), I faced another issue: the websites were no longer accessible through the analytics platform. I decided to [pull the latest images again](https://www.bitdoze.com/updating-container-docker-compose/), hoping this would resolve the problem. However, I was met with a new error:\r\n\r\n```bash\r\nERROR: for plausible_plausible_events_db_1  Cannot create container for service plausible_events_db: open /var/lib/docker/volumes/plausible_event-data/_data: no such file or directory\r\n```\r\n\r\nAdditionally, there was a warning:\r\n\r\n```bash\r\nWARNING: Service \"plausible_events_db\" is using volume \"/var/lib/clickhouse\" from the previous container. Host mapping \"plausible_event-data2\" has no effect. Remove the existing containers (with `docker-compose rm plausible`)\r\n```\r\n\r\nIt became clear to me that I needed to take more drastic measures. I decided to do a clean install of everything and remove the previous plausible.io configurations.\r\n\r\nI have started doing that but in the beginning, I only cleaned the images and volumes had an issue and so on. In the below steps you will find everything you need to do a proper docker cleanup to install the new images fresh.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## How To Clean All Docker Images With Disks and Everything\r\n\r\n### 1.Stop All Running Containers:\r\n\r\nFirst, you need to stop all running containers because you can't remove a container that is currently running.\r\n\r\n```bash\r\ndocker stop $(docker ps -a -q)\r\n```\r\n\r\nThis command stops all running containers by listing all container IDs and then stopping them.\r\n\r\n> If you have more containers there that don't need to be stopped you can only stop them.\r\n\r\n### 2.Remove All Containers:\r\n\r\nAfter stopping all containers, you can remove them.\r\n\r\n```bash\r\ndocker rm $(docker ps -a -q)\r\n```\r\n\r\nThis command removes all containers by listing all container IDs and then removing them.\r\n\r\n> If you have containers that should not be removed just remove them one by one not with all.\r\n\r\n### 3.Remove All Images:\r\n\r\nOnce all containers are removed, you can remove all images.\r\n\r\n```bash\r\ndocker rmi $(docker images -q)\r\n```\r\n\r\nThis command removes all images by listing all image IDs and then removing them.\r\n\r\n> Again if you don't want all images to be removed remove what you don't need.\r\n\r\n### 4.Remove All Volumes:\r\n\r\nDocker volumes are used to persist data from a certain container or to share data between containers. To remove all unused volumes:\r\n\r\n```bash\r\ndocker volume prune -f\r\n```\r\n\r\nThis command removes all unused volumes. The -f or --force flag will bypass the confirmation prompt.\r\n\r\n### 5.Remove All Networks:\r\n\r\nTo remove all unused networks:\r\n\r\n```bash\r\ndocker network prune -f\r\n```\r\n\r\nThis command removes all unused networks. The -f or --force flag will bypass the confirmation prompt.\r\n\r\n### 6.System-wide Cleanup:\r\n\r\nDocker provides a command that cleans up containers, images, volumes, and networks that are not associated with a container:\r\n\r\n```bash\r\ndocker system prune -a -f\r\n```\r\n\r\nThe -a flag tells Docker to remove all unused images, not just dangling ones. The -f or --force flag will bypass the confirmation prompt.\r\n\r\n### 7.Disk Settings and Everything:\r\n\r\nIf you want to clean up disk space further, you may need to look into the Docker data directory, which is usually located at /var/lib/docker/ on Linux systems. Be very careful with this step, as it will remove all Docker data:\r\n\r\n```bash\r\n sudo rm -rf /var/lib/docker\r\n```\r\n\r\nAfter this, you may need to restart the Docker service:\r\n\r\n```bash\r\nsudo systemctl restart docker\r\n```\r\n\r\n> Do this only if you don't have other docker images and you want a fresh start.\r\n\r\n### Warning:\r\n\r\n> These commands will remove all your Docker containers, images, volumes, and networks. They will also free up disk space, but you will lose all data associated with your Docker containers and images. Make sure you have backed up important data before running these commands.\r\n\r\n## Conclusions\r\n\r\nIn this way, you clean up all the docker things if you bump into issues and you want a fresh start. Be sure to take a backup before in case you need something. Also if this is a production environment you should also do a basic test before with a downtime.\r\n\r\nGood luck with your Docker cleanup!","src/content/posts/cleanup-all-docker-things.mdx",[488],"../../assets/images/23/08/docker-cleanup.jpeg","12aa224efbd814cc","cleanup-all-docker-things.mdx","cloudpanel-remote-backups",{id:491,data:493,body:503,filePath:504,assetImports:505,digest:507,legacyId:508,deferredRender:32},{title:494,description:495,date:496,image:497,authors:498,categories:499,tags:500,canonical:502},"Setup CloudPanel Remote Backups to OneDrive or Google Drive","See how you can easily setup remote backups in CloudPanel to OneDrive or Google Drive",["Date","2024-03-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/cloudpanel-remote-backups.png",[19],[98],[501],"cloudpanel","https://www.bitdoze.com/cloudpanel-remote-backups/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/03/cloudpanel-backup.png\";\r\n\r\nBackup is a very important piece in every application, while self-hosting my own apps I need to make sure I have a good backup strategy so in case something happens with the website or the server I have a way to restore it. Your hosting provider account may be hacked and everything can be deleted, that's why you need to have a remote backup that you can rely on. Is not enough to backup your apps locally or in the same provider but you should have also a remote backup that you can use in case something goes wrong.\r\n\r\n[CloudPanel](https://www.cloudpanel.io/) is a good open source alternative that can help you self host your applications and offers a way to do [remote backups](https://www.cloudpanel.io/docs/v2/admin-area/backups/) automatically at certain periods so you can have a better sleep. It uses [Rclone](https://rclone.org/) behind the hood to do remote backups and you can choose various sources like Amazon S3, Wasabi, Digital Ocean Spaces, Dropbox, Google Drive, SFTP.\r\n\r\nSome of them are easy to configure like Digital Ocean Spaces, Dropbox, SFTP as you do it directly in the UI interface but for others like OneDrive or Personal Google Drive you need to run through the Rclone configurations.\r\n\r\nIn this article and video we are going to go through all the steps you need to do to configure remote backups in CloudPanel to OneDrive or Google Drive. The process should be the same and in the end you will have a scheduled backup that will run and store your files securely.\r\n\r\nIn case you are interested on CloudPanel articles you can check:\r\n\r\n- [Setup CloudPanel As Reverse Proxy with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n- [Install CloudPanel and Host Node.js Apps](https://www.bitdoze.com/install-cloudpanel-host-nodejs/)\r\n\r\n## How to Setup CloudPanel Remote Backups to OneDrive or Google Drive\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/ja3TZR083pA\"\r\n  label=\"Setup CloudPanel Remote Backups to OneDrive or Google Drive\"\r\n/>\r\n\r\nLet's go through all the steps you need to have remote backups enabled on CloudPanel and have them run automaticaly.\r\n\r\n### 1. Create the folder in OneDrive or Google Drive\r\n\r\nFirst thing will be to go into your cloud provider that you have chosen to do the backups and create a folder where the backups will be stored. I have seen there are issues if the folder does not exist so that's why you should create one from the start. For this tutorial I will create one called `dragos-cloudpanel-backups` that I will use.\r\n\r\n### 2. Connect to the Server with SSH Tunnel\r\n\r\nAs for configuring the OneDrive or Google Drive you will need to authenticate via Browser you will need to configure an SSH Tunel to your server that will be used. To do this you just need to run:\r\n\r\n```sh\r\nssh -L localhost:53682:localhost:53682 username@remote_server\r\n```\r\n\r\nYou just need to change `username` with your user of the server and `remote_server` with the server IP this will connect your local to the server and with `localhost:53682` you can access the browser to get the token. You can check more on [Rclone doc for remote setup](https://rclone.org/remote_setup/)\r\n\r\nThis will work well on Mac, for Windows you should use Putty and do there the port forwarding, the WSL can also work. You can check: [How to Use SSH Port Forwarding & PuTTY](https://www.redswitches.com/blog/ssh-port-forwarding/)\r\n\r\n### 3. Run Rclone config for OneDrive\r\n\r\nRclone config needs to be run to set this up as you don't have any option in the CloudPanel UI\r\n\r\nto do this you just run :\r\n\r\n```sh\r\nrclone config\r\n```\r\n\r\nand follow the configs as below:\r\n\r\n```sh\r\nroot@cp-dg:~# rclone config\r\n2024/03/14 13:27:43 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\r\nNo remotes found - make a new one\r\nn) New remote\r\ns) Set configuration password\r\nq) Quit config\r\nn/s/q> n\r\nname> remote\r\nType of storage to configure.\r\nEnter a string value. Press Enter for the default (\"\").\r\nChoose a number from below, or type in your own value\r\n 1 / 1Fichier\r\n   \\ \"fichier\"\r\n 2 / Alias for an existing remote\r\n   \\ \"alias\"\r\n 3 / Amazon Drive\r\n   \\ \"amazon cloud drive\"\r\n 4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, Tencent COS, etc)\r\n   \\ \"s3\"\r\n 5 / Backblaze B2\r\n   \\ \"b2\"\r\n 6 / Box\r\n   \\ \"box\"\r\n 7 / Cache a remote\r\n   \\ \"cache\"\r\n 8 / Citrix Sharefile\r\n   \\ \"sharefile\"\r\n 9 / Dropbox\r\n   \\ \"dropbox\"\r\n10 / Encrypt/Decrypt a remote\r\n   \\ \"crypt\"\r\n11 / FTP Connection\r\n   \\ \"ftp\"\r\n12 / Google Cloud Storage (this is not Google Drive)\r\n   \\ \"google cloud storage\"\r\n13 / Google Drive\r\n   \\ \"drive\"\r\n14 / Google Photos\r\n   \\ \"google photos\"\r\n15 / Hubic\r\n   \\ \"hubic\"\r\n16 / In memory object storage system.\r\n   \\ \"memory\"\r\n17 / Jottacloud\r\n   \\ \"jottacloud\"\r\n18 / Koofr\r\n   \\ \"koofr\"\r\n19 / Local Disk\r\n   \\ \"local\"\r\n20 / Mail.ru Cloud\r\n   \\ \"mailru\"\r\n21 / Microsoft Azure Blob Storage\r\n   \\ \"azureblob\"\r\n22 / Microsoft OneDrive\r\n   \\ \"onedrive\"\r\n23 / OpenDrive\r\n   \\ \"opendrive\"\r\n24 / OpenStack Swift (Rackspace Cloud Files, Memset Memstore, OVH)\r\n   \\ \"swift\"\r\n25 / Pcloud\r\n   \\ \"pcloud\"\r\n26 / Put.io\r\n   \\ \"putio\"\r\n27 / SSH/SFTP Connection\r\n   \\ \"sftp\"\r\n28 / Sugarsync\r\n   \\ \"sugarsync\"\r\n29 / Transparently chunk/split large files\r\n   \\ \"chunker\"\r\n30 / Union merges the contents of several upstream fs\r\n   \\ \"union\"\r\n31 / Webdav\r\n   \\ \"webdav\"\r\n32 / Yandex Disk\r\n   \\ \"yandex\"\r\n33 / http Connection\r\n   \\ \"http\"\r\n34 / premiumize.me\r\n   \\ \"premiumizeme\"\r\n35 / seafile\r\n   \\ \"seafile\"\r\nStorage> 22\r\n** See help for onedrive backend at: https://rclone.org/onedrive/ **\r\n\r\nOAuth Client Id\r\nLeave blank normally.\r\nEnter a string value. Press Enter for the default (\"\").\r\nclient_id>\r\nOAuth Client Secret\r\nLeave blank normally.\r\nEnter a string value. Press Enter for the default (\"\").\r\nclient_secret>\r\nEdit advanced config? (y/n)\r\ny) Yes\r\nn) No (default)\r\ny/n>\r\nRemote config\r\nUse auto config?\r\n * Say Y if not sure\r\n * Say N if you are working on a remote or headless machine\r\ny) Yes (default)\r\nn) No\r\ny/n>\r\nIf your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=LK_cdbrOrIT\r\nLog in and authorize rclone for access\r\nWaiting for code...\r\nGot code\r\nChoose a number from below, or type in an existing value\r\n 1 / OneDrive Personal or Business\r\n   \\ \"onedrive\"\r\n 2 / Root Sharepoint site\r\n   \\ \"sharepoint\"\r\n 3 / Type in driveID\r\n   \\ \"driveid\"\r\n 4 / Type in SiteID\r\n   \\ \"siteid\"\r\n 5 / Search a Sharepoint site\r\n   \\ \"search\"\r\nYour choice> 1\r\nFound 1 drives, please select the one you want to use:\r\n0:  (personal) id=f9f661d3066d45ed\r\nChose drive to use:> 0\r\nFound drive 'root' of type 'personal', URL: https://onedrive.live.com/?cid=f9f661d3066d45ed\r\nIs that okay?\r\ny) Yes (default)\r\nn) No\r\ny/n>\r\n--------------------\r\n[remote]\r\ntoken = {\"access_token\":\"EwCIA8l6BAAUTTy6dbu0OLf3Lzl3R/vfUXxq8g8AAYnaLJ3lAxqLzQiNrimK2Z60ZwmFXVR0Buqhnisc5QEW4tUHFH5eFg+/AUNEWMmTptERPkQh4BU3paTlktMnqX4oG4/6LSUPJmRAQZvQrUX5K4tGHzJ/t$\",\"expiry\":\"2024-03-14T14:28:23.186484017Z\"}\r\ndrive_id = f9f661d3066d45ed\r\ndrive_type = personal\r\n--------------------\r\ny) Yes this is OK (default)\r\ne) Edit this remote\r\nd) Delete this remote\r\ny/e/d>\r\nCurrent remotes:\r\n\r\nName                 Type\r\n====                 ====\r\nremote               onedrive\r\n\r\ne) Edit existing remote\r\nn) New remote\r\nd) Delete remote\r\nr) Rename remote\r\nc) Copy remote\r\ns) Set configuration password\r\nq) Quit config\r\ne/n/d/r/c/s/q> q\r\n```\r\n\r\nIn the above you will see that the below have been chosen:\r\n\r\n- **n) New remote** - to create a new config file\r\n- **name> remote** - you need to provide this exact name otherwise CloudPanel will not know what to do\r\n- **Storage> 22** - this is for Microsoft One Drive\r\n- **client_id - client_secret** - just enter, nothing to do\r\n- **Edit advanced config? - n) No (default)** - the default which is No\r\n- **Use auto config? - y) Yes (default)** - the yes default one\r\n- **http://127.0.0.1:53682/auth?state=LK_cdbrOrITOdn9V** - link with a state will be provided that you need to use in the browser to get the token, just open it in browser.\r\n- **1 / OneDrive Personal or Business** - choose the personal\r\n- **Chose drive to use:> 0** - the 0 drive\r\n- **y) Yes this is OK (default)** - the default in here\r\n- \\*_e/n/d/r/c/s/q> q_ - q to finish with settings\r\n\r\nThis would link Rclone to one drive.\r\n\r\n### 4. Run Rclone config for Google Drive\r\n\r\nAs for OneDrive you need to run the same:\r\n\r\n```sh\r\nrclone config\r\n```\r\n\r\nAnd then choose the Google Drive options:\r\n\r\n```sh\r\nroot@cp-dg:~/.config/rclone# rclone config\r\n2024/03/14 13:54:24 NOTICE: Config file \"/root/.config/rclone/rclone.conf\" not found - using defaults\r\nNo remotes found - make a new one\r\nn) New remote\r\ns) Set configuration password\r\nq) Quit config\r\nn/s/q> n\r\nname> remote\r\nType of storage to configure.\r\nEnter a string value. Press Enter for the default (\"\").\r\nChoose a number from below, or type in your own value\r\n 1 / 1Fichier\r\n   \\ \"fichier\"\r\n 2 / Alias for an existing remote\r\n   \\ \"alias\"\r\n 3 / Amazon Drive\r\n   \\ \"amazon cloud drive\"\r\n 4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, Tencent COS, etc)\r\n   \\ \"s3\"\r\n 5 / Backblaze B2\r\n   \\ \"b2\"\r\n 6 / Box\r\n   \\ \"box\"\r\n 7 / Cache a remote\r\n   \\ \"cache\"\r\n 8 / Citrix Sharefile\r\n   \\ \"sharefile\"\r\n 9 / Dropbox\r\n   \\ \"dropbox\"\r\n10 / Encrypt/Decrypt a remote\r\n   \\ \"crypt\"\r\n11 / FTP Connection\r\n   \\ \"ftp\"\r\n12 / Google Cloud Storage (this is not Google Drive)\r\n   \\ \"google cloud storage\"\r\n13 / Google Drive\r\n   \\ \"drive\"\r\n14 / Google Photos\r\n   \\ \"google photos\"\r\n15 / Hubic\r\n   \\ \"hubic\"\r\n16 / In memory object storage system.\r\n   \\ \"memory\"\r\n17 / Jottacloud\r\n   \\ \"jottacloud\"\r\n18 / Koofr\r\n   \\ \"koofr\"\r\n19 / Local Disk\r\n   \\ \"local\"\r\n20 / Mail.ru Cloud\r\n   \\ \"mailru\"\r\n21 / Microsoft Azure Blob Storage\r\n   \\ \"azureblob\"\r\n22 / Microsoft OneDrive\r\n   \\ \"onedrive\"\r\n23 / OpenDrive\r\n   \\ \"opendrive\"\r\n24 / OpenStack Swift (Rackspace Cloud Files, Memset Memstore, OVH)\r\n   \\ \"swift\"\r\n25 / Pcloud\r\n   \\ \"pcloud\"\r\n26 / Put.io\r\n   \\ \"putio\"\r\n27 / SSH/SFTP Connection\r\n   \\ \"sftp\"\r\n28 / Sugarsync\r\n   \\ \"sugarsync\"\r\n29 / Transparently chunk/split large files\r\n   \\ \"chunker\"\r\n30 / Union merges the contents of several upstream fs\r\n   \\ \"union\"\r\n31 / Webdav\r\n   \\ \"webdav\"\r\n32 / Yandex Disk\r\n   \\ \"yandex\"\r\n33 / http Connection\r\n   \\ \"http\"\r\n34 / premiumize.me\r\n   \\ \"premiumizeme\"\r\n35 / seafile\r\n   \\ \"seafile\"\r\nStorage> 13\r\n** See help for drive backend at: https://rclone.org/drive/ **\r\n\r\nGoogle Application Client Id\r\nSetting your own is recommended.\r\nSee https://rclone.org/drive/#making-your-own-client-id for how to create your own.\r\nIf you leave this blank, it will use an internal key which is low performance.\r\nEnter a string value. Press Enter for the default (\"\").\r\nclient_id>\r\nOAuth Client Secret\r\nLeave blank normally.\r\nEnter a string value. Press Enter for the default (\"\").\r\nclient_secret>\r\nScope that rclone should use when requesting access from drive.\r\nEnter a string value. Press Enter for the default (\"\").\r\nChoose a number from below, or type in your own value\r\n 1 / Full access all files, excluding Application Data Folder.\r\n   \\ \"drive\"\r\n 2 / Read-only access to file metadata and file contents.\r\n   \\ \"drive.readonly\"\r\n   / Access to files created by rclone only.\r\n 3 | These are visible in the drive website.\r\n   | File authorization is revoked when the user deauthorizes the app.\r\n   \\ \"drive.file\"\r\n   / Allows read and write access to the Application Data folder.\r\n 4 | This is not visible in the drive website.\r\n   \\ \"drive.appfolder\"\r\n   / Allows read-only access to file metadata but\r\n 5 | does not allow any access to read or download file content.\r\n   \\ \"drive.metadata.readonly\"\r\nscope> 1\r\nID of the root folder\r\nLeave blank normally.\r\n\r\nFill in to access \"Computers\" folders (see docs), or for rclone to use\r\na non root folder as its starting point.\r\n\r\nEnter a string value. Press Enter for the default (\"\").\r\nroot_folder_id>\r\nService Account Credentials JSON file path\r\nLeave blank normally.\r\nNeeded only if you want use SA instead of interactive login.\r\n\r\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`.\r\n\r\nEnter a string value. Press Enter for the default (\"\").\r\nservice_account_file>\r\nEdit advanced config? (y/n)\r\ny) Yes\r\nn) No (default)\r\ny/n>\r\nRemote config\r\nUse auto config?\r\n * Say Y if not sure\r\n * Say N if you are working on a remote or headless machine\r\ny) Yes (default)\r\nn) No\r\ny/n>\r\nIf your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=lJ52bPvVGaG\r\nLog in and authorize rclone for access\r\nWaiting for code...\r\nGot code\r\nConfigure this as a team drive?\r\ny) Yes\r\nn) No (default)\r\ny/n>\r\n\r\n--------------------\r\n[remote]\r\nscope = drive\r\ntoken = {\"access_token\":\"ya29.a0Ad52N3_J72wn8dG4c\",\"token_type\":\"Bearer\",\"refresh_token\":\"1//0czUgUBU95R8GCgYIARAAG-JK9klco7JLyA\",\"expiry\":\"2024-03-14T14:55:20.87391752Z\"}\r\n--------------------\r\ny) Yes this is OK (default)\r\ne) Edit this remote\r\nd) Delete this remote\r\ny/e/d>\r\nCurrent remotes:\r\n\r\nName                 Type\r\n====                 ====\r\nremote               drive\r\n\r\ne) Edit existing remote\r\nn) New remote\r\nd) Delete remote\r\nr) Rename remote\r\nc) Copy remote\r\ns) Set configuration password\r\nq) Quit config\r\ne/n/d/r/c/s/q> q\r\n```\r\n\r\nIn the above you will see that the below have been chosen:\r\n\r\n- **n) New remote** - to create a new config file\r\n- **name> remote** - you need to provide this exact name otherwise CloudPanel will not know what to do\r\n- **Storage> 13** - this is for Google Drive\r\n- **client_id - client_secret** - just enter, nothing to do\r\n- **scope> 1** - Full access all files, excluding Application Data Folder.\r\n- **Edit advanced config? - n) No (default)** - the default which is No\r\n- **root_folder_id - service_account_file** - just hit enter\r\n- **Use auto config? - y) Yes (default)** - the yes default one\r\n- **http://127.0.0.1:53682/auth?state=LK_cdbrOrI** - link with a state will be provided that you need to use in the browser to get the token, just open it in browser.\r\n- **y) Yes this is OK (default)** - the default in here\r\n- \\*_e/n/d/r/c/s/q> q_ - q to finish with settings\r\n\r\nAnd after Rclone is linked to Google Drive and you can configure in CloudPanel UI.\r\n\r\n### 5. Configure CloudPanel Remote Backups\r\n\r\nLogin to CloudPanel and in **Admin Area** - **Backups** choose **Custom Rclone Config**\r\n\r\nIn there, you will have the settings where you choose the Frequency, Time, Retention Period (Days) and Storage Directory, the one you created earlier.\r\n\r\n<Picture src={imag1} alt=\"CloudPanel Create a Reverse Proxy\" />\r\n\r\nAfter you save this you can hit Create Backup so one to start now.\r\n\r\n### 6. Check That Backup has been done in OneDrive or Google Drive.\r\n\r\nIn couple of minutes you should see that files started to be seen in the directory you created, in my case `dragos-cloudpanel-backups`. In function of the size you have for your apps it can take several minutes to 1h, it depends.\r\n\r\n### 7. Alter the default backup schedule for CloudPanel\r\n\r\nIn case you need to have more often backups for the app or for the database you have the file `/etc/cron.d/clp`\r\n\r\nIn here you have the below 2 options that can be altered to change the schedulers, just be careful:\r\n\r\n```sh\r\n15 3 * * * clp /usr/bin/bash -c \"/usr/bin/clpctl db:backup --ignoreDatabases='db1,db2' --retentionPeriod=7\" &> /dev/null\r\n15 4 * * * clp /home/clp/scripts/create_backup.sh &> /dev/null\r\n```\r\n\r\nThey can be altered in function of your needs to have more often DB backups for your applications.\r\n\r\n## Conclusions\r\n\r\nThis is how easy it is to configure the CloudPanel backups, in a couple of minutes you will enable remote backups in CloudPanel and will be protected in case bad things happens.","src/content/posts/cloudpanel-remote-backups.mdx",[506],"../../assets/images/24/03/cloudpanel-remote-backups.png","a9cc2b0d935d320d","cloudpanel-remote-backups.mdx","cloudpanel-setup-dockge",{id:509,data:511,body:521,filePath:522,assetImports:523,digest:525,legacyId:526,deferredRender:32},{title:512,description:513,date:514,image:515,authors:516,categories:517,tags:518,canonical:520},"Setup CloudPanel As Reverse Proxy with Docker and Dockge","Use CloudPanel as a reverse proxy with Dockge for managing your Docker containers.",["Date","2024-02-13T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/cloudpanel-dockge-setup.jpeg",[19],[98],[501,519],"dockge","https://www.bitdoze.com/cloudpanel-setup-dockge/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/02/cp-add-proxy-1.png\";\r\nimport imag2 from \"../../assets/images/24/02/cp-add-proxy-2.png\";\r\nimport imag3 from \"../../assets/images/24/02/cp-open-ports.png\";\r\nimport imag4 from \"../../assets/images/24/01/dockge-add.png\";\r\n\r\n[CloudPanel](https://www.cloudpanel.io/) is a hosting panel that is light and packed with a lot of features. It will help you host PHP, Python or NodeJs apps with just one click. Besides this, it has also a feature that will allow use it as a reverse proxy for your docker apps.\r\n\r\nYou can use other alternatives as reverse proxy for your self-hosted apps but in case you want to host also websites that are not in docker this will be a good alternative.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n**Below is a list of what CloudPanel has to offer:**\r\n\r\n- File Manager\r\n- IP & Bot Blocking\r\n- Varnish Cache & Redis\r\n- SSH/FTP\r\n- Firewall\r\n- Cron Jobs\r\n- Vhost Editor\r\n- Remote Backup with Rclone\r\n- Free Let’s Encrypt Certificates\r\n- Cloudflare Integration\r\n- User Management\r\n- System resources usage graphs\r\n- Multiple PHP versions\r\n- MySQL and MariaDB support\r\n- Node.js, Python support\r\n- Nginx Web Server\r\n\r\n**Minimum requirements:**\r\n\r\n- Minimum 1 core\r\n- Minimum 2 GB RAM\r\n- 10 GB of disk space\r\n\r\nIn this article, we are going to see how you can Install CloudPanel Docker and Dockge and make the most of your VPS. [Dockge](https://dockge.kuma.pet/) is a panel that can help you host docker apps with just one click I have made an article here: [Dockge Install](https://www.bitdoze.com/dockge-install/) that will provide more details on the panel.\r\n\r\nCloudPanel will be used as a proxy server that will send the traffic to your Docker containers and Dockge will be the app that will help you administrate these docker containers and deploy apps easily.\r\n\r\n>If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/BuoyvbDVBe0\"\r\n  label=\"Setup CloudPanel As Reverse Proxy with Docker and Dockge\"\r\n/>\r\n\r\n## 1. Create a VPS server\r\n\r\nYou will need a VPS server where you can host your docker containers I am recommending [Hetzner](https://go.bitdoze.com/hetzner), for more details check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/) and you can check also: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/)\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\nThe VPS process is easy and you just need a Ubuntu server, CloudPanel and Dockge are working on ARM and x86_64 also.\r\n\r\n## 2. Update the VPS server\r\n\r\nFirst thing always is to update your Ubuntu to use the latest packages:\r\n\r\n```sh\r\nsudo apt update && sudo apt upgrade\r\n```\r\n\r\n## 3. Install CloudPanel\r\n\r\n> I have also created a course that will help you get going with Cloudpanel if you are a beginner, check **[CloudPanel Setup Course](https://webdoze.net/courses/cloudpanel-setup/)**\r\n\r\nI use MariaDB 10.11 for this install and Hetzner as a cloud provider you can check [Cloudpanel Install Doc](https://www.cloudpanel.io/docs/v2/getting-started/hetzner-cloud/installation/installer/) to check the options:\r\n\r\n```bash\r\ncurl -sS https://installer.cloudpanel.io/ce/v2/install.sh -o install.sh; \\\r\necho \"85762db0edc00ce19a2cd5496d1627903e6198ad850bbbdefb2ceaa46bd20cbd install.sh\" | \\\r\nsha256sum -c && sudo CLOUD=hetzner DB_ENGINE=MARIADB_10.11 bash install.sh\r\n```\r\n\r\n## 4. Access CloudPanel Admin\r\n\r\nYou can access the CloudPanel admin with the **https://serverIpAddress:8443** , here you will need to create an account that will be used. In the next steps, we are going to go and create a subdomain and use it to access CloudPanel in the future.\r\n\r\n## 5. Create an Admin Subdomain:\r\n\r\nTo access CloudPanel securely and have an SSL certificate we need to create a subdomain and point it to the CloudPanel VPS server. I am using CloudFlare for this and I just go and add an A record under DNS on the domain I want to host. You can use what DNS provider you have, you just need to point the subdomain/domain A record to the server IP.\r\n\r\nNext, we need to go and add it to the CloudPanel Admin area in Settings. And add it there to secure the admin area.\r\n\r\n## 6. Install Docker and Docker Compose\r\n\r\nNow that you have your VPS created and CloudPanel set, you will need to install docker, the below will need to be done to have docker UP and running:\r\n\r\n```sh\r\nsudo apt-get update\r\nsudo apt-get install ca-certificates curl gnupg lsb-release\r\nsudo mkdir -p /etc/apt/keyrings\r\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\r\necho \\\r\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\r\n  jammy stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\r\nsudo apt-get update\r\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose\r\n```\r\n\r\nEverything is also explained in [Install Docker & Docker-compose for Ubuntu](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n\r\n## 7. Install Dockge\r\n\r\nIn the next part, we will go and install Dockge on a subdomain/domain in CloudPanel, under this user we are going to keep our Dockge installation and the future apps.\r\n\r\n> In this way, if you have CloudPanel external backup activated you will backup all the apps and Dockge at once.\r\n\r\n### 7.1 Add Reverse Proxy in CloudPanel\r\n\r\nIn this part, we are going to add the Reverse Proxy in CloudPanel with the port that we are going to use for Dockge. `Under Sites - Add Site` you choose `Create a Reverse Proxy`\r\n\r\n<Picture src={imag1} alt=\"CloudPanel Create a Reverse Proxy\" />\r\n\r\nIn the next part you will add the domain with the user password and port that Dockge will use, in my case will be the `5000` port.\r\n\r\n<Picture src={imag2} alt=\"CloudPanel Create a Reverse Proxy\" />\r\n\r\nYou choose what you want to use.\r\n\r\n### 7.2 Create the Directories For Dockge\r\n\r\nAfter you have the website added you will need to open a terminal to your server and under the path where website is created in CloudPanel you create the dockge directories:\r\n\r\n```sh\r\ncd /home/<user>/htdocs/<website>/\r\n#eg:\r\ncd /home/bitdoze-dockge/htdocs/dockge.bitdoze.com/\r\n```\r\n\r\nYou just replace the `<user>` and `<website>` with your specific details, from point 7.1\r\n\r\nCreate the directories:\r\n\r\n```sh\r\nmkdir dockge-stacks\r\nmkdir dockge\r\n```\r\n\r\n- **dockge** - will be used to have the app installed\r\n- **dockge-stacks** - will be the place where all the apps will reside\r\n\r\n### 7.3 Install Dockge\r\n\r\n```yaml\r\nversion: \"3.8\"\r\nservices:\r\n  dockge:\r\n    image: louislam/dockge:1\r\n    restart: unless-stopped\r\n    ports:\r\n      - 5000:5001\r\n    volumes:\r\n      - /var/run/docker.sock:/var/run/docker.sock\r\n      - ./data:/app/data\r\n      - /home/<user>/htdocs/<website>/dockge-stacks:/home/<user>/htdocs/<website>/dockge-stacks\r\n    environment:\r\n      # Tell Dockge where to find the stacks\r\n      - DOCKGE_STACKS_DIR=/home/<user>/htdocs/<website>/dockge-stacks\r\n```\r\n\r\nIn the above files you have to replace:\r\n\r\n- **Port** - in my case I am using 5000 so you just replace with your port the left one\r\n- **Volumes** - `/home/<user>/htdocs/<website>/dockge-stacks:/home/<user>/htdocs/<website>/dockge-stacks` will need to be replaces with your paths from previous point\r\n- **DOCKGE_STACKS_DIR** - same as previous is the path where you will have your stacks.\r\n\r\nA full example is:\r\n\r\n```yaml\r\nversion: \"3.8\"\r\nservices:\r\n  dockge:\r\n    image: louislam/dockge:1\r\n    restart: unless-stopped\r\n    ports:\r\n      - 5000:5001\r\n    volumes:\r\n      - /var/run/docker.sock:/var/run/docker.sock\r\n      - ./data:/app/data\r\n      - /home/bitdoze-dockge/htdocs/dockge.bitdoze.com/dockge-stacks:/home/bitdoze-dockge/htdocs/dockge.bitdoze.com/dockge-stacks\r\n    environment:\r\n      # Tell Dockge where to find the stacks\r\n      - DOCKGE_STACKS_DIR=/home/bitdoze-dockge/htdocs/dockge.bitdoze.com/dockge-stacks\r\n```\r\n\r\n[Dokge Compose Create](https://dockge.kuma.pet/) can help you create the exact docker-compose file.\r\n\r\nAfter you save the file with the name `compose.yaml` and run:\r\n\r\n```sh\r\ndocker compose up -d\r\n```\r\n\r\nThis will bring up the Dockge app that will run on port `5000` in my case.\r\n\r\n### 7.4 Point Domain/Subdomain to Dockge\r\n\r\nIn your DNS provider, you need to create an A record that will be sent to the server IP.\r\n\r\n### 7.5 Create an SSL certificate\r\n\r\nThe domain that will be used for Dockge will need to be secured with SSL, you need to go into CloudPanel under `Sites - Manage Site - SSL/TLS` and generate an SSL certificate, a Let's Encrypt one.\r\n\r\n### 7.6 Access Dockge and Create a User and Password.\r\n\r\nFirst time when you access Dockge you will be prompted to create a user and a password, you just need to create one. You access Dockge with the domain you have used for installation.\r\n\r\n### 7.7 Open CloudPanel Firewall Ports\r\n\r\nBy default CloudPanel is activating a firewall and only a couple of ports are opened like 8443,443,22 and 80 if you want to not assign domains or subdomains to your docker apps you should go and open the ranges that you are going to use for your docker apps.\r\n\r\nTo open the ports you should go under **Admin Area - Security - Add Rule** and open the ranges as in the picture:\r\n\r\n<Picture src={imag3} alt=\"CloudPanel Port Open\" />\r\n\r\nThis will allow access to the ports from outside.\r\n\r\n## 8. Deploy First App\r\n\r\nNow you are set you can access Dockge and start deploy your apps.\r\n\r\n<Picture src={imag4} alt=\"Dockge Add Compose\" />\r\n\r\nBelow there are some articles with apps that you can deploy on Dockge:\r\n\r\n- [Install Umami Analytics](https://www.bitdoze.com/umami-analytics-install/)\r\n- [Install Outline Wiki](https://www.bitdoze.com/outline-install/)\r\n- [Slash Install](https://www.bitdoze.com/slash-docker-deploy/)\r\n\r\nAt the end when the app is deployed you will need to:\r\n\r\n- **1. Create a Reverse Proxy in CloudPanel** - same as you did for Dockge you will need to Create a website in CloudPanel as a Reverse Proxy and add the domain/subdomain that you will use with the port you have given to that app.\r\n- **2. Point the Subdomain/Domain to Server** - In your DNS manager add A record to the server for domain or subdomain that you are going to use\r\n- **3. Create SSL Certificate** - your app will need to have an SSL certificate you need to go into CloudPanel under `Sites - Manage Site - SSL/TLS` and generate an SSL certificate, a Let's Encrypt one.\r\n\r\n## 9. Activate backups\r\n\r\nTo secure your apps in case something happens you can go and activate CloudPanel external backups to Dropbox, S3, Google Drive or any SMTP you may have. You can go under **Admin Area - Backups** and activate the rClone backups to external source.\r\n\r\nYou can also enable Hetzner snapshots if you want to be better protected, in function of your VPS provider CloudPanel is offering integrations.\r\n\r\n> I have also created a course that will help you get going with Cloudpanel if you are a beginner, check **[CloudPanel Setup Course](https://webdoze.net/courses/cloudpanel-setup/)**","src/content/posts/cloudpanel-setup-dockge.mdx",[524],"../../assets/images/24/02/cloudpanel-dockge-setup.jpeg","7a4f74ca0a78c7d2","cloudpanel-setup-dockge.mdx","compare-folders-content-differences",{id:527,data:529,body:538,filePath:539,assetImports:540,digest:542,legacyId:543,deferredRender:32},{title:530,description:531,date:532,image:533,authors:534,categories:535,tags:536,canonical:537},"How To Compare Two Folders Content and See Different Files in Terminal","Learn how you can compare two folders and see the different files in terminal in a linux or MacOs",["Date","2023-11-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/compare-folders-content-differences.jpeg",[19],[98],[449],"https://www.bitdoze.com/compare-folders-content-differences/","If you have two folders that contain similar files, you might want to compare them and see what are the differences. For example, you might have a backup folder that is supposed to be identical to your original folder, but you are not sure if they are in sync. Or you might have two versions of a project that you want to merge or update.\r\n\r\nFor me I needed to check the different files in two repos, I contracted someone to help me with some issues on this website in astro.js and he provided me the zip archive with the finished code. I needed to see what are the modified files so I could understand what changed.\r\n\r\nIn this article, you will learn how to compare two folders’ content and see different files in the terminal using some simple commands. This can be done in the terminal and will work in MacOS or Linux, on Windows can also work but you need the Windows subsystem to have access to the command.\r\n\r\nBy the end of this article, you will be able to compare any two folders and find out what files are added, deleted, modified, or unchanged.\r\n\r\n## Compare Two Folders Content and See Different Files in Terminal\r\n\r\nThe **diff** command is a command-line utility that compares two files or directories line by line and displays the differences between them. It also tells you what changes you need to make to one file or directory to make it match the other one. The basic syntax of the diff command is:\r\n\r\n```bash\r\ndiff [options] file1 file2\r\n```\r\n\r\nor\r\n\r\n```bash\r\ndiff [options] dir1 dir2\r\n```\r\n\r\nTo compare two directories, you need to use the -rq option\r\n\r\n```bash\r\ndiff -rq dir1 dir2\r\n```\r\n\r\n- **-r** option, which stands for recursive. This option tells the diff command to compare all the files and subdirectories inside the specified directories. For example, if you have two directories named dir1 and dir2, you can compare them by running:\r\n\r\n- **-q** makes the output more concise and readable -q option stands for brief. This option tells the diff command to only report when files differ, without showing the details of the differences. For example, if you run:\r\n\r\n```bash\r\ndiff -rq  /Users/dbalota/websites/bitdoze-astro-bkw  /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze\r\n```\r\n\r\nThe output will look something like this:\r\n\r\n```bash\r\nOnly in /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src: .DS_Store\r\nOnly in /Users/dbalota/websites/bitdoze-astro-bkw/src/assets/favicons: Grammarly.c8npGa4ajbhi8fee1ud80d82.dmg\r\nOnly in /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/assets/favicons: favicon.ico\r\nOnly in /Users/dbalota/websites/bitdoze-astro-bkw/src/assets/favicons: favicon.png\r\nOnly in /Users/dbalota/websites/bitdoze-astro-bkw/src/assets/favicons: favicon.svg\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/config/menu.json and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/config/menu.json differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/content/posts/add-accordion-carrd.mdx and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/content/posts/add-accordion-carrd.mdx differ\r\nOnly in /Users/dbalota/websites/bitdoze-astro-bkw/src/content/posts: compare-folders-content-differences.mdx\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/content/posts/opnform-open-source.mdx and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/content/posts/opnform-open-source.mdx differ\r\nOnly in /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts: .DS_Store\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/Base.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/Base.astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/PostSingle.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/PostSingle.astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/Posts.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/Posts.astro differ\r\nOnly in /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/components: .DS_Store\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/components/SimilarPosts.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/components/SimilarPosts.astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/components/widgets/Button.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/components/widgets/Button.astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/layouts/partials/Footer.astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/layouts/partials/Footer.astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/pages/[regular].astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/pages/[regular].astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/pages/authors/[single].astro and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/pages/authors/[single].astro differ\r\nFiles /Users/dbalota/websites/bitdoze-astro-bkw/src/styles/base.scss and /Users/dbalota/Desktop/orange-facturi/easypanel/bitdoze/src/styles/base.scss diffe\r\n```\r\n\r\nIn the above output, you can see that it will give you the exact files which are different.\r\n\r\n## Enhance The Diff Command and Filter The Results\r\n\r\nOne way to filter the output of the diff command is to use the grep command, which can search for a pattern in the input and print only the matching lines. For example, if we want to compare two directories, dir1 and dir2, and exclude a certain file from the comparison, we can use the following command:\r\n\r\n```bash\r\ndiff -rq dir1 dir2 | grep -v <file>\r\n```\r\n\r\nThe pipe symbol (|) redirects the output of the diff command to the input of the grep command. The **grep -v** option tells the grep command to invert the match, meaning that it will print only the lines that do not contain the pattern. The pattern is the name of the file we want to exclude, enclosed in angle brackets. For example, if we want to exclude the file README.md, we can use:\r\n\r\nAnother way to filter the output of the diff command is to use the **grep -i** option, which tells the grep command to ignore the case of the pattern. This can be useful if we want to compare two directories and find the files that have the same name but different cases. For example, if we want to compare dir1 and dir2 and find the files that have the name file.txt, regardless of the case, we can use the following command:\r\n\r\n```bash\r\ndiff -rq dir1 dir2 | grep -i file.txt\r\n```\r\n\r\nThe grep -i option tells the grep command to match the pattern file.txt in any case, such as File.txt, FILE.TXT, or fiLe.TxT. For example, if dir1 contains File.txt and dir2 contains file.txt, the command will print:\r\n\r\nFiles dir1/File.txt and dir2/file.txt differ\r\n\r\nBy using these filters, we can enhance the diff command and make it more flexible and powerful for our purposes.\r\n\r\n## Conclusions\r\n\r\nThis easy is to compare two folders with subdirectories and see exactly what fails are different, with the terminal on Linux or MacOS you don't need any app besides the command diff. You can enhance the command with grep to find exactly what you are interested in and also you can use **-u** option to see the exact changes.","src/content/posts/compare-folders-content-differences.mdx",[541],"../../assets/images/23/11/compare-folders-content-differences.jpeg","7b7f56858a3559f3","compare-folders-content-differences.mdx","coolify-install-heroku-alternative",{id:544,data:546,body:556,filePath:557,assetImports:558,digest:560,legacyId:561,deferredRender:32},{title:547,description:548,date:549,image:550,authors:551,categories:552,tags:553,canonical:555},"Coolify Install A Free Heroku and Netlify Self-Hosted Alternative","Free Heroku and Netlify alternative? Coolify Install is an easy-to-use self-hosted platform that will help you get started quickly, without any complicated setup.",["Date","2023-02-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/02/coolify-install.jpeg",[19],[77],[554,23,24],"coolify","https://www.bitdoze.com/coolify-install-heroku-alternative/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport img1 from \"../../assets/images/23/02/admin_url_coolify.jpeg\";\r\nimport img2 from \"../../assets/images/23/02/github-coolify.jpeg\";\r\nimport img3 from \"../../assets/images/23/02/astro-Coolify1.jpeg\";\r\nimport img4 from \"../../assets/images/23/02/astro-deploy-coolify.jpeg\";\r\nimport img5 from \"../../assets/images/23/02/Coolify_astro_build.jpeg\";\r\n\r\nIf you want a free Netlify or Heroku alternative that is self-hosted then [Coolify](https://coolify.io/) is the best option out there.\r\n\r\nCoolify is an open-source and self-hosted Heroku alternative that offers users the same features as Heroku, such as automated deployment, scalability, and easy configuration, but with the added benefit of being self-hosted.\r\n\r\nThis gives users full control over their hosting environment and access to features such as multi-application and multi-domain support, application backup and restore, and more.\r\n\r\nMinimum requirements for Coolify\r\n\r\n- 2 CPUs\r\n- 2 GB of memory\r\n- 30+ GB of storage for images.\r\n\r\nWith Coolify you can deploy the following:\r\n\r\n**Applications**\r\n\r\n- Static sites\r\n- NodeJS\r\n- VueJS\r\n- NuxtJS\r\n- NextJS\r\n- React/Preact\r\n- Gatsby\r\n- Svelte\r\n- PHP\r\n- Laravel\r\n- Rust\r\n- Docker\r\n- Python\r\n- Deno\r\n\r\n**Databases**\r\n\r\n- MongoDB\r\n- MariaDB\r\n- MySQL\r\n- PostgreSQL\r\n- CouchDB\r\n- Redis\r\n\r\n**Services**\r\n\r\n- WordPress\r\n- Ghost\r\n- Plausible Analytics\r\n- NocoDB\r\n- VSCode Server\r\n- MinIO\r\n- VaultWarden\r\n- LanguageTool\r\n- n8n\r\n- Uptime Kuma\r\n- MeiliSearch\r\n- Umami\r\n- Fider\r\n- Hasura\r\n\r\nThe full list of [Services Templates for Coolify](https://github.com/coollabsio/coolify-community-templates)\r\n\r\n**Supported Architectures**\r\n\r\nAMD64 and ARM architecture are supported. Debian based servers are supported, due to the installation script.\r\n\r\nWith Coolify you can deploy projects from GitHub or GitLab and like Netlify or Heroku if something is updated in the repo it will be updated in the deployment.\r\n\r\nAnother alternative to Coolify is EasyPanel you can check: [Easypanel.io: A Modern Hosting Panel for Applications and Databases](https://www.bitdoze.com/easypanel-modern-server-control-panel/) for more details.\r\n\r\n## Video With Coolify Install\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/dY-hUI3fHEM\"\r\n  label=\"Coolify Install A Free Heroku and Netlify Self-Hosted Alternative\"\r\n/>\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n## How to Install Coolify on a VPS\r\n\r\nIn this section we will go through all the steps we need to do to get Coolify installed on a VPS server. You can use any VPS provider you want, the most known are [DigitalOcean](https://go.bitdoze.com/do), [Vultr](https://go.bitdoze.com/vultr), [Hetzner](https://go.bitdoze.com/hetzner), I also wrote an article and made a video with the benchmarks here: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/) you can check it out.\r\n\r\nIn this tutorial we are going to use Hetzner for this where we have the VPS created and we can SSH to it to have Coolify installed.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n### Below are some of the tutorials on Coolify:\r\n\r\n- [Install Uptime Kuma With One Click](https://www.bitdoze.com/deploy-uptime-kuma/)\r\n- [Install Plausible With One Click](https://www.bitdoze.com/install-plausible-analytics/)\r\n\r\n### Installing Coolify on Ubuntu 22.04\r\n\r\nWe are going to install Coolify on a Ubuntu 22.04 server. To do this, you just need to SSH to the VPS server and run the following command:\r\n\r\n```bash\r\nwget -q https://get.coollabs.io/coolify/install.sh \\\r\n-O install.sh; sudo bash ./install.sh\r\n```\r\n\r\nWhile I was installing the script I saw that docker 23 version was released and I had to go into the install script and change DOCKER_MAJOR=23 DOCKER_MINOR=0 for the installation to work, this is a new thing as docker was just released.\r\n\r\nIf you are interested is how you can monitor your CPU and have an automatic email sent when load is too big you should check: [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)\r\n\r\n### Create a Coolify user\r\n\r\nAll you need to do is register a new account in the login area with http://ip:3000. You can enter your email and password.\r\n\r\n### Point the domain/subdomain to the server\r\n\r\nTo access Coolify securely, you need to point your domain or subdomain to the server IP. All you need to do is add an A record and point it to the server IP.\r\n\r\n### Add an admin URL\r\n\r\nTo do this, go to Settings - Coolify Settings and in URL (FQDN) add your admin domain.\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"Coolify Admin URL\"\r\n/>\r\n\r\nAfter that, you can use the new URL to sign in and deploy applications or services.\r\n\r\n### Add a GitHub repo\r\n\r\nThis will link your GitHub profile to Coolify so you can choose which repo to deploy from, to do this just go to\r\n\r\nCreate New Resource and select the git source, just add it there and follow the prompts. The video has all the details.\r\n\r\n### Deploy and Node.js static website.\r\n\r\nI have this website that is hosted on CloudFlare Pages and I will deploy it on a subdomain with Coolify to see exactly how it works.\r\n\r\n#### Creating the application\r\n\r\nThe first step is to go to Create New Resource - Application and select Github, as shown in the image below:\r\n\r\n<Picture\r\n  src={img2}\r\n  alt=\"Coolify Gitlab select\"\r\n/>\r\n\r\nAfter you select the repo and branch and hit save. After you get something like the below where you have the type, in my case this is an astro website and it is selected:\r\n\r\n<Picture\r\n  src={img3}\r\n  alt=\"Coolify Astro select\"\r\n/>\r\n\r\n#### Point the domain to the serve\r\n\r\nTo access the static website you just need to go and point the subdomain or website to the server again, just add a record for root or subdomain in function of what you are using.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n#### Adding the domain to Coolify\r\n\r\nAfter you point the domain you just add the URL as in the picture below, everything will be set up as in the picture below as sees that Astro is used:\r\n\r\n<Picture\r\n  src={img4}\r\n  alt=\"Coolify astro deploy\"\r\n/>\r\n\r\nAfter you hit save, just hit deploy. It will take longer the first time, but after the build it will be faster.\r\n\r\nIf you change something, Coolify will automatically fetch the latest version, just like Heroku or Netlify or other cloud deployments out there. This is enabled in **Features - Enable Automatic Deployment**.\r\n\r\n<Picture\r\n  src={img5}\r\n  alt=\"Coolify astro deploy logs\"\r\n/>\r\n\r\nAfter that you can go and deploy a database or an application to see exactly how easy it is and without any problems.\r\n\r\n### Conclusion\r\n\r\nIn conclusion,Coolify Install is an excellent free alternative to Heroku and Netlify for those looking for a self-hosted solution. It's easy to install, configure, and manage with its intuitive user interface. Additionally, the platform supports all of the main programming languages, so it's suitable for development projects in any field.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\nIn case you are interested to have a web panel that can help you manage your applications and be used as a reverse proxy you can check the bellow course:\r\n\r\n<Button\r\n  link=\"https://webdoze.net/courses/cloudpanel-setup/\"\r\n  text=\"CloudPanel Setup Course\"\r\n/>","src/content/posts/coolify-install-heroku-alternative.mdx",[559],"../../assets/images/23/02/coolify-install.jpeg","7388a0ad99a89223","coolify-install-heroku-alternative.mdx","convert-images-to-svg",{id:562,data:564,body:576,filePath:577,assetImports:578,digest:580,legacyId:581,deferredRender:32},{title:565,description:566,date:567,image:568,authors:569,categories:570,tags:571,canonical:575},"Convert Images to SVG Free With Vectorizer.ai","Convert PNG, JPEG images to SVG easily online for free with Vectorizer.ai",["Date","2023-03-30T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/03/img-to-svg-free.jpeg",[19],[77],[572,573,574,24],"free-tools","online-tools","art-tools","https://www.bitdoze.com/convert-images-to-svg/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n## Benefits of SVG\r\n\r\nScalable Vector Graphics, or SVG, is a file format used for 2D graphics on the Web. It is an XML-based format and can be edited with any text editor. One of the main advantages of SVG is its scalability, which means it can be scaled to any size without losing quality. This makes it an excellent option for responsive web design.\r\n\r\nAnother benefit of SVG is its small file size. Unlike raster images, which are made up of pixels, SVG files are created using mathematical equations that define shapes and lines. As a result, they have smaller file sizes than JPEGs or PNGs of the same dimensions. This not only improves Web page loading times but also reduces the strain on server resources.\r\n\r\nSVG also offers superior image clarity and sharpness compared to other image formats because it is resolution-independent.\r\n\r\n## Vectorizer.ai Overview\r\n\r\n[Vectorizer.ai](https://vectorizer.ai/) is an innovative platform that allows users to convert their raster images into high-quality vector graphics. With its powerful AI algorithms, Vectorizer.ai can accurately identify and trace the edges of any image to create a smooth and scalable vector graphic.\r\n\r\nOne of the main advantages of Vectorizer.ai is its ease of use. The platform is incredibly user-friendly and intuitive, which means that even novice users can quickly learn how it works. In addition, the software supports a wide range of raster image formats, including JPEGs, PNGs, GIFs, BMPs and more, meaning that you can convert just about any digital image into a vector graphic with this tool.\r\n\r\nVectorizer.ai also has an API that can be used to convert your images to SVG, currently in beta and free to use.\r\n\r\n## How to use Vectorizer.ai to convert images to SVG\r\n\r\nThe process is simple, you just need to navigate to their website select the image you want and upload it, their AI will take care of the rest and create the SVG image for you. I used it to convert my logo for bitdoze.com and it did a very nice job, also I converted the plausible.io logo to SVG which is more complex and the results were very good.\r\n\r\nYou can check out the video below to see how it did with the plausible.io logo.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/dbbEQssxC6A\"\r\n  label=\"Shots.so - Create Beautiful Mockups\"\r\n/>\r\n\r\n## Conclusions\r\n\r\n[Vectorizer.ai](https://vectorizer.ai/) is free for now, I hope it will stay free in the future and have a free option. If you want to convert your images to SVG, you should do it now.","src/content/posts/convert-images-to-svg.mdx",[579],"../../assets/images/23/03/img-to-svg-free.jpeg","adfb445893a34aa5","convert-images-to-svg.mdx","crowdsec-secure-server",{id:582,data:584,body:593,filePath:594,assetImports:595,digest:597,legacyId:598,deferredRender:32},{title:585,description:586,date:587,image:588,authors:589,categories:590,tags:591,canonical:592},"How To Secure a VPS Sever with CrowdSec","Secure the VPS server by blocking firewall access with CrowdSec by monitoring SSH and Nginx logs.",["Date","2025-02-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/02/crowdsec-secure-server.jpg",[19],[98],[135],"https://www.bitdoze.com/crowdsec-secure-server/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\n\r\nThis tutorial focuses on securing your SSH access using **[CrowdSec](https://www.crowdsec.net/)**, a modern, collaborative security solution designed to protect against various attack vectors, including those targeting VPS servers. Unlike traditional security tools, CrowdSec combines log analysis, behavior detection, and community-driven threat intelligence to proactively block suspicious activity while sharing insights across its user base.\r\n\r\nIn this guide, you’ll learn:\r\n- Why securing SSH is critical for your server's health and data integrity.\r\n- What makes CrowdSec a standout security tool.\r\n- How to install and configure CrowdSec on your VPS.\r\n- Ways to monitor and analyze server logs, extend protection to other applications like Nginx, and link to the CrowdSec Console for centralized management.\r\n\r\nWhether you’re a developer managing multiple servers or a small business owner hosting applications online, CrowdSec offers a reliable and straightforward way to enhance your security posture. This tutorial breaks down every step so you can follow along with confidence, even without extensive prior experience.\r\n\r\n\r\n## **What Is CrowdSec and How Can It Help You?**\r\n---\r\n\r\n### Understanding CrowdSec\r\n\r\nCrowdSec is an open-source, collaborative intrusion prevention system (IPS) designed to protect servers, services, and applications from malicious activities. It works by analyzing system logs, detecting suspicious patterns, and enabling proactive defense mechanisms. Unlike traditional security tools, CrowdSec actively shares threat intelligence with its global user community, allowing it to build a constantly evolving database of malicious IP addresses.\r\n\r\nHere’s what makes CrowdSec a unique and powerful tool:\r\n- **Log Analysis:** CrowdSec parses and analyzes logs from various sources like SSH, Nginx, and custom applications.\r\n- **Real-time Remediation:** It uses \"bouncers\" (remediation components) to automatically block malicious IPs.\r\n- **Collaborative Threat Intelligence:** CrowdSec leverages a community-driven model where users share anonymized attack data to improve overall global security.\r\n\r\n\r\n![Understanding CrowdSec](../../assets/images/25/02/crowdsec1.png)\r\n\r\n\r\n\r\n**What Problem Does CrowdSec Solve?**\r\nCyberattacks such as brute force, Distributed Denial of Service (DDoS), and credential stuffing are increasing in sophistication. While firewalls and intrusion detection systems (IDS) are helpful, they often lack real-time adaptability or community-sourced intelligence. CrowdSec not only detects these attacks but also provides actionable responses and leverages insights from other users to bolster your defenses.\r\n\r\n### Benefits of Using CrowdSec for Securing SSH\r\nIf your VPS uses SSH for remote access, leaving it unprotected can lead to:\r\n- Unauthorized access via brute force attacks.\r\n- Exploitation of known vulnerabilities in outdated SSH configurations.\r\n- Server compromise leading to data loss or service disruption.\r\n\r\nCrowdSec enhances SSH security by:\r\n1. **Detecting Intrusions:** Analyzing SSH attempt logs and identifying patterns like failed login attempts.\r\n2. **Blocking Threats:** Using remediation tools (like the Firewall Bouncer) to block malicious IPs.\r\n3. **Providing Visibility:** Offering detailed metrics and logs for monitoring failed attempts and remediation outcomes.\r\n4. **Adapting Dynamically:** Updating its detection rules automatically through its community-driven hub.\r\n\r\n### Why Choose CrowdSec Over Alternatives?\r\n\r\nHere’s how CrowdSec compares to other security tools:\r\n\r\n| Feature                   | CrowdSec                  | Fail2Ban                    | Traditional Firewalls       |\r\n|---------------------------|---------------------------|-----------------------------|-----------------------------|\r\n| **Community Intelligence**| ✔️ Global threat sharing  | ❌ None                     | ❌ None                     |\r\n| **Ease of Use**           | ✔️ Simple configuration   | ✔️ Moderate complexity      | ❌ Manual configuration required |\r\n| **Multi-service Support** | ✔️ Supports many services | ✔️ Limited (e.g., SSH only) | ✔️ Generic (ports only)     |\r\n| **Scalability**           | ✔️ Cloud-friendly         | ✔️ Local VPS only           | ✔️ Network-based            |\r\n| **Real-time Blocking**    | ✔️ Fast with bouncers     | ✔️ Moderate                 | ❌ Reactive only            |\r\n\r\nBy opting for CrowdSec, you not only protect your VPS but also contribute to a larger global effort to fight cyber threats. It’s an excellent choice for developers, system administrators, and businesses looking for cost-effective, scalable, and collaborative security solutions.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/9y6i2XjCVAw\"\r\n  label=\"CrowdSec Install\"\r\n/>\r\n\r\n\r\n##  Install CrowdSec\r\n---\r\n\r\nInstalling CrowdSec on your VPS is a straightforward process that involves adding a repository, installing the package, and configuring it to monitor logs. Below is a detailed step-by-step guide to help you get started.\r\n\r\n### Step 1: Add the CrowdSec Repository\r\nCrowdSec can be installed using its package repository for most Linux distributions. Add the repository by running the following command:\r\n\r\n```bash\r\ncurl -s https://install.crowdsec.net | sudo sh\r\n```\r\n\r\n#### Expected Output:\r\nYour system should respond with the following information, confirming the repository was successfully added:\r\n```\r\nDetected operating system as ubuntu/22.\r\nDetected apt version as 2.4.13\r\nChecking for gpg...\r\nDetected gpg...\r\n...\r\nInstalling /etc/apt/sources.list.d/crowdsec_crowdsec.list...\r\n```\r\n\r\nThis command imports the necessary GPG keys and sets up the CrowdSec repository.\r\n\r\n\r\n### Step 2: Install the CrowdSec Package\r\n\r\nOnce the repository is added, update your package index and install the CrowdSec package:\r\n\r\n```bash\r\nsudo apt update && sudo apt install crowdsec\r\n```\r\n\r\n#### Important Notes:\r\n- **Port Usage Warning:** CrowdSec’s local API uses **port 8080** by default. Before proceeding, ensure the port is not already in use by checking with:\r\n  ```bash\r\n  netstat -tulpn\r\n  ```\r\n  If port 8080 is busy, you can modify the configuration later (explained below).\r\n\r\n#### Sample Installation Output:\r\nHere’s an example of what you might see during installation:\r\n```\r\nNeed to get 60.9 MB of archives.\r\nAfter this operation, 249 MB of additional disk space will be used.\r\n...\r\nSetting up crowdsec (1.6.4) ...\r\nMachine successfully added to the local API.\r\nAPI credentials written to '/etc/crowdsec/local_api_credentials.yaml'.\r\n...\r\nNot attempting to start crowdsec, port 8080 is already used or lapi was disabled.\r\n```\r\n\r\n\r\n### Step 3: Modify API Port (If Necessary)\r\n\r\nIf you encounter the error stating that port 8080 is in use, you’ll need to edit CrowdSec’s configuration files to change the default API port. Here’s a quick guide:\r\n\r\n1. Open the configuration files for editing:\r\n   ```bash\r\n   sudo nano /etc/crowdsec/config.yaml\r\n   sudo nano /etc/crowdsec/local_api_credentials.yaml\r\n   ```\r\n2. Change the `listen_uri` under `api` to a different port, such as 8081:\r\n   ```yaml\r\n   listen_uri: 127.0.0.1:8081\r\n   ```\r\n3. Save the files and restart CrowdSec:\r\n   ```bash\r\n   sudo service crowdsec start\r\n   sudo service crowdsec status\r\n   ```\r\n\r\nOnce updated, the service should run successfully, and you can verify its status with:\r\n\r\n```bash\r\nsudo service crowdsec status\r\n```\r\n\r\n#### Example Output:\r\n```\r\n● crowdsec.service - Crowdsec agent\r\n     Loaded: loaded (/lib/systemd/system/crowdsec.service; enabled; vendor preset: enabled)\r\n     Active: active (running) since Thu 2025-02-06 06:52:02 UTC; 5s ago\r\n```\r\n\r\n\r\n### Step 4: List Installed Collections\r\n\r\nAfter installation, CrowdSec automatically installs collections to monitor key services like SSH. To check the collections in use, run:\r\n\r\n```bash\r\nsudo cscli collections list\r\n```\r\n\r\n#### Example Output:\r\n```\r\nCOLLECTIONS\r\n Name                               📦 Status    Version  Local Path\r\n─────────────────────────────────────────────────────────────────────\r\n crowdsecurity/linux                ✔️  enabled  0.2      ...\r\n crowdsecurity/sshd                 ✔️  enabled  0.5      ...\r\n```\r\n\r\nThe `crowdsecurity/sshd` collection indicates SSH monitoring is active.\r\n\r\n\r\nWith CrowdSec installed and running, your VPS is now set up to detect and analyze potential threats targeting your SSH and other services.\r\n\r\n\r\n##  Installing the First Remediation Component (Firewall Bouncer)\r\n---\r\n\r\nCrowdSec relies on remediation components called “bouncers” to block malicious actors based on detected attack patterns. The **firewall bouncer** is one of the most critical components for enhancing your VPS’s security since it works with your server’s firewall to block IPs flagged by CrowdSec.\r\n\r\nHere’s how to install and verify the firewall bouncer:\r\n\r\n\r\n\r\n### Step 1: Install the Firewall Bouncer\r\n\r\nTo install the `iptables` firewall bouncer with CrowdSec, run the following command:\r\n\r\n```bash\r\nsudo apt install crowdsec-firewall-bouncer-iptables\r\n```\r\n\r\n#### Output Example:\r\nWhen the installation completes, you’ll see something like this:\r\n```\r\nThe following NEW packages will be installed:\r\n  crowdsec-firewall-bouncer-iptables\r\n0 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\r\n...\r\nSetting up crowdsec-firewall-bouncer (0.0.31) ...\r\n```\r\n\r\nThe bouncer is now integrated into your system and ready to block malicious IPs.\r\n\r\n\r\n\r\n### Step 2: Verify the Bouncer Installation\r\n\r\nAfter installation, check the firewall bouncer’s status and confirm it’s operating correctly:\r\n\r\n```bash\r\nsudo cscli bouncers list\r\n```\r\n\r\n#### Example Output:\r\n```\r\n╭───────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\r\n│ Name                            IP Address  Valid  Last API pull         Type                       Version    │\r\n├───────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\r\n│ cs-firewall-bouncer-1738824978  127.0.0.1   ✔️    2025-02-06T06:57:35Z  crowdsec-firewall-bouncer  v0.0.31    │\r\n╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\r\n```\r\n\r\nKey details in the output:\r\n- **Name:** The unique identifier of the bouncer.\r\n- **Type:** Indicates this is the `crowdsec-firewall-bouncer`.\r\n- **Status:** `Valid` confirms the bouncer is active.\r\n- **Last API pull:** Shows when the bouncer last synced with the CrowdSec engine.\r\n\r\n\r\n### Step 3: How the Firewall Bouncer Works\r\n\r\nThe firewall bouncer automatically updates your firewall rules with decisions made by CrowdSec. For example, if a brute-force attack is detected on your SSH port, the bouncer will add the attacker’s IP to the blocking rules in `iptables`.\r\n\r\nYou can manually verify the active firewall rules with:\r\n\r\n```bash\r\nsudo iptables -L\r\n```\r\n\r\n\r\n\r\n### Troubleshooting Installation Issues\r\n\r\nIf the firewall bouncer doesn’t appear in the `cscli bouncers list` output or isn’t working correctly:\r\n- **Restart the bouncer service:**\r\n  ```bash\r\n  sudo systemctl restart crowdsec-firewall-bouncer\r\n  ```\r\n- **Check the logs for errors:**\r\n  ```bash\r\n  sudo tail -f /var/log/crowdsec-firewall-bouncer.log\r\n  ```\r\n\r\n\r\n\r\n### Why Is the Firewall Bouncer Important?\r\n\r\nThe firewall bouncer ensures that CrowdSec not only detects malicious activity but also takes immediate action to mitigate threats. It’s particularly effective in:\r\n1. Blocking brute force attempts on SSH.\r\n2. Stopping known malicious IPs based on community-shared data.\r\n3. Protecting other services like Nginx when integrated with appropriate configurations.\r\n\r\nBy enabling the firewall bouncer, you’re taking the most critical step in actively defending your VPS from real-time attacks.\r\n\r\n\r\n\r\n##  See What Logs Are Monitored by CrowdSec\r\n---\r\n\r\nCrowdSec is designed to analyze logs from various sources on your server to detect suspicious patterns and potential attacks. Upon installation, CrowdSec includes default collections for commonly used services, such as SSH and Nginx. These collections specify which log files CrowdSec monitors and define the detection scenarios for each service.\r\n\r\n### Viewing the Log Files Monitored\r\n\r\nCrowdSec creates and uses a file called `acquis.yaml` to configure the log files it monitors. You can inspect this file to see the services and log locations that CrowdSec is currently set up to analyze.\r\n\r\nRun the following command to view the log monitoring configuration:\r\n```bash\r\ncat /etc/crowdsec/acquis.yaml\r\n```\r\n\r\n#### Sample Output:\r\n```yaml\r\n# Generated acquisition file - wizard.sh (service: nginx) / files : /var/log/nginx/error.log\r\nfilenames:\r\n  - /var/log/nginx/error.log\r\nlabels:\r\n  type: nginx\r\n---\r\n# Generated acquisition file - wizard.sh (service: ssh) / files :\r\njournalctl_filter:\r\n  - _SYSTEMD_UNIT=ssh.service\r\nlabels:\r\n  type: syslog\r\n---\r\n```\r\n\r\nFrom this example, you can see that CrowdSec is configured to monitor:\r\n1. **Nginx logs:** Located in `/var/log/nginx/error.log`.\r\n2. **SSH logs:** Captured from journald using the `_SYSTEMD_UNIT=ssh.service` filter.\r\n\r\nThis means CrowdSec will automatically analyze logs from these sources to detect scenarios such as brute force attacks, suspicious behavior, or repeated access failures.\r\n\r\n\r\n### Log Files Used by CrowdSec\r\n\r\nCrowdSec itself generates its own set of logs to help you monitor its performance and functionality. These logs are stored in `/var/log/` and include the following files:\r\n\r\n| **File**                          | **Purpose**                                        |\r\n|-----------------------------------|---------------------------------------------------|\r\n| `/var/log/crowdsec.log`           | Primary log for CrowdSec’s main engine.           |\r\n| `/var/log/crowdsec_api.log`       | Logs related to the CrowdSec API.                 |\r\n| `/var/log/crowdsec-firewall-bouncer.log` | Logs for the firewall bouncer performance.       |\r\n\r\nTo view these logs, use commands like:\r\n```bash\r\nsudo tail -f /var/log/crowdsec.log\r\n```\r\n\r\n\r\n\r\n### Why Is Monitoring Logs Important?\r\n\r\nCrowdSec’s ability to monitor logs is the foundation of its detection and response capabilities. By analyzing logs from services like SSH and Nginx, it can:\r\n- Detect brute force attacks and other suspicious patterns.\r\n- Share anonymized attack data with the global CrowdSec network to improve the community database.\r\n- Generate actionable remediation decisions (e.g., banning IPs).\r\n\r\nKeeping an eye on what logs are being monitored ensures that your server is fully protected and that no critical service is left unguarded.\r\n\r\n\r\n\r\n##   Linking to the CrowdSec Console (Optional)\r\n---\r\n\r\nOne of the standout features of CrowdSec is the **CrowdSec Console**, a cloud-based dashboard that provides insights into your server’s security performance. The console offers centralized management, detailed attack visualization, and aggregated metrics, making it easier to monitor security events across different systems or environments.\r\n\r\n### Why Use the CrowdSec Console?\r\n\r\nWhile the local CrowdSec installation provides powerful detection and remediation capabilities, the CrowdSec Console adds the following benefits:\r\n- **Graphical Data Insights:** Visualize attack patterns and blocked threats in an intuitive, graphical format.\r\n- **Centralized Monitoring:** Manage multiple CrowdSec instances across different servers from a single dashboard.\r\n- **Community Intelligence:** View global attack trends based on community-shared intelligence.\r\n- **Easy Bouncer Management:** Track and manage all active bouncers from one place.\r\n\r\nThe console is free and highly valuable for developers, system administrators, and small businesses.\r\n\r\n\r\n\r\n### How to Connect Your VPS to the CrowdSec Console\r\n\r\n#### Step 1: Create a CrowdSec Account\r\nVisit the official **[CrowdSec Console signup page](https://app.crowdsec.net/signup)** to create a free account. You’ll need an email address to register.\r\n\r\nOnce logged in, you’ll gain access to the CrowdSec Console dashboard. From there, follow the steps below to enroll your server.\r\n\r\n\r\n#### Step 2: Generate an Enrollment Token\r\nTo connect a CrowdSec instance (your VPS) to the console, you'll need an enrollment token. Generate this token from the **Security Engines** section of the console dashboard:\r\n\r\n1. Log in to the CrowdSec Console.\r\n2. Navigate to **Security Engines** → **Engines**.\r\n3. Click on **Generate Enrollment Token**.\r\n4. Copy the generated token. You’ll need this in the next step.\r\n\r\n\r\n\r\n#### Step 3: Enroll Your VPS Using the Token\r\nLog in to your VPS and execute the following command, replacing `<your-enrollment-token>` with the token you copied earlier:\r\n\r\n```bash\r\nsudo cscli console enroll <your-enrollment-token>\r\n```\r\n\r\n#### Sample Output:\r\n```bash\r\nINFO[01-10-2025 16:57:08] Successfully enrolled machine 'my-vps.example.com' to the Console\r\n```\r\n\r\n\r\n\r\n#### Step 4: Verify Enrollment\r\nAfter enrolling, return to the CrowdSec Console and navigate back to the **Engines** section. You should see your VPS listed as an active engine.\r\n\r\n\r\n\r\n### Benefits of Using the Console\r\n\r\n- **Real-Time Alerts:** Monitor active alerts and attacks in real time, directly from your browser.\r\n- **Attack Trends:** Understand how often your server is targeted and what type of attacks are most common.\r\n- **Performance Metrics:** Track how well CrowdSec is mitigating threats over time.\r\n- **Centralized Configuration:** Manage configurations and adjust detection scenarios for all your CrowdSec-enabled servers from one interface.\r\n\r\nHere’s an example of what the CrowdSec Console’s overview might look like:\r\n\r\n| **Metric**        | **Description**                             | **Example Insight**                         |\r\n|--------------------|---------------------------------------------|---------------------------------------------|\r\n| **Top IPs Blocked**| Shows the most frequently banned IPs.       | \"IP X.X.X.X blocked 200 times in 24 hours.\"|\r\n| **Attack Trends**  | Displays common attack types.              | \"60% of attacks in past week were SSH brute force.\" |\r\n| **Event Timeline** | Visualizes when attacks occurred.          | \"Spike of 1,000 attacks at 2 a.m. yesterday.\"|\r\n| **Global Intelligence**| Community-sourced blocklists.          | \"50 IPs globally flagged for DoS attacks.\"|\r\n\r\n\r\n\r\n\r\n## Monitoring Application Logs (e.g., Nginx)\r\n---\r\n\r\nCrowdSec can monitor your application-specific logs, such as those from Nginx, Apache, or custom services, to detect and prevent attacks on web applications. This functionality ensures your entire server stack is secured, not just SSH. By analyzing logs from applications like Nginx, CrowdSec can detect patterns like bot attacks, vulnerability probes, or exploitation attempts.\r\n\r\n### Why Monitor Nginx Logs?\r\n\r\nNginx is a common web server used by many VPS setups to host websites and applications. Monitoring Nginx logs with CrowdSec adds the following benefits:\r\n- **Detect Web Attacks:** Identify bots, vulnerability scanners, SQL injection attempts, and other malicious activities.\r\n- **Block Suspicious IPs:** CrowdSec can instantly issue bans based on log analysis, stopping attackers before they cause harm.\r\n- **Centralized Protection:** Combine Nginx log monitoring with SSH and other logs for all-in-one remediation.\r\n\r\n\r\n### Step 1: Install the Nginx Collection\r\n\r\nCrowdSec uses predefined \"collections\" to analyze specific log types. If the Nginx collection is not already installed, you can add it with the following command:\r\n\r\n```bash\r\nsudo cscli collections install crowdsecurity/nginx\r\n```\r\n\r\nThis collection includes detection scenarios tailored for Nginx logs, such as identifying SQL injection probes, directory traversal attempts, or bad user agents.\r\n\r\n#### Example Output:\r\n```bash\r\nINFO[06-02-2025 07:00:02] crowdsecurity/nginx installed successfully\r\n```\r\n\r\n\r\n\r\n### Step 2: Update the `acquis.yaml` File\r\n\r\nTo enable CrowdSec to monitor your Nginx logs, you need to add their file paths to the configuration file located at `/etc/crowdsec/acquis.yaml`:\r\n\r\n1. Open the file for editing:\r\n   ```bash\r\n   sudo vi /etc/crowdsec/acquis.yaml\r\n   ```\r\n\r\n2. Add the following entries under the `filenames` section for Nginx logs:\r\n   ```yaml\r\n   filenames:\r\n     - /var/log/nginx/error.log\r\n     - /home/*/logs/nginx/*.log\r\n   labels:\r\n     type: nginx\r\n   ---\r\n   ```\r\n\r\n   - The first path (`/var/log/nginx/error.log`) includes the default error log for Nginx.\r\n   - The second path (`/home/*/logs/nginx/*.log`) allows for monitoring additional application-specific logs (e.g., virtual hosts).\r\n\r\n3. Save and exit the file.\r\n\r\n\r\n\r\n### Step 3: Restart the CrowdSec Service\r\n\r\nAfter updating the configuration, restart the CrowdSec service to apply the changes:\r\n\r\n```bash\r\nsudo systemctl restart crowdsec\r\n```\r\n\r\n\r\n\r\n### Step 4: Verify Functionality\r\n\r\nTo ensure that CrowdSec is actively monitoring Nginx logs:\r\n\r\n1. Tail the CrowdSec logs in real time to see if it’s processing Nginx logs correctly:\r\n   ```bash\r\n   sudo tail -f /var/log/crowdsec.log\r\n   ```\r\n\r\n2. Check for alerts or decisions related to Nginx:\r\n   ```bash\r\n   sudo cscli alerts list\r\n   sudo cscli decisions list\r\n   ```\r\n\r\nIf everything is working correctly, you will see entries related to Nginx logs, such as blocked IPs or detected attack scenarios.\r\n\r\n\r\n### Example Scenarios for Nginx Monitoring\r\n\r\nWhen you enable Nginx log monitoring, CrowdSec can detect and mitigate the following attack types:\r\n1. **SQL Injection Probes:** Attempts to exploit database vulnerabilities.\r\n2. **Directory Traversal Attacks:** Malicious requests designed to access unauthorized files or directories.\r\n3. **Bad User Agents:** Bots or scrapers with malicious intent.\r\n4. **Rate Limiting Violations:** Excessive request patterns from abusive IPs.\r\n\r\n\r\n\r\nBy monitoring Nginx logs with CrowdSec, you extend protection to your web applications, ensuring that both your server infrastructure and hosted services are secure.\r\n\r\n\r\n## Useful CrowdSec Commands\r\n---\r\n\r\nAfter installing and configuring CrowdSec, it’s crucial to know how to monitor its activity, manage alerts and bans, and optimize its effectiveness. This section provides a breakdown of the most useful CrowdSec commands, empowering you to stay on top of your server's security.\r\n\r\n\r\n\r\n### **1. View Active Decisions (Current Blocks/Bans)**\r\n\r\nThe **decisions list** command shows all IPs currently being blocked by CrowdSec.\r\n\r\n```bash\r\nsudo cscli decisions list\r\n```\r\n\r\n#### Example Output:\r\n```bash\r\n╭────────────┬─────────────┬────────────┬──────────────╮\r\n│  IP        │  Reason     │  Action    │  Expires at  │\r\n├────────────┼─────────────┼────────────┼──────────────┤\r\n│ 192.168.1.1│ ssh-bf      │ ban        │ 2025-02-06T12:00:00Z │\r\n│ 203.0.113.5│ nginx-bot   │ ban        │ 2025-02-06T23:59:59Z │\r\n╰────────────┴─────────────┴────────────┴──────────────╯\r\n```\r\n- **IP:** The IP address being blocked.\r\n- **Reason:** Cause of the block (e.g., SSH brute force).\r\n- **Action:** The remediation applied (e.g., ban).\r\n- **Expires at:** The time when the block will expire.\r\n\r\nThis command is helpful for reviewing which IPs are actively banned and why they were blocked.\r\n\r\n\r\n\r\n### **2. Check Recent Alerts**\r\n\r\nThe **alerts list** command displays recent security alerts generated by CrowdSec. Alerts are triggered by detection scenarios and may result in bans or other actions.\r\n\r\n```bash\r\nsudo cscli alerts list\r\n```\r\n\r\n#### Example Output:\r\n```bash\r\n╭───────┬───────────────────┬────────────────────┬──────────────╮\r\n│  ID   │  Scenario         │  Source IP         │  Created at  │\r\n├───────┼───────────────────┼────────────────────┼──────────────┤\r\n│   1   │ ssh-bf            │ 203.0.113.5        │ 2025-02-06   │\r\n│   2   │ nginx-bad-user-agent │ 198.51.100.10    │ 2025-02-06   │\r\n╰───────┴───────────────────┴────────────────────┴──────────────╯\r\n```\r\n- **Scenario:** The attack type detected (e.g., `ssh-bf` for SSH brute force).\r\n- **Source IP:** The IP address responsible for the attack.\r\n- **Created at:** When the alert was triggered.\r\n\r\nUse this to investigate suspicious activity and identify trends in attack attempts.\r\n\r\n\r\n\r\n### **3. View Security Metrics**\r\n\r\nThe **metrics** command gives an overview of CrowdSec's activity, including parsed log files, detected threats, and actions taken.\r\n\r\n```bash\r\nsudo cscli metrics\r\n```\r\n\r\n#### Example Output:\r\n```bash\r\nLocal API Alerts:\r\n╭───────────────────────────┬───────╮\r\n│ Reason                    │ Count │\r\n├───────────────────────────┼───────┤\r\n│ crowdsecurity/ssh-bf      │ 15    │\r\n│ crowdsecurity/nginx-bot   │ 10    │\r\n╰───────────────────────────┴───────╯\r\n\r\nBouncer Metrics:\r\n╭────────────────────────────┬──────────────────┬───────────╮\r\n│ Origin                     │ Active Decisions │ Dropped   │\r\n├────────────────────────────┼──────────────────┼───────────┤\r\n│ crowdsecurity (engine)     │ 22               │ 14.5k     │\r\n╰────────────────────────────┴──────────────────┴───────────╯\r\n```\r\nThis command provides clear insight into how many attacks were detected, blocked, and their source.\r\n\r\n\r\n\r\n### **4. Monitor Logs in Real Time**\r\n\r\nTo watch CrowdSec in action, use the **tail** command to view log entries as they are being processed.\r\n\r\n```bash\r\nsudo tail -f /var/log/crowdsec.log\r\n```\r\n\r\nThis is especially useful for troubleshooting and for seeing real-time detection events.\r\n\r\n\r\n\r\n### **5. List Active Detection Scenarios**\r\n\r\nCrowdSec uses predefined \"scenarios\" (detection rules) to identify suspicious behavior. To see which scenarios are currently enabled on your system, use:\r\n\r\n```bash\r\nsudo cscli scenarios list\r\n```\r\n\r\n#### Example Output:\r\n```bash\r\n╭───────────────────────────────┬────────╮\r\n│ Name                          │ Status │\r\n├───────────────────────────────┼────────┤\r\n│ crowdsecurity/ssh-bf          │ ✔️ enabled  │\r\n│ crowdsecurity/nginx-bad-user-agent │ ✔️ enabled  │\r\n│ crowdsecurity/http-crawl-non_statics │ ✔️ enabled  │\r\n╰───────────────────────────────┴────────╯\r\n```\r\n\r\nThis ensures that CrowdSec is configured to detect relevant types of attacks, such as SSH brute force and web application abuse.\r\n\r\n\r\n\r\n### **6. Add or Remove a Block Manually**\r\n\r\nYou can manually add or remove an IP block using CrowdSec. This is useful if you want to preemptively block a known bad actor or whitelist a trusted IP.\r\n\r\n**Add a Manual Block:**\r\n```bash\r\nsudo cscli decisions add -i <IP> -t ban -r \"Manual ban\"\r\n```\r\n\r\n**Remove a Block:**\r\n```bash\r\nsudo cscli decisions delete -i <IP>\r\n```\r\n\r\n\r\n\r\n### **7. Unblock an IP**\r\n\r\nIf you need to remove a block for a specific IP address (e.g., if it was mistakenly added), you can do so with:\r\n\r\n```bash\r\nsudo cscli decisions delete --ip <IP>\r\n```\r\n\r\n\r\n\r\n### Summary of Useful Commands\r\n\r\n| **Command**                       | **Purpose**                                  |\r\n|-----------------------------------|---------------------------------------------|\r\n| `sudo cscli decisions list`       | List all active bans and blocks.            |\r\n| `sudo cscli alerts list`          | View recent alerts triggered by attacks.    |\r\n| `sudo cscli metrics`              | Display security metrics and performance.   |\r\n| `sudo tail -f /var/log/crowdsec.log` | Watch logs and detections in real time.     |\r\n| `sudo cscli scenarios list`       | Show all active detection scenarios.        |\r\n| `sudo cscli decisions add`        | Manually block specific IPs.                |\r\n| `sudo cscli decisions delete`     | Remove a block for a specific IP.           |\r\n\r\n\r\n\r\n### Practical Usage Scenarios\r\n\r\n- **Daily Monitoring:** Use `cscli alerts list` and `cscli decisions list` to check for recent activity and blocks.\r\n- **Investigating Attacks:** Use `cscli metrics` to analyze attack trends and identify patterns.\r\n- **Troubleshooting Issues:** Use `tail -f /var/log/crowdsec.log` to debug scenarios or log processing problems.\r\n- **Fine-Tuning Detection:** Use `cscli scenarios list` to enable or disable specific detection rules based on your server's needs.\r\n\r\n\r\n\r\nBy mastering these commands, you can efficiently manage, monitor, and optimize CrowdSec to protect your VPS from cyber threats.\r\n\r\n\r\n\r\n\r\n\r\n##  Conclusion\r\n---\r\n\r\nSecuring your VPS is a critical step in protecting your applications, data, and infrastructure from malicious cyber threats. SSH, being a common entry point for attackers, requires robust protection, and **CrowdSec** provides an effective, scalable, and community-driven solution to meet that need.\r\n\r\nIn this guide, we’ve covered every step you need to take to use CrowdSec to secure your VPS:\r\n\r\n- **Installing CrowdSec:** Setting up the core CrowdSec engine to monitor and analyze server logs.\r\n- **Installing the Firewall Bouncer:** Enabling automatic remediation by blocking malicious IPs at the firewall level.\r\n- **Configuring Log Monitoring:** Extending protection to application logs (e.g., Nginx) to safeguard your entire server stack.\r\n- **Using Metrics and Commands:** Leveraging CrowdSec's metrics and commands to monitor and fine-tune your security setup.\r\n- **Optional: Linking to CrowdSec Console:** Utilizing a centralized management platform for streamlined and graphical security insights.\r\n\r\nCrowdSec offers key advantages over traditional security tools by combining detection, remediation, and collaborative intelligence. Here’s why it stands out:\r\n- **Ease of Use:** Simple installation and configuration, even for beginners.\r\n- **Real-Time Defense:** Proactive blocking of attackers based on server activity logs.\r\n- **Community Intelligence:** Benefit from global shared threat data to stay ahead of emerging threats.\r\n- **Scalable:** Protects one VPS or an entire server fleet with cloud-friendly tools.\r\n\r\nBy following this tutorial, you’ve not only secured your SSH service but also empowered your VPS to block malicious actors proactively, helping to reduce the risk of data breaches and server compromise.\r\n\r\n\r\nBy taking the time to secure your server with CrowdSec, you’ve made a significant investment in your VPS's security. Keep the system updated, monitor its performance, and continue exploring CrowdSec's features to strengthen your defenses further. Security is an ongoing process, and with CrowdSec, you’re well-equipped to stay ahead of the curve.","src/content/posts/crowdsec-secure-server.mdx",[596],"../../assets/images/25/02/crowdsec-secure-server.jpg","67c89dd3d414ecad","crowdsec-secure-server.mdx","benchmark-cloud-servers",{id:599,data:601,body:610,filePath:611,assetImports:612,digest:614,rendered:615,legacyId:634},{title:602,description:603,date:604,image:605,authors:606,categories:607,tags:608,canonical:609},"How to Benchmark Cloud Servers (VPS)","Let's see how we can Benchmark a VPS and see exactly of what is capable off",["Date","2022-09-15T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/benchmark_vps.jpeg",[19],[98],[135],"https://www.bitdoze.com/benchmark-cloud-servers/","In case you want to use a cloud server for your projects then you should benchmark them to have an idea of what they have to offer. There are a lot of cloud providers out there and the best way to see if the performance is good on a server is to run a test. A good one that you can try is [Hetzner](https://go.bitdoze.com/hetzner), it has decent prices and good performance, in this article we are going to benchmark one of their servers, for more details check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/).\r\n\r\n## What a Good Benchmark Should Check\r\n\r\nA good benchmark should take a look on below:\r\n\r\n- **Disk Speeds** > These days you have HDD, SSD, or NVME so the speeds may vary from a couple of MB on second to GB. A good VPS should have from hundreds of MB to GB write/read speed.\r\n- **Network Speeds** > in case you are hosting internet-facing applications these speeds are very important the fastest the better, as you need to see the connections with the different data centers.\r\n- **CPU** > The CPU plays a major role and it needs to be tested to see how fast it is. These days relevant tests are the [Geekbench](https://www.geekbench.com/) ones.\r\n\r\n## What Tool to Use to Benchmark Al\r\n\r\n[Yabs.sh](https://github.com/masonr/yet-another-bench-script) it’s a small tool that can help you benchmark all of these with a run. This will work on all servers Ubuntu or RedHat and you don’t need to install it to run. For benchmarks to start you just need to run:\r\n\r\n    curl -sL yabs.sh | bash\r\n\r\nThis uses curl to fetch it and run it so you need curl on your VPS (this should be there by default in most distros)\r\n\r\n**Benchmark Output**\r\n\r\n    # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\r\n    #              Yet-Another-Bench-Script              #\r\n    #                     v2022-08-20                    #\r\n    # https://github.com/masonr/yet-another-bench-script #\r\n    # ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #\r\n\r\n    Wed Aug 31 12:14:29 PM UTC 2022\r\n\r\n    Basic System Information:\r\n    ---------------------------------\r\n    Uptime     : 6 days, 5 hours, 18 minutes\r\n    Processor  : AMD EPYC Processor\r\n    CPU cores  : 3 @ 2445.404 MHz\r\n    AES-NI     : ✔ Enabled\r\n    VM-x/AMD-V : ❌ Disabled\r\n    RAM        : 3.7 GiB\r\n    Swap       : 2.0 GiB\r\n    Disk       : 75.0 GiB\r\n    Distro     : Ubuntu 22.04.1 LTS\r\n    Kernel     : 5.15.0-46-generic\r\n\r\n    fio Disk Speed Tests (Mixed R/W 50/50):\r\n    ---------------------------------\r\n    Block Size | 4k            (IOPS) | 64k           (IOPS)\r\n      ------   | ---            ----  | ----           ----\r\n    Read       | 113.01 MB/s  (28.2k) | 1.39 GB/s    (21.7k)\r\n    Write      | 113.31 MB/s  (28.3k) | 1.39 GB/s    (21.8k)\r\n    Total      | 226.32 MB/s  (56.5k) | 2.79 GB/s    (43.6k)\r\n       |                      |\r\n    Block Size | 512k          (IOPS) | 1m            (IOPS)\r\n      ------   | ---            ----  | ----           ----\r\n    Read       | 2.07 GB/s     (4.0k) | 2.72 GB/s     (2.6k)\r\n    Write      | 2.18 GB/s     (4.2k) | 2.90 GB/s     (2.8k)\r\n    Total      | 4.26 GB/s     (8.3k) | 5.62 GB/s     (5.4k)\r\n\r\n    iperf3 Network Speed Tests (IPv4):\r\n    ---------------------------------\r\n    Provider        | Location (Link)           | Send Speed      | Recv Speed\r\n            |                           |                 |\r\n    Clouvider       | London, UK (10G)          | 1.08 Gbits/sec  | 2.38 Gbits/sec\r\n    Online.net      | Paris, FR (10G)           | 2.84 Gbits/sec  | 2.34 Gbits/sec\r\n    Hybula          | The Netherlands (40G)     | 2.05 Gbits/sec  | 2.11 Gbits/sec\r\n    Uztelecom       | Tashkent, UZ (10G)        | 1.07 Gbits/sec  | 948 Mbits/sec\r\n    Clouvider       | NYC, NY, US (10G)         | 8.47 Gbits/sec  | 8.09 Gbits/sec\r\n    Clouvider       | Dallas, TX, US (10G)      | 4.23 Gbits/sec  | 5.35 Gbits/sec\r\n    Clouvider       | Los Angeles, CA, US (10G) | 2.32 Gbits/sec  | 3.35 Gbits/sec\r\n\r\n    iperf3 Network Speed Tests (IPv6):\r\n    ---------------------------------\r\n    Provider        | Location (Link)           | Send Speed      | Recv Speed\r\n            |                           |                 |\r\n    Clouvider       | London, UK (10G)          | 1.99 Gbits/sec  | 2.39 Gbits/sec\r\n    Online.net      | Paris, FR (10G)           | 2.96 Gbits/sec  | 2.41 Gbits/sec\r\n    Hybula          | The Netherlands (40G)     | 1.97 Gbits/sec  | 2.06 Gbits/sec\r\n    Uztelecom       | Tashkent, UZ (10G)        | 1.25 Gbits/sec  | 1.05 Gbits/sec\r\n    Clouvider       | NYC, NY, US (10G)         | 8.46 Gbits/sec  | 7.40 Gbits/sec\r\n    Clouvider       | Dallas, TX, US (10G)      | 4.31 Gbits/sec  | 4.80 Gbits/sec\r\n    Clouvider       | Los Angeles, CA, US (10G) | 3.15 Gbits/sec  | 3.38 Gbits/sec\r\n\r\n    Geekbench 5 Benchmark Test:\r\n    ---------------------------------\r\n    Test            | Value\r\n            |\r\n    Single Core     | 924\r\n    Multi Core      | 2582\r\n    Full Test       | https://browser.geekbench.com/v5/cpu/16984496\r\n\r\nIn the output above you see that you have all the 3 major things benchmarked and Hetzner does a very nice job for the price point, you pay for a VPS server.","src/content/posts/benchmark-cloud-servers.md",[613],"../../assets/images/benchmark_vps.jpeg","7103eaebeff5bd28",{html:616,metadata:617},"<p>In case you want to use a cloud server for your projects then you should benchmark them to have an idea of what they have to offer. There are a lot of cloud providers out there and the best way to see if the performance is good on a server is to run a test. A good one that you can try is <a href=\"https://go.bitdoze.com/hetzner\">Hetzner</a>, it has decent prices and good performance, in this article we are going to benchmark one of their servers, for more details check this <a href=\"https://www.wpdoze.com/hetzner-cloud-review/\">Hetzner Review</a>.</p>\n<h2 id=\"what-a-good-benchmark-should-check\">What a Good Benchmark Should Check</h2>\n<p>A good benchmark should take a look on below:</p>\n<ul>\n<li><strong>Disk Speeds</strong> > These days you have HDD, SSD, or NVME so the speeds may vary from a couple of MB on second to GB. A good VPS should have from hundreds of MB to GB write/read speed.</li>\n<li><strong>Network Speeds</strong> > in case you are hosting internet-facing applications these speeds are very important the fastest the better, as you need to see the connections with the different data centers.</li>\n<li><strong>CPU</strong> > The CPU plays a major role and it needs to be tested to see how fast it is. These days relevant tests are the <a href=\"https://www.geekbench.com/\">Geekbench</a> ones.</li>\n</ul>\n<h2 id=\"what-tool-to-use-to-benchmark-al\">What Tool to Use to Benchmark Al</h2>\n<p><a href=\"https://github.com/masonr/yet-another-bench-script\">Yabs.sh</a> it’s a small tool that can help you benchmark all of these with a run. This will work on all servers Ubuntu or RedHat and you don’t need to install it to run. For benchmarks to start you just need to run:</p>\n<pre class=\"astro-code one-dark-pro\" style=\"background-color:#282c34;color:#abb2bf; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>curl -sL yabs.sh | bash</span></span></code></pre>\n<p>This uses curl to fetch it and run it so you need curl on your VPS (this should be there by default in most distros)</p>\n<p><strong>Benchmark Output</strong></p>\n<pre class=\"astro-code one-dark-pro\" style=\"background-color:#282c34;color:#abb2bf; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span># ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #</span></span>\n<span class=\"line\"><span>#              Yet-Another-Bench-Script              #</span></span>\n<span class=\"line\"><span>#                     v2022-08-20                    #</span></span>\n<span class=\"line\"><span># https://github.com/masonr/yet-another-bench-script #</span></span>\n<span class=\"line\"><span># ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>Wed Aug 31 12:14:29 PM UTC 2022</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>Basic System Information:</span></span>\n<span class=\"line\"><span>---------------------------------</span></span>\n<span class=\"line\"><span>Uptime     : 6 days, 5 hours, 18 minutes</span></span>\n<span class=\"line\"><span>Processor  : AMD EPYC Processor</span></span>\n<span class=\"line\"><span>CPU cores  : 3 @ 2445.404 MHz</span></span>\n<span class=\"line\"><span>AES-NI     : ✔ Enabled</span></span>\n<span class=\"line\"><span>VM-x/AMD-V : ❌ Disabled</span></span>\n<span class=\"line\"><span>RAM        : 3.7 GiB</span></span>\n<span class=\"line\"><span>Swap       : 2.0 GiB</span></span>\n<span class=\"line\"><span>Disk       : 75.0 GiB</span></span>\n<span class=\"line\"><span>Distro     : Ubuntu 22.04.1 LTS</span></span>\n<span class=\"line\"><span>Kernel     : 5.15.0-46-generic</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>fio Disk Speed Tests (Mixed R/W 50/50):</span></span>\n<span class=\"line\"><span>---------------------------------</span></span>\n<span class=\"line\"><span>Block Size | 4k            (IOPS) | 64k           (IOPS)</span></span>\n<span class=\"line\"><span>  ------   | ---            ----  | ----           ----</span></span>\n<span class=\"line\"><span>Read       | 113.01 MB/s  (28.2k) | 1.39 GB/s    (21.7k)</span></span>\n<span class=\"line\"><span>Write      | 113.31 MB/s  (28.3k) | 1.39 GB/s    (21.8k)</span></span>\n<span class=\"line\"><span>Total      | 226.32 MB/s  (56.5k) | 2.79 GB/s    (43.6k)</span></span>\n<span class=\"line\"><span>   |                      |</span></span>\n<span class=\"line\"><span>Block Size | 512k          (IOPS) | 1m            (IOPS)</span></span>\n<span class=\"line\"><span>  ------   | ---            ----  | ----           ----</span></span>\n<span class=\"line\"><span>Read       | 2.07 GB/s     (4.0k) | 2.72 GB/s     (2.6k)</span></span>\n<span class=\"line\"><span>Write      | 2.18 GB/s     (4.2k) | 2.90 GB/s     (2.8k)</span></span>\n<span class=\"line\"><span>Total      | 4.26 GB/s     (8.3k) | 5.62 GB/s     (5.4k)</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>iperf3 Network Speed Tests (IPv4):</span></span>\n<span class=\"line\"><span>---------------------------------</span></span>\n<span class=\"line\"><span>Provider        | Location (Link)           | Send Speed      | Recv Speed</span></span>\n<span class=\"line\"><span>        |                           |                 |</span></span>\n<span class=\"line\"><span>Clouvider       | London, UK (10G)          | 1.08 Gbits/sec  | 2.38 Gbits/sec</span></span>\n<span class=\"line\"><span>Online.net      | Paris, FR (10G)           | 2.84 Gbits/sec  | 2.34 Gbits/sec</span></span>\n<span class=\"line\"><span>Hybula          | The Netherlands (40G)     | 2.05 Gbits/sec  | 2.11 Gbits/sec</span></span>\n<span class=\"line\"><span>Uztelecom       | Tashkent, UZ (10G)        | 1.07 Gbits/sec  | 948 Mbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | NYC, NY, US (10G)         | 8.47 Gbits/sec  | 8.09 Gbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | Dallas, TX, US (10G)      | 4.23 Gbits/sec  | 5.35 Gbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | Los Angeles, CA, US (10G) | 2.32 Gbits/sec  | 3.35 Gbits/sec</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>iperf3 Network Speed Tests (IPv6):</span></span>\n<span class=\"line\"><span>---------------------------------</span></span>\n<span class=\"line\"><span>Provider        | Location (Link)           | Send Speed      | Recv Speed</span></span>\n<span class=\"line\"><span>        |                           |                 |</span></span>\n<span class=\"line\"><span>Clouvider       | London, UK (10G)          | 1.99 Gbits/sec  | 2.39 Gbits/sec</span></span>\n<span class=\"line\"><span>Online.net      | Paris, FR (10G)           | 2.96 Gbits/sec  | 2.41 Gbits/sec</span></span>\n<span class=\"line\"><span>Hybula          | The Netherlands (40G)     | 1.97 Gbits/sec  | 2.06 Gbits/sec</span></span>\n<span class=\"line\"><span>Uztelecom       | Tashkent, UZ (10G)        | 1.25 Gbits/sec  | 1.05 Gbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | NYC, NY, US (10G)         | 8.46 Gbits/sec  | 7.40 Gbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | Dallas, TX, US (10G)      | 4.31 Gbits/sec  | 4.80 Gbits/sec</span></span>\n<span class=\"line\"><span>Clouvider       | Los Angeles, CA, US (10G) | 3.15 Gbits/sec  | 3.38 Gbits/sec</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>Geekbench 5 Benchmark Test:</span></span>\n<span class=\"line\"><span>---------------------------------</span></span>\n<span class=\"line\"><span>Test            | Value</span></span>\n<span class=\"line\"><span>        |</span></span>\n<span class=\"line\"><span>Single Core     | 924</span></span>\n<span class=\"line\"><span>Multi Core      | 2582</span></span>\n<span class=\"line\"><span>Full Test       | https://browser.geekbench.com/v5/cpu/16984496</span></span></code></pre>\n<p>In the output above you see that you have all the 3 major things benchmarked and Hetzner does a very nice job for the price point, you pay for a VPS server.</p>",{headings:618,localImagePaths:626,remoteImagePaths:627,frontmatter:628,imagePaths:633},[619,623],{depth:620,slug:621,text:622},2,"what-a-good-benchmark-should-check","What a Good Benchmark Should Check",{depth:620,slug:624,text:625},"what-tool-to-use-to-benchmark-al","What Tool to Use to Benchmark Al",[],[],{date:629,title:602,description:603,image:613,categories:630,authors:631,tags:632,canonical:609},["Date","2022-09-15T00:00:00.000Z"],[98],[19],[135],[],"benchmark-cloud-servers.md","copy-multiple-files-in-one-layer-using-a-dockerfile",{id:635,data:637,body:646,filePath:647,assetImports:648,digest:650,legacyId:651,deferredRender:32},{title:638,description:639,date:640,image:641,authors:642,categories:643,tags:644,canonical:645},"How to Copy Multiple Files in One Layer Using a Dockerfile","Learn how to How to Copy Multiple Files in One Layer Using a Dockerfile to maker your work easier",["Date","2023-07-03T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/07/copy-multiple-files-one-layer-docker.jpeg",[19],[98],[100],"https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/","If you are working with Docker containers, you may find yourself needing to copy multiple files into a single layer using a Dockerfile. This can be a common task when setting up a container with all the necessary files and configurations. Fortunately, Docker provides a straightforward way to achieve this.\r\n\r\nTo copy multiple files in one layer using a Dockerfile, you can utilize the \"COPY\" command. This command allows you to copy files or directories from your build context into the container's file system. By specifying multiple source files or directories and a destination directory in the container, you can copy all the required files in one go.\r\n\r\nFor example, you can use the following command in your Dockerfile to copy multiple files into a specific directory within the container:\r\n\r\n```bash\r\nCOPY file1.txt file2.txt /app/files/\r\n```\r\n\r\nIn this case, \"file1.txt\" and \"file2.txt\" from your build context will be copied into the \"/app/files/\" directory within the container. You can add more files or directories as needed, separating them with spaces.\r\n\r\nCopying multiple files in one layer using a Dockerfile can save you time and effort while building your containers. This method ensures that all the necessary files are available within the container, simplifying your deployment process.\r\n\r\nSome other docker articles that can help you in your docker journey:\r\n\r\n- [Add Users to a Docker Container](https://www.bitdoze.com/add-users-to-docker-container/)\r\n- [Install Docker & Docker-compose for Ubuntu ARM](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n- [Redirect Docker Logs to a Single File](https://www.bitdoze.com/redirect-docker-logs-to-a-single-file/)\r\n- [Environment Variables ARG and ENV in Docker](https://www.bitdoze.com/docker-env-vars/)\r\n\r\n## Using COPY instruction to copy files in a Dockerfile\r\n\r\nIn a Dockerfile, the COPY instruction is used to copy files from the host machine to the container's file system. This instruction allows you to include multiple files in a single layer, which can help improve the build time and efficiency of your Docker image.\r\n\r\nHere is how you can use the COPY instruction to copy files in a Dockerfile:\r\n\r\n1. **Syntax**: The syntax for the COPY instruction is as follows:\r\n\r\n   ```bash\r\n   COPY <src> <dest>\r\n   ```\r\n\r\n   - `<src>`: Specifies the source file or directory on the host machine.\r\n   - `<dest>`: Indicates the destination directory in the container's file system.\r\n\r\n2. **Copying Files**: To copy a single file, you can specify the file's path as the source and provide the destination directory in the container's file system.\r\n\r\n   ```bash\r\n   COPY app.js /app/\r\n   ```\r\n\r\n   This command will copy the `app.js` file from the host machine to the `/app/` directory inside the container.\r\n\r\n3. **Copying Directories**: If you want to copy an entire directory, you can specify the directory's path as the source and provide the destination directory.\r\n\r\n   ```dockerfile\r\n   COPY src/ /app/src/\r\n   ```\r\n\r\n   This command will copy the `src` directory and all its contents to the `/app/src/` directory inside the container.\r\n\r\n4. **Wildcard Patterns**: The COPY instruction also supports wildcard patterns, allowing you to copy multiple files matching a certain pattern.\r\n\r\n   ```bash\r\n   COPY *.txt /app/\r\n   ```\r\n\r\n   This command will copy all the text files in the current directory on the host machine to the `/app/` directory inside the container.\r\n\r\n5. **Copying Multiple Files**: To copy multiple files, you can specify each file individually, separating them with a space.\r\n\r\n   ```bash\r\n   COPY file1.txt file2.txt /app/\r\n   ```\r\n\r\n   This command will copy both `file1.txt` and `file2.txt` to the `/app/` directory inside the container.\r\n\r\nUsing the COPY instruction in your Dockerfile allows you to easily include the necessary files in your container at build time. By efficiently copying multiple files in a single layer, you can enhance the performance of your Docker image and streamline the containerization process.\r\n\r\n## Including multiple source files in the COPY instruction\r\n\r\nThe COPY instruction in a Dockerfile is used to copy files from the host machine to the Docker image. It allows you to include multiple source files in a single instruction, which can be beneficial when you need to copy multiple files efficiently. Here's how you can do it:\r\n\r\n1. Specify the source files: Begin by listing all the source files you want to copy inside the Dockerfile. These files can be located in the same directory or different directories.\r\n\r\n2. Use wildcards or glob patterns: Docker supports using wildcards or glob patterns to specify the source files. This allows you to copy multiple files that match a certain pattern, reducing the number of COPY instructions needed. For example, you can use `*` to match all files, or `*.txt` to match all text files.\r\n\r\n3. Specify the destination directory: After specifying the source files, you need to mention the destination directory inside the Docker image where these files will be copied. It's important to ensure that the destination directory exists or create it if necessary.\r\n\r\nHere's an example of how the COPY instruction can be used to copy multiple source files:\r\n\r\n```bash\r\nCOPY file1.txt file2.txt /app/\r\nCOPY *.csv /data/\r\nCOPY src/. /code/\r\n```\r\n\r\nIn this example, we are copying `file1.txt` and `file2.txt` to the `/app/` directory, all CSV files in the current directory to the `/data/` directory, and all files and directories inside the `src/` directory to the `/code/` directory.\r\n\r\nBy including multiple source files in a single COPY instruction, you can minimize the number of instructions in your Dockerfile and make the image build process more efficient.\r\n\r\nRemember to also consider the order in which you list the source files, as the COPY instruction will preserve that order when copying them into the image.\r\n\r\n## Using wildcards to copy multiple files in a single layer\r\n\r\nWhen building Docker images, it is common to include multiple files in a single layer to optimize the image layering process. One way to achieve this is by using wildcards in your Dockerfile's `COPY` instruction. Wildcards allow you to match patterns and copy multiple files at once, reducing the number of individual `COPY` instructions needed.\r\n\r\nHere's a breakdown of how you can use wildcards effectively to copy multiple files in a Dockerfile:\r\n\r\n1. **Understand wildcard patterns**: Wildcard patterns use special characters to match multiple files or directories. The two most commonly used wildcards are `*` and `?`. The `*` character matches any sequence of characters, while the `?` character matches any single character. For example, `*.txt` matches all files with the `.txt` extension, and `file?.txt` matches any file with the name \"file\" followed by any single character and then the `.txt` extension.\r\n\r\n2. **Specify the source and destination in the COPY instruction**: In your Dockerfile, use the `COPY` instruction to copy files from the build context (source) to the image (destination). By specifying wildcards in the source path, you can copy multiple files that match the pattern to a single directory in the destination path. For example, `COPY src/*.txt dest/` would copy all files with the `.txt` extension from the `src` directory to the `dest` directory in the image.\r\n\r\n3. **Consider directory structures**: When using wildcards, be mindful of the directory structures in your build context. You can use wildcards to match files in specific directories or nested subdirectories. For instance, `COPY src/**/*.txt dest/` would recursively copy all files with the `.txt` extension from any subdirectory within the `src` directory to the `dest` directory in the image.\r\n\r\nUsing wildcards in your Dockerfile's `COPY` instruction can greatly simplify the process of copying multiple files in a single layer. It allows you to leverage the power of pattern matching to efficiently include desired files in your image. Remember to experiment and test your Dockerfile to ensure that the expected files are being copied correctly.\r\n\r\nHere's a table summarizing the usage of wildcards for convenience:\r\n\r\n| Wildcard | Description                                                                                                                      |\r\n| -------- | -------------------------------------------------------------------------------------------------------------------------------- |\r\n| `*`      | Matches any sequence of characters                                                                                               |\r\n| `?`      | Matches any single character                                                                                                     |\r\n| `**`     | Matches any directory and its subdirectories                                                                                     |\r\n| `{}`     | Matches specific patterns within a set of alternatives. (e.g., `{*.txt, *.md}` matches all files with `.txt` or `.md` extension) |\r\n\r\nWith this knowledge, you can make your Dockerfile more concise and efficient by using wildcards to copy multiple files in a single layer. Happy containerizing!\r\n\r\n## Organizing files in separate directories for better readability\r\n\r\nWhen working with a Dockerfile, it's important to maintain a clear and organized file structure to ensure better readability of your project. In this section, we will explore how to organize files in separate directories, making it easier to manage and understand your Dockerfiles.\r\n\r\n**1. Create a directory structure:** Start by creating a dedicated directory for your Docker project. Within this directory, you can further organize your files into separate folders based on their purpose or functionality. For example, you can have separate directories for source code, configurations, and any other related files.\r\n\r\n**2. Store related files together:** Group all the relevant files for a particular function or component within their respective directories. This approach aids in quickly identifying and locating the necessary files when working on different parts of your project.\r\n\r\n**3. Utilize subdirectories:** If your project has multiple layers or components, consider using subdirectories within each component directory. This can help maintain a hierarchical structure and further improve the readability of your Dockerfile.\r\n\r\n**4. Avoid clutter:** Keep your directories clean and avoid including unnecessary files or directories that do not directly contribute to your Dockerfile. Excessive clutter can make it challenging to navigate through your project and hinder readability.\r\n\r\n**5. Document your directory structure:** Make sure to include a README.md file within your project's main directory, detailing the purpose and organization of each directory. This helps other developers understand the layout of your project quickly.\r\n\r\nBy organizing your files in separate directories, you give your Dockerfile a clear and concise structure, making it easier to understand and maintain. Remember, a well-organized project enhances collaboration and allows for smoother development processes.\r\n\r\n## Best practices for efficient file copying in a Dockerfile\r\n\r\nWhen it comes to copying multiple files in one layer using a Dockerfile, there are a few best practices that can help ensure efficiency and smooth operation. Let's dive into some key tips:\r\n\r\n### 1. Group related files together\r\n\r\n- To minimize layer size and optimize build time, it's advisable to group files that are related to each other and likely to change together into a single directory.\r\n- This approach reduces the number of separate copy instructions in the Dockerfile.\r\n\r\n### 2. Leverage .dockerignore file\r\n\r\n- Use a `.dockerignore` file to exclude unnecessary files and directories from being copied into the Docker image.\r\n- Avoid copying unnecessary files that are not required for the application to run correctly.\r\n- This reduces the build context and significantly improves the build speed.\r\n\r\n### 3. Order files based on frequency of change\r\n\r\n- Arrange the files in the Dockerfile's COPY instruction according to their frequency of change.\r\n- Files that change less frequently should be placed before files that change more often.\r\n- This way, you can take advantage of Docker's layer caching mechanism and avoid unnecessary rebuilds.\r\n\r\n### 4. Use wildcards cautiously\r\n\r\n- When copying files, be cautious when using wildcards (e.g., `*` or `**`) as it may unintentionally include unwanted files or directories.\r\n- Explicitly specifying file paths is generally safer and more predictable.\r\n\r\n### 5. Consider volume mounting for development\r\n\r\n- In development environments, rather than copying files into the image, consider using volume mounting.\r\n- Volume mounting allows for live code changes, enabling faster development cycles without the need for rebuilding the Docker image.\r\n\r\nBy following these best practices, you can optimize the file copying process in your Dockerfile, resulting in faster and more efficient builds. Remember, keep your layers small, exclude unnecessary files, and order the files strategically to take full advantage of Docker's caching capabilities. Happy and efficient Dockerfile building!\r\n\r\n## Conclusion\r\n\r\nIn this article, we have explored the process of copying multiple files in one layer using a Dockerfile. By implementing this technique, you can efficiently manage and organize your files within a Docker image.\r\n\r\nHere's a quick recap of the steps involved in achieving this:\r\n\r\n1. Begin by creating a Dockerfile in your project directory.\r\n2. Use the `COPY` instruction to copy multiple files from your local directory to the desired location within the Docker image.\r\n3. Specify the source files and the destination directory in the Dockerfile.\r\n4. Repeat the `COPY` instruction as needed to copy all the required files.\r\n5. Build your Docker image using the `docker build` command, providing the path to your Dockerfile.\r\n6. Test the resulting image to ensure that the files have been copied correctly.\r\n\r\nUsing this approach, you can streamline the process of copying multiple files into a Docker image, making your containerized applications more efficient and easier to distribute.\r\n\r\nRemember, Docker provides a powerful platform for building and deploying applications, and knowing how to effectively manage files is an essential skill. By mastering techniques like copying multiple files in one layer, you can leverage the full potential of Docker to enhance your development workflow.\r\n\r\nIf you have any questions or need further assistance, feel free to reach out to the Docker community or consult the official Docker documentation. Happy coding!","src/content/posts/copy-multiple-files-in-one-layer-using-a-dockerfile.mdx",[649],"../../assets/images/23/07/copy-multiple-files-one-layer-docker.jpeg","2e59711c9099dce1","copy-multiple-files-in-one-layer-using-a-dockerfile.mdx","deploy-astro-easypanel",{id:652,data:654,body:664,filePath:665,assetImports:666,digest:668,legacyId:669,deferredRender:32},{title:655,description:656,date:657,image:658,authors:659,categories:660,tags:661,canonical:663},"How to Deploy Astro on Your VPS with EasyPanel","Learn how to deploy Astro static website on your own VPS with EasyPanel",["Date","2023-11-15T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/deploy-astro-easypanel.jpeg",[19],[21],[662,23],"easypanel","https://www.bitdoze.com/deploy-astro-easypanel/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/11/easypanel-general.png\";\r\nimport imag2 from \"../../assets/images/23/11/ep-domains.png\";\r\n\r\nAstro is a web framework that lets you build fast and scalable websites with your favorite UI library. Astro can render your pages to static HTML for better performance, SEO, and accessibility. You can also use Astro to create dynamic web applications and APIs with any backend service. Astro supports multiple UI frameworks, such as React, Svelte, Vue, and more.\r\n\r\nYou can mix and match them in the same project, or use Astro’s template syntax. Astro also has a rich ecosystem of plugins, themes, and integrations that make it easy to extend and customize your website. Whether you want to create a blog, a portfolio, a marketing site, or a web app, Astro can help you achieve your goals.\r\n\r\nIn this tutorial, we are going to see how you can deploy Astro on your own VPS with the help of [EasyPanel](https://easypanel.io/). I have created a detailed article and video of how you can install EasyPanel and what has to offer it can be found under: [Easypanel.io: A Modern Hosting Panel for Applications and Databases](https://www.bitdoze.com/easypanel-modern-server-control-panel/)\r\n\r\nOther useful Astro articles you may want to check:\r\n\r\n- [How To Deploy Static Website Astro.JS on VPS Servers](https://www.bitdoze.com/deploy-astro-on-vps/)\r\n- [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/)\r\n- [How To Deploy An Astro.JS Blog On Cloudflare](https://www.bitdoze.com/deploy-astrojs-cloudflare/)\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## Deploy Astro on Your VPS with EasyPanel\r\n\r\nIn this part, we are going to see all the steps we need to do to have an Astro static website deployed in EasyPanel and have it hosted on your server.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/l-yohC7xD38\"\r\n  label=\"Deploy Astro on Your VPS with EasyPanel\"\r\n/>\r\n\r\n### 1. Instal EasyPanel\r\n\r\nThe first step is to have EasyPanel installed and used in a VPS, this can be done with a simple command you can check the full tutorial in: [Easypanel.io: A Modern Hosting Panel for Applications and Databases](https://www.bitdoze.com/easypanel-modern-server-control-panel/)\r\n\r\n### 2. Create a GitHub Repo\r\n\r\nTo deploy Astro you need a GitHub account and you need to create a repo where you will add Astro with all the packages. You can follow the [Link GitHub with A SSH Key to MacOS or Linux](https://www.bitdoze.com/link-github-with-ssh-maco-linux/) it has all the details you need to create the repo and link it to your laptop.\r\n\r\n### 3. Install Astro and Make the Configs\r\n\r\nNow the fun part is coming and we will start installing Astro. You can create a new installation or you can use a predefined theme, in this tutorial we are going to deploy it from scratch.\r\n\r\n#### 3.1 Deploy An Astro Blog\r\n\r\nFirst, you need to go into the repo you have created and be sure is empty, then you can run:\r\n\r\n```sh\r\nnpm create astro@latest\r\n```\r\n\r\nYou just use the \".\" for the project name as already you are in the repo that will be used for GitGub. You can also initiate a new repo with Astro is up to you.\r\n\r\n#### 3.2 Configure Astro Static\r\n\r\nTo have this working on EasyPanel we will need to install a node.js package and make some configurations in the package.json file so EasyPanel knows how to work with Astro.\r\nFirst, install **serve** package:\r\n\r\n```sh\r\nnpm install serve\r\n```\r\n\r\nServe will be needed in the start option so it tells to use /dist folder as: **\"start\": \"serve dist/\"**\r\n\r\nThe package.json will look like this:\r\n\r\n```json\r\n{\r\n  \"name\": \"\",\r\n  \"type\": \"module\",\r\n  \"version\": \"0.0.1\",\r\n  \"scripts\": {\r\n    \"dev\": \"astro dev\",\r\n    \"start\": \"serve dist/\",\r\n    \"build\": \"astro build\",\r\n    \"preview\": \"astro preview\",\r\n    \"astro\": \"astro\"\r\n  },\r\n  \"dependencies\": {\r\n    \"@astrojs/mdx\": \"^1.1.5\",\r\n    \"@astrojs/node\": \"^6.0.3\",\r\n    \"@astrojs/rss\": \"^3.0.0\",\r\n    \"@astrojs/sitemap\": \"^3.0.3\",\r\n    \"astro\": \"^3.5.4\",\r\n    \"serve\": \"^14.2.1\"\r\n  }\r\n}\r\n```\r\n\r\nIn the script section, you see the command and in dependencies you see the serve package.\r\n\r\nYou can check that everything is working fine with :\r\n\r\n```sh\r\nnpm run dev\r\nnpm run build\r\n```\r\n\r\nThe above commands will be used by EasyPanel and in case there are issues you will want to know before pushing everything to the repo.\r\n\r\n#### 3.3 Push to GitHub\r\n\r\nNow we finished with the configs and we will need to update the repo with the changes and Astro, just follow the below commands:\r\n\r\n```sh\r\ngit add .\r\ngit commit -m \"added astro\"\r\ngit push\r\n```\r\n\r\n### 4. Add Astro to EasyPanel\r\n\r\n#### 4.1 Configure Astro Repo\r\n\r\nNow what you need to do is to add your repo in EasyPanel, to do that you need to create a project then on **Service** add an **App** in the video you will see the details.\r\n\r\nIn the **Project - General - Source** you configure your GitHub repo with the owner and repo like in the picture:\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"easypanel general\"\r\n/>\r\n\r\nAfter just hit **Save**.\r\n\r\nThen another window will appear called **Build\\*\\*** where you should choose the **Nixpacks** and **Save**\r\n\r\nAfter you hit **Deploy** and under **Deployments** you watch how the magic is happening and the Astro application starts to be deployed.\r\n\r\nThe GitHub repo needs to be public, in case you have a private one you will need to add a GitHub token in **EasyPanel - Settings - GitHub**.\r\n\r\n#### 4.2 Enable Auto Deploy\r\n\r\nThe auto update will make the image to be updated with the latest changes from the repo. So in case you modify a file or add a new one EasyPanel will pull the latest version. This can be done in 2 ways:\r\n\r\n- **Hitting Auto Deploy** - in case you are using a token with EasyPanel you can just hit Auto Deply. This will add a webkook on the GitHub repo and EasyPanel will get notified when the update is happening.\r\n- **Add The WebHook** - you can manually fetch the Webhook from **General** in the project and add it to your repo.\r\n\r\n#### 4.3 Add A Domain\r\n\r\nNow you can check that everything is working by opening the domain that EasyPanel is assigning and add your own. You can go under **Domains** and add your URL. You need first to point the domain to the server IP with an **A** record.\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"easypanel domain\"\r\n/>\r\n\r\n## Conclusions\r\n\r\nNow you should have a fully functional static Astro website working on your VPS server with the help of EasyPanel. You can deploy as many you want including in the free option.","src/content/posts/deploy-astro-easypanel.mdx",[667],"../../assets/images/23/11/deploy-astro-easypanel.jpeg","ed6f5f8eb652afcc","deploy-astro-easypanel.mdx","deploy-astro-on-vps",{id:670,data:672,body:681,filePath:682,assetImports:683,digest:685,legacyId:686,deferredRender:32},{title:673,description:674,date:675,image:676,authors:677,categories:678,tags:679,canonical:680},"How To Deploy Static Website Astro.JS on VPS Servers","Learn how to deploy a node.js static website Astro on a VPS server easily.",["Date","2023-02-03T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/deploy-astro-vps.jpeg",[19],[98],[501,23,24],"https://www.bitdoze.com/deploy-astro-on-vps/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/cloudpanel-change-root.jpeg\";\r\nimport img2 from \"../../assets/images/23/cloudflare_dns.jpeg\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nThere are a lot of free options where you can deploy Astro.js websites like CloudFlare Pages, Vercel, or Netlify all of them for free but with some limitations.\r\n\r\nI have even created a tutorial of how you can [deploy astro.js on CloudFlare Pages](https://www.bitdoze.com/deploy-astrojs-cloudflare/) but you may want to use your own VPS server for it to not reach the limits of a free solution.\r\n\r\nIn this tutorial we are going to see how you can deploy an astro.js blog or any website on your own VPS server, you can use any VPS provider you want, the most known are [DigitalOcean](https://go.bitdoze.com/do), [Vultr](https://go.bitdoze.com/vultr), [Hetzner](https://go.bitdoze.com/hetzner), I have also written an article and made a video with the benchmarks here: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/) you can check it out.\r\n\r\nTo deploy Astro in this tutorial we will need to have the VPS server created and have CloudPanel installed, if you don't know CloudPanel is a hosting panel that can help you host different websites from PHP ones to Node.JS ones to static ones. It is fast and can help you manage your VPS easily, you can check the [Install CloudPanel and Host Node.js Apps](https://www.bitdoze.com/install-cloudpanel-host-nodejs/) to see exactly how you can install it.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\nIf you are interested on how you can monitor your CPU and have an automatic email sent when load is to big you should check: [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## Video With Astro.JS Deployment on VPS\r\n\r\nYou can check this tutorial showing how you can deploy Astro on your VPS with a Heroko or Netlify alternative Coolify: [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/) or [How to Deploy Astro on Your VPS with EasyPanel](https://www.bitdoze.com/deploy-astro-easypanel/)\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/kMtVBvO87pg\"\r\n  label=\"How To Deploy Static Website Astro.JS on VPS Servers\"\r\n/>\r\n\r\n## Deploy Astro.JS on Your VPS With CloudPanel\r\n\r\nLet's get into this tutorial and see how we can have our website deployed on a VPS server.\r\n\r\n### 1. Add The Website to CloudPanel\r\n\r\nThe first step is to go and add the website to CloudPanel, you just go and hit the Add Site and choose the static site option, you need to add your website URL with the website user and password.\r\n\r\nAfter you need to go and edit the root directory to point to **www.domain.com/dist** the dist directory is used by Astro to deploy the static content when you build it. Check the below picture:\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"CloudPanel change root directory\"\r\n/>\r\n\r\n### 2. Point the Website DNS to the server IP\r\n\r\nThe next thing to do is to go and point the A record of the domain to your server IP, I am doing this in CloudFlare and I have also activated the proxy for this domain.\r\n\r\n<Picture\r\n  src={img2}\r\n  alt=\"Clodflare DNS settings\"\r\n/>\r\n\r\n### 3. Generate an SSL Certificate for the Website\r\n\r\nThis is done from the CloudPanel SSL/TLS interface you just need to go there on the website and hit the new Let's Encrypt Certificate.\r\n\r\nNow the website will have its certificate and can be accessed over HTTPS.\r\n\r\n### 4. Install Node.js on your website user\r\n\r\nAstro is using node.js so we need to have node.js working on the user used for the website for this you just need to follow the below steps, also the [Install Node.js using NVM on MacOS and Ubuntu](https://www.bitdoze.com/install-nodejs-using-nvm-macos-ubuntu/) can help you:\r\n\r\nLogin with SSH to the VPS server and go under the user used for the website:\r\n\r\n```bash\r\nssh -i <key> root@serverip\r\nsudo su - <website_user>\r\n```\r\n\r\nInstall NVM:\r\n\r\n```bash\r\ncurl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash\r\nsource ~/.bashrc\r\n```\r\n\r\nInstall Node.js:\r\n\r\n```bash\r\nnvm install --lts\r\n```\r\n\r\n### 5. Remove the current website to add a new one with Astro:\r\n\r\n```bash\r\ncd htdocs\r\nrm -rf www.domain.com\r\n```\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n### 6. Add an Astro Website\r\n\r\nAdd the Astro website, if you have a GitHub repo you can use the [Link GitHub with A SSH Key](https://www.bitdoze.com/link-github-with-ssh-maco-linux/) for more details. In this article, I will add a clean Astro website\r\nBe sure to know Where would you like to create your new project? to add the exact folder with the domain name you deleted.\r\n\r\n### 7. Build your website\r\n\r\nAfter you make the modifications you can build your website:\r\n\r\n```bash\r\nnpm run astro build\r\n```\r\n\r\n## Conclusions\r\n\r\nThis is the easiest way to host astro.js on your own VPS, this can be used on any static website you just need to add the proper destination folder for your static website.\r\n\r\nUsing CloudFlare on the website will assure that will be fast and can be secured with additional security rules.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\nIn case you are interested to have a web panel that can help you manage your applications and be used as a reverse proxy you can check the bellow course:\r\n\r\n<Button\r\n  link=\"https://webdoze.net/courses/cloudpanel-setup/\"\r\n  text=\"CloudPanel Setup Course\"\r\n/>","src/content/posts/deploy-astro-on-vps.mdx",[684],"../../assets/images/23/deploy-astro-vps.jpeg","8a863b63f933bfca","deploy-astro-on-vps.mdx","deploy-astrojs-cloudflare",{id:687,data:689,body:698,filePath:699,assetImports:700,digest:702,legacyId:703,deferredRender:32},{title:690,description:691,date:692,image:693,authors:694,categories:695,tags:696,canonical:697},"How To Deploy An Astro.JS Blog On Cloudflare","Deploy an Astro.JS blog or website to CloudFlare Pages for free.",["Date","2022-10-18T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/2210/deploy-astro-cloudflare-blog.jpeg",[19],[98],[23,24],"https://www.bitdoze.com/deploy-astrojs-cloudflare/","import { Picture } from \"astro:assets\";\r\nimport imag1 from \"../../assets/images/2210/use-this-template.jpeg\";\r\nimport imag2 from \"../../assets/images/2210/deploy-cloudflare-project.jpeg\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nI am a WordPress user for years and lately, I have wanted to try something new and see how different it is. I have another website [WP Doze](https://www.wpdoze.com/) where I write about WordPress things but wanted to try something different.\r\nAfter searching online I found [Gatsby](https://www.gatsbyjs.com/) a static website generator that can help you have a blog or website fast, did a lot of [Gatsby courses](https://www.bitdoze.com/gatsby-js-online-courses/) to find out what is about and eventually came across [Astro](https://astro.build/).\r\n\r\nAstro is not around for a long time but it is here to stay, I like it as it is fast and it is easy to set it up. Same as with Gatsby in the beginning I have searched online for a lot of [Astro.js Trainings](https://www.bitdoze.com/best-astrojs-online-courses/) to see what is about and modify some aspects myself if I need it.\r\n\r\nIn this article, we will see how you can deploy your Astro website on Cloudflare pages and have a free blog or website in the function of your needs.\r\n\r\nI will use the [GitHub Bit Doze blog theme](https://github.com/bitdoze/bitdoze.com) for the deployment and guide you through what you need to change to host your blog. The Theme is built on [AstroWind theme](https://github.com/onwidget/astrowind) and you can install either as it's the same and you should change the same thing just mine has only the blog things active.\r\n\r\n## How To Deploy An Astro.Js Blog On Cloudflare\r\n\r\nYou can check this tutorial showing how you can deploy Astro on your VPS with a Heroko or Netlify alternative Coolify: [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/) or [How to Deploy Astro on Your VPS with EasyPanel](https://www.bitdoze.com/deploy-astro-easypanel/)\r\n\r\n### 1. Setup Node.js and Github\r\n\r\nCloudflare Pages are using GitHub to fetch the details and node.js is needed so you can view the project before deploying it on Cloudflare to be sure is working to prepare your laptop just follow the below 2 tutorials to get you going:\r\n\r\n- [How to Install Node.js using NVM on MacOS and Ubuntu](https://www.bitdoze.com/install-nodejs-using-nvm-macos-ubuntu/)\r\n- [Link GitHub with A SSH Key to MacOS or Linux](https://www.bitdoze.com/link-github-with-ssh-maco-linux/)\r\n\r\n### 2. Fetch the [AstroWind theme](https://github.com/onwidget/astrowind) or [Bit Doze simpified theme](https://github.com/bitdoze/bitdoze.com)\r\n\r\nTo do this you need to open the GitHub project and hit **Use this template** button. Be sure you are logged in to have this enabled.\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"GitHub Use Template\"\r\n/>\r\n\r\nAfter you just need to go and clone the project on your laptop with:\r\n\r\n```bash\r\ngit clone <path to the repo you have>\r\n#Example:\r\ngit clone git@github.com:bitdoze/bitdoze.com.git\r\n```\r\n\r\nThen you will need to go and install all the packages and update them if you want. To do That you should do:\r\n\r\n```bash\r\nnpm install\r\nnpm update\r\n```\r\n\r\n## Video With Astro.js Deploy\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/T7PY55WudZ4\"\r\n  label=\"How To Deploy An Astro.JS Blog On Cloudflare\"\r\n/>{\" \"}\r\n\r\n### 3. Modify the theme files\r\n\r\n#### 3.1 Modify URL with Title and Description\r\n\r\n[AstroWind theme](https://github.com/onwidget/astrowind) or [Bit Doze simplified theme](https://github.com/bitdoze/bitdoze.com) has a config file that you need to modify it and change the details for your website. The location is under _src/config.mjs_\r\n\r\nHere replace the:\r\n\r\n- name - with your website name\r\n- origin - with the URL for your website\r\n- basePathname - add something in case this is using a subdirectory\r\n- title - will be used in the main URL\r\n- description - website description\r\n- googleAnalyticsId - in case you want analytics added to the website\r\n- googleSiteVerificationId- for Google Web Central ( I recommend the use of DNS verification instead)\r\n\r\nOther changes are for the url, category, tags. You can let them like this if you want.\r\nBelow is the file used for bitdoze.com:\r\n\r\n```js\r\nexport const SITE = {\r\n  name: \"Bit Doze\",\r\n\r\n  origin: \"https://www.bitdoze.com\",\r\n  basePathname: \"/\",\r\n\r\n  title: \"Bit Doze Website\",\r\n  description:\r\n    \"An Website about Linux, blogging, CMS and other internet news..\",\r\n\r\n  googleAnalyticsId: false, // or \"G-XXXXXXXXXX\",\r\n  googleSiteVerificationId: \"jyQEUqY6oYZKY6ZhkHatU7g4vVBBSb7z3Zw5bA\",\r\n};\r\n\r\nexport const BLOG = {\r\n  disabled: false,\r\n  postsPerPage: 9,\r\n\r\n  blog: {\r\n    disabled: false,\r\n    pathname: \"blog\", // blog main path, you can change this to \"articles\" (/articles)\r\n  },\r\n\r\n  post: {\r\n    disabled: false,\r\n    pathname: \"\", // empty for /some-post, value for /pathname/some-post\r\n  },\r\n\r\n  category: {\r\n    disabled: false,\r\n    pathname: \"\", // set empty to change from /category/some-category to /some-category\r\n  },\r\n\r\n  tag: {\r\n    disabled: false,\r\n    pathname: \"tag\", // set empty to change from /tag/some-tag to /some-tag\r\n  },\r\n};\r\n```\r\n\r\n#### 3.2 Modify Header Links\r\n\r\nThe themes is using by default my blog categories so you need to add the categories you want and pages in the menu. To alter them you just need to open the file under _src/components/widgets/Header.astro_\r\n\r\nIn here just add or delete the _li_ by modifying the _href_ with the path you have, eg:\r\n\r\n```html\r\n<li>\r\n  <a\r\n    class=\"font-medium hover:text-gray-900 dark:hover:text-white px-4 py-3 flex items-center transition duration-150 ease-in-out\"\r\n    href=\"/cms/\"\r\n    >CMS</a\r\n  >\r\n</li>\r\n```\r\n\r\nTo add a new thing just add a new li and change the url and name.\r\n\r\n#### 3.3 Modify the footer links\r\n\r\nIn the footer I have some links to my twitter and GitHub profile, you should change them or delete them as per your needs. This can be done from _src/components/widgets/Footer.astro_\r\n\r\n```html\r\n<li>\r\n  <a\r\n    class=\"text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm p-2.5 inline-flex items-center\"\r\n    aria-label=\"Twitter\"\r\n    href=\"https://twitter.com/bitdozecom\"\r\n  >\r\n    <Icon name=\"tabler:brand-twitter\" class=\"w-5 h-5\" />\r\n  </a>\r\n</li>\r\n```\r\n\r\n#### 3.4 Modify the Logo\r\n\r\nLogo is a text one, created from a gradient colour of primary and secondary, to modify it you just need to go under _src/components/atoms/Logo.astro_ and modify the text:\r\n\r\n```html\r\n<span\r\n  class=\"bg-clip-text font-bold text-4xl text-transparent bg-gradient-to-r from-primary-500 to-secondary-500 sm:whitespace-nowrap\"\r\n>\r\n  Bit Doze</span\r\n>\r\n```\r\n\r\n#### 3.5 Modify the Hero with your description and social links\r\n\r\nIn the front page I have Hero with the welcome and social links, to modify the text icons and links you should alter the _src/components/widgets/Hero.astro_ In here you have the h1 and div with text and ul with the social icons. This is using Astra Icon so you can add anything you want.\r\n\r\n```html\r\n<h1\r\n  class=\"text-5xl md:text-[3.50rem] font-bold leading-tighter tracking-tighter mb-4 font-heading\"\r\n>\r\n  Welcome to\r\n  <span\r\n    class=\"bg-clip-text text-transparent bg-gradient-to-r from-primary-500 to-secondary-500 sm:whitespace-nowrap\"\r\n    >Bit Doze</span\r\n  >\r\n</h1>\r\n<div class=\"max-w-3xl mx-auto\">\r\n  <p class=\"text-xl text-gray-600 mb-8 dark:text-slate-400\">\r\n    Join the community to find out more about, VPS, blogging, CMS.\r\n  </p>\r\n</div>\r\n\r\n<ul\r\n  class=\"flex justify-center items-center mb-4 md:order-1 -ml-2 md:ml-4 md:mb-0\"\r\n>\r\n  <li>\r\n    <a\r\n      class=\"text-gray-500 dark:text-gray-400 hover:bg-gray-100 dark:hover:bg-gray-700 focus:outline-none focus:ring-4 focus:ring-gray-200 dark:focus:ring-gray-700 rounded-lg text-sm p-2.5 inline-flex items-center\"\r\n      aria-label=\"Twitter\"\r\n      href=\"https://twitter.com/bitdoze\"\r\n    >\r\n      <Icon name=\"tabler:brand-twitter\" class=\"w-20 h-20\" />\r\n    </a>\r\n  </li>\r\n</ul>\r\n```\r\n\r\n#### 3.5 Modify the main colours\r\n\r\nThe theme is coming with the main colour that is used around the components to change the color you can take a look on [tailwind colors](https://tailwindcss.com/docs/customizing-colors) and modify the _tailwind.config.cjs_ under colors primary and secondary\r\n\r\n```js\r\ncolors: {\r\n\t\t\t\tprimary: colors.blue,\r\n\t\t\t\tsecondary: colors.indigo,\r\n\t\t\t},\r\n```\r\n\r\n#### 3.6 Cleanup Assets folder and data folder\r\n\r\nCurrently, the theme has my mdx posts and images, you may want to clean up the _src/assets/images/_ and _data/blog/_ to add your md files with articles and things.\r\n\r\n#### 3.7 Add You Articles to the first page Blog Roll\r\n\r\nAfter you added your own articles with picture to modify the first page articles you need to go under _src/components/blog/HighlightedPosts.astro_ and add your articles. Just replace the below with your own:\r\n\r\n```js\r\nconst ids = [\r\n  \"deploy-astrojs-cloudflare\",\r\n  \"best-astrojs-online-courses\",\r\n  \"link-github-with-ssh-maco-linux\",\r\n  \"install-nodejs-using-nvm-macos-ubuntu\",\r\n  \"gatsby-js-online-courses\",\r\n  \"embed-youtube-videos-to-gatsby\",\r\n];\r\n```\r\n\r\n### 4. Preview the changes and update the GitLab project with the latest changes\r\n\r\nAfter we confirm the project and have added our article we can see if everything is OK and we can push the project to GitLab to host it in Cloudflare Pages.\r\n\r\nTo preview the changes just run:\r\n\r\n```bash\r\nnpm run dev\r\n```\r\n\r\nOpen the localhost:3000 or the link specified to see if you have everything in order. In case everything looks good you should proceed and push the project to GitLab:\r\n\r\n```bash\r\ngit add .\r\ngit commit -m \"configured my website\"\r\ngit push\r\n```\r\n\r\n### 5. Host the project in CloudFlare\r\n\r\nCloudflare can be used to host Astro, they have also this documentation [Deploy Astro Sites](https://developers.cloudflare.com/pages/framework-guides/astro/)\r\n\r\nWith the free account, you have 500 monthly builds and you benefit from their CDN so the website is very fast. These 500 builds are more than enough to host a blog or have a few. Every time you push an article or a change to GitHub Cloudflare will pick the latest change and will count it as a build.\r\n\r\nYou need to go under Pages and add create a project. There you connect it to your git repo and begin the setup.\r\n\r\nYou choose:\r\n\r\n- Production branch: main\r\n- Build command: npm run build\r\n- Build directory: dist\r\n- Environment Variables NODE_VERSION: 16\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"Cloudflare Astro Deploy\"\r\n/>\r\n\r\nThen you hit save and deploy and that is all. When you want to add an article you just need to push the change and Cloudflare will do the rest.\r\n\r\nNext, you can check the below articles about Astro:\r\n\r\n- [Add Responsive YouTube Videos to Astro.JS MDX](https://www.bitdoze.com/responsive-youtube-astrojs/)","src/content/posts/deploy-astrojs-cloudflare.mdx",[701],"../../assets/images/2210/deploy-astro-cloudflare-blog.jpeg","01337dabec0bc205","deploy-astrojs-cloudflare.mdx","deploy-filebrowser-docker",{id:704,data:706,body:715,filePath:716,assetImports:717,digest:719,legacyId:720,deferredRender:32},{title:707,description:708,date:709,image:710,authors:711,categories:712,tags:713,canonical:714},"Simplify File Management with Docker Filebrowser: Easy Setup Guide","Learn how you can install Filebrowser with the help of Docker and docker compose to manage your server files with a UI",["Date","2024-07-26T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/04/easy-file-management.jpeg",[19],[98],[242],"https://www.bitdoze.com/deploy-filebrowser-docker/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\n\r\n[Filebrowser](https://filebrowser.org/) is a powerful web-based file manager that allows you to easily manage your files and folders from any device with a web browser. It provides a user-friendly interface for uploading, deleting, previewing, renaming, and editing files, as well as creating and managing user accounts with customizable permissions.\r\n\r\nOne of the best ways to set up Filebrowser is by using Docker, which simplifies the installation and deployment process. In this guide, we'll walk you through the steps to set up Filebrowser with Docker Compose, a tool that makes it easy to manage and deploy multi-container Docker applications.\r\n\r\nIf you're interested in exploring other useful Docker containers for your home server setup, check out our comprehensive guide: [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of information on various containers that can enhance your home server's capabilities, including media management, networking tools, and productivity applications.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## How To Set Up Filebrowser with Docker Compose\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/9KT2sF7CQsg\"\r\n  label=\"Docker Filebrowser Install\"\r\n/>\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host FlowiseAI, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\nHaving all of this you will be ready to move to next step and add the containers in Dockge.\r\n\r\n### 2. Create A Docker Compose File\r\n\r\nCreate a new file named `docker-compose.yml` in your preferred directory and add the following configuration:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  filebrowser:\r\n    image: filebrowser/filebrowser:v2-s6\r\n    container_name: filebrowser\r\n    restart: unless-stopped\r\n    volumes:\r\n      - /opt/stacks:/srv # Change to match your directory\r\n      - ./filebrowser.db:/database/filebrowser.db # Change to match your directory\r\n      - ./settings.json:/config/settings.json # Change to match your directory\r\n    environment:\r\n      PUID: $(id -u)\r\n      PGID: $(id -g)\r\n    ports:\r\n      - 5040:80 # Change the port if needed\r\n```\r\n\r\nIn this configuration, we're using the `filebrowser/filebrowser:v2-s6` Docker image, which is the latest stable version of Filebrowser. The `volumes` section maps the necessary directories and files for Filebrowser to function, including the file storage directory (`/opt/stacks`), the database file (`filebrowser.db`), and the settings file (`settings.json`). You'll need to update these paths to match your own setup.\r\n\r\nThe `environment` section sets the user and group IDs for the Filebrowser container, which is important for file permissions. The `ports` section exposes the Filebrowser web interface on port `5040`, which you can change if needed.\r\n\r\n### 3. Create the `filebrowser.db` and `settings.json`\r\n\r\nIn the same directory as your `docker-compose.yml` file, create the following files:\r\n\r\n**filebrowser.db:**\r\n\r\n```sh\r\ntouch filebrowser.db\r\n```\r\n\r\n**settings.json:**\r\nFetch the [Filebrowser settings file](https://github.com/filebrowser/filebrowser/blob/master/docker/root/defaults/settings.json) and create one with:\r\n\r\n```sh\r\nvi settings.json\r\n```\r\n\r\nThen paste the following configuration:\r\n\r\n```json\r\n{\r\n  \"port\": 80,\r\n  \"baseURL\": \"\",\r\n  \"address\": \"\",\r\n  \"log\": \"stdout\",\r\n  \"database\": \"/database/filebrowser.db\",\r\n  \"root\": \"/srv\"\r\n}\r\n```\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\nIn case you are seeing the bellow error:\r\n\r\n```sh\r\nfilebrowser  | 2024/04/25 07:17:26 Using database: /database/filebrowser.db\r\nfilebrowser  | 2024/04/25 07:17:26 open /database/filebrowser.db: permission denied\r\n```\r\n\r\nJust grant the `filebrowser.db` the rights you are seeing in the startup of the container, this happens when you are running with root the docker install:\r\n\r\n```sh\r\nfilebrowser  | GID/UID\r\nfilebrowser  | ───────────────────────────────────────\r\nfilebrowser  |\r\nfilebrowser  | User UID:    911\r\nfilebrowser  | User GID:    1001\r\nfilebrowser  | ───────────────────────────────────────\r\n```\r\n\r\n```sh\r\nchown 911:1001 filebrowser.db\r\n```\r\n\r\n### 4. Deploy Filebrowser\r\n\r\nIn the same directory as your `docker-compose.yml` file, run the following command to start the Filebrowser container:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nYou can check that the container is running with the following command:\r\n\r\n```sh\r\ndocker ps | grep -i filebrowser\r\n```\r\n\r\n### 5. Configure the CloudFlare Tunnels for SSL and Domain Access\r\n\r\nTo securely access your Filebrowser instance from the internet, you'll need to set up a reverse proxy like CloudFlare Tunnels. This will provide SSL/TLS encryption and allow you to access Filebrowser using a domain or subdomain.\r\n\r\nGo in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 6. Access the Filebrowser UI\r\n\r\nOnce you've set up the CloudFlare Tunnels (or another reverse proxy), you can access the Filebrowser web interface using the domain or subdomain you configured. The default login credentials are `admin/admin`, but you should change the password immediately after your first login.\r\n\r\n## Conclusion\r\n\r\nBy following this guide, you've successfully set up Filebrowser using Docker Compose, which simplifies the installation and deployment process. Filebrowser provides a user-friendly way to manage your files and folders, and the Docker-based setup ensures that it's easy to maintain and scale as your needs grow.","src/content/posts/deploy-filebrowser-docker.mdx",[718],"../../assets/images/24/04/easy-file-management.jpeg","6f198047c4936abc","deploy-filebrowser-docker.mdx","deploy-uptime-kuma",{id:721,data:723,body:732,filePath:733,assetImports:734,digest:736,legacyId:737,deferredRender:32},{title:724,description:725,date:726,image:727,authors:728,categories:729,tags:730,canonical:731},"How To Deploy Uptime Kuma With One Click","Learn how to deploy Uptime Kuma with 1 click in docker via Coolify.",["Date","2023-03-16T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/03/Uptime_Kuma_Install_Coolify.jpeg",[19],[98],[554,24],"https://www.bitdoze.com/deploy-uptime-kuma/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/03/01createservice.png\";\r\nimport img2 from \"../../assets/images/23/03/02chooseuptimekuma.png\";\r\nimport img3 from \"../../assets/images/23/03/03uptimekumaconfigs.jpeg\";\r\nimport img4 from \"../../assets/images/23/03/04accessuptimekuma.png\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[Uptime Kuma](https://www.bitdoze.com/uptime-kuma-tool/) is an open source monitoring tool that you can install on your VPS via Docker. Uptime Kuma will help you monitor your website and send alerts when it is down. In this tutorial, we will see how we can install it via Coolify with just 1 click.\r\n\r\nIf you don't know, Coolify is a self-hosted Heroku or Netlify alternative that also allows you to deploy various Docker apps. For more details you can check out: [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/) where we go into more detail.\r\n\r\n## 1. Deploy A VPS server\r\n\r\nTo host Uptime Kume in a Docker container you need a VPS server, there are a lot of services that can help you with this, the most known are [DigitalOcean](https://go.bitdoze.com/do), [Vultr](https://go.bitdoze.com/vultr), [Hetzner](https://go.bitdoze.com/hetzner), I also wrote an article and made a video with the benchmarks here: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/) you can check it out.\r\n\r\nIn this tutorial, we are going to use Hetzner for this where we have the VPS created and we can ssh to it to have Coolify installed and then deploy Uptime Kuma.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n## 2. Installing Coolify\r\n\r\nCoolify can be installed with a simple command, you can also check my tutorial for complete steps including the SSL certificate: [Coolify Install](https://www.bitdoze.com/coolify-install-heroku-alternative/)\r\n\r\nWe will install Coolify on a Ubuntu 22.04 server. To do this, you just need to SSH to the VPS server and run the following command\r\n\r\n```bash\r\nwget -q https://get.coollabs.io/coolify/install.sh \\\r\n-O install.sh; sudo bash ./install.sh\r\n```\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\nIf you are interested in how to monitor your CPU and have an automatic email sent when the load is too high, you should have a look at [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)\r\n\r\n## 🎥 Uptime Kuma Video with Deployment\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/GYbqyhs4suk\"\r\n  label=\"How To Deploy Uptime Kuma With One Click\"\r\n/>{\" \"}\r\n\r\n## 3. Deploy the Uptime Kuma service with Coolify\r\n\r\n### 3.1 Point Domain or Subdomain to VPS\r\n\r\nWe need a domain or subdomain that we can use to access Uptime Kuma, in this tutorial we are going to use CloudFlare and point a subdomain to the VPS server where we have Coolify installed. We will not be using the CloudFlare proxy and will leave it disabled. All you need to do is add an A record and point it to the server IP.\r\n\r\n### 3.2 Create a service\r\n\r\nYou need to go to Create New Resource and select **Service**. From there select the **UptimeKuma** service as shown in the images below:\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"Coolify service create\"\r\n/>\r\n<Picture\r\n  src={img2}\r\n  alt=\"Coolify choose UptimeKuma\"\r\n/>\r\n\r\n### 3.3 Configure Coolify Uptime Kuma\r\n\r\nThe next thing on the list is to configure the Coolify service, in there you need to add the below:\r\n\r\n- **Name** - here you add the name of your deployment\r\n- **Version / Tag** - you choose the Uptime Kuma version you want to install\r\n- **URL (FQDN)** - you add the domain or sudomain that you want to use\r\n\r\n<Picture\r\n  src={img3}\r\n  alt=\"Coolify UptimeKuma Configs\"\r\n/>\r\n\r\nAfter what needs to be done is to hit Save and Deploy button.\r\n\r\n### 3.4 Access Uptime Kuma\r\n\r\nAfter deploy is finished you should go and access Uptime Kuma dashboard with the URL you have added in the configs. The first time you will be prompted to create a user and a password.\r\n\r\n<Picture\r\n  src={img4}\r\n  alt=\" UptimeKuma Access\"\r\n/>\r\n\r\nAfter you create the account you can add your website that needs to be monitored and configure the notifications. The configs will be stored on the disks with docker volums and in case you redeploy it you will not lose any data.\r\n\r\n## Conclusions\r\n\r\nThis is how you can easely deploy Uptime Kuma with the help off Coolify, the config has everything that it needs and you can access it securely with an SSL certificate.","src/content/posts/deploy-uptime-kuma.mdx",[735],"../../assets/images/23/03/Uptime_Kuma_Install_Coolify.jpeg","6dec18ba3128ac52","deploy-uptime-kuma.mdx","deploy-pgvector-pgadmin-docker",{id:738,data:740,body:749,filePath:750,assetImports:751,digest:753,legacyId:754,deferredRender:32},{title:741,description:742,date:743,image:744,authors:745,categories:746,tags:747,canonical:748},"How to Deploy PGvector and PGadmin on Docker and Ditch Pinecone","Learn how you can install Postgres PGvector database and PGadmin on Docker Compose for your projects.",["Date","2024-06-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/05/deploy-pgvector-docker.jpeg",[19],[98],[242],"https://www.bitdoze.com/deploy-pgvector-pgadmin-docker/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\n\r\n**PostgreSQL** is a powerful, open-source object-relational database system known for its robustness, extensibility, and compliance with SQL standards. It has been actively developed for over 35 years and is widely used for managing complex data workloads.\r\n\r\n**PGVector** is an extension for PostgreSQL that enables vector similarity search, allowing you to store, query, and index vectors. This is particularly useful for applications involving high-dimensional data, such as recommendation systems, image search, and natural language processing.\r\n\r\n**PGAdmin** is a web-based graphical user interface (GUI) management tool for PostgreSQL. It simplifies database administration tasks, providing a user-friendly interface for executing SQL queries, managing database objects, and monitoring database performance.\r\n\r\nPgvector can store embeddings generated by AI models, such as those from OpenAI. This integration allows for efficient similarity searches and retrieval-augmented generation (RAG), where embeddings help find relevant information to augment AI model responses\r\n\r\nPgvector transforms PostgreSQL into a powerful vector database, enabling efficient storage and querying of embeddings. This capability is essential for AI applications that rely on vector similarity, such as recommendation systems, NLP, and image search. With enhancements from pgvectorscale, PostgreSQL can now compete with specialized vector databases, offering high performance and scalability at a lower cost\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## How to Deploy PGvector and PGadmin on Docker\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/WSkP9EkBsh0\"\r\n  label=\"How to Deploy PGvector and PGadmin on Docker and Ditch Pinecone\"\r\n/>\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host PGvector, you can use one from [Hetzner](https://go.bitdoze.com/hetzner)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n\r\nDockge and CloudFlare tunnels are optional, they are just here to help us deploy easier the compose file and have a subdomain added to the PGAdmin so we can access it via SSL with CloudFlare Protection. They are not mandatory you can use what ever stack you like.\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\n### 2. Docker Compose File\r\n\r\nThe following Docker Compose file sets up a PostgreSQL database with the PGVector extension and a PGAdmin instance for database management.\r\n\r\n```yaml\r\nversion: \"3.8\"\r\nservices:\r\n  db:\r\n    hostname: pgvector_db\r\n    container_name: pgvector_db_container\r\n    image: ankane/pgvector\r\n    ports:\r\n      - 5432:5432\r\n    restart: unless-stopped\r\n    environment:\r\n      - POSTGRES_DB=${POSTGRES_DB}\r\n      - POSTGRES_USER=${POSTGRES_USER}\r\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\r\n      - POSTGRES_HOST_AUTH_METHOD=trust\r\n    volumes:\r\n      - ./local_pgdata:/var/lib/postgresql/data\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n  pgadmin:\r\n    image: dpage/pgadmin4\r\n    container_name: pgadmin4_container\r\n    restart: unless-stopped\r\n    ports:\r\n      - 5016:80\r\n    user: \"$UID:$GID\"\r\n    environment:\r\n      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}\r\n      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD}\r\n    volumes:\r\n      - ./pgadmin-data:/var/lib/pgadmin\r\n```\r\n\r\n**Explanation of the Docker Compose File**\r\n\r\n- **services**: Defines the services to be run.\r\n  - **db**: This service sets up the PostgreSQL database with the PGVector extension.\r\n    - **hostname**: Sets the hostname for the container.\r\n    - **container_name**: Names the container for easier identification.\r\n    - **image**: Uses the `ankane/pgvector` image, which includes PostgreSQL with the PGVector extension.\r\n    - **ports**: Maps port 5432 on the host to port 5432 in the container.\r\n    - **restart**: Ensures the container restarts unless stopped manually.\r\n    - **environment**: Sets environment variables for the PostgreSQL database.\r\n    - **volumes**: Mounts a local directory to persist PostgreSQL data.\r\n    - **healthcheck**: Configures a health check to ensure the database is ready.\r\n  - **pgadmin**: This service sets up the PGAdmin web interface.\r\n    - **image**: Uses the `dpage/pgadmin4` image.\r\n    - **container_name**: Names the container for easier identification.\r\n    - **restart**: Ensures the container restarts unless stopped manually.\r\n    - **ports**: Maps port 5016 on the host to port 80 in the container.\r\n    - **user**: Sets the user ID and group ID for the container.\r\n    - **environment**: Sets environment variables for PGAdmin.\r\n    - **volumes**: Mounts a local directory to persist PGAdmin data.\r\n\r\n### 3. Environment Variables File\r\n\r\nCreate a `.env` file with the following content to set the necessary environment variables:\r\n\r\n```\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='vector'\r\nPGADMIN_DEFAULT_PASSWORD=pgpass\r\nPGADMIN_DEFAULT_EMAIL=bitdoze1@gmail.com\r\n```\r\n\r\nHere you can add your users and passwords, POSTGRES ones are for the database and PGADMIN are to access the UI for administrating the PGvector database.\r\n\r\n### 4. Deploying PGVector and PGAdmin\r\n\r\nTo deploy the services, run the following command:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nThis command will start the PostgreSQL database with PGVector and the PGAdmin web interface in detached mode.\r\n\r\n### 5. Accessing the PostgreSQL Database with PGAdmin\r\n\r\n1. Open your web browser and navigate to `http://ip:5016`.\r\n2. Log in using the email and password specified in the `.env` file.\r\n3. Register a new server in PGAdmin using the following details:\r\n   - **Host name/address**: `pgvector_db`\r\n   - **Port**: `5432`\r\n   - **Username**: The value of `POSTGRES_USER` from the `.env` file.\r\n   - **Password**: The value of `POSTGRES_PASSWORD` from the `.env` file.\r\n\r\n### 6. Configure the CloudFlare Tunnels\r\n\r\nYou need to let CloudFlare Tunel know which port to use, you just need to go in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers.\r\n\r\n### Conclusion\r\n\r\nDeploying PostgreSQL with PGVector and PGAdmin using Docker simplifies the setup process and provides a powerful environment for managing high-dimensional data and performing vector similarity searches. This setup is ideal for AI-based applications and other use cases requiring efficient data management and analysis.","src/content/posts/deploy-pgvector-pgadmin-docker.mdx",[752],"../../assets/images/24/05/deploy-pgvector-docker.jpeg","cebd4a3e954ab08c","deploy-pgvector-pgadmin-docker.mdx","docker-commands",{id:755,data:757,body:766,filePath:767,assetImports:768,digest:770,legacyId:771,deferredRender:32},{title:758,description:759,date:760,image:761,authors:762,categories:763,tags:764,canonical:765},"Top 50+ Docker Commands You MUST Know","Master Docker with our comprehensive guide to 50+ essential commands. From basic operations to advanced container management, this resource covers everything DevOps professionals and Docker enthusiasts need to know.",["Date","2024-07-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/docker-commands.jpeg",[19],[98],[100],"https://www.bitdoze.com/docker-commands/","Docker has revolutionized the way software is developed, shipped, and deployed. By providing a consistent environment across different stages of development, Docker ensures that applications run seamlessly from development to production. This containerization technology has become a cornerstone in modern DevOps practices, enabling developers and system administrators to work more efficiently and reliably.\r\n\r\nKnowing Docker commands is essential for anyone involved in software development or system administration. These commands allow you to manage containers, images, networks, and volumes with ease, providing you with the tools needed to build, deploy, and maintain applications effectively. This article aims to provide a comprehensive list of the top 50+ Docker commands that are crucial for effective container management. Whether you are a beginner or an experienced user, mastering these commands will enhance your productivity and streamline your workflows.\r\n\r\n## Section 1: Basic Docker Commands\r\n\r\n### 1.1 Docker Version and Info\r\n\r\nUnderstanding the version and system-wide information of Docker installed on your machine is the first step in mastering Docker commands. Here are the essential commands:\r\n\r\n- `docker --version`: This command displays the version of Docker installed on your system. It's a quick way to verify that Docker is installed and to check which version you are running.\r\n\r\n  ```sh\r\n  $ docker --version\r\n  Docker version 20.10.7, build f0df350\r\n  ```\r\n\r\n- `docker version`: This command provides detailed version information about the Docker client and server. It includes the version number, Git commit, and build date.\r\n\r\n  ```sh\r\n  $ docker version\r\n  Client:\r\n   Version:           20.10.7\r\n   API version:       1.41\r\n   Go version:        go1.13.15\r\n   Git commit:        f0df350\r\n   Built:             Wed Jun  2 11:56:24 2021\r\n   OS/Arch:           linux/amd64\r\n   Context:           default\r\n   Experimental:      true\r\n\r\n  Server:\r\n   Engine:\r\n    Version:          20.10.7\r\n    API version:      1.41 (minimum version 1.12)\r\n    Go version:       go1.13.15\r\n    Git commit:       b0f5bc3\r\n    Built:            Wed Jun  2 11:55:04 2021\r\n    OS/Arch:          linux/amd64\r\n    Experimental:     false\r\n  ```\r\n\r\n- `docker info`: This command displays system-wide information about Docker, including the number of containers, images, and the storage driver in use. It's useful for getting a comprehensive overview of your Docker environment.\r\n\r\n  ```sh\r\n  $ docker info\r\n  Client:\r\n   Context:    default\r\n   Debug Mode: false\r\n\r\n  Server:\r\n   Containers: 2\r\n    Running: 1\r\n    Paused: 0\r\n    Stopped: 1\r\n   Images: 5\r\n   Storage Driver: overlay2\r\n   Logging Driver: json-file\r\n   Cgroup Driver: cgroupfs\r\n   Plugins:\r\n    Volume: local\r\n    Network: bridge host ipvlan macvlan null overlay\r\n    Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\r\n   Swarm: inactive\r\n   Runtimes: runc\r\n   Default Runtime: runc\r\n   Init Binary: docker-init\r\n   containerd version: 7ad184331fa3e55e52b890ea95e65ba581ae3429\r\n   runc version: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\r\n   init version: de40ad0\r\n   Security Options:\r\n    seccomp\r\n     Profile: default\r\n   Kernel Version: 5.8.0-53-generic\r\n   Operating System: Ubuntu 20.04.2 LTS\r\n   OSType: linux\r\n   Architecture: x86_64\r\n   CPUs: 4\r\n   Total Memory: 7.771GiB\r\n   Name: docker-desktop\r\n   ID: 3Z4Q:KZ3F:3Z4Q:KZ3F:3Z4Q:KZ3F:3Z4Q:KZ3F\r\n   Docker Root Dir: /var/lib/docker\r\n   Debug Mode: false\r\n   Registry: https://index.docker.io/v1/\r\n   Labels:\r\n   Experimental: false\r\n   Insecure Registries:\r\n    127.0.0.0/8\r\n   Live Restore Enabled: false\r\n  ```\r\n\r\n### 1.2 Docker Help\r\n\r\nDocker provides built-in help commands to assist you in understanding the usage and options of various Docker commands:\r\n\r\n- `docker --help`: This command displays a list of all available Docker commands along with a brief description of each. It's a handy reference for quickly finding the command you need.\r\n\r\n  ```sh\r\n  $ docker --help\r\n\r\n  Usage:  docker [OPTIONS] COMMAND\r\n\r\n  A self-sufficient runtime for containers\r\n\r\n  Options:\r\n    --config string      Location of client config files (default\r\n                         \"/root/.docker\")\r\n    -D, --debug          Enable debug mode\r\n    -H, --host list      Daemon socket(s) to connect to\r\n    -l, --log-level string   Set the logging level (debug, info, warn, error, fatal)\r\n                             (default \"info\")\r\n    --tls                Use TLS; implied by --tlsverify\r\n    --tlscacert string   Trust certs signed only by this CA (default\r\n                         \"/root/.docker/ca.pem\")\r\n    --tlscert string     Path to TLS certificate file (default\r\n                         \"/root/.docker/cert.pem\")\r\n    --tlskey string      Path to TLS key file (default \"/root/.docker/key.pem\")\r\n    --tlsverify          Use TLS and verify the remote\r\n    -v, --version        Print version information and quit\r\n\r\n  Management Commands:\r\n    builder     Manage builds\r\n    config      Manage Docker configs\r\n    container   Manage containers\r\n    context     Manage contexts\r\n    engine      Manage the docker engine\r\n    image       Manage images\r\n    network     Manage networks\r\n    node        Manage Swarm nodes\r\n    plugin      Manage plugins\r\n    secret      Manage [Docker secrets](https://www.bitdoze.com/docker-compose-secrets/)\r\n    service     Manage services\r\n    stack       Manage Docker stacks\r\n    swarm       Manage Swarm\r\n    system      Manage Docker\r\n    trust       Manage trust on Docker images\r\n    volume      Manage volumes\r\n\r\n  Commands:\r\n    attach      Attach local standard input, output, and error streams to a running container\r\n    build       Build an image from a Dockerfile\r\n    commit      Create a new image from a container's changes\r\n    cp          Copy files/folders between a container and the local filesystem\r\n    create      Create a new container\r\n    diff        Inspect changes to files or directories on a container's filesystem\r\n    events      Get real time events from the server\r\n    exec        Run a command in a running container\r\n    export      Export a container's filesystem as a tar archive\r\n    history     Show the history of an image\r\n    images      List images\r\n    import      Import the contents from a tarball to create a filesystem image\r\n    info        Display system-wide information\r\n    inspect     Return low-level information on Docker objects\r\n    kill        Kill one or more running containers\r\n    load        Load an image from a tar archive or STDIN\r\n    login       Log in to a Docker registry\r\n    logout      Log out from a Docker registry\r\n    logs        Fetch the logs of a container\r\n    pause       Pause all processes within one or more containers\r\n    port        List port mappings or a specific mapping for the container\r\n    ps          List containers\r\n    pull        Pull an image or a repository from a registry\r\n    push        Push an image or a repository to a registry\r\n    rename      Rename a container\r\n    restart     Restart one or more containers\r\n    rm          Remove one or more containers\r\n    rmi         Remove one or more images\r\n    run         Run a command in a new container\r\n    save        Save one or more images to a tar archive (streamed to STDOUT by default)\r\n    search      Search the Docker Hub for images\r\n    start       Start one or more stopped containers\r\n    stats       Display a live stream of container(s) resource usage statistics\r\n    stop        Stop one or more running containers\r\n    tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\r\n    top         Display the running processes of a container\r\n    unpause     Unpause all processes within one or more containers\r\n    update      Update configuration of one or more containers\r\n    version     Show the Docker version information\r\n    wait        Block until one or more containers stop, then print their exit codes\r\n\r\n  Run 'docker COMMAND --help' for more information on a command.\r\n  ```\r\n\r\n- `docker <command> --help`: This command provides detailed help for a specific Docker command, including its usage, options, and examples. For instance, if you want to learn more about the `docker run` command, you can use:\r\n\r\n  ```sh\r\n  $ docker run --help\r\n\r\n  Usage:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\r\n\r\n  Run a command in a new container\r\n\r\n  Options:\r\n    -d, --detach                         Run container in background and print container ID\r\n    --name string                        Assign a name to the container\r\n    -p, --publish list                   Publish a container's port(s) to the host\r\n    -v, --volume list                    Bind mount a volume\r\n    --rm                                 Automatically remove the container when it exits\r\n    -i, --interactive                    Keep STDIN open even if not attached\r\n    -t, --tty                            Allocate a pseudo-TTY\r\n    --network string                     Connect a container to a network\r\n    --restart string                     Restart policy to apply when a container exits\r\n    --env list                           Set environment variables\r\n    --label list                         Set metadata on a container\r\n    --log-driver string                  Logging driver for the container\r\n    --log-opt list                       Log driver options\r\n    --privileged                         Give extended privileges to this container\r\n    --user string                        Username or UID (format: <name|uid>[:<group|gid>])\r\n    --workdir string                     Working directory inside the container\r\n    --entrypoint string                  Overwrite the default ENTRYPOINT of the image\r\n    --cpus decimal                       Number of CPUs\r\n    --memory string                      Memory limit\r\n    --memory-swap string                 Swap limit equal to memory plus swap: '-1' to enable unlimited swap\r\n    --memory-reservation string          Memory soft limit\r\n    --cpu-shares int                     CPU shares (relative weight)\r\n    --cpu-period int                     Limit CPU CFS (Completely Fair Scheduler) period\r\n    --cpu-quota int                      Limit CPU CFS (Completely Fair Scheduler) quota\r\n    --cpuset-cpus string                 CPUs in which to allow execution (0-3, 0,1)\r\n    --cpuset-mems string                 MEMs in which to allow execution (0-3, 0,1)\r\n  ```\r\n\r\nThese basic commands form the foundation of working with Docker, providing you with essential information and help to get started.\r\n\r\n## Section 2: Working with Docker Images\r\n\r\n### 2.1 Pulling Images\r\n\r\nDocker images are the blueprints for containers, and pulling images from a registry is often the first step in using Docker. Here are the key commands for pulling images:\r\n\r\n- `docker pull <image>`: This command downloads an image from a Docker registry (such as Docker Hub). If no tag is specified, Docker defaults to pulling the `latest` tag.\r\n\r\n  ```sh\r\n  $ docker pull ubuntu\r\n  Using default tag: latest\r\n  latest: Pulling from library/ubuntu\r\n  6d28e14ab8c8: Pull complete\r\n  1c9383292a8f: Pull complete\r\n  6c0b1f2a1dcd: Pull complete\r\n  Digest: sha256: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\r\n  ```\r\n\r\n- `docker pull <image>:<tag>`: This command downloads a specific version of an image from a Docker registry. Tags are used to specify different versions of an image.\r\n\r\n  ```sh\r\n  $ docker pull nginx:alpine\r\n  alpine: Pulling from library/nginx\r\n  6d28e14ab8c8: Pull complete\r\n  1c9383292a8f: Pull complete\r\n  6c0b1f2a1dcd: Pull complete\r\n  Digest: sha256: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\r\n  ```\r\n\r\n### 2.2 Listing Images\r\n\r\nOnce you have pulled images, you may want to list them to see what is available on your local system. Docker provides commands to list all images:\r\n\r\n- `docker images`: This command lists all images on the local system, including their repository, tag, image ID, creation date, and size.\r\n\r\n  ```sh\r\n  $ docker images\r\n  REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\r\n  ubuntu              latest              4e9f2cdfd5d6        2 weeks ago         72.9MB\r\n  nginx               alpine              6d28e14ab8c8        3 weeks ago         22.3MB\r\n  ```\r\n\r\n### 2.3 Removing Images\r\n\r\nManaging disk space is crucial, especially when dealing with multiple images. Docker provides commands to remove images that are no longer needed:\r\n\r\n- `docker rmi <image>`: This command removes one or more images by their image ID or repository name and tag. You can specify multiple images to remove by separating them with spaces.\r\n\r\n  ```sh\r\n  $ docker rmi nginx:alpine\r\n  Untagged: nginx:alpine\r\n  Deleted: sha256:6d28e14ab8c8\r\n  ```\r\n\r\n- `docker rmi $(docker images -q)`: This command removes all images on the local system. The `-q` flag with `docker images` outputs only the image IDs, which are then passed to `docker rmi` for removal.\r\n\r\n  ```sh\r\n  $ docker rmi $(docker images -q)\r\n  Untagged: ubuntu:latest\r\n  Deleted: sha256:4e9f2cdfd5d6\r\n  ```\r\n\r\n### 2.4 Building Images\r\n\r\nBuilding custom images from a Dockerfile is a common practice in Docker. Here are the commands to build images:\r\n\r\n- `docker build -t <image>:<tag> <path>`: This command builds an image from a Dockerfile located at the specified path. The `-t` flag tags the image with a name and optional tag.\r\n\r\n  ```sh\r\n  $ docker build -t myapp:1.0 .\r\n  Sending build context to Docker daemon  2.048kB\r\n  Step 1/3 : FROM ubuntu\r\n   ---> 4e9f2cdfd5d6\r\n  Step 2/3 : COPY . /app\r\n   ---> Using cache\r\n   ---> 1c9383292a8f\r\n  Step 3/3 : CMD [\"python\", \"/app/app.py\"]\r\n   ---> Using cache\r\n   ---> 6c0b1f2a1dcd\r\n  Successfully built 6c0b1f2a1dcd\r\n  Successfully tagged myapp:1.0\r\n  ```\r\n\r\n- `docker build --no-cache -t <image>:<tag> <path>`: This command builds an image without using the cache. It's useful when you want to ensure that all steps in the Dockerfile are executed fresh.\r\n\r\n  ```sh\r\n  $ docker build --no-cache -t myapp:1.0 .\r\n  Sending build context to Docker daemon  2.048kB\r\n  Step 1/3 : FROM ubuntu\r\n   ---> 4e9f2cdfd5d6\r\n  Step 2/3 : COPY . /app\r\n   ---> 1c9383292a8f\r\n  Step 3/3 : CMD [\"python\", \"/app/app.py\"]\r\n   ---> 6c0b1f2a1dcd\r\n  Successfully built 6c0b1f2a1dcd\r\n  Successfully tagged myapp:1.0\r\n  ```\r\n\r\nThese commands cover the essential operations for working with Docker images, from pulling and listing to removing and building them.\r\n\r\n## Section 3: Managing Docker Containers\r\n\r\n### 3.1 Running Containers\r\n\r\nRunning containers is one of the core functionalities of Docker. Here are the key commands for running containers:\r\n\r\n- `docker run <image>`: This command runs a container from a specified image. If the image is not available locally, Docker will pull it from the registry.\r\n\r\n  ```sh\r\n  $ docker run ubuntu\r\n  ```\r\n\r\n- `docker run -d <image>`: This command runs a container in detached mode, meaning it runs in the background.\r\n\r\n  ```sh\r\n  $ docker run -d nginx\r\n  5d8c5c4b5b6a\r\n  ```\r\n\r\n- `docker run -it <image>`: This command runs a container in interactive mode with a terminal attached. It's useful for debugging or running commands inside the container.\r\n\r\n  ```sh\r\n  $ docker run -it ubuntu\r\n  root@5d8c5c4b5b6a:/#\r\n  ```\r\n\r\n### 3.2 Listing Containers\r\n\r\nTo manage running and stopped containers, Docker provides commands to list them:\r\n\r\n- `docker ps`: This command lists all running containers, displaying their container ID, image, command, creation time, status, ports, and names.\r\n\r\n  ```sh\r\n  $ docker ps\r\n  CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES\r\n  5d8c5c4b5b6a   nginx     \"nginx -g 'daemon of...\"   2 minutes ago   Up 2 minutes   80/tcp    amazing_bell\r\n  ```\r\n\r\n- `docker ps -a`: This command lists all containers, including stopped ones. It provides a comprehensive view of all containers on the system.\r\n\r\n  ```sh\r\n  $ docker ps -a\r\n  CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS                     PORTS     NAMES\r\n  5d8c5c4b5b6a   nginx     \"nginx -g 'daemon of...\"   2 minutes ago   Up 2 minutes               80/tcp    amazing_bell\r\n  1c9383292a8f   ubuntu    \"/bin/bash\"              5 minutes ago   Exited (0) 3 minutes ago             compassionate_morse\r\n  ```\r\n\r\n### 3.3 Stopping and Starting Containers\r\n\r\nManaging the state of containers involves stopping, starting, and restarting them:\r\n\r\n- `docker stop <container>`: This command stops a running container. You can specify the container by its ID or name.\r\n\r\n  ```sh\r\n  $ docker stop amazing_bell\r\n  amazing_bell\r\n  ```\r\n\r\n- `docker start <container>`: This command starts a stopped container.\r\n\r\n  ```sh\r\n  $ docker start amazing_bell\r\n  amazing_bell\r\n  ```\r\n\r\n- `docker restart <container>`: This command restarts a running or stopped container.\r\n\r\n  ```sh\r\n  $ docker restart amazing_bell\r\n  amazing_bell\r\n  ```\r\n\r\n### 3.4 Removing Containers\r\n\r\nTo free up system resources, you can remove stopped containers:\r\n\r\n- `docker rm <container>`: This command removes a stopped container. You can specify the container by its ID or name.\r\n\r\n  ```sh\r\n  $ docker rm compassionate_morse\r\n  compassionate_morse\r\n  ```\r\n\r\n- `docker rm $(docker ps -a -q)`: This command removes all stopped containers. The `-q` flag with `docker ps -a` outputs only the container IDs, which are then passed to `docker rm` for removal.\r\n\r\n  ```sh\r\n  $ docker rm $(docker ps -a -q)\r\n  compassionate_morse\r\n  amazing_bell\r\n  ```\r\n\r\nThese commands cover the essential operations for managing Docker containers, from running and listing to stopping, starting, and removing them.\r\n\r\n## Section 4: Inspecting and Logging\r\n\r\n### 4.1 Inspecting Containers and Images\r\n\r\nInspecting containers and images allows you to retrieve detailed information about their configuration and state. Here are the key commands for inspection:\r\n\r\n- `docker inspect <container>`: This command provides detailed information about a container in JSON format. It includes details such as the container's configuration, state, network settings, and more.\r\n\r\n  ```sh\r\n  $ docker inspect amazing_bell\r\n  [\r\n      {\r\n          \"Id\": \"5d8c5c4b5b6a\",\r\n          \"Created\": \"2021-06-02T11:56:24.123456789Z\",\r\n          \"Path\": \"nginx\",\r\n          \"Args\": [\r\n              \"-g\",\r\n              \"daemon off;\"\r\n          ],\r\n          \"State\": {\r\n              \"Status\": \"running\",\r\n              \"Running\": true,\r\n              \"Paused\": false,\r\n              \"Restarting\": false,\r\n              \"OOMKilled\": false,\r\n              \"Dead\": false,\r\n              \"Pid\": 1234,\r\n              \"ExitCode\": 0,\r\n              \"Error\": \"\",\r\n              \"StartedAt\": \"2021-06-02T11:56:25.123456789Z\",\r\n              \"FinishedAt\": \"0001-01-01T00:00:00Z\"\r\n          },\r\n          \"Image\": \"sha256:6d28e14ab8c8\",\r\n          \"ResolvConfPath\": \"/var/lib/docker/containers/5d8c5c4b5b6a/resolv.conf\",\r\n          \"HostnamePath\": \"/var/lib/docker/containers/5d8c5c4b5b6a/hostname\",\r\n          \"HostsPath\": \"/var/lib/docker/containers/5d8c5c4b5b6a/hosts\",\r\n          \"LogPath\": \"/var/lib/docker/containers/5d8c5c4b5b6a/5d8c5c4b5b6a-json.log\",\r\n          \"Name\": \"/amazing_bell\",\r\n          \"RestartCount\": 0,\r\n          \"Driver\": \"overlay2\",\r\n          \"Platform\": \"linux\",\r\n          \"MountLabel\": \"\",\r\n          \"ProcessLabel\": \"\",\r\n          \"AppArmorProfile\": \"\",\r\n          \"ExecIDs\": null,\r\n          \"HostConfig\": {\r\n              \"Binds\": null,\r\n              \"ContainerIDFile\": \"\",\r\n              \"LogConfig\": {\r\n                  \"Type\": \"json-file\",\r\n                  \"Config\": {}\r\n              },\r\n              \"NetworkMode\": \"default\",\r\n              \"PortBindings\": {},\r\n              \"RestartPolicy\": {\r\n                  \"Name\": \"no\",\r\n                  \"MaximumRetryCount\": 0\r\n              },\r\n              \"AutoRemove\": false,\r\n              \"VolumeDriver\": \"\",\r\n              \"VolumesFrom\": null,\r\n              \"CapAdd\": null,\r\n              \"CapDrop\": null,\r\n              \"Dns\": [],\r\n              \"DnsOptions\": [],\r\n              \"DnsSearch\": [],\r\n              \"ExtraHosts\": null,\r\n              \"GroupAdd\": null,\r\n              \"IpcMode\": \"private\",\r\n              \"Cgroup\": \"\",\r\n              \"Links\": null,\r\n              \"OomScoreAdj\": 0,\r\n              \"PidMode\": \"\",\r\n              \"Privileged\": false,\r\n              \"PublishAllPorts\": false,\r\n              \"ReadonlyRootfs\": false,\r\n              \"SecurityOpt\": null,\r\n              \"UTSMode\": \"\",\r\n              \"UsernsMode\": \"\",\r\n              \"ShmSize\": 67108864,\r\n              \"Runtime\": \"runc\",\r\n              \"ConsoleSize\": [\r\n                  0,\r\n                  0\r\n              ],\r\n              \"Isolation\": \"\",\r\n              \"CpuShares\": 0,\r\n              \"Memory\": 0,\r\n              \"NanoCpus\": 0,\r\n              \"CgroupParent\": \"\",\r\n              \"BlkioWeight\": 0,\r\n              \"BlkioWeightDevice\": null,\r\n              \"BlkioDeviceReadBps\": null,\r\n              \"BlkioDeviceWriteBps\": null,\r\n              \"BlkioDeviceReadIOps\": null,\r\n              \"BlkioDeviceWriteIOps\": null,\r\n              \"CpuPeriod\": 0,\r\n              \"CpuQuota\": 0,\r\n              \"CpuRealtimePeriod\": 0,\r\n              \"CpuRealtimeRuntime\": 0,\r\n              \"CpusetCpus\": \"\",\r\n              \"CpusetMems\": \"\",\r\n              \"Devices\": [],\r\n              \"DeviceCgroupRules\": null,\r\n              \"DeviceRequests\": null,\r\n              \"KernelMemory\": 0,\r\n              \"KernelMemoryTCP\": 0,\r\n              \"MemoryReservation\": 0,\r\n              \"MemorySwap\": 0,\r\n              \"MemorySwappiness\": null,\r\n              \"OomKillDisable\": false,\r\n              \"PidsLimit\": null,\r\n              \"Ulimits\": null,\r\n              \"CpuCount\": 0,\r\n              \"CpuPercent\": 0,\r\n              \"IOMaximumIOps\": 0,\r\n              \"IOMaximumBandwidth\": 0,\r\n              \"MaskedPaths\": [\r\n                  \"/proc/asound\",\r\n                  \"/proc/acpi\",\r\n                  \"/proc/kcore\",\r\n                  \"/proc/keys\",\r\n                  \"/proc/latency_stats\",\r\n                  \"/proc/timer_list\",\r\n                  \"/proc/timer_stats\",\r\n                  \"/proc/sched_debug\",\r\n                  \"/proc/scsi\",\r\n                  \"/sys/firmware\"\r\n              ],\r\n              \"ReadonlyPaths\": [\r\n                  \"/proc/bus\",\r\n                  \"/proc/fs\",\r\n                  \"/proc/irq\",\r\n                  \"/proc/sys\",\r\n                  \"/proc/sysrq-trigger\"\r\n              ]\r\n          },\r\n          \"GraphDriver\": {\r\n              \"Data\": {\r\n                  \"LowerDir\": \"/var/lib/docker/overlay2/6d28e14ab8c8/lower\",\r\n                  \"MergedDir\": \"/var/lib/docker/overlay2/6d28e14ab8c8/merged\",\r\n                  \"UpperDir\": \"/var/lib/docker/overlay2/6d28e14ab8c8/upper\",\r\n                  \"WorkDir\": \"/var/lib/docker/overlay2/6d28e14ab8c8/work\"\r\n              },\r\n              \"Name\": \"overlay2\"\r\n          },\r\n          \"Mounts\": [],\r\n          \"Config\": {\r\n              \"Hostname\": \"5d8c5c4b5b6a\",\r\n              \"Domainname\": \"\",\r\n              \"User\": \"\",\r\n              \"AttachStdin\": false,\r\n              \"AttachStdout\": false,\r\n              \"AttachStderr\": false,\r\n              \"ExposedPorts\": {\r\n                  \"80/tcp\": {}\r\n              },\r\n              \"Tty\": false,\r\n              \"OpenStdin\": false,\r\n              \"StdinOnce\": false,\r\n              \"Env\": [\r\n                  \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\r\n              ],\r\n              \"Cmd\": [\r\n                  \"nginx\",\r\n                  \"-g\",\r\n                  \"daemon off;\"\r\n              ],\r\n              \"Image\": \"nginx\",\r\n              \"Volumes\": null,\r\n              \"WorkingDir\": \"\",\r\n              \"Entrypoint\": null,\r\n              \"OnBuild\": null,\r\n              \"Labels\": {},\r\n              \"StopSignal\": \"SIGTERM\",\r\n              \"StopTimeout\": null,\r\n              \"Shell\": null\r\n          },\r\n          \"NetworkSettings\": {\r\n              \"Bridge\": \"\",\r\n              \"SandboxID\": \"6d28e14ab8c8\",\r\n              \"HairpinMode\": false,\r\n              \"LinkLocalIPv6Address\": \"\",\r\n              \"LinkLocalIPv6PrefixLen\": 0,\r\n              \"Ports\": {\r\n                  \"80/tcp\": [\r\n                      {\r\n                          \"HostIp\": \"0.0.0.0\",\r\n                          \"HostPort\": \"32768\"\r\n                      }\r\n                  ]\r\n              },\r\n              \"SandboxKey\": \"/var/run/docker/netns/6d28e14ab8c8\",\r\n              \"SecondaryIPAddresses\": null,\r\n              \"SecondaryIPv6Addresses\": null,\r\n              \"EndpointID\": \"6d28e14ab8c8\",\r\n              \"Gateway\": \"172.17.0.1\",\r\n              \"GlobalIPv6Address\": \"\",\r\n              \"GlobalIPv6PrefixLen\": 0,\r\n              \"IPAddress\": \"172.17.0.2\",\r\n              \"IPPrefixLen\": 16,\r\n              \"IPv6Gateway\": \"\",\r\n              \"MacAddress\": \"02:42:ac:11:00:02\",\r\n              \"Networks\": {\r\n                  \"bridge\": {\r\n                      \"IPAMConfig\": null,\r\n                      \"Links\": null,\r\n                      \"Aliases\": null,\r\n                      \"NetworkID\": \"6d28e14ab8c8\",\r\n                      \"EndpointID\": \"6d28e14ab8c8\",\r\n                      \"Gateway\": \"172.17.0.1\",\r\n                      \"IPAddress\": \"172.17.0.2\",\r\n                      \"IPPrefixLen\": 16,\r\n                      \"IPv6Gateway\": \"\",\r\n                      \"GlobalIPv6Address\": \"\",\r\n                      \"GlobalIPv6PrefixLen\": 0,\r\n                      \"MacAddress\": \"02:42:ac:11:00:02\",\r\n                      \"DriverOpts\": null\r\n                  }\r\n              }\r\n          }\r\n      }\r\n  ]\r\n  ```\r\n\r\n- `docker inspect <image>`: This command provides detailed information about an image in JSON format. It includes details such as the image's configuration, layers, and more.\r\n\r\n  ```sh\r\n  $ docker inspect nginx:alpine\r\n  [\r\n      {\r\n          \"Id\": \"sha256:6d28e14ab8c8\",\r\n          \"RepoTags\": [\r\n              \"nginx:alpine\"\r\n          ],\r\n          \"RepoDigests\": [\r\n              \"nginx@sha256:4e9f2cdfd5d6b3e5d5e5c5e5d5e5e5d5e5d5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e5e\r\n  ```\r\n\r\n### 4.2 Viewing Logs\r\n\r\nLogs are crucial for debugging and monitoring the behavior of your containers. Docker provides commands to view the logs of a container:\r\n\r\n- `docker logs <container>`: This command fetches the logs of a container. You can specify the container by its ID or name. It displays all the logs generated by the container since it started.\r\n\r\n  ```sh\r\n  $ docker logs amazing_bell\r\n  2021/06/02 11:56:25 [notice] 1#1: using the \"epoll\" event method\r\n  2021/06/02 11:56:25 [notice] 1#1: nginx/1.19.10\r\n  2021/06/02 11:56:25 [notice] 1#1: built by gcc 9.3.0 (Alpine 9.3.0)\r\n  2021/06/02 11:56:25 [notice] 1#1: OS: Linux 5.8.0-53-generic\r\n  2021/06/02 11:56:25 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker processes\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker process 30\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker process 31\r\n  ```\r\n\r\n- `docker logs -f <container>`: This command follows the logs of a container in real-time. The `-f` flag stands for \"follow,\" similar to the `tail -f` command in Unix-like systems. It continues to display new log entries as they are generated.\r\n\r\n  ```sh\r\n  $ docker logs -f amazing_bell\r\n  2021/06/02 11:56:25 [notice] 1#1: using the \"epoll\" event method\r\n  2021/06/02 11:56:25 [notice] 1#1: nginx/1.19.10\r\n  2021/06/02 11:56:25 [notice] 1#1: built by gcc 9.3.0 (Alpine 9.3.0)\r\n  2021/06/02 11:56:25 [notice] 1#1: OS: Linux 5.8.0-53-generic\r\n  2021/06/02 11:56:25 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker processes\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker process 30\r\n  2021/06/02 11:56:25 [notice] 1#1: start worker process 31\r\n  ```\r\n\r\nThese commands provide essential tools for inspecting and logging, enabling you to retrieve detailed information and monitor the behavior of your containers and images.\r\n\r\n## Section 5: Networking\r\n\r\n### 5.1 Managing Networks\r\n\r\nDocker networking is a crucial aspect of container management, enabling communication between containers and external systems. Here are the key commands for managing Docker networks:\r\n\r\n- `docker network ls`: This command lists all Docker networks on the local system, including their names, IDs, drivers, and scope.\r\n\r\n  ```sh\r\n  $ docker network ls\r\n  NETWORK ID     NAME      DRIVER    SCOPE\r\n  6d28e14ab8c8   bridge    bridge    local\r\n  1c9383292a8f   host      host      local\r\n  6c0b1f2a1dcd   none      null      local\r\n  ```\r\n\r\n- `docker network create <network>`: This command creates a new Docker network with the specified name. By default, it uses the bridge driver, but you can specify other drivers such as overlay.\r\n\r\n  ```sh\r\n  $ docker network create my_network\r\n  6d28e14ab8c8\r\n  ```\r\n\r\n- `docker network rm <network>`: This command removes a Docker network by its name or ID. You can only remove networks that are not in use by any containers.\r\n\r\n  ```sh\r\n  $ docker network rm my_network\r\n  my_network\r\n  ```\r\n\r\n### 5.2 Connecting and Disconnecting Containers\r\n\r\nManaging the connections between containers and networks is essential for ensuring proper communication and isolation. Here are the commands to connect and disconnect containers from networks:\r\n\r\n- `docker network connect <network> <container>`: This command connects a container to a specified network. You can specify the container by its ID or name.\r\n\r\n  ```sh\r\n  $ docker network connect my_network amazing_bell\r\n  ```\r\n\r\n- `docker network disconnect <network> <container>`: This command disconnects a container from a specified network.\r\n\r\n  ```sh\r\n  $ docker network disconnect my_network amazing_bell\r\n  ```\r\n\r\nThese commands provide the essential tools for managing Docker networks, enabling you to create, list, remove networks, and manage container connections.\r\n\r\n## Section 6: Volumes and Data Management\r\n\r\n### 6.1 Managing Volumes\r\n\r\nDocker volumes are used to persist data generated by and used by Docker containers. Here are the key commands for managing Docker volumes:\r\n\r\n- `docker volume ls`: This command lists all Docker volumes on the local system, including their names and driver information.\r\n\r\n  ```sh\r\n  $ docker volume ls\r\n  DRIVER    VOLUME NAME\r\n  local     my_volume\r\n  local     another_volume\r\n  ```\r\n\r\n- `docker volume create <volume>`: This command creates a new Docker volume with the specified name.\r\n\r\n  ```sh\r\n  $ docker volume create my_volume\r\n  my_volume\r\n  ```\r\n\r\n- `docker volume rm <volume>`: This command removes a Docker volume by its name. You can only remove volumes that are not in use by any containers.\r\n\r\n  ```sh\r\n  $ docker volume rm my_volume\r\n  my_volume\r\n  ```\r\n\r\n### 6.2 Using Volumes\r\n\r\nMounting volumes to containers allows you to persist data and share it between containers. Here are the commands to use volumes with containers:\r\n\r\n- `docker run -v <volume>:/path/in/container <image>`: This command mounts a volume to a specified path inside the container. The `-v` flag is used to specify the volume and the mount path.\r\n\r\n  ```sh\r\n  $ docker run -v my_volume:/data ubuntu\r\n  ```\r\n\r\n- `docker run --mount source=<volume>,target=/path/in/container <image>`: This command is another way to mount a volume to a container using the `--mount` flag. It provides more options and flexibility compared to the `-v` flag.\r\n\r\n  ```sh\r\n  $ docker run --mount source=my_volume,target=/data ubuntu\r\n  ```\r\n\r\nThese commands provide the essential tools for managing Docker volumes, enabling you to create, list, remove volumes, and mount them to containers for data persistence and sharing.\r\n\r\n## Section 7: Docker Compose\r\n\r\n### 7.1 Basic Commands\r\n\r\nDocker Compose is a tool for defining and running multi-container Docker applications. Here are the key commands for working with Docker Compose:\r\n\r\n- `docker-compose up`: This command starts the services defined in a `docker-compose.yml` file. It creates and starts containers, networks, and volumes as specified in the file.\r\n\r\n  ```sh\r\n  $ docker-compose up\r\n  Creating network \"myapp_default\" with the default driver\r\n  Creating volume \"myapp_data\" with default driver\r\n  Creating myapp_db_1 ... done\r\n  Creating myapp_web_1 ... done\r\n  Attaching to myapp_db_1, myapp_web_1\r\n  db_1   | 2021-06-02 11:56:25.123456789 [Note] mysqld: ready for connections.\r\n  web_1  |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\r\n  ```\r\n\r\n- `docker-compose down`: This command stops and removes containers, networks, and volumes defined in a `docker-compose.yml` file. It cleans up the environment created by `docker-compose up`.\r\n\r\n  ```sh\r\n  $ docker-compose down\r\n  Stopping myapp_web_1 ... done\r\n  Stopping myapp_db_1  ... done\r\n  Removing myapp_web_1 ... done\r\n  Removing myapp_db_1  ... done\r\n  Removing network myapp_default\r\n  Removing volume myapp_data\r\n  ```\r\n\r\n### 7.2 Managing Services\r\n\r\nDocker Compose provides commands to manage the services defined in a `docker-compose.yml` file:\r\n\r\n- `docker-compose ps`: This command lists the containers managed by Docker Compose, including their names, states, and ports.\r\n\r\n  ```sh\r\n  $ docker-compose ps\r\n  Name                Command               State           Ports\r\n  ----------------------------------------------------------------------------\r\n  myapp_db_1          docker-entrypoint.sh   Up      3306/tcp\r\n  myapp_web_1         flask run              Up      0.0.0.0:5000->5000/tcp\r\n  ```\r\n\r\n- `docker-compose logs`: This command displays the logs of services managed by Docker Compose. It aggregates the logs from all containers defined in the `docker-compose.yml` file.\r\n\r\n  ```sh\r\n  $ docker-compose logs\r\n  db_1   | 2021-06-02 11:56:25.123456789 [Note] mysqld: ready for connections.\r\n  web_1  |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\r\n  ```\r\n\r\nThese commands provide the essential tools for working with Docker Compose, enabling you to start, stop, and manage multi-container applications efficiently.\r\n\r\n## Section 8: Advanced Commands\r\n\r\n### 8.1 Docker Exec\r\n\r\nExecuting commands inside a running container is often necessary for debugging or performing administrative tasks. Here are the key commands for using `docker exec`:\r\n\r\n- `docker exec -it <container> <command>`: This command runs a command in a running container. The `-it` flags are used to run the command in interactive mode with a terminal attached.\r\n\r\n  ```sh\r\n  $ docker exec -it amazing_bell /bin/bash\r\n  root@5d8c5c4b5b6a:/#\r\n  ```\r\n\r\n- `docker exec -u <user> -it <container> <command>`: This command runs a command as a specific user inside a running container. The `-u` flag specifies the user.\r\n\r\n  ```sh\r\n  $ docker exec -u www-data -it amazing_bell /bin/bash\r\n  www-data@5d8c5c4b5b6a:/$\r\n  ```\r\n\r\n### 8.2 Docker Export and Import\r\n\r\nExporting and importing containers allows you to save and restore container states. Here are the key commands for exporting and importing containers:\r\n\r\n- `docker export <container> > <file.tar>`: This command exports a container's filesystem as a tar archive. It does not include the container's metadata (such as environment variables and linked containers).\r\n\r\n  ```sh\r\n  $ docker export amazing_bell > amazing_bell.tar\r\n  ```\r\n\r\n- `docker import <file.tar>`: This command imports a tar archive as a Docker image. You can specify a repository name and tag for the imported image.\r\n\r\n  ```sh\r\n  $ docker import amazing_bell.tar myimage:latest\r\n  ```\r\n\r\n### 8.3 Docker Save and Load\r\n\r\nSaving and loading images allows you to transfer images between different Docker environments. Here are the key commands for saving and loading images:\r\n\r\n- `docker save -o <file.tar> <image>`: This command saves an image to a tar archive. It includes all layers and metadata of the image.\r\n\r\n  ```sh\r\n  $ docker save -o myimage.tar myimage:latest\r\n  ```\r\n\r\n- `docker load -i <file.tar>`: This command loads an image from a tar archive. It restores the image, including all layers and metadata.\r\n\r\n  ```sh\r\n  $ docker load -i myimage.tar\r\n  ```\r\n\r\nThese advanced commands provide powerful tools for executing commands inside containers, exporting and importing container states, and saving and loading images for transfer and backup.\r\n\r\nDocker commands are essential for effectively managing containers and images in your development and production environments. By mastering these commands, you can streamline your workflow, troubleshoot issues more efficiently, and take full advantage of Docker's capabilities. Remember that Docker is constantly evolving, so it's a good idea to regularly check the official Docker documentation for the most up-to-date information and best practices.\r\n\r\nFor a comprehensive list of Docker containers that can enhance your home server setup, check out our guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you make the most of your containerized environment.","src/content/posts/docker-commands.mdx",[769],"../../assets/images/24/07/docker-commands.jpeg","ff80549c52469ce3","docker-commands.mdx","docker-bypasses-firewall",{id:772,data:774,body:783,filePath:784,assetImports:785,digest:787,legacyId:788,deferredRender:32},{title:775,description:776,date:777,image:778,authors:779,categories:780,tags:781,canonical:782},"How to Fix Docker Bypassing Firewall: A Complete Guide","Learn how you can fix the docker security issue that is bypassing the firewall",["Date","2025-01-17T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/01/fix-docker-bypass-firewall.jpg",[19],[98],[100],"https://www.bitdoze.com/docker-bypasses-firewall/","I recently discovered something quite surprising while setting up Docker containers on my VPS - Docker can actually bypass your system's firewall rules! If you're [managing your own servers](https://www.bitdoze.com/secure-ssh-server-linux/) and running Docker containers, this is something you definitely need to know about.\r\n\r\n## Understanding the Problem: Why Docker Bypasses the Firewall\r\n\r\nHere's what I learned: When you set up Docker on your system, it creates its own networking setup that works a bit differently than you might expect. Let me break down what's actually happening behind the scenes.\r\n\r\nDocker's default behavior prioritizes ease of use for container networking, which involves automatically setting up these firewall rules. While this makes container networking more straightforward and user-friendly, it can inadvertently create security vulnerabilities if not managed properly. Docker needs this level of network control to efficiently handle inter-container communication and external access, but it's crucial to understand and manage these automatic configurations.\r\n\r\nBy default, Docker sets up something called a bridge network (usually named `docker0`). Think of it as Docker creating its own little private network inside your system. When you run a container with a port mapping like this:\r\n\r\n```yaml\r\nservices:\r\n  myapp:\r\n    image: nginx\r\n    ports:\r\n      - \"8080:80\"\r\n```\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\nYou might think you're just opening port 8080 on localhost, but what's actually happening is more complex. Docker is binding to `0.0.0.0:8080`, which means it's accessible from anywhere, not just your local machine. This is similar to the issue I encountered when [setting up containers for my home server](https://www.bitdoze.com/docker-containers-home-server/).\r\n\r\nThe real kicker is that Docker modifies your iptables (the Linux firewall) directly to make this work. It adds its own rules before your regular firewall rules, which means those carefully crafted firewall configurations you set up might not be doing what you expect for your Docker containers.\r\n\r\nHere's a quick way to see this in action. Try running:\r\n```bash\r\nsudo iptables -L -n -v | grep DOCKER\r\n```\r\n\r\nYou'll probably see several rules that Docker has added to your firewall configuration - these are what can bypass your regular firewall rules.\r\n\r\nThis behavior is by design - Docker needs to efficiently manage container networking and provide seamless communication between containers and the outside world. However, this convenience comes with security implications that need to be carefully considered and managed. In the following sections, I'll show you exactly how to fix this and secure your Docker containers properly while maintaining the functionality you need.\r\n\r\n\r\n## Solutions: Configuring the Firewall to Control Docker Traffic\r\n\r\nAfter discovering this security concern, I've found several effective ways to handle it. Let me share the most practical solutions I've implemented and tested.\r\n\r\n### 1. Using Localhost Binding (The Simplest Fix)\r\n\r\nThe easiest solution I've found is to explicitly bind containers to localhost. Instead of letting Docker bind to all interfaces (`0.0.0.0`), we can force it to only listen on `127.0.0.1`. Here's how to do it in your docker-compose.yml:\r\n\r\n```yaml\r\nservices:\r\n  myapp:\r\n    image: nginx\r\n    ports:\r\n      - \"127.0.0.1:8080:80\"\r\n```\r\n\r\nWhile this is the simplest solution, it does have limitations. If you need other services on the same host to access the container, or if you plan to expose the service directly without a reverse proxy, this method won't work. In such cases, you'll need to consider alternative approaches like those discussed below.\r\n\r\n### 2. Using a Cloud Firewall\r\n\r\nOne approach I've come to really appreciate is using a cloud firewall instead of relying solely on the VPS firewall. Cloud providers offer various firewall solutions:\r\n\r\n- AWS Security Groups: Configure inbound/outbound rules at the instance level\r\n- Google Cloud Firewall Rules: Network-level filtering with tag-based rules\r\n- DigitalOcean Cloud Firewalls: Similar to AWS security groups\r\n- Hetzner Cloud Firewall: Network-level filtering before traffic reaches your VPS\r\n\r\nThese firewalls operate at the network level, meaning they can't be bypassed by Docker's internal networking configuration.\r\n\r\nThe advantage here is that these firewalls can't be bypassed by Docker's networking tricks because they operate at a higher level. Here's what I typically do:\r\n\r\n1. Allow only necessary ports (usually 80, 443 for web traffic, and 22 for SSH)\r\n2. Block all other incoming traffic\r\n3. Configure specific rules for any additional services\r\n\r\n### 3. Using Traefik as a Reverse Proxy\r\n\r\nI've found that using [Traefik as a reverse proxy](https://www.bitdoze.com/traefik-proxy-docker/) is one of the most elegant solutions. Here's a basic setup:\r\n\r\n```yaml\r\nversion: '3'\r\n\r\nservices:\r\n  traefik:\r\n    image: traefik:v2.10\r\n    command:\r\n      - \"--providers.docker=true\"\r\n      - \"--api.dashboard=true\"\r\n      - \"--api.authentication.basic.users=admin:$$apr1$$xyz123$$\" # Use htpasswd to generate\r\n      - \"--certificatesresolvers.letsencrypt.acme.tlschallenge=true\"\r\n      - \"--certificatesresolvers.letsencrypt.acme.email=your@email.com\"\r\n    ports:\r\n      - \"127.0.0.1:80:80\"\r\n    volumes:\r\n      - /var/run/docker.sock:/var/run/docker.sock:ro\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.http.routers.dashboard.middlewares=auth@file\"\r\n\r\n  myapp:\r\n    image: nginx\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.http.routers.myapp.rule=Host(`your-domain.com`)\"\r\n    # Notice: no ports exposed directly!\r\n```\r\n\r\nWhen using Traefik, ensure you secure the proxy itself:\r\n- Enable HTTPS with automatic Let's Encrypt certificates\r\n- Add authentication for the Traefik dashboard\r\n- Use secure headers and middleware\r\n- Regularly update Traefik to the latest version\r\n\r\nThis setup is particularly secure because:\r\n- Only Traefik's ports are exposed, and they're bound to localhost\r\n- Other containers don't expose ports directly\r\n- All traffic is routed through Traefik\r\n- Built-in SSL/TLS support with automatic certificate management\r\n\r\n### 4. Advanced Firewall Configurations: Working with iptables and UFW\r\n\r\nAfter setting up the basic protections, I learned there are more sophisticated ways to control Docker's network access. Let me share how to handle this using different firewall tools.\r\n\r\n#### Working with iptables\r\n\r\nThe DOCKER-USER chain is special because Docker specifically avoids modifying rules within this chain, making it a safe place for administrators to add their custom firewall rules that will persist. This chain is processed before Docker's automatic rules, giving you precise control over incoming container traffic.\r\n\r\nHere's how I set it up:\r\n\r\n```bash\r\n# Allow established connections\r\nsudo iptables -A DOCKER-USER -i eth0 -j ACCEPT -m conntrack --ctstate ESTABLISHED,RELATED\r\n\r\n# Allow specific IPs (replace with your trusted IPs)\r\nsudo iptables -A DOCKER-USER -i eth0 -s 203.0.113.1 -j ACCEPT\r\n\r\n# Drop everything else\r\nsudo iptables -A DOCKER-USER -i eth0 -j DROP\r\n```\r\n\r\nTo make these rules permanent (survive reboots), I use:\r\n```bash\r\nsudo netfilter-persistent save\r\nsudo netfilter-persistent reload\r\n```\r\n\r\n#### Working with firewalld\r\n\r\n'firewalld' uses the concept of \"zones\" to represent different network environments and applies different rules based on the active zone. To effectively manage Docker traffic, you can create specific zones for your Docker interfaces and networks.\r\n\r\nHere's how you can configure firewalld to control traffic to Docker networks:\r\n\r\n1. Create a Zone for Docker:\r\n```sh\r\nsudo firewall-cmd --permanent --new-zone=docker\r\n```\r\n\r\n2. Bind Docker Interfaces:\r\n```sh\r\n# For default docker0 bridge\r\nsudo firewall-cmd --permanent --zone=docker --add-interface=docker0\r\n\r\n# For user-defined networks (replace br-xxx with your network interface)\r\nsudo firewall-cmd --permanent --zone=docker --add-interface=br-xxx\r\n```\r\n\r\n3. Define Rules within the Docker Zone:\r\n```sh\r\n# Allow specific ports\r\nsudo firewall-cmd --permanent --zone=docker --add-port=8080/tcp\r\n\r\n# Allow specific services\r\nsudo firewall-cmd --permanent --zone=docker --add-service=http\r\n```\r\n\r\n4. Apply and Verify:\r\n```sh\r\n# Reload firewalld\r\nsudo firewall-cmd --reload\r\n\r\n# Verify configuration\r\nsudo firewall-cmd --zone=docker --list-all\r\n```\r\n\r\nFor user-defined Docker networks, you'll need to:\r\n1. Identify the bridge interface name:\r\n```bash\r\ndocker network ls\r\ndocker network inspect network_name | grep \"Interface\"\r\n```\r\n\r\n2. Add the interface to your firewall zone:\r\n```bash\r\nsudo firewall-cmd --permanent --zone=docker --add-interface=br-xxxxx\r\n```\r\n\r\n#### Using UFW (Uncomplicated Firewall)\r\n\r\nSetting DEFAULT_FORWARD_POLICY to \"ACCEPT\" is necessary because Docker requires the ability to forward traffic through its bridge network. This setting allows containers to communicate with the outside world through the host's network interface. However, this makes it even more important to carefully configure your container-specific rules.\r\n\r\n1. First, edit `/etc/default/ufw`:\r\n```bash\r\nDEFAULT_FORWARD_POLICY=\"ACCEPT\"\r\n```\r\n\r\n2. Then, create Docker-specific rules in `/etc/ufw/after.rules`:\r\n```bash\r\n# NAT table rules\r\n*nat\r\n:POSTROUTING ACCEPT [0:0]\r\n\r\n# Forward traffic through eth0\r\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\r\n\r\nCOMMIT\r\n\r\n# Don't delete these required lines\r\n*filter\r\n:ufw-user-forward - [0:0]\r\n:ufw-docker-logging-deny - [0:0]\r\n:DOCKER-USER - [0:0]\r\n\r\n# Docker user chain\r\n-A DOCKER-USER -j RETURN -s 10.0.0.0/8\r\n-A DOCKER-USER -j RETURN -s 172.16.0.0/12\r\n-A DOCKER-USER -j RETURN -s 192.168.0.0/16\r\n\r\n-A DOCKER-USER -j ufw-user-forward\r\n\r\n-A DOCKER-USER -j DROP\r\n\r\nCOMMIT\r\n```\r\n\r\n3. Apply the changes:\r\n```bash\r\nsudo ufw reload\r\n```\r\n\r\n#### Using Docker Compose Networks\r\n\r\nOne thing that really helped me secure my containers was using custom networks in Docker Compose. Understanding the difference between internal and external networks is crucial for security:\r\n\r\n- Internal networks (`internal: true`):\r\n  - Completely isolated from external networks\r\n  - Containers can't make outbound connections\r\n  - Perfect for sensitive services like databases\r\n  - Only containers within the same internal network can communicate\r\n\r\n- External networks (`internal: false`):\r\n  - Allow containers to access the internet\r\n  - Can be accessed from outside the Docker host\r\n  - Suitable for public-facing services\r\n  - Need careful firewall configuration\r\n\r\nHere's an example of how I organize my containers with network isolation:\r\n\r\n```yaml\r\nversion: '3'\r\n\r\nnetworks:\r\n  frontend:\r\n    internal: false  # Allows external access\r\n  backend:\r\n    internal: true   # Completely isolated from external access\r\n\r\nservices:\r\n  web:\r\n    image: nginx\r\n    networks:\r\n      - frontend\r\n    ports:\r\n      - \"127.0.0.1:8080:80\"\r\n    security_opt:      # Advanced security options\r\n      - no-new-privileges:true\r\n\r\n  api:\r\n    image: node\r\n    networks:\r\n      - frontend\r\n      - backend\r\n    depends_on:\r\n      - database\r\n\r\n  database:\r\n    image: mysql\r\n    networks:\r\n      - backend    # Only connected to internal network\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password\r\n    secrets:\r\n      - db_root_password\r\n    security_opt:\r\n      - no-new-privileges:true\r\n\r\nsecrets:\r\n  db_root_password:\r\n    file: ./secrets/db_password.txt\r\n```\r\n\r\nThis setup ensures that:\r\n- The database is completely isolated from external access\r\n- The API service can communicate with both web and database\r\n- All external access must go through localhost\r\n- Additional security measures like secrets and security options are implemented\r\n\r\nFor more complex setups, you might want to implement additional security measures:\r\n\r\n1. Network Segmentation:\r\n```yaml\r\nnetworks:\r\n  frontend:\r\n    internal: false\r\n    ipam:\r\n      config:\r\n        - subnet: 172.20.0.0/24\r\n  backend:\r\n    internal: true\r\n    ipam:\r\n      config:\r\n        - subnet: 172.20.1.0/24\r\n```\r\n\r\n2. Network Encryption (for swarm mode):\r\n```yaml\r\nnetworks:\r\n  backend:\r\n    driver: overlay\r\n    driver_opts:\r\n      encrypted: \"true\"\r\n```\r\n\r\n3. Rate Limiting with Traefik:\r\n```yaml\r\nservices:\r\n  web:\r\n    labels:\r\n      - \"traefik.http.middlewares.ratelimit.ratelimit.average=100\"\r\n      - \"traefik.http.middlewares.ratelimit.ratelimit.burst=50\"\r\n```\r\n\r\n4. Container Resource Limits:\r\n```yaml\r\nservices:\r\n  web:\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: '0.50'\r\n          memory: 512M\r\n        reservations:\r\n          cpus: '0.25'\r\n          memory: 256M\r\n```\r\n\r\nWhen managing these networks, it's important to regularly audit their configuration:\r\n\r\n```bash\r\n# List all networks\r\ndocker network ls\r\n\r\n# Inspect network configuration\r\ndocker network inspect frontend\r\n\r\n# Check container connectivity\r\ndocker network connect test-network container-name\r\n\r\n# Monitor network usage\r\ndocker stats --format \"table {{.Name}}\\t{{.NetIO}}\"\r\n```\r\n\r\nRemember to periodically clean up unused networks:\r\n\r\n```bash\r\n# Remove unused networks\r\ndocker network prune\r\n\r\n# Remove specific network\r\ndocker network rm network-name\r\n```\r\n\r\nWhen managing multiple containers, I also find it helpful to use [Docker Compose secrets](https://www.bitdoze.com/docker-compose-secrets/) for additional security.\r\n\r\n\r\n## Conclusion: Keeping Your Docker Containers Secure\r\n\r\nAfter spending considerable time learning about Docker's networking quirks and implementing various security measures, I've realized that securing Docker containers isn't as straightforward as it might seem at first. However, it's definitely manageable once you understand what's happening under the hood.\r\n\r\nHere's what I've learned to be the most important takeaways:\r\n\r\n1. **Always Start with the Basics**\r\n   - Bind containers to localhost when possible\r\n   - Use reverse proxies like [Traefik](https://www.bitdoze.com/traefik-proxy-docker/) for external access\r\n   - Regularly [clean up unused Docker resources](https://www.bitdoze.com/clean-docker-overlay2-dir/) to maintain a clean, secure environment\r\n\r\n2. **Layer Your Security**\r\n   - Don't rely on just one security measure\r\n   - Combine host firewall rules with Docker network isolation\r\n   - Consider using cloud firewalls as an additional security layer\r\n\r\n3. **Best Practices I Now Follow**\r\n   - Regularly audit exposed ports using `docker ps`\r\n   - Keep Docker and containers updated\r\n   - Use the principle of least privilege when [managing Docker users](https://www.bitdoze.com/add-users-to-docker-container/)\r\n   - Document all [Docker commands](https://www.bitdoze.com/docker-commands/) and configurations for easy maintenance\r\n\r\nRemember, security is an ongoing process. I recommend regularly reviewing your Docker networking setup and firewall rules to ensure they're still appropriate for your needs.\r\n\r\nA quick checklist I use for any new Docker deployment:\r\n```bash\r\n# Check exposed ports\r\ndocker ps\r\n\r\n# Review network configurations\r\ndocker network ls\r\n\r\n# Verify firewall rules\r\nsudo iptables -L -n -v | grep DOCKER\r\n\r\n# Check Docker daemon configuration\r\ndocker info\r\n```\r\n\r\nThe most important thing I've learned is that while Docker's default networking behavior might seem concerning at first, there are plenty of tools and techniques available to secure it properly. The key is understanding how these pieces fit together and implementing the right combination of security measures for your specific needs.\r\n\r\nIf you're just getting started with Docker, you might want to check out how to [copy multiple files efficiently in Dockerfiles](https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/) and other basic Docker concepts before diving deep into security configurations.\r\n\r\nBy following these guidelines and regularly reviewing your security setup, you can run Docker containers confidently while maintaining proper security controls. Remember, security isn't about implementing every possible measure, but about choosing the right combination of tools and practices that work for your specific situation.","src/content/posts/docker-bypasses-firewall.mdx",[786],"../../assets/images/25/01/fix-docker-bypass-firewall.jpg","431b41ff6aab112c","docker-bypasses-firewall.mdx","docker-containers-business",{id:789,data:791,body:800,filePath:801,assetImports:802,digest:804,legacyId:805,deferredRender:32},{title:792,description:793,date:794,image:795,authors:796,categories:797,tags:798,canonical:799},"Best 20+ Self-hosted Apps Docker Containers for A Business","Check out this list with 20+ self hosted apps with docker containers that you can use on your business to grow it.",["Date","2024-11-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/11/docker-containers-business.jpeg",[19],[98],[100],"https://www.bitdoze.com/docker-containers-business/","In today's digital landscape, businesses of all sizes are increasingly turning to self-hosted solutions to maintain control over their data, reduce costs, and customize their software environment. Docker containers have revolutionized the way applications are deployed and managed, offering a lightweight, portable, and efficient method for running software. This article explores the world of Docker containers for businesses, discussing where they can be hosted and highlighting 20+ essential containers that can significantly enhance your business operations.\r\n\r\n## Where Docker Containers For Business Can be Hosted\r\n\r\nWhen it comes to hosting self hosted apps on docker containers for your business, you have several options, each with its own advantages. Let's explore two popular choices: VPS servers and home servers.\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### VPS Server With Hetzner, DigitalOcean, etc.\r\n\r\nVirtual Private Servers (VPS) offer a robust and scalable solution for hosting Docker containers. Companies like Hetzner and DigitalOcean provide reliable VPS options with competitive pricing and excellent performance. Here are some key benefits of using a VPS for your Docker containers:\r\n\r\n- **Scalability**: Easily upgrade resources as your business grows\r\n- **High availability**: Benefit from enterprise-grade infrastructure and redundancy\r\n- **Managed services**: Many providers offer managed Kubernetes clusters for easier container orchestration\r\n- **Global reach**: Choose from data centers worldwide to reduce latency for your users\r\n- **Cost-effective**: Pay only for the resources you use, with flexible pricing plans\r\n\r\nWhen selecting a VPS provider, consider factors such as pricing, performance, data center locations, and support options. Both Hetzner and DigitalOcean offer user-friendly interfaces and extensive documentation to help you get started with Docker containers.\r\n\r\n### Home Server\r\n\r\nFor businesses with specific security requirements or those looking to maximize control over their infrastructure, a home server can be an excellent option for hosting Docker containers. There are two main approaches to setting up a home server:\r\n\r\n#### Mini PC\r\n\r\nMini PCs have gained popularity as home servers due to their compact size, energy efficiency, and sufficient power for running Docker containers. Some advantages of using a mini PC as a home server include:\r\n\r\n- **Low power consumption**: Ideal for 24/7 operation without significant energy costs\r\n- **Quiet operation**: Many models are fanless or have low-noise cooling systems\r\n- **Customizable**: Choose from a variety of models to fit your specific needs\r\n- **Cost-effective**: Often more affordable than traditional server hardware\r\n\r\nFor more information on selecting the best mini PC for your home server, check out our comprehensive guide on [the best mini PCs for home servers](https://www.bitdoze.com/best-mini-pc-home-server/).\r\n\r\n#### NAS\r\n\r\nNetwork Attached Storage (NAS) devices are another popular option for home servers, offering a balance of storage capacity and processing power. Many modern NAS systems support Docker containers, making them versatile solutions for small businesses. Benefits of using a NAS for Docker containers include:\r\n\r\n- **Built-in storage**: Large storage capacity with RAID support for data redundancy\r\n- **Easy management**: User-friendly interfaces for managing both storage and containers\r\n- **Power efficiency**: Designed for 24/7 operation with low power consumption\r\n- **Backup solutions**: Often include built-in backup software and cloud integration\r\n\r\nWhether you choose a VPS, mini PC, or NAS solution, hosting Docker containers on-premises or in the cloud can significantly enhance your business's IT infrastructure and capabilities.\r\n\r\n\r\n\r\n\r\n## Best 20+ Docker Containers for A Business\r\n\r\nDocker containers offer a wide range of applications that can streamline your business operations, improve collaboration, and enhance productivity. Let's explore some of the most useful Docker containers for businesses:\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n| Category | Application | Complexity | Resource Usage | Key Benefit |\r\n|----------|-------------|------------|----------------|-------------|\r\n| Collaboration | NextCloud | Medium | High | Complete office suite replacement |\r\n| Communication | Zulip | Easy | Medium | Organized team discussions |\r\n| Support | Zammad | Medium | Medium | Customer service automation |\r\n| Remote Work | Kasm Workspaces | High | High | Secure remote applications |\r\n| Monitoring | Uptime Kuma | Easy | Low | Service availability tracking |\r\n| Database | NocoDB/Baserow | Easy | Medium | No-code database solutions |\r\n| Finance | Firefly III | Easy | Low | Financial management |\r\n| Document Management | Paperless-ngx | Medium | Medium | Digital document organization |\r\n| Automation | n8n/Activepieces | Medium | Medium | Workflow automation |\r\n| Analytics | Plausible | Easy | Low | Privacy-focused analytics |\r\n| Marketing | Mautic/ListMonk | Medium | Medium | Marketing automation |\r\n| Container Management | Dockge/Portainer | Medium | Low | Docker administration |\r\n| AI Integration | Flowise AI | Medium | High | Custom AI solutions |\r\n| Version Control | Gitea | Easy | Low | Code repository management |\r\n| Knowledge Base | Docmost | Easy | Low | Team documentation |\r\n| Backup | Duplicati | Easy | Low | Data protection |\r\n| Business Management | ERPNext | High | High | Complete business solution |\r\n| Feedback | Formbricks | Easy | Low | User feedback collection |\r\n| Monitoring | beszel | Easy | Low | Resource monitoring |\r\n| Billing | Invoice Ninja | Easy | Low | Invoice management |\r\n| Time Tracking | Kimai | Easy | Low | Time management |\r\n\r\n### NextCloud\r\n\r\n[NextCloud](https://nextcloud.com/) is a powerful, open-source file sharing and collaboration platform that can serve as a central hub for your business data and communication.\r\n\r\n**Key features:**\r\n- File sharing and synchronization across devices\r\n- Collaborative document editing\r\n- Calendar and contact management\r\n- Video conferencing and chat\r\n- Task management and project planning\r\n\r\n**How it helps small businesses:**\r\nNextCloud provides a secure, self-hosted alternative to services like Dropbox or Google Drive, allowing businesses to maintain full control over their data while offering similar functionality. It can significantly improve team collaboration and file management, all within a customizable environment that adheres to your specific security and compliance requirements.\r\n\r\n### Zulip\r\n\r\n[Zulip](https://zulip.com/) is an open-source team chat application that combines the best features of real-time and asynchronous communication.\r\n\r\n**Key features:**\r\n- Topic-based threading for organized discussions\r\n- Powerful search functionality\r\n- Integrations with various tools and services\r\n- Mobile apps for iOS and Android\r\n- Customizable notifications\r\n\r\n**How it helps small businesses:**\r\nZulip's unique approach to team communication can help small businesses stay organized and focused, even as they grow. The topic-based threading system allows for more structured conversations, reducing information overload and making it easier to catch up on discussions. This can lead to improved productivity and better decision-making across the organization.\r\n\r\n### Zammad\r\n\r\n[Zammad](https://zammad.com/) is an open-source help desk and customer support system that can help businesses manage customer inquiries efficiently.\r\n\r\n**Key features:**\r\n- Multi-channel support (email, chat, social media)\r\n- Ticket management and automation\r\n- Knowledge base for self-service support\r\n- Customizable workflows and integrations\r\n- Reporting and analytics\r\n\r\n**How it helps small businesses:**\r\nBy centralizing customer support operations, Zammad can help small businesses provide better service with limited resources. The automation features can reduce response times, while the knowledge base can empower customers to find answers independently. This can lead to improved customer satisfaction and retention, which are crucial for small business growth.\r\n\r\n### Kasm Workspaces\r\n\r\n[Kasm Workspaces](https://www.kasmweb.com/) is a Docker container streaming platform that delivers browser-based access to desktops, applications, and web services.\r\n\r\n**Key features:**\r\n- Secure remote access to applications and desktops\r\n- Customizable workspaces for different user roles\r\n- Integration with existing authentication systems\r\n- Usage analytics and monitoring\r\n- Support for GPU-accelerated applications\r\n\r\n**How it helps small businesses:**\r\nKasm Workspaces can enable small businesses to implement secure remote work solutions without significant infrastructure investments. It allows employees to access necessary applications and resources from any device, improving flexibility and productivity. Additionally, it can help businesses maintain better control over sensitive data by keeping it within the containerized environment.\r\n\r\n### Uptime Kuma\r\n\r\n[Uptime Kuma](https://uptime.kuma.pet/) is a self-hosted monitoring tool that helps businesses keep track of their websites and services' availability.\r\n\r\n**Key features:**\r\n- Real-time monitoring of websites and services\r\n- Multiple notification channels (email, SMS, chat apps)\r\n- Status page generation\r\n- Supports various monitoring methods (HTTP, TCP, Ping, etc.)\r\n- User-friendly interface with customizable dashboard\r\n\r\n**How it helps small businesses:**\r\nFor small businesses, website and service uptime is crucial for maintaining customer trust and preventing revenue loss. Uptime Kuma provides an affordable, easy-to-use solution for monitoring critical infrastructure, allowing businesses to quickly identify and respond to issues before they impact customers. The status page feature can also improve transparency and communication during outages or maintenance periods.\r\n\r\n### NocoDB or Baserow\r\n\r\n[NocoDB](https://nocodb.com/) and [Baserow](https://baserow.io/) are open-source alternatives to Airtable, providing flexible database and spreadsheet functionality. You can also see [best aitable self hosted alternatives](https://www.bitdoze.com/self-hosted-airtable-alternatives/) for a more in detail list.\r\n\r\n**Key features:**\r\n- Spreadsheet-like interface for database management\r\n- Views: Grid, Gallery, Kanban, Form, and more\r\n- API access for integration with other tools\r\n- User roles and permissions\r\n- Automation and workflow capabilities\r\n\r\n**How it helps small businesses:**\r\nThese tools can help small businesses organize and manage their data without the need for complex database systems or expensive software licenses. They offer the flexibility to create custom applications for various business needs, such as inventory management, project tracking, or customer relationship management. The familiar spreadsheet-like interface makes it easy for non-technical users to work with data, improving overall productivity.\r\n\r\n### Firefly III\r\n\r\n[Firefly III](https://www.firefly-iii.org/) is a personal finance manager that can be adapted for small business use, helping to track income, expenses, and budgets.\r\n\r\n**Key features:**\r\n- Multi-currency support\r\n- Budgeting and financial goal setting\r\n- Bill management and recurring transactions\r\n- Detailed reports and charts\r\n- Import data from various sources\r\n\r\n**How it helps small businesses:**\r\nFor small businesses, especially sole proprietorships or partnerships, Firefly III can provide a cost-effective solution for managing finances. It offers detailed insights into cash flow, helping businesses make informed financial decisions. The ability to set budgets and track expenses can be particularly useful for startups and small businesses looking to optimize their spending and growth.\r\n\r\n### Paperless-ngx\r\n\r\n[Paperless-ngx](https://docs.paperless-ngx.com/) is a document management system that helps businesses go paperless by digitizing and organizing documents.\r\n\r\n**Key features:**\r\n- OCR (Optical Character Recognition) for searchable PDFs\r\n- Automatic tagging and categorization of documents\r\n- Full-text search capabilities\r\n- Mobile-friendly web interface\r\n- Integration with scanners and email\r\n\r\n**How it helps small businesses:**\r\nSmall businesses can significantly reduce physical storage needs and improve document retrieval times with Paperless-ngx. The OCR and tagging features make it easy to find specific information within large document collections, which can be particularly useful for compliance and auditing purposes. By digitizing documents, businesses can also improve collaboration and enable remote access to important files.\r\n\r\n### n8n or Activepieces\r\n\r\n[n8n](https://n8n.io/) and [Activepieces](https://www.activepieces.com/) are workflow automation tools that allow businesses to connect various applications and automate repetitive tasks.\r\n\r\n**Key features:**\r\n- Visual workflow builder\r\n- Wide range of integrations with popular services\r\n- Ability to create custom nodes/actions\r\n- Scheduling and trigger-based automation\r\n- Self-hosted for data privacy\r\n\r\n**How it helps small businesses:**\r\nAutomation can be a game-changer for small businesses, allowing them to do more with limited resources. These tools can help automate various processes, such as lead generation, data synchronization between systems, social media management, and customer follow-ups. By reducing manual work, businesses can focus on more strategic activities and improve overall efficiency.\r\n\r\n### Plausible\r\n\r\n[Plausible](https://plausible.io/) is a lightweight, open-source website analytics platform that prioritizes user privacy.\r\n\r\n**Key features:**\r\n- Simple, intuitive dashboard\r\n- GDPR compliant and cookie-free\r\n- Lightweight script for minimal impact on site performance\r\n- Custom event tracking\r\n- Email reports and API access\r\n\r\n**How it helps small businesses:**\r\nFor small businesses concerned about user privacy or looking for a simpler alternative to Google Analytics, Plausible offers a compelling solution. Its lightweight nature ensures that it won't slow down your website, which is crucial for maintaining good search engine rankings and user experience. The straightforward dashboard provides key metrics without overwhelming users with data, making it easier for small business owners to make informed decisions about their online presence.\r\n\r\n### Mautic or ListMonk\r\n\r\n[Mautic](https://www.mautic.org/) and [ListMonk](https://listmonk.app/) are open-source marketing automation and email marketing platforms that can help businesses manage their marketing campaigns.\r\n\r\n**Key features:**\r\n- Email campaign management\r\n- Landing page and form builders\r\n- Lead scoring and segmentation\r\n- Marketing automation workflows\r\n- Integration with CRM systems\r\n\r\n**How it helps small businesses:**\r\nThese tools provide small businesses with enterprise-level marketing capabilities at a fraction of the cost. They allow for sophisticated email marketing campaigns, lead nurturing, and customer segmentation, which can significantly improve marketing effectiveness. The ability to create targeted campaigns based on user behavior can help small businesses compete more effectively with larger competitors.\r\n\r\n### Dockge, Portainer, or Dockploy\r\n\r\n[Dockge](https://dockge.kuma.pet/), [Portainer](https://www.portainer.io/), or [Dockploy](https://dokploy.com/) tools are Docker management platforms that simplify the process of deploying and managing Docker containers.\r\n\r\n**Key features:**\r\n- User-friendly web interface for container management\r\n- Container templating and stack deployment\r\n- Resource monitoring and logging\r\n- Role-based access control\r\n- Support for Docker Swarm or Kubernetes (varies by tool)\r\n\r\n**How it helps small businesses:**\r\nFor small businesses adopting Docker, these management tools can significantly reduce the complexity of container operations. They provide an intuitive interface for deploying and managing containers, which can help businesses without dedicated IT staff to leverage containerization effectively. This can lead to improved application deployment processes, better resource utilization, and easier scaling of services as the business grows.\r\n\r\nFor more information on installing and using these tools, check out our guides on [Dockge installation](https://www.bitdoze.com/dockge-install/) and [Dockploy installation](https://www.bitdoze.com/dokploy-install/).\r\n\r\n### Flowise AI\r\n\r\n[Flowise AI](https://flowiseai.com/) is an open-source tool for building customized AI agents and chatbots using a visual interface.\r\n\r\n**Key features:**\r\n- Drag-and-drop interface for creating AI workflows\r\n- Integration with various AI models and APIs\r\n- Customizable chatbot interfaces\r\n- API endpoints for integration with other applications\r\n- Support for multiple languages\r\n\r\n**How it helps small businesses:**\r\nFlowise AI enables small businesses to leverage AI technologies without the need for extensive programming knowledge. This can be particularly useful for creating customer service chatbots, automating repetitive tasks, or developing AI-powered features for products or services. By providing an accessible way to work with AI, Flowise AI can help small businesses innovate and improve their offerings.\r\n\r\nFor a detailed guide on setting up Flowise AI, visit our [Flowise AI installation tutorial](https://www.bitdoze.com/flowiseai-install/).\r\n\r\n### Gitea\r\n\r\n[Gitea](https://about.gitea.com/) is a lightweight, self-hosted Git service that provides version control and collaboration features similar to GitHub or GitLab.\r\n\r\n**Key features:**\r\n- Git repository management\r\n- Issue tracking and project management\r\n- Pull request and code review functionality\r\n- Wiki for documentation\r\n- Integration with CI/CD tools\r\n\r\n**How it helps small businesses:**\r\nFor small businesses involved in software development or those managing code-based projects, Gitea offers a cost-effective alternative to hosted Git services. It allows teams to keep their code and intellectual property on-premises while still benefiting from modern version control and collaboration features. This can be particularly important for businesses working on sensitive or proprietary projects.\r\n\r\n### Docmost\r\n\r\n[Docmost](https://docmost.com/) is an open-source document collaboration platform that combines the features of a wiki and a document editor.\r\n\r\n**Key features:**\r\n- Real-time collaborative editing\r\n- Version history and document comparison\r\n- Markdown and WYSIWYG editing modes\r\n- Nested document structure\r\n- Full-text search capabilities\r\n\r\n**How it helps small businesses:**\r\nDocmost can serve as a central knowledge base and collaboration tool for small businesses. It's particularly useful for creating and maintaining internal documentation, project plans, and team wikis. The real-time collaboration features can improve team productivity and ensure that everyone has access to the most up-to-date information.\r\n\r\nFor installation instructions, check out our [Docmost Docker installation guide](https://www.bitdoze.com/docmost-docker-install/).\r\n\r\n### Duplicati\r\n\r\n[Duplicati](https://duplicati.com/) is an open-source backup solution that supports various storage backends, including cloud storage services.\r\n\r\n**Key features:**\r\n- Encrypted and compressed backups\r\n- Incremental backups to save space and bandwidth\r\n- Scheduling and retention policies\r\n- Support for multiple storage providers\r\n- Web-based interface for easy management\r\n\r\n**How it helps small businesses:**\r\nData loss can be catastrophic for small businesses. Duplicati provides a robust, cost-effective backup solution that can help protect critical business data. Its support for various storage backends allows businesses to choose the most cost-effective storage option, while encryption ensures that sensitive data remains secure even when stored in the cloud.\r\n\r\n### ERPNext or Twenty CRM\r\n\r\n[ERPNext](https://erpnext.com/) and [Twenty CRM](https://twenty.com/) are open-source business management solutions that cover various aspects of business operations.\r\n\r\n**Key features:**\r\n- Customer Relationship Management (CRM)\r\n- Inventory and warehouse management\r\n- Human Resources and payroll\r\n- Accounting and financial management\r\n- Project management and time tracking\r\n\r\n**How it helps small businesses:**\r\nThese comprehensive business management tools can help small businesses streamline their operations by integrating various business functions into a single platform. This can lead to improved efficiency, better data consistency, and more informed decision-making. As open-source solutions, they offer flexibility and customization options to fit specific business needs without the high costs associated with proprietary ERP systems.\r\n\r\n### Formbricks\r\n\r\n[Formbricks](https://formbricks.com/) is an open-source survey and feedback collection tool that helps businesses gather insights from their customers and users.\r\n\r\n**Key features:**\r\n- Customizable survey templates\r\n- In-app survey targeting\r\n- Response analysis and reporting\r\n- Integration with various platforms and tools\r\n- GDPR-compliant data collection\r\n\r\n**How it helps small businesses:**\r\nUnderstanding customer needs and preferences is crucial for small businesses. Formbricks provides an affordable way to collect and analyze customer feedback, which can inform product development, marketing strategies, and customer service improvements. The ability to embed surveys within applications or websites allows businesses to gather contextual feedback at key points in the customer journey.\r\n\r\n### beszel server resource monitoring\r\n\r\n[beszel](https://github.com/henrygd/beszel) is a lightweight server monitoring tool that helps businesses keep track of their server resources and performance.\r\n\r\n**Key features:**\r\n- Real-time monitoring of CPU, memory, and disk usage\r\n- Network traffic analysis\r\n- Customizable alerts and notifications\r\n- Historical data and trend analysis\r\n- API for integration with other\r\n\r\n\r\n\r\n### Invoice Ninja\r\n\r\n[Invoice Ninja](https://invoiceninja.com/) is an open-source invoicing and billing solution that can help small businesses manage their finances more effectively.\r\n\r\n**Key features:**\r\n- Customizable invoice templates\r\n- Automated recurring invoices and payments\r\n- Time tracking and project management\r\n- Integration with multiple payment gateways\r\n- Client portal for easy invoice access\r\n\r\n**How it helps small businesses:**\r\nInvoice Ninja can streamline the billing process for small businesses, saving time and improving cash flow. Its automation features can reduce the administrative burden of invoicing, while the client portal can enhance customer experience by providing easy access to invoices and payment history. The integration with various payment gateways allows businesses to offer multiple payment options to their clients, potentially speeding up payments.\r\n\r\n### [Kimai](https://www.kimai.org/)\r\n\r\nKimai is an open-source time tracking application that can help businesses monitor employee work hours and project durations.\r\n\r\n**Key features:**\r\n- User-friendly interface for time entry\r\n- Project and task management\r\n- Detailed reporting and export options\r\n- User roles and permissions\r\n- Integration with invoicing systems\r\n\r\n**How it helps small businesses:**\r\nAccurate time tracking is crucial for project management, billing, and productivity analysis. Kimai provides small businesses with a flexible tool to track employee hours, manage projects, and generate reports. This can lead to more accurate client billing, better resource allocation, and improved project profitability. The insights gained from time tracking can also help businesses identify inefficiencies and optimize their workflows.\r\n\r\n\r\n## Security Considerations for Self-Hosted Containers\r\n\r\nWhen deploying these containers, consider implementing:\r\n- Reverse proxy with SSL (like [Traefik](https://www.bitdoze.com/traefik-proxy-docker/) or Nginx Proxy Manager)\r\n- Regular backup solutions\r\n- Container update automation\r\n- Network segregation\r\n- Access control and MFA\r\n- Monitoring and logging solutions\r\n\r\n\r\n## Getting Started with Docker Containers\r\n\r\nEssential tools for managing your container infrastructure:\r\n1. **Docker Compose** for container orchestration\r\n2. **Reverse Proxy** (Traefik/Nginx Proxy Manager)\r\n3. **Backup solution** (Duplicati/Borgbackup)\r\n4. **Monitoring stack** (Prometheus/Grafana)\r\n5. **Container management** (Portainer/Dockge)\r\n\r\nBasic deployment checklist:\r\n- [ ] Set up server with adequate resources\r\n- [ ] Install Docker and Docker Compose\r\n- [ ] Configure reverse proxy and SSL\r\n- [ ] Implement backup strategy\r\n- [ ] Set up monitoring\r\n- [ ] Document deployment procedures\r\n\r\n## Minimum Resource Requirements by Usage Scale\r\n\r\n| Scale | Users | CPU | RAM | Storage | Recommended VPS |\r\n|-------|--------|-----|-----|---------|----------------|\r\n| Small | 1-10 | 2 cores | 4GB | 50GB | Basic VPS |\r\n| Medium | 10-50 | 4 cores | 8GB | 100GB | Standard VPS |\r\n| Large | 50+ | 8+ cores | 16GB+ | 200GB+ | Performance VPS |\r\n\r\n## Conclusions\r\n\r\nSelf-hosted Docker containers offer small businesses a powerful way to deploy and manage a wide range of applications that can significantly enhance their operations. From collaboration tools like NextCloud and Zulip to financial management solutions like Firefly III and Invoice Ninja, these containers provide enterprise-level functionality at a fraction of the cost of traditional software solutions.\r\n\r\nBy leveraging these tools, small businesses can:\r\n\r\n1. **Improve collaboration and communication**: Tools like NextCloud, Zulip, and Docmost facilitate better teamwork and information sharing.\r\n\r\n2. **Enhance customer support**: Zammad and Uptime Kuma help businesses provide better service and maintain high availability.\r\n\r\n3. **Streamline operations**: ERPNext, NocoDB, and n8n allow for process automation and efficient data management.\r\n\r\n4. **Boost marketing efforts**: Mautic, ListMonk, and Plausible provide powerful marketing and analytics capabilities.\r\n\r\n5. **Strengthen security and compliance**: Self-hosting gives businesses greater control over their data and helps meet regulatory requirements.\r\n\r\n6. **Reduce costs**: Open-source solutions eliminate expensive software licenses while providing similar functionality.\r\n\r\nWhen considering which containers to deploy, businesses should assess their specific needs, available resources, and technical expertise. It's often beneficial to start with a few core applications and gradually expand the container ecosystem as the business grows and becomes more comfortable with the technology.\r\n\r\nRemember that while self-hosting offers many advantages, it also comes with responsibilities such as security management, updates, and backups. Proper planning and implementation are crucial to ensure a smooth and secure deployment of these Docker containers.\r\n\r\nBy embracing these self-hosted solutions, small businesses can level the playing field with larger competitors, improve their operational efficiency, and provide better services to their customers. As the Docker ecosystem continues to grow, the opportunities for businesses to innovate and optimize their IT infrastructure will only increase.","src/content/posts/docker-containers-business.mdx",[803],"../../assets/images/24/11/docker-containers-business.jpeg","f9180f85b7b2128c","docker-containers-business.mdx","docker-compose-secrets",{id:806,data:808,body:817,filePath:818,assetImports:819,digest:821,legacyId:822,deferredRender:32},{title:809,description:810,date:811,image:812,authors:813,categories:814,tags:815,canonical:816},"Secure Your Docker Stack: A Comprehensive Guide to Docker Compose Secrets","Learn how to effectively use secrets in Docker Compose to secure your containerized applications. This comprehensive guide covers setup, implementation, best practices, and real-world examples.",["Date","2024-09-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/09/docker-compose-secrets.jpeg",[19],[98],[100],"https://www.bitdoze.com/docker-compose-secrets/","Docker Compose is a powerful tool for defining and running multi-container Docker applications. It allows developers to use a YAML file to configure application services, networks, and volumes, simplifying the process of managing complex containerized environments. However, as applications grow in complexity and sensitivity, managing confidential information such as API keys, passwords, and other secrets becomes increasingly challenging.\r\n\r\nIn today's security-conscious world, protecting sensitive data is paramount. Hardcoding secrets directly into Dockerfiles or Docker Compose files is a risky practice that can lead to security breaches if these files are exposed. This is where Docker secrets come into play, offering a secure way to manage sensitive information in containerized environments.\r\n\r\nThis article will guide you through the process of using secrets in Docker Compose, helping you enhance the security of your Docker-based applications without compromising on convenience or functionality.\r\n\r\n## Understanding Docker Secrets\r\n\r\n### What are Docker Secrets?\r\n\r\nDocker secrets are a feature provided by Docker to manage sensitive data securely. A secret is a blob of data, such as a password, SSH private key, SSL certificate, or any other piece of data that should not be transmitted over a network or stored unencrypted in a Dockerfile or in your application's source code.\r\n\r\nDocker secrets are:\r\n- Encrypted at rest\r\n- Encrypted during transit\r\n- Mounted as files in the container\r\n- Centrally managed by Docker Swarm (required for Docker secrets)\r\n\r\n### Benefits of using secrets in Docker Compose\r\n\r\n1. **Enhanced Security**: Secrets are encrypted and only accessible to services that explicitly need them, reducing the risk of exposure.\r\n\r\n2. **Centralized Management**: All secrets are managed in one place, making it easier to update, rotate, or revoke them as needed.\r\n\r\n3. **Separation of Concerns**: Developers can work with Docker Compose files without needing access to the actual secret values.\r\n\r\n4. **Version Control Friendly**: Since secrets are not stored in the Docker Compose file itself, you can safely version control your configuration without exposing sensitive data.\r\n\r\n5. **Runtime Injection**: Secrets are injected into containers at runtime, not build time, allowing for more flexible and secure deployments.\r\n\r\n6. **Scalability**: As your application scales, Docker secrets provide a consistent way to manage sensitive data across multiple containers and nodes.\r\n\r\nBy leveraging Docker secrets in your Docker Compose setup, you can significantly improve the security posture of your containerized applications while maintaining the ease of use that Docker Compose provides.\r\n\r\n\r\n## Setting Up Docker Secrets\r\n\r\nSetting up Docker secrets involves two main steps: creating the secret files and using the Docker CLI to add these secrets to your Docker environment.\r\n\r\n### Creating secret files\r\n\r\n1. Create a file containing your secret information. For example:\r\n   ```sh\r\n   echo \"mysupersecretpassword\" > db_password.txt\r\n   ```\r\n\r\n2. Ensure the file permissions are restricted:\r\n   ```sh\r\n   chmod 400 db_password.txt\r\n   ```\r\n\r\n### Using the `docker secret create` command\r\n\r\nOnce you have your secret file, you can create a Docker secret using the following command:\r\n\r\n```bash\r\ndocker secret create my_db_password db_password.txt\r\n```\r\n\r\nThis command does the following:\r\n- `docker secret create` is the command to create a new secret\r\n- `my_db_password` is the name you're giving to this secret in Docker\r\n- `db_password.txt` is the file containing the secret\r\n\r\nYou can verify the secret was created by running:\r\n\r\n```bash\r\ndocker secret ls\r\n```\r\n\r\n**Common Commands**\r\n\r\n| Command                       | Description                        |\r\n|-------------------------------|------------------------------------|\r\n| `docker secret create`        | Creates a new secret               |\r\n| `docker secret ls`            | Lists all secrets                  |\r\n| `docker secret inspect`       | Shows details of a specific secret |\r\n| `docker secret rm <secret>`   | Removes a secret                   |\r\n\r\nRemember, after creating the secret, you can safely delete the local file as Docker now securely stores the secret.\r\n\r\n## Implementing Secrets in Docker Compose\r\n\r\n### Add the Docker External Secret Created Previousely\r\n\r\nTo use secrets in your Docker Compose setup, you need to declare them in your `docker-compose.yml` file and then configure your services to access these secrets. This will use the secret that you have defined initialy external.\r\n\r\n#### Syntax for declaring secrets in docker-compose.yml\r\n\r\nHere's an example of how to declare and use secrets in a Docker Compose file:\r\n\r\n```yaml\r\nversion: '3.8'\r\n\r\nservices:\r\n  myapp:\r\n    image: myapp:latest\r\n    secrets:\r\n      - my_db_password\r\n    environment:\r\n      - DB_PASSWORD_FILE=/run/secrets/my_db_password\r\n\r\nsecrets:\r\n  my_db_password:\r\n    external: true\r\n```\r\n\r\nLet's break this down:\r\n\r\n- The `secrets` section at the root level declares which secrets this compose file uses.\r\n- `external: true` indicates that this secret has been created externally (using `docker secret create`).\r\n- In the `services` section, we list the secrets that should be made available to the service.\r\n- The `environment` section shows how to reference the secret file path\r\n#### Accessing secrets in containers\r\n\r\nDocker mounts secrets as files under `/run/secrets/<secret_name>` in the container. In the example above, the secret would be accessible at `/run/secrets/my_db_password`.\r\n\r\nTo use the secret in your application, you would read the contents of this file. Here's an example in Python:\r\n\r\n```python\r\nwith open('/run/secrets/my_db_password', 'r') as secret_file:\r\n    db_password = secret_file.read().strip()\r\n\r\n# Now use db_password to connect to your database\r\n```\r\n\r\nRemember, it's the responsibility of your application code to read the secret from the file and use it appropriately.\r\n\r\n### Integrating Secrets Directly into Docker Compose Files\r\n\r\n\r\nThis section uses directly the docker compose file to define the secret you don't need to create it before:\r\n\r\n#### Step 1: Define Secrets\r\n\r\nFirst, define the secrets in your `docker-compose.yml` file.\r\n\r\n```yaml\r\nversion: '3.7'\r\nsecrets:\r\n  my_secret:\r\n    file: ./my_secret.txt\r\n```\r\n\r\n#### Step 2: Use Secrets in Services\r\n\r\nNext, reference the secrets in your service definitions.\r\n```yaml\r\nservices:\r\n  my_service:\r\n    image: my_image\r\n    secrets:\r\n      - my_secret\r\n```\r\n\r\n#### Step 3: Access Secrets in Containers\r\n\r\nThe secrets will be available in your container at `/run/secrets`.\r\n\r\n```bash\r\ncat /run/secrets/my_secret\r\n```\r\n\r\n**Example Configuration**\r\n\r\nHere's a complete example:\r\n```yaml\r\nversion: '3.7'\r\nsecrets:\r\n  db_password:\r\n    file: ./db_password.txt\r\nservices:\r\n  database:\r\n    image: mysql:latest\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_password\r\n    secrets:\r\n      - db_password\r\n```\r\nA fule exemple confiuration can be checked for [traefik setup with wildcard certificat](https://www.bitdoze.com/traefik-wildcard-certificate/) in here you have a full docker compose file will the things.\r\n\r\n### Differences Between External and Internal Docker Compose Secrets\r\n\r\nDocker Compose allows you to work with two types of secrets: external and internal. Understanding the differences between these types is crucial for effective secret management.\r\n\r\n#### External Secrets\r\n\r\nExternal secrets are created and managed outside of the Docker Compose file, typically using the Docker CLI or a secret management system.\r\n\r\nCharacteristics:\r\n1. Created independently of the Docker Compose file\r\n2. Referenced in the Compose file using the `external: true` attribute\r\n3. Provide better separation of concerns and enhanced security\r\n4. Can be shared across multiple services and stacks\r\n\r\n\r\n#### Internal Secrets\r\n\r\nInternal secrets are defined directly within the Docker Compose file.\r\n\r\nCharacteristics:\r\n1. Defined and managed within the Docker Compose file\r\n2. Can be created from files, [environment variables](https://www.bitdoze.com/docker-env-vars/), or directly in the Compose file\r\n3. Easier to set up for development and testing environments\r\n4. Less secure as they may be visible in the Compose file\r\n\r\n\r\n\r\n#### Key Differences\r\n\r\n1. **Management:** External secrets are managed outside the Compose file, while internal secrets are defined within it.\r\n2. **Security:** External secrets offer better security as they're not visible in the Compose file.\r\n3. **Reusability:** External secrets can be easily shared across multiple services and stacks.\r\n4. **Deployment:** Internal secrets are easier to deploy in development environments but may require additional steps in production.\r\n5. **Versioning:** External secrets can be versioned independently of the application code.\r\n\r\n#### When to Use Each Type\r\n\r\n- Use external secrets for:\r\n  - Production environments\r\n  - Sensitive data that needs to be shared across multiple services\r\n  - When you need to manage secrets independently of your application code\r\n\r\n- Use internal secrets for:\r\n  - Development and testing environments\r\n  - Quick prototyping\r\n  - When the secret is specific to a single service and doesn't need to be shared\r\n\r\nBy understanding these differences, you can choose the most appropriate method for managing secrets in your Docker Compose projects, balancing security needs with ease of use and deployment considerations.\r\n\r\n\r\n## Best Practices\r\n\r\nWhen working with Docker secrets, following best practices ensures optimal security and manageability. Here are some key recommendations:\r\n\r\n### Naming conventions\r\n\r\n- Use descriptive names: Choose secret names that clearly indicate their purpose, e.g., `prod_db_password` instead of `secret1`.\r\n- Use consistent prefixes: For example, use `prod_` for production secrets and `dev_` for development secrets.\r\n- Avoid including sensitive information in names: The secret name itself shouldn't reveal any confidential data.\r\n\r\nExample:\r\n```yaml\r\nsecrets:\r\n  prod_api_key:\r\n    external: true\r\n  dev_db_password:\r\n    external: true\r\n```\r\n\r\n### Version control considerations\r\n\r\n- Never commit actual secret values to version control.\r\n- Use `.gitignore` to exclude files containing secrets.\r\n- Consider using a `.env.example` file to show which environment variables are needed, without including actual values.\r\n\r\nExample `.gitignore` entry:\r\n```\r\n*.env\r\nsecrets/\r\n```\r\n\r\n### Rotating secrets\r\n\r\n- Regularly update secrets to minimize the impact of potential breaches.\r\n- Use Docker's secret rotation feature in Swarm mode:\r\n  1. Create a new secret with a versioned name.\r\n  2. Update your service to use the new secret.\r\n  3. Remove the old secret after ensuring all services are updated.\r\n\r\nExample:\r\n```bash\r\ndocker secret create db_password_v2 new_password.txt\r\ndocker service update --secret-rm db_password_v1 --secret-add db_password_v2 myservice\r\ndocker secret rm db_password_v1\r\n```\r\n\r\n### Additional best practices\r\n\r\n- Limit secret access: Only give services access to the secrets they absolutely need.\r\n- Use secret drivers: Consider using external secret management systems through Docker secret drivers for advanced use cases.\r\n- Audit secret usage: Regularly review which services are using which secrets and revoke unnecessary access.\r\n\r\n## Example Use Cases\r\n\r\nDocker secrets can be used in various scenarios to enhance the security of your containerized applications. Here are some common use cases:\r\n\r\n### Database credentials\r\n\r\nSecurely manage database passwords without hardcoding them in your application or Docker files.\r\n\r\n```yaml\r\nversion: '3.8'\r\nservices:\r\n  db:\r\n    image: postgres\r\n    environment:\r\n      POSTGRES_PASSWORD_FILE: /run/secrets/db_password\r\n    secrets:\r\n      - db_password\r\n\r\n  app:\r\n    image: myapp\r\n    secrets:\r\n      - db_password\r\n    environment:\r\n      DB_PASSWORD_FILE: /run/secrets/db_password\r\n\r\nsecrets:\r\n  db_password:\r\n    external: true\r\n```\r\n\r\n### API keys\r\n\r\nSafely use API keys in your services without exposing them in your code or configuration files.\r\n\r\n```yaml\r\nversion: '3.8'\r\nservices:\r\n  api_service:\r\n    image: api_service\r\n    secrets:\r\n      - api_key\r\n    environment:\r\n      API_KEY_FILE: /run/secrets/api_key\r\n\r\nsecrets:\r\n  api_key:\r\n    external: true\r\n```\r\n\r\n### SSL certificates\r\n\r\nManage SSL certificates securely for services that require HTTPS.\r\n\r\n```yaml\r\nversion: '3.8'\r\nservices:\r\n  web:\r\n    image: nginx\r\n    secrets:\r\n      - site_certificate\r\n      - site_key\r\n    volumes:\r\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\r\n\r\nsecrets:\r\n  site_certificate:\r\n    file: ./certs/site.crt\r\n  site_key:\r\n    file: ./certs/site.key\r\n```\r\n\r\n### JWT signing keys\r\n\r\nSecurely manage keys used for signing JSON Web Tokens (JWTs) in authentication services.\r\n\r\n```yaml\r\nversion: '3.8'\r\nservices:\r\n  auth_service:\r\n    image: auth_service\r\n    secrets:\r\n      - jwt_private_key\r\n      - jwt_public_key\r\n    environment:\r\n      JWT_PRIVATE_KEY_FILE: /run/secrets/jwt_private_key\r\n      JWT_PUBLIC_KEY_FILE: /run/secrets/jwt_public_key\r\n\r\nsecrets:\r\n  jwt_private_key:\r\n    external: true\r\n  jwt_public_key:\r\n    external: true\r\n```\r\n\r\nBy leveraging Docker secrets in these scenarios, you can significantly enhance the security of your containerized applications while maintaining flexibility and ease of management.\r\n\r\n\r\n## Limitations and Alternatives\r\n\r\nWhile Docker secrets provide a robust solution for managing sensitive information in containerized environments, it's important to be aware of their limitations and consider alternatives when necessary.\r\n\r\n### Swarm mode requirement\r\n\r\nThe primary limitation of Docker secrets is that they require Docker Swarm mode to be enabled. This can be a significant drawback for users who prefer to use Docker in standalone mode or with other orchestration tools like Kubernetes.\r\n\r\nImplications:\r\n- Not suitable for single-node deployments without Swarm\r\n- May require architectural changes in existing non-Swarm setups\r\n\r\n### Other options for managing sensitive data\r\n\r\nGiven the limitations of Docker secrets, here are some alternatives to consider:\r\n\r\n1. **Environment Variables**:\r\n   - Pros: Simple to use, widely supported\r\n   - Cons: Can be exposed through system calls, not encrypted at rest\r\n\r\n   Example:\r\n   ```yaml\r\n   services:\r\n     app:\r\n       image: myapp\r\n       environment:\r\n         - DB_PASSWORD=mysecretpassword\r\n   ```\r\n\r\n2. **Config Management Tools**:\r\n   - Options: Ansible, Puppet, Chef\r\n   - Pros: Powerful, flexible, can manage configurations across multiple servers\r\n   - Cons: Require additional setup and knowledge\r\n\r\n3. **Cloud Provider Secret Management Services**:\r\n   - Options: AWS Secrets Manager, Google Cloud Secret Manager, Azure Key Vault\r\n   - Pros: Highly secure, integrated with cloud ecosystems\r\n   - Cons: Vendor lock-in, potential cost implications\r\n\r\n4. **HashiCorp Vault**:\r\n   - Pros: Highly secure, platform-agnostic, supports dynamic secrets\r\n   - Cons: Complex setup, requires additional infrastructure\r\n\r\n   Example integration:\r\n   ```yaml\r\n   services:\r\n     app:\r\n       image: myapp\r\n       environment:\r\n         - VAULT_ADDR=http://vault:8200\r\n       entrypoint: [\"vault-agent\", \"-config=/vault-agent-config.hcl\"]\r\n   ```\r\n\r\n5. **Docker Config**:\r\n   - Similar to Docker secrets but for non-sensitive configuration data\r\n   - Can be used in conjunction with secrets for a comprehensive configuration management strategy\r\n\r\nWhen choosing an alternative, consider factors such as your specific security requirements, existing infrastructure, team expertise, and scalability needs.\r\n\r\n## Conclusions\r\n\r\nUsing secrets in Docker Compose adds a vital layer of security to your applications. By keeping sensitive information out of your codebase, you reduce the risk of leaks and unauthorized access.\r\n\r\nStart implementing secrets today to bolster your application's security. With these steps, you'll handle confidential data more responsibly and effectively, ensuring a safer deployment environment.","src/content/posts/docker-compose-secrets.mdx",[820],"../../assets/images/24/09/docker-compose-secrets.jpeg","31fdb4598d8efa23","docker-compose-secrets.mdx","docker-env-vars",{id:823,data:825,body:834,filePath:835,assetImports:836,digest:838,legacyId:839,deferredRender:32},{title:826,description:827,date:828,image:829,authors:830,categories:831,tags:832,canonical:833},"How to Use Environment Variables ARG and ENV in Docker, Dockerfile or Docker Compose","Learn how to use environments variables ARG and ENV into Docker command, Dockerfile or Docker Compose",["Date","2024-01-31T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/docker-user-env-vars.jpeg",[19],[98],[100],"https://www.bitdoze.com/docker-env-vars/","In Docker, environment variables play a crucial role in configuring and customizing containerized applications. Two commonly used methods for setting environment variables are ARG and ENV. In this article, we will explore how to set Docker environment variables using ARG and ENV, providing you with the necessary knowledge to effectively manage your container environments.\r\n\r\nWhen it comes to setting environment variables in Docker, two primary approaches are widely adopted: ARG (build-time) and ENV (runtime). The former is utilized during image build time while the latter is employed when running containers. By leveraging these techniques, developers can seamlessly pass information into their containerized applications without modifying the underlying codebase. Join us as we delve into the details of using ARG and ENV in Docker to empower your container deployments.\r\n\r\n## What are Docker environment variables?\r\n\r\nDocker environment variables play a crucial role in configuring and customizing containerized applications. They allow you to define runtime values that can be accessed by processes running within the Docker container. Here's what you need to know about Docker environment variables:\r\n\r\n1. **Definition**: Environment variables are dynamic values that are set outside of an application but can be accessed by it during runtime. In the context of Docker, these variables provide a flexible way to configure containers without modifying their underlying code.\r\n\r\n2. **Usage**: Environment variables in Docker can be used for various purposes, such as providing configuration settings, defining connection strings, specifying API keys, or storing sensitive information like passwords.\r\n\r\n3. **ARG vs ENV**: There are two types of environment variable instructions in Docker: ARG and ENV.\r\n\r\n   - `ARG` (Build-time): ARG allows you to pass build-time arguments when building your image using the `--build-arg` flag with the `docker build` command. These arguments act as placeholders and can only be referenced during the build process.\r\n   - `ENV` (Runtime): ENV sets environment variables that will persist when the container is running. You can specify them directly in your Dockerfile or through command-line options with `-e` when running a container.\r\n\r\n4. **Benefits**:\r\n\r\n   - Flexibility: Using environment variables makes it easier to customize your application's behavior without modifying its codebase.\r\n   - Portability: By externalizing configuration details into environment variables, you create reusable images that work across different environments.\r\n   - Security: Sensitive information like credentials or API keys can be securely stored and managed as environment variables rather than hardcoding them into source files.\r\n\r\n5. **Accessing Variables**: Inside a running container, accessing these environment variable values depends on the programming language or framework being used by your application.\r\n\r\n6. **Best Practices**:\r\n   - Use clear naming conventions for your environment variables to improve readability and maintainability.\r\n   - Avoid hardcoding sensitive information directly in Dockerfiles or source code files.\r\n   - Consider using a secrets management solution to securely manage sensitive data stored as environment variables.\r\n\r\nUnderstanding Docker environment variables is essential for effectively configuring and deploying containerized applications. They offer flexibility, portability, and security while allowing you to control different aspects of your containers without modifying their core functionality.\r\n\r\nSome other docker articles that can help you in your docker journey:\r\n\r\n- [Add Users to a Docker Container](https://www.bitdoze.com/add-users-to-docker-container/)\r\n- [Copy Multiple Files in One Layer Using a Dockerfile](https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/)\r\n- [Install Docker & Docker-compose for Ubuntu ARM](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n- [Redirect Docker Logs to a Single File](https://www.bitdoze.com/redirect-docker-logs-to-a-single-file/)\r\n\r\n## How to use ARG and ENV variables in Dockerfiles and docker-compose files\r\n\r\nIn this section, you will learn how to use ARG and ENV variables in your Dockerfiles and docker-compose files to build and run your Docker images and containers. You will also see some examples of how to use these variables in different scenarios.\r\n\r\n### Using ARG variables in Dockerfiles\r\n\r\nTo use ARG variables in your Dockerfiles, you need to follow these steps:\r\n\r\n- Define the ARG variables using the `ARG` instruction, optionally with a default value. You can define multiple ARG variables in your Dockerfile, but they must come before the first `FROM` instruction.\r\n- Use the ARG variables in your Dockerfile instructions, such as `FROM`, `RUN`, `COPY`, or `ADD`. You can use the `$` syntax to reference the ARG variables, such as `$my_arg`.\r\n- Override the default values of the ARG variables using the `--build-arg` option of the `docker build` command. You can specify multiple `--build-arg` options, one for each ARG variable. The format is `--build-arg my_arg=my_value`.\r\n\r\nHere is an example of a Dockerfile that uses ARG variables to specify the base image and the version of a library:\r\n\r\n```\r\n# Define ARG variables\r\nARG base_image=ubuntu:20.04\r\nARG lib_version=1.0.0\r\n\r\n# Use ARG variables in FROM instruction\r\nFROM $base_image\r\n\r\n# Use ARG variables in RUN instruction\r\nRUN apt-get update && apt-get install -y libfoo=$lib_version\r\n```\r\n\r\nTo build this image, you can use the following command:\r\n\r\n```bash\r\ndocker build -t my_image --build-arg base_image=debian:10 --build-arg lib_version=1.1.0 .\r\n```\r\n\r\nThis command overrides the default values of the ARG variables and builds the image using `debian:10` as the base image and `libfoo=1.1.0` as the library version.\r\n\r\n### Using ENV variables in Dockerfiles\r\n\r\nTo use ENV variables in your Dockerfiles, you need to follow these steps:\r\n\r\n- Define the ENV variables using the `ENV` instruction, optionally with a default value. You can define multiple ENV variables in your Dockerfile, and they can come after the `FROM` instruction.\r\n- Use the ENV variables in your Dockerfile instructions, such as `RUN`, `CMD`, or `ENTRYPOINT`. You can use the `$` syntax to reference the ENV variables, such as `$my_env`.\r\n- Override the default values of the ENV variables using the `-e` option of the `docker run` command or the `environment` or `env_file` options of the `docker-compose` command. You can specify multiple options, one for each ENV variable. The format is `-e my_env=my_value` or `environment: - my_env=my_value` or `env_file: my_env_file`.\r\n\r\nHere is an example of a Dockerfile that uses ENV variables to specify the database URL and the API key for an application:\r\n\r\n```\r\n# Define ENV variables\r\nENV db_url=postgres://user:pass@localhost:5432/db\r\nENV api_key=secret\r\n\r\n# Use ENV variables in CMD instruction\r\nCMD [\"python\", \"app.py\", \"$db_url\", \"$api_key\"]\r\n```\r\n\r\nTo run this image, you can use the following command:\r\n\r\n```bash\r\ndocker run -d -p 5000:5000 -e db_url=postgres://user:pass@host:port/db -e api_key=supersecret my_image\r\n```\r\n\r\nThis command overrides the default values of the ENV variables and runs the container using `postgres://user:pass@host:port/db` as the database URL and `supersecret` as the API key.\r\n\r\nAlternatively, you can use a docker-compose file to run this image, such as:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  app:\r\n    image: my_image\r\n    ports:\r\n      - \"5000:5000\"\r\n    environment:\r\n      - db_url=postgres://user:pass@host:port/db\r\n      - api_key=supersecret\r\n```\r\n\r\nOr, you can use an env_file to store the ENV variables, such as:\r\n\r\n```sh\r\ndb_url=postgres://user:pass@host:port/db\r\napi_key=supersecret\r\n```\r\n\r\nAnd then reference the env_file in your docker-compose file, such as:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  app:\r\n    image: my_image\r\n    ports:\r\n      - \"5000:5000\"\r\n    env_file:\r\n      - my_env_file\r\n```\r\n\r\nI hope this section helps you understand how to use ARG and ENV variables in Dockerfiles and docker-compose files. If you want me to write more sections, please let me know. 😊\r\n\r\n## Using ARG and ENV together\r\n\r\nWhen working with Docker, it is common to use both ARG and ENV instructions in combination to set environment variables. Here's how you can use them together effectively:\r\n\r\n1. **Using ARG instruction:** The ARG instruction allows you to pass build-time variables during the image build process. These variables are accessible only during the build stage and not at runtime.\r\n\r\n2. **Setting ARG values:** To set an argument value, you can either specify it directly in your Dockerfile or pass it as a command-line parameter using the `--build-arg` flag when running `docker build`. For example:\r\n\r\n   ```\r\n   # Set a default value for your argument\r\n   ARG MY_ARG=default_value\r\n\r\n   # Use the argument within your Dockerfile\r\n   ENV MY_ENV=$MY_ARG\r\n   ```\r\n\r\n3. **Using ENV instruction:** The ENV instruction sets environment variables that will be available in containers based on the image at runtime.\r\n\r\n4. **Combining ARG and ENV instructions:** You can leverage both instructions by setting an environment variable using an argument value defined earlier in your Dockerfile.\r\n\r\n5. **Example usage:**\r\n\r\n   ```\r\n   # Define an argument with a default value\r\n   ARG PORT=8080\r\n\r\n   # Set an environment variable using the argument value\r\n   ENV APP_PORT=$PORT\r\n\r\n   # Use the environment variable within your container commands/scripts\r\n   CMD [\"node\", \"app.js\", \"--port\", \"$APP_PORT\"]\r\n   ```\r\n\r\n6. **Building images with custom arguments:**\r\n\r\n- If no `--build-arg` flag is provided, the default values specified in your Dockerfile will be used.\r\n\r\n- To override default values during builds, use `--build-arg` followed by `<ARG_NAME>=<VALUE>` syntax while executing `docker build`.\r\n\r\n7. Remember that any changes made to ARG values during the build process won't affect the environment variables used within the container. The ENV instruction is responsible for setting those runtime variables.\r\n\r\nBy using both ARG and ENV instructions together, you can dynamically set environment variables at build time while allowing flexibility to override default values during image builds. This approach helps streamline your Docker workflow and ensures consistent behavior across different environments.\r\n\r\n## Best Practices for Managing Docker Environment Variables\r\n\r\nWhen managing environment variables in Docker, it's important to follow best practices to ensure smooth and secure container deployments. Here are some recommendations:\r\n\r\n1. **Use ARG for build-time variables**: When you need to pass information during the build process, such as version numbers or credentials, use ARG instead of ENV. ARG values are only available during the build stage and won't be accessible in the final image.\r\n\r\n2. **Prefer ENV for runtime variables**: For configuration settings that need to be available inside your running container, use ENV variables. These can be set at runtime using either command-line flags or a docker-compose file.\r\n\r\n3. **Avoid sensitive data in plain text**: Never store sensitive information like passwords or API keys directly in your Dockerfile or source code repositories. Instead, consider using external services like secrets managers or encrypted files mounted as volumes.\r\n\r\n4. **Keep environment variable names consistent**: Use meaningful and standardized names for your environment variables across different containers and projects. This will make it easier to understand configurations when working with multiple containers or microservices.\r\n\r\n5. **Document required environment variables**: Clearly document which environment variables are required by each container so that other developers can easily understand how to run and configure them properly.\r\n\r\n6. **Consider default values**: Provide sensible default values wherever possible for non-mandatory environment variables, reducing friction when deploying containers without explicitly setting every variable.\r\n\r\n7. **Use .env files sparingly**: While convenient for local development purposes with docker-compose, avoid relying heavily on .env files in production environments where more robust configuration management solutions should be used.\r\n\r\n| Best Practice | Description                           |\r\n| ------------- | ------------------------------------- |\r\n| 1             | Use ARG for build-time vars           |\r\n| 2             | Prefer ENV for runtime vars           |\r\n| 3             | Avoid storing sensitive data directly |\r\n| 4             | Keep env var names consistent         |\r\n| 5             | Document required env vars            |\r\n| 6             | Consider default values               |\r\n| 7             | Use .env files sparingly              |\r\n\r\nFollowing these best practices will help you manage Docker environment variables effectively, ensuring security and scalability in your containerized applications.\r\n\r\n## Common Issues with Docker Environment Variables\r\n\r\nWhen working with Docker environment variables, there are a few common issues that you may encounter. It's important to be aware of these issues and know how to address them:\r\n\r\n1. **Missing or Incorrect Variable Names**: Make sure that you are using the correct variable names when defining your environment variables in Docker. Misspelling or using incorrect names can lead to errors and unexpected behavior.\r\n\r\n2. **Overwriting Existing Environment Variables**: In some cases, setting an environment variable in Docker may overwrite an existing variable on your host system. This can cause conflicts and lead to undesired results. Always check for any potential clashes before defining new variables.\r\n\r\n3. **Ordering Dependencies**: If your application depends on certain environment variables being set before others, it's crucial to define their order correctly within your Dockerfile or docker-compose.yml file. Otherwise, you may run into initialization issues and errors during runtime.\r\n\r\n4. **Handling Sensitive Information**: Be cautious when dealing with sensitive information such as passwords or API keys as environment variables in Docker containers. Storing them directly in plain text is not secure and exposes them to potential risks. Consider using secrets management tools provided by container orchestration platforms like Kubernetes instead.\r\n\r\n5. **Variable Scope**: Remember that each container has its own isolated environment scope within a Docker network stack, which means that changes made inside one container will not affect other containers unless explicitly linked or connected together via networking configuration.\r\n\r\n6. **Updating Running Containers**: Updating the values of running containers' environment variables might require restarting those containers so that they pick up the new values properly.\r\n\r\nBy understanding these common issues related to Docker environment variables, you'll be better equipped to handle any challenges that arise while working with them effectively.\r\n\r\n## Summary\r\n\r\nIn this section, we will summarize the key points regarding setting Docker environment variables using `ARG` and `ENV`.\r\n\r\n- Environment variables play a crucial role in configuring applications running within Docker containers.\r\n- Docker provides two methods for setting environment variables during image building: `ARG` and `ENV`.\r\n- The `ARG` instruction allows you to define build-time variables that can be passed to the builder with the docker build command.\r\n- These ARG values can also be used as default values for ENV instructions or overridden during the build process.\r\n- On the other hand, the `ENV` instruction sets environment variables that are persistent within the container at runtime.\r\n- This means that any processes running inside the container have access to these defined environment variables.\r\n- You can use both ARG and ENV instructions together to create flexible configuration options for your Docker images.\r\n\r\nHere is a basic example of how these instructions can be used:\r\n\r\n```\r\n# Set an ARG variable\r\nARG my_variable=default_value\r\n\r\n# Use it as a default value when defining an ENV variable\r\nENV MY_VAR=${my_variable}\r\n\r\n# Other instructions...\r\n```\r\n\r\nBy utilizing ARG and ENV effectively, you can easily configure your containers with dynamic values while maintaining flexibility. Remember that ARG is primarily intended for passing arguments during image builds, while ENV is suitable for defining persistent runtime configuration.\r\n\r\nKeep in mind that proper usage of these features enhances portability and enables better customization of your Dockerized applications. Experimenting with different combinations of ARG and ENV will help you optimize your development workflow and make managing environments easier.\r\n\r\n## Conclusion\r\n\r\nIn this article, we explored the concept of setting Docker environment variables using the `ARG` and `ENV` directives. We learned that environment variables play a crucial role in configuring containers and can be used to pass important information such as database credentials, API keys, or application-specific settings.\r\n\r\nBy leveraging the `ARG` directive, we can define build-time variables that are accessible during the image building process but not available at runtime. On the other hand, with the `ENV` directive, we can set environment variables that are present both during build time and when running a container.\r\n\r\nUnderstanding how to properly set Docker environment variables is essential for creating flexible and configurable containers. By utilizing `ARG` and `ENV`, developers gain greater control over their containerized applications' behavior without hard-coding values into images.\r\n\r\nWith these powerful techniques in our toolkit, we can create dynamic Docker images that adapt to different environments seamlessly. Whether it's customizing configurations or securely passing sensitive data, leveraging environmental variables enhances the portability and flexibility of our containerized applications.\r\n\r\nRemembering to use proper syntax and follow best practices will ensure smooth deployments across various platforms while keeping our codebase clean and maintainable. So go ahead and start harnessing the power of Docker environment variables with confidence!","src/content/posts/docker-env-vars.mdx",[837],"../../assets/images/24/01/docker-user-env-vars.jpeg","467936b283197467","docker-env-vars.mdx","docker-run-python",{id:840,data:842,body:851,filePath:852,assetImports:853,digest:855,legacyId:856,deferredRender:32},{title:843,description:844,date:845,image:846,authors:847,categories:848,tags:849,canonical:850},"How To Run Any Python App in Docker with Docker Compose","Learn how Run Any Python App in Docker with Docker Compose and add an SSL certificate thru CloudFlare tunnels",["Date","2024-03-25T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/python-docker.jpeg",[19],[98],[100,294],"https://www.bitdoze.com/docker-run-python/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nRunning Python apps in Docker containers using Docker Compose makes deployment simple, reproducible and scalable. In this article, we'll guide you through the process of running any Python application in Docker using Docker Compose, targeting both beginners and intermediate users. We'll cover everything from creating a Dockerfile and Docker Compose file to adding new PIP packages and setting up a domain with SSL using Cloudflare Tunnels.\r\n\r\n## Prerequisites\r\n\r\nBefore we dive into the details, ensure you have the following prerequisites covered:\r\n\r\n- VPS where you can host WordPress, you can use one from [Hetzner](https://go.bitdoze.com/hetzner)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n- **Docker and Docker Compose Installed**: Docker to run containers and Docker Compose to define and share multi-container applications. Installation guides can be found on: [How To Install Docker & Docker-compose](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/7Nu7r8y_bDA\"\r\n  label=\"How To Run Any Python App in Docker with Docker Compose\"\r\n/>\r\n\r\n## Create a Dockerfile\r\n\r\nThe Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Here's a simple Dockerfile for a Python application:\r\n\r\n```yaml\r\nFROM python:3.12\r\n\r\nWORKDIR /app\r\nCOPY ./my-app/requirements.txt /app\r\n\r\nRUN pip install --upgrade pip\r\nRUN pip install --no-cache-dir -r requirements.txt\r\n```\r\n\r\n- `FROM python:3.12`: This line tells Docker to use the official Python 3.12 image as the base image for your application.\r\n- `WORKDIR /app`: Sets the working directory inside the container to `/app`.\r\n- `COPY ./my-app/requirements.txt /app`: Copies the `requirements.txt` file from your project into the `/app` directory in the container.\r\n- `RUN pip install --upgrade pip`: Upgrades pip to its latest version inside the container.\r\n- `RUN pip install --no-cache-dir -r requirements.txt`: Installs the Python dependencies defined in `requirements.txt` without storing the cache, to keep the image size small.\r\n\r\n## Create a my-app directory with your Python scripts and requirements.txt\r\n\r\nTo prepare your Python application for Docker:\r\n\r\n1. **Create a `requirements.txt` file** with the following content:\r\n\r\n   ```\r\n   nicegui\r\n   streamlit\r\n   ```\r\n\r\n   This file lists all the Python packages your application needs. In this case, `nicegui` and `streamlit`.\r\n\r\n2. **Create a `main.py` file** with a simple NiceGUI application:\r\n\r\n   ```python\r\n   from nicegui import ui\r\n\r\n   ui.label('Hello NiceGUI!')\r\n\r\n   ui.run()\r\n   ```\r\n\r\n   This script creates a web interface with a label displaying \"Hello NiceGUI!\".\r\n   In case you want to know more about NiceGUI you can check:\r\n\r\n- [NiceGUI For Beginners: Build An UI to Python App in 5 Minutes](https://www.bitdoze.com/nicegui-get-started/)\r\n- [How To Add Multiple Pages to NiceGUI](https://www.bitdoze.com/nicegui-pages/)\r\n\r\n## Create a docker compose file `compose.yml`\r\n\r\nDocker Compose allows you to define and run multi-container Docker applications. Here's how to set up your `compose.yml` for a basic Python web server:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  web:\r\n    container_name: python-server\r\n    command: python main.py\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    volumes:\r\n      - ./my-app:/app\r\n    ports:\r\n      - \"5021:8080\"\r\n    restart: unless-stopped\r\n```\r\n\r\nFor running Streamlit, modify the `command` and `ports`:\r\n\r\n```yaml\r\ncommand: streamlit hello\r\nports:\r\n  - \"5021:8501\"\r\n```\r\n\r\n- `version: \"3\"`: Specifies the version of the Docker Compose file format.\r\n- `services`: Defines the services your application consists of. In this case, just one service named `web`.\r\n- `container_name`: Names your container for easier reference.\r\n- `command`: The command that runs your application. For NiceGUI, it's `python main.py`. For Streamlit, it's `streamlit hello`.\r\n- `build`: Instructions to build the Docker image. It uses the current directory (`.`) and the Dockerfile named `Dockerfile`.\r\n- `volumes`: Maps the `./my-app` directory on your host to `/app` in the container, allowing for live updates without rebuilding the image.\r\n- `ports`: Maps port `5021` on your host to port `8080` (for NiceGUI) or `8501` (for Streamlit) in the container, making your application accessible via `http://localhost:5021`.\r\n\r\n## Start the docker compose file\r\n\r\nTo start your application, run:\r\n\r\n```sh\r\ndocker-compose up -d --force-recreate --build\r\n```\r\n\r\nThis command builds the images if they don't exist, starts the containers in detached mode, forces recreation of the containers, and rebuilds the images even if they exist.\r\n\r\n## Add new PIP Package\r\n\r\nTo add a new PIP package:\r\n\r\n1. **Update the `requirements.txt`** with the package you want.\r\n2. **Rebuild and restart your containers** with the same command:\r\n\r\n   ```sh\r\n   docker-compose up -d --force-recreate --build\r\n   ```\r\n\r\nThis ensures your application always runs with the latest dependencies.\r\n\r\n## Add domain with SSL with Cloudflare Tunnels\r\n\r\nYou need to let CloudFlare Tunel to know which port is using, you just need to go in the Access - Tunnels and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port . This will need to be as for the URL you have set in the .env file.\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers.\r\n\r\nAnd that's about it, now you can use your Python app, test it and see how it works.\r\n\r\n## Conclusions\r\n\r\nRunning a Python application in Docker using Docker Compose simplifies dependency management, ensures consistency across environments, and makes your application easy to deploy and scale. By following the steps outlined in this article, you can containerize any Python application, from simple scripts to complex web applications, and enjoy the benefits of Docker's containerization technology. Whether you're a beginner or an intermediate user, the process is straightforward and significantly enhances your development and deployment workflows.","src/content/posts/docker-run-python.mdx",[854],"../../assets/images/24/03/python-docker.jpeg","23ce0741018a2ef7","docker-run-python.mdx","dockge-install",{id:857,data:859,body:868,filePath:869,assetImports:870,digest:872,legacyId:873,deferredRender:32},{title:860,description:861,date:862,image:863,authors:864,categories:865,tags:866,canonical:867},"Dockge Install - Portainer Alternative for Docker Management","Learn how Dockge can help you manage your Docker containers, stacks, and services with ease. Dockge is a web-based Docker manager that lets you self-host your own applications and services with minimal hassle.",["Date","2024-01-09T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/dockge-install.jpeg",[19],[77],[519,242],"https://www.bitdoze.com/dockge-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/dockge-main.png\";\r\nimport imag2 from \"../../assets/images/24/01/dockge-container.png\";\r\nimport imag3 from \"../../assets/images/24/01/dockge-add.png\";\r\n\r\nDocker is a popular technology that allows you to run applications and services in isolated containers. Docker containers are lightweight, portable, and scalable, making them ideal for self-hosting your own applications and services.\r\n\r\nHowever, managing Docker containers can be challenging, especially if you have multiple containers, stacks, and services running on your server. You need to use the command line, edit configuration files, monitor the status and logs, and deal with networking and security issues.\r\n\r\nThat’s where Dockge comes in. Dockge is a web-based Docker manager that simplifies the process of self-hosting your own applications and services. With Dockge, you can easily create, manage, and update your Docker containers, stacks, and services from a user-friendly interface.\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nIn this article, we will introduce Dockge, explain how it works, and show you how to install it on your server. We will also highlight some of the features and benefits of using Dockge for your self-hosting needs.\r\n\r\n## What is Dockge?\r\n\r\n[Dockge](https://github.com/louislam/dockge) is a web-based Docker manager that lets you self-host your own applications and services with minimal hassle. Dockge is a Portainer alternative that is a lot simpler and can help you deploy docker-compose applications, it was developed by Louis the creator of [Uptime Kuma](https://github.com/louislam/uptime-kuma) which is a well known tool that can help you monitor your applications and websites more on: [Uptime Kuma Self Hosted Monitoring Tool](https://www.bitdoze.com/uptime-kuma-tool/)\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"Dockge UI\"\r\n/>\r\n\r\nDockge, you can access everything you need on a single page, without switching between multiple tabs. Whether you want to create a new container, view the logs, edit the YAML file, use the terminal, or control the container, you can do it all from one place. Dockge integrates all the essential functions into a single interface, making Docker management easy and convenient.\r\n\r\nDockge is also a powerful tool that helps you manage Docker Compose.yaml stacks with ease. You can create, edit, start, stop, restart, and delete Docker containers and services, all defined by your compose.yaml files. Dockge provides an interactive compose.yaml editor that lets you configure your stacks without hassle. You can also communicate with your containers and services in real time through the web terminal.\r\n\r\nDockge lets you track the progress of your container operations, such as image pulls, in real time. Dockge has a sleek and stylish user interface that enhances your productivity and enjoyment. If you like Uptime Kuma’s elegant UI/UX, you will love Dockge’s interface too. Moreover, Dockge makes it easy to convert complex “docker run” commands into Compose.yaml configurations, keeping your Compose files safe and organized on your drive. You can access and manage them using standard Docker Compose commands, giving you full control over your configuration files.\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"Dockge Container\"\r\n/>\r\n\r\n## Installing Dockge\r\n\r\nIn this section, we are going to see everything that needs to be done to have Dockge installed and running on a VPS server.\r\n\r\n<YouTubeEmbed url=\"https://www.youtube.com/embed/ouyOyAqRDyI\" label=\"Dockge\" />\r\n\r\n### 1. Create a VPS server\r\n\r\nYou will need a VPS server where you can host your docker containers I am recommending [Hetzner](https://go.bitdoze.com/hetzner), for more details check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/) and you can check also: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/)\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\nOr use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n\r\nThe VPS process is easy and you just need a Ubuntu server, Dockge is working on ARM and x86_64 also.\r\n\r\n### 2. Install Docker\r\n\r\nNow that you have your VPS created you will need to update it and have docker installed, the below will need to be done to have docker UP and running:\r\n\r\n```sh\r\nsudo apt-get update\r\nsudo apt-get install ca-certificates curl gnupg lsb-release\r\nsudo mkdir -p /etc/apt/keyrings\r\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\r\necho \\\r\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\r\n  jammy stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\r\nsudo apt-get update\r\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose\r\n```\r\n\r\nEverything is also explained in: [Install Docker & Docker-compose for Ubuntu](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n\r\n### 3. Install Dockge\r\n\r\nInstalling Dockge is easy. Here are the steps they lay out on their [GitHub Page](https://github.com/louislam/dockge?l#basic)\r\n\r\n```sh\r\n# Create directories that store your stacks and stores Dockge's stack\r\nmkdir -p /opt/stacks /opt/dockge\r\ncd /opt/dockge\r\n\r\n# Download the compose.yaml\r\ncurl https://raw.githubusercontent.com/louislam/dockge/master/compose.yaml --output compose.yaml\r\n\r\n# Start the server\r\ndocker compose up -d\r\n```\r\n\r\nThe default port in the YAML file is 5001 but you can change it if you want with your port. Below is the YAML file:\r\n\r\n```yaml\r\nversion: \"3.8\"\r\nservices:\r\n  dockge:\r\n    image: louislam/dockge:1\r\n    restart: unless-stopped\r\n    ports:\r\n      # Host Port : Container Port\r\n      - 5001:5001\r\n    volumes:\r\n      - /var/run/docker.sock:/var/run/docker.sock\r\n      - ./data:/app/data\r\n\r\n      # If you want to use private registries, you need to share the auth file with Dockge:\r\n      # - /root/.docker/:/root/.docker\r\n\r\n      # Stacks Directory\r\n      # ⚠️ READ IT CAREFULLY. If you did it wrong, your data could end up writing into a WRONG PATH.\r\n      # ⚠️ 1. FULL path only. No relative path (MUST)\r\n      # ⚠️ 2. Left Stacks Path === Right Stacks Path (MUST)\r\n      - /opt/stacks:/opt/stacks\r\n    environment:\r\n      # Tell Dockge where is your stacks directory\r\n      - DOCKGE_STACKS_DIR=/opt/stacks\r\n```\r\n\r\n### 4. Access Dockge\r\n\r\nTo access the dockge installation and create your first user you will need to use the server IP and the port(5001 by default) in the compose.yaml file. First, you will be prompted to create your first user. After you can go and start deploying docker compose apps.\r\n\r\n<Picture\r\n  src={imag3}\r\n  alt=\"Dockge Add Compose\"\r\n/>\r\n\r\n### 5. Adding Admin URL with SSL to Dockge and the APPS\r\n\r\nIf this is an internet-facing server and the APPs hosted on it will be internet facing then you will need to add a container that is handling the reverse proxy to your containers you can do that with CloudFlare tunnels or you can use the [nginx-proxy-manager](https://nginxproxymanager.com/)\r\nIn the video we will see how this can be done with CloudFlare Tunnels.\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## Final Notes\r\n\r\nI have only started to use Dockge and I really like it, it allows me to have also an UI to my docker compose apps. I will use it starting now and create some tutorials that will help deploy verious apps with it.\r\n\r\nDockge is a powerful and user-friendly tool that simplifies Docker management, making it an excellent alternative to Portainer for many users. Its intuitive interface and integrated features make it easier to manage your Docker containers, especially for those who prefer working with docker-compose files.\r\n\r\nAs you explore Dockge and start deploying various applications, you might be interested in discovering more Docker containers that can enhance your self-hosted setup. For a comprehensive list of useful Docker containers for your home server or personal projects, check out our guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource can provide you with ideas for new services to deploy and manage using Dockge, helping you make the most of your Docker environment.","src/content/posts/dockge-install.mdx",[871],"../../assets/images/24/01/dockge-install.jpeg","77879bbfef0993da","dockge-install.mdx","docker-containers-home-server",{id:874,data:876,body:885,filePath:886,assetImports:887,digest:889,legacyId:890,deferredRender:32},{title:877,description:878,date:879,image:880,authors:881,categories:882,tags:883,canonical:884},"Best 100+ Docker Containers for Home Server","Check out this list with 100+ docker containers that you can use on your home server.",["Date","2025-01-16T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/docker-containers.jpeg",[19],[98],[100],"https://www.bitdoze.com/docker-containers-home-server/","[Docker](https://www.docker.com/) is an open-source platform that allows you to develop, ship, and run applications in containers. Containers are lightweight, standalone, and executable packages that include everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings.\r\n\r\nAs we are in 2025, Docker containers continue to evolve and provide even more powerful solutions for home servers. The ecosystem has grown significantly, with new applications offering enhanced features, better performance, and improved security. This comprehensive list covers both well-established containers and exciting new additions that are shaping the future of home server deployments.\r\n\r\nKey benefits of Docker:\r\n\r\n- Consistency across different environments\r\n- Isolation of applications\r\n- Efficient resource utilization\r\n- Easy deployment and scaling\r\n\r\nUsing Docker containers for your home server offers several advantages:\r\n\r\n- **Simplified management**: Easily install, update, and remove applications without affecting the host system or other containers.\r\n- **Resource efficiency**: Containers share the host OS kernel, making them more lightweight than traditional virtual machines.\r\n- **Improved security**: Containers provide isolation, reducing the risk of one compromised application affecting others.\r\n- **Portability**: Easily move your applications between different systems or hardware.\r\n- **Version control**: Quickly roll back to previous versions if issues arise.\r\n\r\n## Setting Up Your Home Server\r\n\r\n### Hardware requirements\r\n\r\nTo run a Docker-based home server, you'll need:\r\n\r\n| Component | Minimum Recommendation |\r\n| --------- | ---------------------- |\r\n| CPU       | Dual-core processor    |\r\n| RAM       | 4GB                    |\r\n| Storage   | 32GB (SSD preferred)   |\r\n| Network   | Gigabit Ethernet       |\r\n\r\nNote: Actual requirements may vary depending on the number and type of containers you plan to run.\r\n\r\n> you can check [Best Mini PC For Home Servers](https://www.bitdoze.com/best-mini-pc-home-server/) to choose one.\r\n\r\n### Operating system considerations\r\n\r\nDocker can run on various operating systems, including:\r\n\r\n- Linux distributions (Ubuntu, Debian, CentOS, etc.)\r\n- Windows 10/11 Pro or Enterprise (using Hyper-V)\r\n- macOS\r\n\r\nFor home servers, a lightweight Linux distribution is often the best choice due to its lower resource overhead and better Docker support.\r\n\r\nPopular options include:\r\n\r\n- Ubuntu Server\r\n- Debian\r\n- Proxmox VE (for advanced users who want to combine Docker with virtual machines)\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### Installing Docker\r\n\r\nTo install Docker on your chosen operating system, follow these general steps:\r\n\r\n1. Update your system's package manager\r\n2. Install required dependencies\r\n3. Add Docker's official GPG key and repository\r\n4. Install Docker Engine and Docker Compose\r\n\r\nFor detailed, up-to-date installation instructions, refer to the [official Docker documentation](https://docs.docker.com/engine/install/).\r\n\r\nAfter installation, it's recommended to:\r\n\r\n- Add your user to the Docker group to avoid using sudo for Docker commands\r\n- Enable and start the Docker service\r\n- Test the installation by running a simple container, such as:\r\n  ```\r\n  docker run hello-world\r\n  ```\r\n\r\nWith Docker installed and running on your home server, you're now ready to explore the wide world of Docker containers and start building your personalized home server environment.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## Media Management Containers\r\n\r\nDocker containers offer excellent solutions for managing and organizing your media library. Let's explore some of the most popular options:\r\n\r\n### Plex\r\n\r\n[Plex](https://www.plex.tv/) is a comprehensive media server that organizes and streams your personal media collection.\r\n\r\nKey features:\r\n\r\n- Automatic metadata fetching\r\n- Transcoding for various devices\r\n- User management and sharing\r\n\r\nDocker image: `linuxserver/plex`\r\n\r\n### Jellyfin\r\n\r\n[Jellyfin](https://jellyfin.org/) is a free, open-source alternative to Plex with similar functionality.\r\n\r\nKey features:\r\n\r\n- Completely free and open-source\r\n- No central servers or tracking\r\n- Supports live TV and DVR\r\n\r\nDocker image: `jellyfin/jellyfin`\r\n\r\n### Emby\r\n\r\n[Emby](https://emby.media/) is another media server option that offers a balance between features and pricing.\r\n\r\nKey features:\r\n\r\n- Live TV and DVR support\r\n- Parental controls\r\n- Mobile sync (premium feature)\r\n\r\nDocker image: `emby/embyserver`\r\n\r\n### Comparison Table\r\n\r\n| Feature     | Plex                       | Jellyfin | Emby                       |\r\n| ----------- | -------------------------- | -------- | -------------------------- |\r\n| Cost        | Free (with premium option) | Free     | Free (with premium option) |\r\n| Open-source | No                         | Yes      | Partially                  |\r\n| Live TV     | Yes (with Plex Pass)       | Yes      | Yes                        |\r\n| Mobile sync | Yes (with Plex Pass)       | No       | Yes (with premium)         |\r\n\r\n### Sonarr\r\n\r\n[Sonarr](https://sonarr.tv/) is an automated TV show downloader and manager.\r\n\r\nKey features:\r\n\r\n- Automated TV show searching and downloading\r\n- Calendar view of upcoming episodes\r\n- Integration with media servers and download clients\r\n\r\nDocker image: `linuxserver/sonarr`\r\n\r\n### Radarr\r\n\r\n[Radarr](https://radarr.video/) is similar to Sonarr but focuses on movies instead of TV shows.\r\n\r\nKey features:\r\n\r\n- Automated movie searching and downloading\r\n- Integration with media servers and download clients\r\n- Customizable quality profiles\r\n\r\nDocker image: `linuxserver/radarr`\r\n\r\n### Lidarr\r\n\r\n[Lidarr](https://lidarr.audio/) completes the \\*arr trio by providing similar functionality for music.\r\n\r\nKey features:\r\n\r\n- Automated music searching and downloading\r\n- Artist and album management\r\n- Integration with various music sources and download clients\r\n\r\nDocker image: `linuxserver/lidarr`\r\n\r\n### Jackett\r\n\r\n[Jackett](https://github.com/Jackett/Jackett) works as a proxy server between your media management apps and torrent trackers.\r\n\r\nKey features:\r\n\r\n- Supports a wide range of torrent trackers\r\n- Provides a unified search interface\r\n- Integrates with Sonarr, Radarr, and Lidarr\r\n\r\nDocker image: `linuxserver/jackett`\r\n\r\n### Transmission\r\n\r\n[Transmission](https://transmissionbt.com/) is a lightweight and user-friendly BitTorrent client.\r\n\r\nKey features:\r\n\r\n- Web interface for remote management\r\n- Scheduling and bandwidth controls\r\n- Support for magnet links\r\n\r\nDocker image: `linuxserver/transmission`\r\n\r\n### qBittorrent\r\n\r\n[qBittorrent](https://www.qbittorrent.org/) is another popular BitTorrent client with a feature-rich web interface.\r\n\r\nKey features:\r\n\r\n- Built-in search engine\r\n- RSS feed support\r\n- IP filtering and encryption\r\n\r\nDocker image: `linuxserver/qbittorrent`\r\n\r\nThese Docker containers can work together to create a powerful, automated media management system for your home server. By combining media servers like Plex or Jellyfin with automated downloaders like Sonarr, Radarr, and Lidarr, and using Jackett as a search proxy, you can create a seamless entertainment experience tailored to your preferences. The addition of torrent clients like Transmission or qBittorrent completes the setup, allowing for efficient downloading of your desired content.\r\n\r\n## File Sharing and Sync Containers\r\n\r\nDocker containers offer excellent solutions for file sharing and synchronization across devices. Let's explore some of the most popular options:\r\n\r\n### Nextcloud\r\n\r\n[Nextcloud](https://nextcloud.com/) is a powerful, self-hosted file sync and share solution that provides a platform for collaboration and productivity.\r\n\r\nKey features:\r\n\r\n- File synchronization across devices\r\n- Collaborative document editing\r\n- Calendar and contact management\r\n- Built-in chat and video calling\r\n\r\nDocker image: `nextcloud`\r\n\r\n### Syncthing\r\n\r\n[Syncthing](https://syncthing.net/) is a continuous file synchronization program that securely synchronizes files between two or more computers in real-time.\r\n\r\nKey features:\r\n\r\n- Decentralized and peer-to-peer\r\n- End-to-end encryption\r\n- Cross-platform support\r\n- No central server required\r\n\r\nDocker image: `syncthing/syncthing`\r\n\r\n### Seafile\r\n\r\n[Seafile](https://www.seafile.com/) is an open-source file sync and share solution with high reliability and performance.\r\n\r\nKey features:\r\n\r\n- File versioning and snapshots\r\n- Selective sync\r\n- Two-factor authentication\r\n- Built-in wiki\r\n\r\nDocker image: `seafileltd/seafile-mc`\r\n\r\n### ownCloud\r\n\r\n[ownCloud](https://owncloud.com/) is another popular open-source file sync and share platform that offers a range of features for personal and business use.\r\n\r\nKey features:\r\n\r\n- File sharing and synchronization\r\n- Collaborative editing\r\n- Mobile and desktop clients\r\n- Extensible through apps\r\n\r\nDocker image: `owncloud/server`\r\n\r\n### Filebrowser\r\n\r\n[Filebrowser](https://filebrowser.org/) is a lightweight, web-based file manager that allows you to manage files and directories on your server through a clean, easy-to-use interface.\r\n\r\nKey features:\r\n\r\n- Simple and intuitive web interface\r\n- File upload and download\r\n- User management with configurable permissions\r\n- Customizable look and feel\r\n\r\nDocker image: `filebrowser/filebrowser`\r\n\r\nFor a detailed guide on how to deploy Filebrowser using Docker, check out this tutorial: [How to Deploy Filebrowser with Docker](https://www.bitdoze.com/deploy-filebrowser-docker/)\r\n\r\n\r\n### SFTPGo\r\n[SFTPGo](https://sftpgo.com/) is a fully featured and highly configurable SFTP server with optional HTTP/S, FTP/S and WebDAV support.\r\n\r\nKey features:\r\n\r\n- Virtual users and folders\r\n- Bandwidth throttling\r\n- Public key and password authentication\r\n- Web admin interface\r\n- REST API\r\n- Quota restrictions\r\n\r\nDocker image: `drakkan/sftpgo`\r\n\r\n\r\n### Comparison Table\r\n\r\n| Feature               | Nextcloud | Syncthing | Seafile | ownCloud | Filebrowser | SFTPGo |\r\n| --------------------- | --------- | --------- | ------- | -------- | ----------- | ------- |\r\n| File sync             | Yes       | Yes       | Yes     | Yes      | No          | No      |\r\n| Collaboration tools   | Yes       | No        | Limited | Yes      | No          | No      |\r\n| Self-hosted           | Yes       | Yes       | Yes     | Yes      | Yes         | Yes     |\r\n| Mobile apps           | Yes       | Yes       | Yes     | Yes      | No          | No      |\r\n| End-to-end encryption | Yes       | Yes       | Yes     | Yes      | No          | Yes     |\r\n| Open-source           | Yes       | Yes       | Yes     | Yes      | Yes         | Yes     |\r\n| Web-based file manager| Yes       | No        | Yes     | Yes      | Yes         | Yes     |\r\n\r\nThese file sharing and sync containers offer a range of options to suit different needs and preferences. Whether you're looking for a comprehensive collaboration platform like Nextcloud or ownCloud, a simple yet powerful sync solution like Syncthing, a high-performance option like Seafile, or a lightweight file manager like Filebrowser, there's a Docker container that can meet your requirements.\r\n\r\nBy setting up one of these containers on your home server, you can create your own personal cloud storage system, ensuring that your files are always accessible and synchronized across all your devices while maintaining control over your data.\r\n\r\nRemember to consider factors such as ease of use, scalability, and specific features that matter to you when choosing the right file sharing and sync solution for your home server setup.\r\n\r\n\r\n## AI Applications Containers\r\n\r\nDocker containers offer powerful solutions for running various AI applications on your home server. Let's explore some of the most popular options:\r\n\r\n### Ollama with OpenWebUI\r\n\r\n[Ollama](https://ollama.ai/) is an open-source project that allows you to run large language models locally. [OpenWebUI](https://github.com/open-webui/open-webui) provides a user-friendly interface for interacting with Ollama.\r\n\r\nKey features:\r\n\r\n- Run various large language models locally\r\n- User-friendly web interface\r\n- Customizable prompts and settings\r\n- No need for API keys or internet connection for inference\r\n\r\nDocker images:\r\n- `ollama/ollama`\r\n- `ghcr.io/open-webui/open-webui:main`\r\n\r\nFor a detailed guide on how to install Ollama with OpenWebUI using Docker, check out this tutorial: [How to Install Ollama with Docker](https://www.bitdoze.com/ollama-docker-install/)\r\n\r\n### Flowise\r\n\r\n[Flowise](https://flowiseai.com/) is an open-source UI visual tool for building LLM apps, chatbots, and agents with a drag-and-drop interface.\r\n\r\nKey features:\r\n\r\n- Visual workflow builder\r\n- Integration with various AI models and tools\r\n- Customizable components\r\n- API generation for created flows\r\n\r\nDocker image: `flowiseai/flowise`\r\n\r\nFor a step-by-step installation guide, see: [How to Install Flowise AI](https://www.bitdoze.com/flowiseai-install/)\r\n\r\n### Langflow\r\n\r\n[Langflow](https://www.langflow.org/) is an GUI for LangChain, designed to provide an easy way to experiment and prototype flows.\r\n\r\nKey features:\r\n\r\n- Drag-and-drop interface for LangChain components\r\n- Code export functionality\r\n- Easy integration with various LLMs and tools\r\n- Customizable nodes and edges\r\n\r\nDocker image: `logspace/langflow`\r\n\r\nLearn how to set up Langflow with this guide: [How to Install Langflow with Docker](https://www.bitdoze.com/langflow-docker-install/)\r\n\r\n### Langfuse\r\n\r\n[Langfuse](https://langfuse.com/) is an open-source observability and analytics solution for LLM applications.\r\n\r\nKey features:\r\n\r\n- Tracing and logging for LLM interactions\r\n- Performance monitoring and analytics\r\n- Integration with popular LLM frameworks\r\n- Customizable dashboards\r\n\r\nDocker image: `langfuse/langfuse`\r\n\r\nFor installation instructions, check out: [How to Install Langfuse with Docker](https://www.bitdoze.com/langfuse-docker-install/)\r\n\r\n### LiteLLM\r\n\r\n[LiteLLM](https://github.com/BerriAI/litellm) is a lightweight package to standardize inputs and outputs across LLM providers.\r\n\r\nKey features:\r\n\r\n- Unified interface for multiple LLM providers\r\n- Caching and rate limiting\r\n- Logging and monitoring\r\n- Easy integration with various AI applications\r\n\r\nDocker image: `ghcr.io/berriai/litellm`\r\n\r\nFor a guide on setting up LiteLLM, see: [How to Install LiteLLM with Docker](https://www.bitdoze.com/litellm-docker-install/)\r\n\r\n### Comparison Table\r\n\r\n| Feature | Ollama + OpenWebUI | Flowise | Langflow | Langfuse | LiteLLM |\r\n|---------|-------------------|---------|----------|----------|---------|\r\n| Primary Function | Local LLM Inference | LLM App Builder | LangChain GUI | LLM Observability | LLM Standardization |\r\n| User Interface | Web-based | Web-based | Web-based | Web-based | API |\r\n| Local Models | Yes | No | No | N/A | Depends on setup |\r\n| Visualization | Basic chat | Flow diagram | Flow diagram | Analytics dashboards | N/A |\r\n| Extensibility | Limited | High | High | High | High |\r\n\r\nThese AI application containers can work together to create a powerful ecosystem for developing, running, and monitoring AI applications:\r\n\r\n1. Use Ollama with OpenWebUI for running and interacting with local language models.\r\n2. Implement Flowise or Langflow for building complex LLM applications with a visual interface.\r\n3. Set up Langfuse to monitor and analyze the performance of your LLM applications.\r\n4. Utilize LiteLLM to standardize interactions with various LLM providers and simplify your codebase.\r\n\r\nBy combining these containers, you can create a robust AI development and deployment environment that:\r\n\r\n- Allows for local LLM inference without relying on external APIs\r\n- Provides tools for rapid prototyping and development of LLM applications\r\n- Offers monitoring and analytics capabilities for optimizing your AI systems\r\n- Simplifies integration with multiple LLM providers\r\n\r\nRemember to consider the following when setting up your AI application environment:\r\n\r\n- Ensure you have sufficient computational resources, especially for running local LLM models\r\n- Implement proper security measures, especially if exposing services to the internet\r\n- Regularly update your containers to benefit from the latest features and security patches\r\n- Be mindful of licensing and usage terms, especially when using commercial LLM models\r\n\r\nWith these Docker containers, you can build a powerful, flexible, and efficient AI application environment on your home server, enabling you to explore and leverage the latest advancements in language models and AI technologies.\r\n\r\n\r\n## Home Automation Containers\r\n\r\nDocker containers offer powerful solutions for home automation, allowing you to control and monitor various aspects of your smart home. Let's explore some of the most popular options:\r\n\r\n### Home Assistant\r\n\r\n[Home Assistant](https://www.home-assistant.io/) is an open-source home automation platform that puts local control and privacy first.\r\n\r\nKey features:\r\n\r\n- Supports over 1,000 integrations\r\n- Powerful automation engine\r\n- Local processing for faster response times\r\n- Customizable dashboard\r\n\r\nDocker image: `homeassistant/home-assistant`\r\n\r\n### Node-RED\r\n\r\n[Node-RED](https://nodered.org/) is a flow-based programming tool for connecting hardware devices, APIs, and online services.\r\n\r\nKey features:\r\n\r\n- Visual programming interface\r\n- Extensive library of nodes\r\n- Easy integration with IoT devices\r\n- Customizable dashboard\r\n\r\nDocker image: `nodered/node-red`\r\n\r\n### Mosquitto MQTT Broker\r\n\r\n[Eclipse Mosquitto](https://mosquitto.org/) is an open-source message broker that implements the MQTT protocol, widely used in IoT and home automation.\r\n\r\nKey features:\r\n\r\n- Lightweight and efficient\r\n- Supports MQTT v3.1 and v3.1.1\r\n- SSL/TLS encryption\r\n- Authentication and access control\r\n\r\nDocker image: `eclipse-mosquitto`\r\n\r\n### OpenHAB\r\n\r\n[openHAB](https://www.openhab.org/) (Open Home Automation Bus) is a vendor and technology-agnostic open-source automation software for your home.\r\n\r\nKey features:\r\n\r\n- Supports over 200 technologies and systems\r\n- Rule-based automation engine\r\n- Mobile apps for iOS and Android\r\n- Customizable user interfaces\r\n\r\nDocker image: `openhab/openhab`\r\n\r\n### Comparison Table\r\n\r\n| Feature          | Home Assistant           | Node-RED               | Mosquitto                      | openHAB                  |\r\n| ---------------- | ------------------------ | ---------------------- | ------------------------------ | ------------------------ |\r\n| Primary Function | Home Automation Platform | Flow-based Programming | MQTT Broker                    | Home Automation Platform |\r\n| User Interface   | Web-based                | Web-based              | Command-line                   | Web-based                |\r\n| Automation       | Yes                      | Yes                    | No (facilitates communication) | Yes                      |\r\n| Device Support   | Extensive                | Extensive              | N/A (protocol-based)           | Extensive                |\r\n| Customization    | High                     | High                   | Limited                        | High                     |\r\n\r\nThese home automation containers can work together to create a powerful and flexible smart home system:\r\n\r\n1. Use Home Assistant or openHAB as your central home automation platform.\r\n2. Implement Node-RED for complex automations and integrations.\r\n3. Set up Mosquitto MQTT Broker to facilitate communication between various smart devices and your automation platform.\r\n\r\nBy combining these containers, you can create a robust home automation system that:\r\n\r\n- Controls your smart lights, thermostats, and other IoT devices\r\n- Automates routines based on time, presence, or other triggers\r\n- Monitors energy usage and environmental conditions\r\n- Provides a unified interface for managing your entire smart home\r\n\r\nRemember to secure your home automation setup by:\r\n\r\n- Using strong passwords\r\n- Enabling two-factor authentication where possible\r\n- Keeping your containers and host system updated\r\n- Implementing network segmentation for IoT devices\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and secure home automation system that puts you in control of your smart home.\r\n\r\n## Network Management Containers\r\n\r\nDocker containers offer powerful solutions for managing and optimizing your home network. Let's explore some of the most popular options:\r\n\r\n### Pi-hole\r\n\r\n[Pi-hole](https://pi-hole.net/) is a network-wide ad blocker that acts as a DNS sinkhole to block unwanted content.\r\n\r\nKey features:\r\n\r\n- Network-wide ad blocking\r\n- Customizable blocklists\r\n- Detailed statistics and reporting\r\n- DHCP server functionality\r\n\r\nDocker image: `pihole/pihole`\r\n\r\n### Unbound\r\n\r\n[Unbound](https://nlnetlabs.nl/projects/unbound/about/) is a validating, recursive, and caching DNS resolver.\r\n\r\nKey features:\r\n\r\n- DNSSEC validation\r\n- Improved privacy and security\r\n- Caching for faster DNS resolution\r\n- Can work alongside Pi-hole for enhanced functionality\r\n\r\nDocker image: `mvance/unbound`\r\n\r\n### Traefik\r\n\r\n[Traefik](https://traefik.io/) is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy.\r\n\r\nKey features:\r\n\r\n- Automatic SSL certificate generation with Let's Encrypt\r\n- Dynamic configuration\r\n- Docker integration\r\n- Metrics and monitoring\r\n\r\nDocker image: `traefik`\r\n\r\n### Nginx Proxy Manager\r\n\r\n[Nginx Proxy Manager](https://nginxproxymanager.com/) provides a user-friendly interface to manage Nginx proxy hosts with SSL termination.\r\n\r\nKey features:\r\n\r\n- Easy-to-use web interface\r\n- Automatic SSL certificate management\r\n- Access lists and basic authentication\r\n- Docker container support\r\n\r\nDocker image: `jc21/nginx-proxy-manager`\r\n\r\n### Portainer\r\n\r\n[Portainer](https://www.portainer.io/) is a lightweight management UI for Docker environments.\r\n\r\nKey features:\r\n\r\n- Web-based Docker management\r\n- Container and image management\r\n- User authentication and role-based access control\r\n- Monitoring and logging\r\n\r\nDocker image: `portainer/portainer-ce`\r\n\r\n### Dockge\r\n\r\n[Dockge](https://github.com/louislam/dockge) is a simple, lightweight, and powerful Docker compose stack manager and UI.\r\n\r\nKey features:\r\n\r\n- User-friendly web interface for managing Docker compose stacks\r\n- Easy deployment and management of Docker containers\r\n- Built-in code editor for compose files\r\n- Support for environment variables and secrets\r\n\r\nDocker image: `louislam/dockge`\r\n\r\nFor a detailed guide on how to install and set up Dockge, check out this tutorial: [How to Install Dockge](https://www.bitdoze.com/dockge-install/)\r\n\r\n### Comparison Table\r\n\r\n| Feature            | Pi-hole     | Unbound      | Traefik       | Nginx Proxy Manager | Portainer         | Dockge                   |\r\n| ------------------ | ----------- | ------------ | ------------- | ------------------- | ----------------- | ------------------------ |\r\n| Primary Function   | Ad Blocking | DNS Resolver | Reverse Proxy | Reverse Proxy       | Docker Management | Docker Compose Management|\r\n| User Interface     | Web-based   | Command-line | Web-based     | Web-based           | Web-based         | Web-based                |\r\n| SSL Management     | No          | No           | Yes           | Yes                 | No                | No                       |\r\n| Docker Integration | N/A         | N/A          | Yes           | Yes                 | Yes               | Yes                      |\r\n| Ease of Use        | High        | Medium       | Medium        | High                | High              | High                     |\r\n\r\nThese network management containers can work together to create a powerful and secure home network setup:\r\n\r\n1. Use Pi-hole for network-wide ad blocking and DNS management.\r\n2. Implement Unbound as a recursive DNS resolver for improved privacy and security.\r\n3. Set up Traefik or Nginx Proxy Manager to handle reverse proxy and SSL termination for your services.\r\n4. Use Portainer to manage your Docker containers and images easily.\r\n5. Implement Dockge for simplified management of Docker compose stacks.\r\n\r\nBy combining these containers, you can create a robust network management system that:\r\n\r\n- Blocks ads and unwanted content across all devices on your network\r\n- Improves DNS resolution speed and security\r\n- Provides easy access to your self-hosted services with automatic SSL management\r\n- Simplifies Docker container and stack management\r\n\r\nRemember to secure your network management setup by:\r\n\r\n- Using strong passwords for all interfaces\r\n- Keeping your containers and host system updated\r\n- Implementing proper firewall rules\r\n- Regularly reviewing and updating your configurations\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and secure network management system that gives you full control over your home network infrastructure.\r\n\r\n## Monitoring and Analytics Containers\r\n\r\nMonitoring and analytics are crucial for maintaining a healthy and efficient home server setup. Docker containers offer powerful solutions for tracking system performance, resource usage, and network activity. Let's explore some of the most popular options:\r\n\r\n### Grafana\r\n\r\n[Grafana](https://grafana.com/) is an open-source platform for monitoring and observability, allowing you to query, visualize, and alert on metrics from various data sources.\r\n\r\nKey features:\r\n\r\n- Beautiful and customizable dashboards\r\n- Support for multiple data sources (Prometheus, InfluxDB, etc.)\r\n- Alerting and notification system\r\n- User authentication and team-based access control\r\n\r\nDocker image: `grafana/grafana`\r\n\r\n### Prometheus\r\n\r\n[Prometheus](https://prometheus.io/) is an open-source systems monitoring and alerting toolkit, designed for reliability and scalability.\r\n\r\nKey features:\r\n\r\n- Multi-dimensional data model with time series data\r\n- Flexible query language (PromQL)\r\n- Pull model for collecting metrics\r\n- Service discovery and dynamic configuration\r\n\r\nDocker image: `prom/prometheus`\r\n\r\n### InfluxDB\r\n\r\n[InfluxDB](https://www.influxdata.com/) is an open-source time series database designed to handle high write and query loads.\r\n\r\nKey features:\r\n\r\n- Purpose-built for time series data\r\n- High performance and data compression\r\n- SQL-like query language\r\n- Retention policies and continuous queries\r\n\r\nDocker image: `influxdb`\r\n\r\n### Telegraf\r\n\r\n[Telegraf](https://www.influxdata.com/time-series-platform/telegraf/) is an agent for collecting, processing, aggregating, and writing metrics.\r\n\r\nKey features:\r\n\r\n- Plugin-driven for both inputs and outputs\r\n- Supports a wide variety of services and systems\r\n- Low memory footprint\r\n- Can be used with InfluxDB and other time series databases\r\n\r\nDocker image: `telegraf`\r\n\r\n### Netdata\r\n\r\n[Netdata](https://www.netdata.cloud/) is a distributed, real-time performance and health monitoring system for systems, hardware, containers, and applications.\r\n\r\nKey features:\r\n\r\n- Real-time, per-second metrics\r\n- Highly optimized data collection and visualization\r\n- Automatic configuration and zero maintenance\r\n- Extensible through plugins\r\n\r\nDocker image: `netdata/netdata`\r\n\r\n\r\n### Beszel\r\n\r\n[Beszel](https://beszel.dev/) is a lightweight, modern monitoring solution designed for small to medium-sized deployments.\r\n\r\nKey features:\r\n\r\n- Real-time server metrics monitoring\r\n- Notifications\r\n- Docker container statistics\r\n- Custom notification channels\r\n- Low resource footprint\r\n- Simple deployment process\r\n- Public key authentication\r\n- Multi-server support\r\n\r\nDocker image: `henrygd/beszel`\r\n\r\n> For more details on server monitoring you can check: [How To Monitor Server and Docker Resources:CPU,Memory...](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### Comparison Table\r\n\r\n| Feature          | Grafana       | Prometheus            | InfluxDB             | Telegraf        | Netdata              |\r\n| ---------------- | ------------- | --------------------- | -------------------- | --------------- | -------------------- |\r\n| Primary Function | Visualization | Monitoring & Alerting | Time Series Database | Data Collection | Real-time Monitoring |\r\n| Data Sources     | Multiple      | Self & Exporters      | Self                 | Multiple        | Self                 |\r\n| Visualization    | Yes           | Basic                 | No                   | No              | Yes                  |\r\n| Alerting         | Yes           | Yes                   | No                   | No              | Yes                  |\r\n| Ease of Setup    | Medium        | Medium                | Easy                 | Easy            | Very Easy            |\r\n\r\nThese monitoring and analytics containers can work together to create a comprehensive monitoring solution for your home server:\r\n\r\n1. Use Prometheus as your primary metrics collection and storage system.\r\n2. Implement Grafana for creating beautiful dashboards and visualizations.\r\n3. Set up InfluxDB for storing high-resolution metrics data.\r\n4. Use Telegraf to collect metrics from various sources and send them to InfluxDB or Prometheus.\r\n5. Implement Netdata for real-time, per-second monitoring of your systems.\r\n\r\nBy combining these containers, you can create a robust monitoring and analytics system that:\r\n\r\n- Provides real-time insights into your server's performance\r\n- Allows you to create custom dashboards for different aspects of your setup\r\n- Sends alerts when issues arise or thresholds are exceeded\r\n- Helps you identify trends and optimize your server's resources\r\n\r\nRemember to consider the following when setting up your monitoring solution:\r\n\r\n- Properly configure retention policies to manage data storage\r\n- Set up appropriate alerting thresholds to avoid alert fatigue\r\n- Regularly review and update your dashboards and metrics collection\r\n- Ensure your monitoring setup doesn't overly tax your system resources\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and insightful monitoring and analytics system that gives you full visibility into your home server's performance and health.\r\n\r\n## Security and Privacy Containers\r\n\r\nEnsuring the security and privacy of your home server is crucial. Docker containers offer several solutions to help protect your data and network. Let's explore some of the most popular options:\r\n\r\n### Bitwarden\r\n\r\n[Bitwarden](https://bitwarden.com/) is an open-source password management solution that allows you to store, manage, and share sensitive information securely.\r\n\r\nKey features:\r\n\r\n- End-to-end encryption\r\n- Self-hosting option for complete control\r\n- Cross-platform support (desktop, mobile, browser extensions)\r\n- Secure password sharing and organization\r\n\r\nDocker image: `bitwardenrs/server`\r\n\r\n### OpenVPN\r\n\r\n[OpenVPN](https://openvpn.net/) is a popular open-source VPN solution that allows you to create secure connections to your home network from remote locations.\r\n\r\nKey features:\r\n\r\n- Strong encryption and authentication\r\n- Supports various authentication methods\r\n- Cross-platform compatibility\r\n- Extensible through plugins\r\n\r\nDocker image: `kylemanna/openvpn`\r\n\r\n### WireGuard\r\n\r\n[WireGuard](https://www.wireguard.com/) is a modern, fast, and secure VPN protocol that aims to be simpler and more efficient than traditional VPN solutions.\r\n\r\nKey features:\r\n\r\n- Lightweight and high-performance\r\n- Strong, modern cryptography\r\n- Simple configuration\r\n- Cross-platform support\r\n\r\nDocker image: `linuxserver/wireguard`\r\n\r\n### Fail2Ban\r\n\r\n[Fail2Ban](https://www.fail2ban.org/) is an intrusion prevention software framework that protects computer servers from brute-force attacks.\r\n\r\nKey features:\r\n\r\n- Monitors log files for suspicious activity\r\n- Automatically blocks IP addresses of potential attackers\r\n- Customizable rules and actions\r\n- Supports various services (SSH, Apache, etc.)\r\n\r\nDocker image: `crazymax/fail2ban`\r\n\r\n### Comparison Table\r\n\r\n| Feature          | Bitwarden           | OpenVPN | WireGuard | Fail2Ban             |\r\n| ---------------- | ------------------- | ------- | --------- | -------------------- |\r\n| Primary Function | Password Management | VPN     | VPN       | Intrusion Prevention |\r\n| Encryption       | Yes                 | Yes     | Yes       | N/A                  |\r\n| Self-hosting     | Yes                 | Yes     | Yes       | Yes                  |\r\n| Ease of Setup    | Medium              | Medium  | Easy      | Medium               |\r\n| Cross-platform   | Yes                 | Yes     | Yes       | Linux-focused        |\r\n\r\nThese security and privacy containers can work together to create a comprehensive protection system for your home server:\r\n\r\n1. Use Bitwarden to securely store and manage passwords for all your services.\r\n2. Implement OpenVPN or WireGuard to create secure remote access to your home network.\r\n3. Set up Fail2Ban to protect your server from brute-force attacks and other malicious activities.\r\n\r\nBy combining these containers, you can create a robust security and privacy system that:\r\n\r\n- Keeps your sensitive information encrypted and easily accessible\r\n- Provides secure remote access to your home network\r\n- Protects your server from common attack vectors\r\n\r\nRemember to consider the following when setting up your security and privacy solution:\r\n\r\n- Regularly update your containers and host system to patch security vulnerabilities\r\n- Use strong, unique passwords for all services and accounts\r\n- Enable two-factor authentication where possible\r\n- Regularly review logs and security reports\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and secure system that helps protect your home server and personal data from various threats.\r\n\r\n## Productivity Containers\r\n\r\nDocker containers offer a range of solutions to boost your productivity and organization. Let's explore some of the most popular options:\r\n\r\n### Bookstack\r\n\r\n[Bookstack](https://www.bookstackapp.com/) is a free and open-source wiki system that provides a simple, self-hosted platform for organizing and storing information.\r\n\r\nKey features:\r\n\r\n- User-friendly interface\r\n- Markdown support\r\n- File attachments\r\n- Full-text search\r\n\r\nDocker image: `linuxserver/bookstack`\r\n\r\n### Joplin\r\n\r\n[Joplin](https://joplinapp.org/) is an open-source note-taking and to-do application with synchronization capabilities.\r\n\r\nKey features:\r\n\r\n- End-to-end encryption\r\n- Markdown support\r\n- Web clipper for saving web pages\r\n- Cross-platform (desktop, mobile, terminal)\r\n\r\nDocker image: `joplin/server`\r\n\r\n### Kanboard\r\n\r\n[Kanboard](https://kanboard.org/) is a free and open-source Kanban project management software.\r\n\r\nKey features:\r\n\r\n- Visual task board\r\n- Drag and drop tasks\r\n- Multiple projects and users\r\n- Automatic actions and subtasks\r\n\r\nDocker image: `kanboard/kanboard`\r\n\r\n### Wekan\r\n\r\n[Wekan](https://wekan.github.io/) is an open-source kanban board that allows real-time collaboration.\r\n\r\nKey features:\r\n\r\n- Customizable boards and lists\r\n- Card attachments and comments\r\n- User management and permissions\r\n- REST API for integrations\r\n\r\nDocker image: `wekanteam/wekan`\r\n\r\n### Docmost\r\n\r\n[Docmost](https://docmost.com/) is a self-hosted, open-source knowledge base and documentation platform that helps teams organize and share information efficiently.\r\n\r\nKey features:\r\n\r\n- Markdown and WYSIWYG editor support\r\n- Version history and document comparison\r\n- Full-text search\r\n- User and group management\r\n- Custom branding options\r\n\r\nDocker image: `docmost/docmost`\r\n\r\nFor a detailed guide on how to install Docmost using Docker, check out this tutorial: [How to Install Docmost with Docker](https://www.bitdoze.com/docmost-docker-install/)\r\n\r\n### Comparison Table\r\n\r\n| Feature          | Bookstack | Joplin      | Kanboard           | Wekan             | Docmost           |\r\n| ---------------- | --------- | ----------- | ------------------ | ----------------- | ----------------- |\r\n| Primary Function | Wiki      | Note-taking | Project Management | Kanban Board      | Knowledge Base    |\r\n| Collaboration    | Yes       | Limited     | Yes                | Yes               | Yes               |\r\n| Encryption       | No        | Yes         | No                 | No                | No                |\r\n| Mobile App       | No        | Yes         | No                 | Yes (third-party) | No                |\r\n| Markdown Support | Yes       | Yes         | Limited            | Yes               | Yes               |\r\n| Version History  | No        | Yes         | No                 | No                | Yes               |\r\n\r\nThese productivity containers can work together to create a comprehensive system for managing information, tasks, and projects:\r\n\r\n1. Use Bookstack or Docmost as your central knowledge base and documentation system.\r\n2. Implement Joplin for personal note-taking and information gathering.\r\n3. Set up Kanboard or Wekan (or both) for project management and task tracking.\r\n\r\nBy combining these containers, you can create a robust productivity system that:\r\n\r\n- Centralizes your team's knowledge and documentation\r\n- Provides a secure platform for personal note-taking and information management\r\n- Offers visual project management tools for better task organization and collaboration\r\n- Enables efficient information sharing and collaboration within your team\r\n\r\nRemember to consider the following when setting up your productivity solution:\r\n\r\n- Integrate these tools into your daily workflow to maximize their benefits\r\n- Regularly backup your data to prevent loss of important information\r\n- Set up proper user permissions to ensure data security and privacy\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) to access these services securely from outside your network\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and efficient productivity system that helps you stay organized and focused on your tasks and projects.\r\n\r\n## Development Containers\r\n\r\nDocker containers offer excellent solutions for setting up development environments and continuous integration/continuous deployment (CI/CD) pipelines. Let's explore some of the most popular options:\r\n\r\n### GitLab\r\n\r\n[GitLab](https://about.gitlab.com/) is a complete DevOps platform that combines Git repository management, issue tracking, CI/CD, and more.\r\n\r\nKey features:\r\n\r\n- Git repository management\r\n- Built-in CI/CD pipelines\r\n- Issue tracking and project management\r\n- Container registry\r\n\r\nDocker image: `gitlab/gitlab-ce`\r\n\r\n### Jenkins\r\n\r\n[Jenkins](https://www.jenkins.io/) is a popular open-source automation server that enables developers to build, test, and deploy their software.\r\n\r\nKey features:\r\n\r\n- Extensible through plugins\r\n- Distributed builds\r\n- Pipeline support\r\n- Easy configuration via web interface\r\n\r\nDocker image: `jenkins/jenkins`\r\n\r\n### Gitea\r\n\r\n[Gitea](https://gitea.io/) is a lightweight, self-hosted Git service written in Go, designed to be easy to install and use.\r\n\r\nKey features:\r\n\r\n- Git repository hosting\r\n- Issue tracking and pull requests\r\n- Webhooks and API\r\n- Low resource requirements\r\n\r\nDocker image: `gitea/gitea`\r\n\r\n### Drone CI\r\n\r\n[Drone](https://www.drone.io/) is a modern CI/CD platform that uses simple YAML configuration files to define and execute pipelines.\r\n\r\nKey features:\r\n\r\n- Docker-native pipeline execution\r\n- Parallel and distributed builds\r\n- Extensible plugin system\r\n- Built-in secret management\r\n\r\nDocker image: `drone/drone`\r\n\r\n### Comparison Table\r\n\r\n| Feature            | GitLab            | Jenkins | Gitea       | Drone CI |\r\n| ------------------ | ----------------- | ------- | ----------- | -------- |\r\n| Primary Function   | All-in-one DevOps | CI/CD   | Git Hosting | CI/CD    |\r\n| Repository Hosting | Yes               | No      | Yes         | No       |\r\n| Built-in CI/CD     | Yes               | Yes     | No          | Yes      |\r\n| Resource Usage     | High              | Medium  | Low         | Low      |\r\n| Extensibility      | High              | High    | Medium      | High     |\r\n\r\nThese development containers can work together to create a comprehensive development environment:\r\n\r\n1. Use GitLab or Gitea for source code management and repository hosting.\r\n2. Implement Jenkins or Drone CI for continuous integration and deployment pipelines.\r\n\r\nBy combining these containers, you can create a robust development system that:\r\n\r\n- Provides version control and collaboration tools for your projects\r\n- Automates your build, test, and deployment processes\r\n- Offers flexibility in choosing the right tools for your specific needs\r\n\r\nRemember to consider the following when setting up your development environment:\r\n\r\n- Ensure proper backup and disaster recovery plans for your repositories and build artifacts\r\n- Implement security best practices, such as access controls and secrets management\r\n- Regularly update your containers and plugins to benefit from the latest features and security patches\r\n- Consider resource requirements when choosing between lightweight options like Gitea and more comprehensive solutions like GitLab\r\n\r\nWith these Docker containers, you can build a powerful, customizable, and efficient development environment that streamlines your software development lifecycle and improves collaboration among team members.\r\n\r\n## Database Containers\r\n\r\nDocker containers offer excellent solutions for running various database systems. Let's explore some of the most popular options:\r\n\r\n### MariaDB\r\n\r\n[MariaDB](https://mariadb.org/) is a community-developed fork of MySQL that aims to remain free and open-source software.\r\n\r\nKey features:\r\n\r\n- Drop-in replacement for MySQL\r\n- Enhanced performance and features\r\n- Strong data consistency and integrity\r\n- Galera Cluster for multi-master replication\r\n\r\nDocker image: `mariadb`\r\n\r\n### PostgreSQL\r\n\r\n[PostgreSQL](https://www.postgresql.org/) is a powerful, open-source object-relational database system with a strong reputation for reliability and data integrity.\r\n\r\nKey features:\r\n\r\n- Advanced SQL support\r\n- ACID compliance\r\n- Extensible through custom functions and data types\r\n- Full-text search capabilities\r\n\r\nDocker image: `postgres`\r\n\r\n### MongoDB\r\n\r\n[MongoDB](https://www.mongodb.com/) is a popular NoSQL database that provides high performance, high availability, and easy scalability.\r\n\r\nKey features:\r\n\r\n- Document-oriented storage\r\n- Flexible schema design\r\n- Powerful query language\r\n- Built-in sharding and replication\r\n\r\nDocker image: `mongo`\r\n\r\n### Redis\r\n\r\n[Redis](https://redis.io/) is an open-source, in-memory data structure store, used as a database, cache, and message broker.\r\n\r\nKey features:\r\n\r\n- High performance and low latency\r\n- Supports various data structures\r\n- Built-in replication and clustering\r\n- Pub/sub messaging paradigm\r\n\r\nDocker image: `redis`\r\n\r\n### Comparison Table\r\n\r\n| Feature         | MariaDB         | PostgreSQL            | MongoDB                           | Redis                        |\r\n| --------------- | --------------- | --------------------- | --------------------------------- | ---------------------------- |\r\n| Type            | Relational      | Relational            | Document-oriented                 | Key-value                    |\r\n| SQL Support     | Yes             | Yes                   | No (but has query language)       | Limited                      |\r\n| ACID Compliance | Yes             | Yes                   | Optional                          | No                           |\r\n| Scalability     | Good            | Good                  | Excellent                         | Excellent                    |\r\n| Use Cases       | General-purpose | Complex queries, OLTP | Flexible schema, large-scale apps | Caching, real-time analytics |\r\n\r\nThese database containers can be used individually or in combination to support various applications and services:\r\n\r\n- Use MariaDB or PostgreSQL for traditional relational database needs\r\n- Implement MongoDB for applications requiring flexible schema and horizontal scalability\r\n- Utilize Redis for caching, session management, and real-time data processing\r\n\r\nWhen setting up database containers, consider the following best practices:\r\n\r\n- Use volumes to persist data outside the container\r\n- Implement regular backups and test restoration procedures\r\n- Configure appropriate resource limits (CPU, memory) for each container\r\n- Use Docker networks to isolate database containers from public access\r\n- Regularly update your database containers to benefit from security patches and new features\r\n\r\nExample docker-compose snippet for setting up a MariaDB container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  mariadb:\r\n    image: mariadb:latest\r\n    container_name: mariadb\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD: your_root_password\r\n      MYSQL_DATABASE: your_database_name\r\n      MYSQL_USER: your_username\r\n      MYSQL_PASSWORD: your_password\r\n    volumes:\r\n      - ./mariadb_data:/var/lib/mysql\r\n    ports:\r\n      - \"3306:3306\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage various database systems for your home server, providing robust data storage and management capabilities for your applications and services.\r\n\r\n## Backup and Recovery Containers\r\n\r\nEnsuring the safety and recoverability of your data is crucial for any home server setup. Docker containers offer several excellent solutions for backup and recovery. Let's explore some of the most popular options:\r\n\r\n### Duplicati\r\n\r\n[Duplicati](https://www.duplicati.com/) is a free, open-source backup software that securely stores encrypted, incremental, compressed backups on cloud storage services and remote file servers.\r\n\r\nKey features:\r\n\r\n- Strong encryption (AES-256)\r\n- Incremental backups\r\n- Supports various cloud storage providers\r\n- Web-based user interface\r\n\r\nDocker image: `linuxserver/duplicati`\r\n\r\n### Restic\r\n\r\n[Restic](https://restic.net/) is a fast, secure, and efficient backup program that supports multiple storage backends.\r\n\r\nKey features:\r\n\r\n- Deduplication and compression\r\n- Encryption by default\r\n- Fast incremental backups\r\n- Support for various storage backends (local, SFTP, S3, etc.)\r\n\r\nDocker image: `restic/restic`\r\n\r\n### Borgmatic\r\n\r\n[Borgmatic](https://torsion.org/borgmatic/) is a simple, configuration-driven backup software that uses BorgBackup as its backend.\r\n\r\nKey features:\r\n\r\n- Deduplication and compression\r\n- Encryption\r\n- Flexible retention policies\r\n- Hooks for pre and post-backup actions\r\n\r\nDocker image: `b3vis/borgmatic`\r\n\r\n### Veeam\r\n\r\n[Veeam](https://www.veeam.com/) offers various backup solutions, including Veeam Backup & Replication Community Edition, which is free for up to 10 instances.\r\n\r\nKey features:\r\n\r\n- Full, incremental, and synthetic full backups\r\n- Instant VM recovery\r\n- Built-in WAN acceleration\r\n- Comprehensive reporting\r\n\r\nNote: Veeam doesn't have an official Docker image, but you can run it in a VM alongside your Docker host.\r\n\r\n### Comparison Table\r\n\r\n| Feature               | Duplicati | Restic | Borgmatic | Veeam  |\r\n| --------------------- | --------- | ------ | --------- | ------ |\r\n| Encryption            | Yes       | Yes    | Yes       | Yes    |\r\n| Deduplication         | Yes       | Yes    | Yes       | Yes    |\r\n| Incremental Backups   | Yes       | Yes    | Yes       | Yes    |\r\n| Web UI                | Yes       | No     | No        | Yes    |\r\n| Cloud Storage Support | Extensive | Good   | Limited   | Good   |\r\n| Ease of Use           | High      | Medium | Medium    | Medium |\r\n\r\nThese backup and recovery containers can be used to create a comprehensive backup strategy for your home server:\r\n\r\n1. Use Duplicati for user-friendly, web-based backups to various cloud storage providers.\r\n2. Implement Restic for fast, efficient backups to local or remote storage.\r\n3. Set up Borgmatic for flexible, configuration-driven backups using BorgBackup.\r\n4. Consider Veeam for more advanced backup and recovery needs, especially if you're running VMs.\r\n\r\nWhen setting up your backup solution, consider the following best practices:\r\n\r\n- Follow the 3-2-1 backup rule: Keep at least three copies of your data, on two different storage types, with one copy off-site.\r\n- Regularly test your backups by performing test restores.\r\n- Encrypt your backups, especially when storing them off-site or in the cloud.\r\n- Set up automated backup schedules to ensure consistent backups.\r\n- Monitor your backup jobs and set up notifications for successful completions and failures.\r\n\r\nExample docker-compose snippet for setting up a Duplicati container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  duplicati:\r\n    image: linuxserver/duplicati\r\n    container_name: duplicati\r\n    environment:\r\n      - PUID=1000\r\n      - PGID=1000\r\n      - TZ=Your/Timezone\r\n    volumes:\r\n      - ./duplicati_config:/config\r\n      - /path/to/source/files:/source\r\n      - /path/to/backups:/backups\r\n    ports:\r\n      - 8200:8200\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage robust backup and recovery solutions for your home server, ensuring that your important data is protected and recoverable in case of any issues.\r\n\r\n## Personal Finance Containers\r\n\r\nDocker containers offer several solutions for managing personal finances. Let's explore some of the most popular options:\r\n\r\n### Firefly III\r\n\r\n[Firefly III](https://www.firefly-iii.org/) is a free and open-source personal finance manager.\r\n\r\nKey features:\r\n\r\n- Double-entry bookkeeping system\r\n- Budgeting and financial goal tracking\r\n- Bill management and recurring transactions\r\n- Detailed reports and charts\r\n\r\nDocker image: `fireflyiii/core`\r\n\r\n### GnuCash\r\n\r\n[GnuCash](https://www.gnucash.org/) is a free, open-source accounting software designed for personal and small business use.\r\n\r\nKey features:\r\n\r\n- Double-entry bookkeeping\r\n- Scheduled transactions\r\n- Financial reports and graphs\r\n- Multi-currency support\r\n\r\nDocker image: `jrwrigh/gnucash`\r\n\r\n### HomeBank\r\n\r\n[HomeBank](http://homebank.free.fr/) is a free, easy-to-use personal accounting software.\r\n\r\nKey features:\r\n\r\n- Multiple account management\r\n- Transaction categorization\r\n- Budgeting tools\r\n- Graphical reports\r\n\r\nDocker image: `linuxserver/homebank`\r\n\r\n### Comparison Table\r\n\r\n| Feature        | Firefly III            | GnuCash | HomeBank |\r\n| -------------- | ---------------------- | ------- | -------- |\r\n| Web-based      | Yes                    | No      | No       |\r\n| Double-entry   | Yes                    | Yes     | No       |\r\n| Mobile App     | No (but API available) | No      | No       |\r\n| Multi-currency | Yes                    | Yes     | Yes      |\r\n| Ease of Use    | Medium                 | Medium  | High     |\r\n\r\nThese personal finance containers can help you manage your finances more effectively:\r\n\r\n1. Use Firefly III for a comprehensive, web-based personal finance management solution.\r\n2. Implement GnuCash for more advanced accounting features and small business use.\r\n3. Set up HomeBank for a simple, user-friendly approach to personal finance tracking.\r\n\r\nWhen setting up your personal finance solution, consider the following:\r\n\r\n- Ensure proper security measures are in place to protect your sensitive financial data.\r\n- Regularly backup your financial data to prevent loss.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to web-based solutions.\r\n- Keep your containers updated to benefit from the latest features and security patches.\r\n\r\nExample docker-compose snippet for setting up a Firefly III container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  firefly_iii:\r\n    image: fireflyiii/core:latest\r\n    container_name: firefly_iii\r\n    environment:\r\n      - APP_KEY=your_app_key_here\r\n      - DB_HOST=firefly_db\r\n      - DB_PORT=3306\r\n      - DB_CONNECTION=mysql\r\n      - DB_DATABASE=firefly\r\n      - DB_USERNAME=firefly\r\n      - DB_PASSWORD=your_password_here\r\n    volumes:\r\n      - ./firefly_iii_upload:/var/www/html/storage/upload\r\n    ports:\r\n      - \"8080:8080\"\r\n    depends_on:\r\n      - firefly_db\r\n    restart: unless-stopped\r\n\r\n  firefly_db:\r\n    image: mariadb:latest\r\n    container_name: firefly_db\r\n    environment:\r\n      - MYSQL_ROOT_PASSWORD=your_root_password_here\r\n      - MYSQL_DATABASE=firefly\r\n      - MYSQL_USER=firefly\r\n      - MYSQL_PASSWORD=your_password_here\r\n    volumes:\r\n      - ./firefly_db_data:/var/lib/mysql\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage personal finance tools on your home server, helping you track expenses, manage budgets, and achieve your financial goals.\r\n\r\n## Photography and Image Management Containers\r\n\r\nDocker containers offer excellent solutions for managing and organizing your photo collections. Let's explore some of the most popular options:\r\n\r\n### PhotoPrism\r\n\r\n[PhotoPrism](https://photoprism.app/) is a feature-rich, AI-powered photo management solution that automatically organizes and tags your photo collection.\r\n\r\nKey features:\r\n\r\n- AI-powered image classification and face recognition\r\n- Automatic tagging and geocoding\r\n- Full-text search\r\n- Web-based user interface with mobile support\r\n\r\nDocker image: `photoprism/photoprism`\r\n\r\n### Lychee\r\n\r\n[Lychee](https://lycheeorg.github.io/) is a free, open-source photo-management tool that allows you to create a self-hosted photo server.\r\n\r\nKey features:\r\n\r\n- Clean, minimalist interface\r\n- Album creation and management\r\n- Password protection for albums\r\n- Import from various sources (local, URL, Dropbox)\r\n\r\nDocker image: `lycheeorg/lychee`\r\n\r\n### Piwigo\r\n\r\n[Piwigo](https://piwigo.org/) is a free and open-source photo gallery software that offers a wide range of features for managing your photo collection.\r\n\r\nKey features:\r\n\r\n- Customizable themes and plugins\r\n- User management and access control\r\n- Batch import and upload\r\n- Tagging and search functionality\r\n\r\nDocker image: `linuxserver/piwigo`\r\n\r\n### Immich\r\n\r\n[Immich](https://github.com/immich-app/immich) is a high-performance self-hosted photo and video backup solution.\r\n\r\nKey features:\r\n\r\n- Real-time backup from mobile devices\r\n- Face recognition and detection\r\n- Location-based organization\r\n- Support for RAW photos\r\n- Built-in map view\r\n- Shared albums and user management\r\n\r\nDocker image: `ghcr.io/immich-app/immich-server`\r\n\r\n\r\n### Comparison Table\r\n\r\n| Feature                 | PhotoPrism | Lychee  | Piwigo     | Immich |\r\n| ----------------------- | ---------- | ------- | ---------- | ---------|\r\n| AI-powered organization | Yes        | No      | No         | Yes |\r\n| Face recognition        | Yes        | No      | Via plugin | Yes |\r\n| Mobile support          | Yes        | Yes     | Yes        | Yes |\r\n| User management         | Basic      | No      | Advanced   | Yes |\r\n| Customization           | Limited    | Limited | Extensive  | Limited |\r\n\r\nThese photography and image management containers can help you organize and showcase your photo collection:\r\n\r\n1. Use PhotoPrism for advanced, AI-powered photo organization and management.\r\n2. Implement Lychee for a simple, clean photo gallery solution.\r\n3. Set up Piwigo for a highly customizable photo gallery with advanced user management.\r\n\r\nWhen setting up your photography and image management solution, consider the following:\r\n\r\n- Ensure you have sufficient storage space for your photo collection.\r\n- Set up regular backups of your photo library and database.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your photo gallery.\r\n- Keep your containers updated to benefit from the latest features and security patches.\r\n\r\nExample docker-compose snippet for setting up a PhotoPrism container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  photoprism:\r\n    image: photoprism/photoprism:latest\r\n    container_name: photoprism\r\n    depends_on:\r\n      - mariadb\r\n    environment:\r\n      PHOTOPRISM_ADMIN_PASSWORD: \"your_admin_password\"\r\n      PHOTOPRISM_SITE_URL: \"http://your-site-url.com/\"\r\n      PHOTOPRISM_DATABASE_DRIVER: \"mysql\"\r\n      PHOTOPRISM_DATABASE_SERVER: \"mariadb:3306\"\r\n      PHOTOPRISM_DATABASE_NAME: \"photoprism\"\r\n      PHOTOPRISM_DATABASE_USER: \"photoprism\"\r\n      PHOTOPRISM_DATABASE_PASSWORD: \"your_database_password\"\r\n    volumes:\r\n      - ./photos:/photoprism/originals\r\n      - ./storage:/photoprism/storage\r\n    ports:\r\n      - \"2342:2342\"\r\n    restart: unless-stopped\r\n\r\n  mariadb:\r\n    image: mariadb:10.5\r\n    container_name: photoprism-mariadb\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD: \"your_root_password\"\r\n      MYSQL_DATABASE: \"photoprism\"\r\n      MYSQL_USER: \"photoprism\"\r\n      MYSQL_PASSWORD: \"your_database_password\"\r\n    volumes:\r\n      - ./database:/var/lib/mysql\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage powerful photography and image management solutions on your home server, helping you organize, showcase, and enjoy your photo collection.\r\n\r\n## E-book Management Containers\r\n\r\nDocker containers offer excellent solutions for managing and organizing your e-book collection. Let's explore some of the most popular options:\r\n\r\n### Calibre-web\r\n\r\n[Calibre-web](https://github.com/janeczku/calibre-web) is a web app providing a clean interface for browsing, reading and downloading e-books using an existing Calibre database.\r\n\r\nKey features:\r\n\r\n- Clean, responsive web interface\r\n- OPDS feed for remote e-book reading\r\n- User management with fine-grained permissions\r\n- Integrates with existing Calibre libraries\r\n\r\nDocker image: `linuxserver/calibre-web`\r\n\r\n### COPS (Calibre OPDS PHP Server)\r\n\r\n[COPS](https://github.com/seblucas/cops) is a lightweight e-book server that serves your Calibre library through a web interface and an OPDS feed.\r\n\r\nKey features:\r\n\r\n- Lightweight and fast\r\n- OPDS feed support\r\n- Multiple output formats (HTML, EPUB, MOBI)\r\n- Integrates with existing Calibre libraries\r\n\r\nDocker image: `lscr.io/linuxserver/cops:latest`\r\n\r\n### Ubooquity\r\n\r\n[Ubooquity](https://vaemendis.net/ubooquity/) is a free, lightweight, and easy-to-use home server for your comics and ebooks.\r\n\r\nKey features:\r\n\r\n- Supports both e-books and comics\r\n- Web-based reader\r\n- OPDS feed support\r\n- User management and access control\r\n\r\nDocker image: `linuxserver/ubooquity`\r\n\r\n### Comparison Table\r\n\r\n| Feature             | Calibre-web | COPS | Ubooquity |\r\n| ------------------- | ----------- | ---- | --------- |\r\n| Web interface       | Yes         | Yes  | Yes       |\r\n| OPDS support        | Yes         | Yes  | Yes       |\r\n| Comic support       | No          | No   | Yes       |\r\n| User management     | Yes         | No   | Yes       |\r\n| Calibre integration | Yes         | Yes  | No        |\r\n\r\nThese e-book management containers can help you organize and access your digital library:\r\n\r\n1. Use Calibre-web for a feature-rich, web-based interface to your Calibre library.\r\n2. Implement COPS for a lightweight, fast solution focused on OPDS support.\r\n3. Set up Ubooquity if you need support for both e-books and comics in a single application.\r\n\r\nWhen setting up your e-book management solution, consider the following:\r\n\r\n- Ensure you have sufficient storage space for your e-book collection.\r\n- Set up regular backups of your e-book library and database.\r\n- Use a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your e-book server.\r\n- Keep your containers updated to benefit from the latest features and security patches.\r\n\r\nExample docker-compose snippet for setting up a Calibre-web container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  calibre-web:\r\n    image: linuxserver/calibre-web\r\n    container_name: calibre-web\r\n    environment:\r\n      - PUID=1000\r\n      - PGID=1000\r\n      - TZ=Your/Timezone\r\n    volumes:\r\n      - ./config:/config\r\n      - /path/to/your/calibre/library:/books\r\n    ports:\r\n      - 8083:8083\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage powerful e-book management solutions on your home server, helping you organize, access, and enjoy your digital library from any device.\r\n\r\n\r\n\r\n## Self-Hosted Database Solutions (Airtable Alternatives)\r\n\r\nDocker containers offer excellent solutions for creating self-hosted database management systems similar to Airtable. Let's explore some of the most popular options:\r\n\r\n### Baserow\r\n\r\n[Baserow](https://baserow.io/) is an open-source no-code database tool that provides a user-friendly interface for creating and managing relational databases.\r\n\r\nKey features:\r\n\r\n- Intuitive drag-and-drop interface\r\n- Multiple field types\r\n- Views (Grid, Gallery, Form)\r\n- User management and permissions\r\n- REST API\r\n- Extensible plugin system\r\n\r\nDocker image: `baserow/baserow`\r\n\r\n### NocoDB\r\n\r\n[NocoDB](https://nocodb.com/) transforms any MySQL, PostgreSQL, Microsoft SQL Server, SQLite, or MariaDB into a smart spreadsheet.\r\n\r\nKey features:\r\n\r\n- Multiple views (Grid, Gallery, Form, Kanban)\r\n- Role-based access control\r\n- REST and GraphQL APIs\r\n- Webhooks and integrations\r\n- Automations and workflows\r\n\r\nDocker image: `nocodb/nocodb`\r\n\r\n### Teable\r\n\r\n[Teable](https://github.com/teableio/teable) is a super fast, real-time, professional, developer-friendly database tool that provides spreadsheet-like interactions.\r\n\r\nKey features:\r\n\r\n- Direct PostgreSQL connection\r\n- Real-time collaboration\r\n- Professional data management\r\n- Developer-friendly API\r\n- Mobile-friendly interface\r\n- Extensible architecture\r\n\r\nDocker image: `teableio/teable`\r\n\r\n### Comparison Table\r\n\r\n| Feature | Baserow | NocoDB | Teable |\r\n|---------|----------|---------|---------|\r\n| Interface | Modern, intuitive | Spreadsheet-like | Modern, spreadsheet-like |\r\n| Database Support | PostgreSQL | Multiple | PostgreSQL |\r\n| API Support | REST | REST, GraphQL | REST |\r\n| Real-time | Yes | Limited | Yes |\r\n| Mobile Support | Yes | Yes | Yes |\r\n\r\nFor more detailed information about self-hosted Airtable alternatives, check out our comprehensive guide: [Best Open Source Self-hosted Airtable Alternatives](https://www.bitdoze.com/self-hosted-airtable-alternatives/)\r\n\r\n\r\n## Game Server Containers\r\n\r\nDocker containers offer an excellent way to set up and manage game servers for various popular games. Let's explore some of the most popular options:\r\n\r\n### Minecraft Server\r\n\r\n[Minecraft](https://www.minecraft.net/) is a popular sandbox game that allows players to build, explore, and survive in a blocky, procedurally-generated world.\r\n\r\nKey features:\r\n\r\n- Customizable server settings\r\n- Support for various game modes (Survival, Creative, Adventure)\r\n- Plugin support for extended functionality\r\n- Cross-platform play between Java and Bedrock editions (with additional setup)\r\n\r\nDocker image: `itzg/minecraft-server`\r\n\r\n### Terraria Server\r\n\r\n[Terraria](https://terraria.org/) is a 2D action-adventure sandbox game that offers a vast world to explore, craft, and fight in.\r\n\r\nKey features:\r\n\r\n- Customizable world generation\r\n- Support for various game modes\r\n- Multiplayer support for up to 255 players\r\n- Mod support (tModLoader)\r\n\r\nDocker image: `ryshe/terraria`\r\n\r\n### Factorio Server\r\n\r\n[Factorio](https://www.factorio.com/) is a game about building and maintaining factories while defending against alien threats.\r\n\r\nKey features:\r\n\r\n- Highly optimized for multiplayer\r\n- Mod support\r\n- Customizable map settings\r\n- Automatic saving and server management\r\n\r\nDocker image: `factoriotools/factorio`\r\n\r\n### Valheim Server\r\n\r\n[Valheim](https://www.valheimgame.com/) is a survival and exploration game set in a procedurally-generated world inspired by Norse mythology.\r\n\r\nKey features:\r\n\r\n- Cooperative gameplay for up to 10 players\r\n- Customizable server settings\r\n- World persistence\r\n- Cross-platform play between Windows and Linux\r\n\r\nDocker image: `lloesche/valheim-server`\r\n\r\n### Comparison Table\r\n\r\n| Feature        | Minecraft    | Terraria  | Factorio  | Valheim  |\r\n| -------------- | ------------ | --------- | --------- | -------- |\r\n| Player Limit   | Configurable | Up to 255 | Unlimited | Up to 10 |\r\n| Mod Support    | Yes          | Yes       | Yes       | Limited  |\r\n| Cross-platform | Yes\\*        | Yes       | No        | Yes      |\r\n| Resource Usage | Moderate     | Low       | High      | Moderate |\r\n\r\n\\*Requires additional setup for Java-Bedrock cross-play\r\n\r\nWhen setting up game server containers, consider the following best practices:\r\n\r\n- Allocate sufficient resources (CPU, RAM) based on the game's requirements and expected player count\r\n- Use volumes to persist game data and world saves outside the container\r\n- Configure automatic backups to prevent data loss\r\n- Use a reverse proxy or VPN for secure remote access to your game servers\r\n- Keep your game server containers updated to benefit from the latest features and security patches\r\n\r\nExample docker-compose snippet for setting up a Minecraft server container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  minecraft:\r\n    image: itzg/minecraft-server\r\n    container_name: minecraft\r\n    environment:\r\n      EULA: \"TRUE\"\r\n      MEMORY: \"2G\"\r\n      TYPE: \"PAPER\"\r\n      DIFFICULTY: \"normal\"\r\n      ALLOW_NETHER: \"true\"\r\n      ENABLE_COMMAND_BLOCK: \"true\"\r\n    volumes:\r\n      - ./minecraft_data:/data\r\n    ports:\r\n      - \"25565:25565\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage game servers on your home server, providing a fun and customizable gaming experience for you and your friends. Remember to check the specific documentation for each game server container, as they may have additional configuration options and requirements.\r\n\r\n## Communication Containers\r\n\r\nDocker containers offer excellent solutions for setting up your own communication platforms. Let's explore some of the most popular options:\r\n\r\n### Mattermost\r\n\r\n[Mattermost](https://mattermost.com/) is an open-source, self-hostable alternative to Slack, providing team messaging and collaboration features.\r\n\r\nKey features:\r\n\r\n- Team messaging and file sharing\r\n- Integrations with various tools and services\r\n- Customizable themes and branding\r\n- End-to-end encryption (in Enterprise edition)\r\n\r\nDocker image: `mattermost/mattermost-team-edition`\r\n\r\n### Rocket.Chat\r\n\r\n[Rocket.Chat](https://rocket.chat/) is a free and open-source team communication platform that offers chat, video conferencing, and file sharing capabilities.\r\n\r\nKey features:\r\n\r\n- Real-time chat with channels and direct messages\r\n- Video and audio calls\r\n- File sharing and screen sharing\r\n- Customizable with a marketplace of apps and integrations\r\n\r\nDocker image: `rocket.chat`\r\n\r\n### Jitsi Meet\r\n\r\n[Jitsi Meet](https://jitsi.org/jitsi-meet/) is a fully encrypted, 100% open-source video conferencing solution that you can use all day, every day, for free.\r\n\r\nKey features:\r\n\r\n- High-quality video and audio conferencing\r\n- Screen sharing and collaborative document editing\r\n- No account required for participants\r\n- Mobile apps for iOS and Android\r\n\r\nDocker image: `jitsi/web`\r\n\r\n### Matrix Synapse\r\n\r\n[Matrix Synapse](https://matrix.org/docs/projects/server/synapse) is the reference homeserver implementation of the Matrix protocol, an open standard for secure, decentralized real-time communication.\r\n\r\nKey features:\r\n\r\n- End-to-end encryption\r\n- Federated architecture for decentralized communication\r\n- Supports various clients (e.g., Element)\r\n- Bridges to other communication platforms\r\n\r\nDocker image: `matrixdotorg/synapse`\r\n\r\n### Comparison Table\r\n\r\n| Feature        | Mattermost      | Rocket.Chat | Jitsi Meet         | Matrix Synapse              |\r\n| -------------- | --------------- | ----------- | ------------------ | --------------------------- |\r\n| Primary Focus  | Team Chat       | Team Chat   | Video Conferencing | Decentralized Communication |\r\n| Video Calls    | Via plugins     | Built-in    | Built-in           | Via clients                 |\r\n| E2E Encryption | Enterprise only | Optional    | Yes                | Yes                         |\r\n| Federation     | No              | No          | No                 | Yes                         |\r\n| Mobile Apps    | Yes             | Yes         | Yes                | Via clients                 |\r\n\r\nWhen setting up communication containers, consider the following best practices:\r\n\r\n- Use HTTPS with valid SSL certificates for secure communication\r\n- Implement proper user authentication and access control\r\n- Regularly backup your data and configuration\r\n- Keep your containers updated to benefit from the latest features and security patches\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your communication platforms\r\n\r\nExample docker-compose snippet for setting up a Mattermost container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  mattermost:\r\n    image: mattermost/mattermost-team-edition:latest\r\n    container_name: mattermost\r\n    depends_on:\r\n      - db\r\n    environment:\r\n      - MM_SQLSETTINGS_DRIVERNAME=postgres\r\n      - MM_SQLSETTINGS_DATASOURCE=postgres://mmuser:mmuser_password@db:5432/mattermost?sslmode=disable&connect_timeout=10\r\n    volumes:\r\n      - ./mattermost-data:/mattermost/data\r\n      - ./mattermost-logs:/mattermost/logs\r\n      - ./mattermost-config:/mattermost/config\r\n    ports:\r\n      - \"8065:8065\"\r\n    restart: unless-stopped\r\n\r\n  db:\r\n    image: postgres:13\r\n    container_name: mattermost-db\r\n    environment:\r\n      - POSTGRES_USER=mmuser\r\n      - POSTGRES_PASSWORD=mmuser_password\r\n      - POSTGRES_DB=mattermost\r\n    volumes:\r\n      - ./postgres-data:/var/lib/postgresql/data\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage various communication platforms on your home server, providing secure and customizable communication solutions for your team or personal use.\r\n\r\n## Document Management Containers\r\n\r\nDocker containers offer excellent solutions for managing and organizing your documents. Let's explore some of the most popular options:\r\n\r\n### Paperless-ng\r\n\r\n[Paperless-ng](https://github.com/jonaswinkler/paperless-ng) is a document management system that transforms your physical documents into a searchable online archive.\r\n\r\nKey features:\r\n\r\n- OCR (Optical Character Recognition) for scanned documents\r\n- Automatic tagging and classification\r\n- Full-text search\r\n- Mobile-friendly web interface\r\n\r\nDocker image: `jonaswinkler/paperless-ng`\r\n\r\n### Mayan EDMS\r\n\r\n[Mayan EDMS](https://www.mayan-edms.com/) is a free and open-source electronic document management system.\r\n\r\nKey features:\r\n\r\n- Document versioning\r\n- Fine-grained access control\r\n- Workflow automation\r\n- OCR and full-text search\r\n\r\nDocker image: `mayanedms/mayanedms`\r\n\r\n### OpenKM\r\n\r\n[OpenKM](https://www.openkm.com/) is a document management system that helps you manage, track, and store documents.\r\n\r\nKey features:\r\n\r\n- Document versioning and workflow\r\n- Metadata and taxonomy management\r\n- Full-text indexing and search\r\n- Integration with external applications\r\n\r\nDocker image: `openkm/openkm-ce`\r\n\r\n### Comparison Table\r\n\r\n| Feature             | Paperless-ng | Mayan EDMS | OpenKM  |\r\n| ------------------- | ------------ | ---------- | ------- |\r\n| OCR                 | Yes          | Yes        | Yes     |\r\n| Full-text search    | Yes          | Yes        | Yes     |\r\n| Workflow automation | Limited      | Yes        | Yes     |\r\n| Mobile-friendly     | Yes          | Yes        | Limited |\r\n| Document versioning | No           | Yes        | Yes     |\r\n\r\nThese document management containers can help you organize and access your digital documents:\r\n\r\n- Use Paperless-ng for a lightweight, user-friendly solution focused on personal document management.\r\n- Implement Mayan EDMS for a more comprehensive document management system with advanced features.\r\n- Set up OpenKM if you need extensive integration capabilities and advanced workflow management.\r\n\r\nWhen setting up your document management solution, consider the following:\r\n\r\n- Ensure you have sufficient storage space for your document collection.\r\n- Set up regular backups of your document library and database.\r\n- Use a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your document management system.\r\n- Keep your containers updated to benefit from the latest features and security patches.\r\n\r\nExample docker-compose snippet for setting up a Paperless-ng container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  paperless:\r\n    image: jonaswinkler/paperless-ng:latest\r\n    container_name: paperless\r\n    depends_on:\r\n      - redis\r\n      - db\r\n    environment:\r\n      - PAPERLESS_TIME_ZONE=Your/Timezone\r\n      - PAPERLESS_OCR_LANGUAGE=eng\r\n      - PAPERLESS_SECRET_KEY=your_secret_key_here\r\n    volumes:\r\n      - ./data:/usr/src/paperless/data\r\n      - ./media:/usr/src/paperless/media\r\n      - ./export:/usr/src/paperless/export\r\n      - ./consume:/usr/src/paperless/consume\r\n    ports:\r\n      - \"8000:8000\"\r\n    restart: unless-stopped\r\n\r\n  redis:\r\n    image: redis:6\r\n    container_name: paperless-redis\r\n    restart: unless-stopped\r\n\r\n  db:\r\n    image: postgres:13\r\n    container_name: paperless-db\r\n    environment:\r\n      - POSTGRES_DB=paperless\r\n      - POSTGRES_USER=paperless\r\n      - POSTGRES_PASSWORD=paperless\r\n    volumes:\r\n      - ./pgdata:/var/lib/postgresql/data\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage powerful document management solutions on your home server, helping you organize, search, and access your digital documents efficiently.\r\n\r\n## Web Hosting Containers\r\n\r\nDocker containers offer excellent solutions for hosting websites and content management systems. Let's explore some of the most popular options:\r\n\r\n### WordPress\r\n\r\n[WordPress](https://wordpress.org/) is the world's most popular content management system, powering over 40% of all websites on the internet.\r\n\r\nKey features:\r\n\r\n- User-friendly content management\r\n- Extensive plugin and theme ecosystem\r\n- SEO-friendly\r\n- Suitable for blogs, e-commerce, and complex websites\r\n\r\nDocker image: `wordpress`\r\n\r\n### Ghost\r\n\r\n[Ghost](https://ghost.org/) is a modern, open-source publishing platform designed for creating professional publications and newsletters.\r\n\r\nKey features:\r\n\r\n- Clean, minimalist interface\r\n- Built-in SEO tools\r\n- Membership and subscription features\r\n- Native email newsletter functionality\r\n\r\nDocker image: `ghost`\r\n\r\n### Drupal\r\n\r\n[Drupal](https://www.drupal.org/) is a flexible and powerful content management system known for its robustness and scalability.\r\n\r\nKey features:\r\n\r\n- Highly customizable\r\n- Strong security features\r\n- Excellent for complex, content-heavy websites\r\n- Large community and extensive module ecosystem\r\n\r\nDocker image: `drupal`\r\n\r\n### Joomla\r\n\r\n[Joomla](https://www.joomla.org/) is a popular content management system that offers a balance between ease of use and powerful features.\r\n\r\nKey features:\r\n\r\n- User-friendly administration\r\n- Multilingual support out of the box\r\n- Extensive extension directory\r\n- Suitable for various types of websites\r\n\r\nDocker image: `joomla`\r\n\r\n### Comparison Table\r\n\r\n| Feature          | WordPress       | Ghost                  | Drupal        | Joomla                |\r\n| ---------------- | --------------- | ---------------------- | ------------- | --------------------- |\r\n| Ease of Use      | High            | High                   | Medium        | Medium                |\r\n| Customizability  | High            | Medium                 | Very High     | High                  |\r\n| Performance      | Good            | Excellent              | Good          | Good                  |\r\n| Plugin Ecosystem | Extensive       | Limited                | Extensive     | Extensive             |\r\n| Best For         | General-purpose | Blogging, Publications | Complex sites | Medium to large sites |\r\n\r\nWhen setting up web hosting containers, consider the following best practices:\r\n\r\n- Use separate containers for your web server, database, and application\r\n- Implement proper security measures, including SSL certificates and regular updates\r\n- Set up automated backups for your website data and database\r\n- Use a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access and easy SSL management\r\n- Monitor resource usage and scale your containers as needed\r\n\r\nExample docker-compose snippet for setting up a WordPress container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  wordpress:\r\n    image: wordpress:latest\r\n    container_name: wordpress\r\n    depends_on:\r\n      - db\r\n    environment:\r\n      - WORDPRESS_DB_HOST=db\r\n      - WORDPRESS_DB_USER=wordpress\r\n      - WORDPRESS_DB_PASSWORD=wordpress_password\r\n      - WORDPRESS_DB_NAME=wordpress\r\n    volumes:\r\n      - ./wordpress:/var/www/html\r\n    ports:\r\n      - \"8080:80\"\r\n    restart: unless-stopped\r\n\r\n  db:\r\n    image: mysql:5.7\r\n    container_name: wordpress-db\r\n    environment:\r\n      - MYSQL_DATABASE=wordpress\r\n      - MYSQL_USER=wordpress\r\n      - MYSQL_PASSWORD=wordpress_password\r\n      - MYSQL_RANDOM_ROOT_PASSWORD=1\r\n    volumes:\r\n      - ./mysql:/var/lib/mysql\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage various content management systems and web hosting solutions on your home server. This allows you to create and maintain websites, blogs, or online publications with ease, while maintaining full control over your hosting environment.\r\n\r\n## Personal Dashboard Containers\r\n\r\nDocker containers offer excellent solutions for creating personal dashboards that help you organize and access your self-hosted services. Let's explore some of the most popular options:\r\n\r\n### Heimdall\r\n\r\n[Heimdall](https://heimdall.site/) is a sleek, customizable application dashboard that allows you to organize all your web applications and services in one place.\r\n\r\nKey features:\r\n\r\n- Clean, modern interface\r\n- Customizable icons and colors\r\n- Support for enhanced apps with additional functionality\r\n- Multiple user support with fine-grained access control\r\n\r\nDocker image: `linuxserver/heimdall`\r\n\r\n### Organizr\r\n\r\n[Organizr](https://github.com/causefx/Organizr) is a PHP-based dashboard that brings all your services together in one place, with a focus on media server management.\r\n\r\nKey features:\r\n\r\n- Tabbed interface for easy navigation\r\n- Integration with popular media server applications\r\n- Customizable themes and layouts\r\n- User management and authentication\r\n\r\nDocker image: `organizr/organizr`\r\n\r\n### Homer\r\n\r\n[Homer](https://github.com/bastienwirtz/homer) is a simple, lightweight, and highly customizable static dashboard for your server.\r\n\r\nKey features:\r\n\r\n- Minimalist design\r\n- Easy configuration using YAML files\r\n- Support for custom CSS and icons\r\n- Lightweight and fast-loading\r\n\r\nDocker image: `b4bz/homer`\r\n\r\n### Comparison Table\r\n\r\n| Feature              | Heimdall           | Organizr | Homer                |\r\n| -------------------- | ------------------ | -------- | -------------------- |\r\n| Interface            | Modern, tile-based | Tabbed   | Static, customizable |\r\n| Customization        | High               | High     | Very High            |\r\n| User Management      | Yes                | Yes      | No                   |\r\n| Enhanced App Support | Yes                | Yes      | No                   |\r\n| Ease of Setup        | Easy               | Medium   | Easy                 |\r\n\r\nThese personal dashboard containers can help you organize and access your self-hosted services:\r\n\r\n- Use Heimdall for a polished, feature-rich dashboard with enhanced app support.\r\n- Implement Organizr if you're primarily focused on media server management and want deep integration.\r\n- Set up Homer for a lightweight, highly customizable static dashboard that's easy to configure.\r\n\r\nWhen setting up your personal dashboard solution, consider the following:\r\n\r\n- Choose a dashboard that fits your specific needs and aesthetic preferences.\r\n- Ensure proper security measures, such as using HTTPS and implementing authentication.\r\n- Regularly update your dashboard container to benefit from the latest features and security patches.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your dashboard.\r\n\r\nExample docker-compose snippet for setting up a Heimdall container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  heimdall:\r\n    image: linuxserver/heimdall\r\n    container_name: heimdall\r\n    environment:\r\n      - PUID=1000\r\n      - PGID=1000\r\n      - TZ=Your/Timezone\r\n    volumes:\r\n      - ./config:/config\r\n    ports:\r\n      - \"80:80\"\r\n      - \"443:443\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage a personal dashboard on your home server, providing a centralized point of access for all your self-hosted services and applications. This not only improves organization but also enhances the overall user experience of your home server setup.\r\n\r\n## RSS Feed Containers\r\n\r\nDocker containers offer excellent solutions for managing and reading RSS feeds. Let's explore some of the most popular options:\r\n\r\n### FreshRSS\r\n\r\n[FreshRSS](https://freshrss.org/) is a free, self-hostable RSS feed aggregator that's lightweight and customizable.\r\n\r\nKey features:\r\n\r\n- Clean, responsive web interface\r\n- Supports keyboard shortcuts for efficient navigation\r\n- Customizable themes and extensions\r\n- API support for mobile apps (compatible with Google Reader API)\r\n\r\nDocker image: `linuxserver/freshrss`\r\n\r\n### Tiny Tiny RSS\r\n\r\n[Tiny Tiny RSS](https://tt-rss.org/) is a free and open-source web-based news feed (RSS/Atom) reader and aggregator.\r\n\r\nKey features:\r\n\r\n- Customizable interface with themes\r\n- Plugins for extended functionality\r\n- Mobile-friendly web interface\r\n- API for third-party clients and mobile apps\r\n\r\nDocker image: `linuxserver/tt-rss`\r\n\r\n### Miniflux\r\n\r\n[Miniflux](https://miniflux.app/) is a minimalist and opinionated feed reader, focusing on simplicity and performance.\r\n\r\nKey features:\r\n\r\n- Clean, distraction-free interface\r\n- Built-in support for various content scraping methods\r\n- Fever API support for third-party clients\r\n- Keyboard shortcuts for efficient navigation\r\n\r\nDocker image: `miniflux/miniflux`\r\n\r\n### Comparison Table\r\n\r\n| Feature            | FreshRSS            | Tiny Tiny RSS | Miniflux    |\r\n| ------------------ | ------------------- | ------------- | ----------- |\r\n| Interface          | Clean, customizable | Customizable  | Minimalist  |\r\n| Extensions/Plugins | Yes                 | Yes           | No          |\r\n| API Support        | Yes (Google Reader) | Yes (Custom)  | Yes (Fever) |\r\n| Performance        | Good                | Good          | Excellent   |\r\n| Ease of Setup      | Easy                | Medium        | Easy        |\r\n\r\nThese RSS feed containers can help you manage and read your favorite news and content sources:\r\n\r\n- Use FreshRSS for a feature-rich, customizable RSS reader with good mobile app support.\r\n- Implement Tiny Tiny RSS if you need extensive plugin support and don't mind a slightly more complex setup.\r\n- Set up Miniflux for a minimalist, high-performance RSS reader that focuses on the essentials.\r\n\r\nWhen setting up your RSS feed solution, consider the following:\r\n\r\n- Choose a reader that fits your reading habits and preferences.\r\n- Ensure proper security measures, such as using HTTPS and implementing authentication.\r\n- Regularly update your RSS reader container to benefit from the latest features and security patches.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your RSS reader.\r\n\r\nExample docker-compose snippet for setting up a FreshRSS container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  freshrss:\r\n    image: linuxserver/freshrss\r\n    container_name: freshrss\r\n    environment:\r\n      - PUID=1000\r\n      - PGID=1000\r\n      - TZ=Your/Timezone\r\n    volumes:\r\n      - ./config:/config\r\n    ports:\r\n      - \"8080:80\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage an RSS feed reader on your home server, allowing you to stay up-to-date with your favorite news sources, blogs, and websites in a centralized, self-hosted solution. This gives you full control over your news consumption while ensuring your reading habits remain private.\r\n\r\n## Weather Monitoring Containers\r\n\r\nDocker containers offer excellent solutions for monitoring and analyzing weather data. Let's explore some of the most popular options:\r\n\r\n### Weewx\r\n\r\n[Weewx](http://www.weewx.com/) is a free, open-source weather station software that collects data from various weather sensors and generates reports, graphs, and web pages.\r\n\r\nKey features:\r\n\r\n- Supports a wide range of weather stations and sensors\r\n- Customizable reports and graphs\r\n- Extensible through plugins\r\n- Can upload data to popular weather services\r\n\r\nDocker image: `felddy/weewx`\r\n\r\n### Meteobridge\r\n\r\n[Meteobridge](https://www.meteobridge.com/) is a commercial weather station data logger and uploader that can be run in a Docker container.\r\n\r\nKey features:\r\n\r\n- Supports various weather stations and sensors\r\n- Real-time data processing and uploading\r\n- Customizable web interface\r\n- Integration with multiple weather services\r\n\r\nDocker image: `acperez/meteobridge-docker` (unofficial)\r\n\r\n### Comparison Table\r\n\r\n| Feature            | Weewx      | Meteobridge           |\r\n| ------------------ | ---------- | --------------------- |\r\n| Open-source        | Yes        | No                    |\r\n| Supported Stations | Wide range | Wide range            |\r\n| Customization      | High       | Medium                |\r\n| Ease of Setup      | Medium     | Easy                  |\r\n| Cost               | Free       | Paid license required |\r\n\r\nThese weather monitoring containers can help you collect, analyze, and display weather data from your personal weather station:\r\n\r\n- Use Weewx for a free, highly customizable solution with extensive support for various weather stations.\r\n- Implement Meteobridge if you prefer a commercial solution with easier setup and real-time data processing.\r\n\r\nWhen setting up your weather monitoring solution, consider the following:\r\n\r\n- Ensure your weather station or sensors are compatible with the chosen software.\r\n- Set up proper networking to allow the container to communicate with your weather station.\r\n- Configure regular backups of your weather data.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your weather dashboard.\r\n\r\nExample docker-compose snippet for setting up a Weewx container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  weewx:\r\n    image: felddy/weewx\r\n    container_name: weewx\r\n    environment:\r\n      - TZ=Your/Timezone\r\n      - WEEWX_UID=1000\r\n      - WEEWX_GID=1000\r\n    volumes:\r\n      - ./config:/config\r\n      - ./data:/home/weewx/archive\r\n    devices:\r\n      - /dev/ttyUSB0:/dev/ttyUSB0\r\n    ports:\r\n      - \"8080:80\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage a weather monitoring system on your home server. This allows you to collect and analyze data from your personal weather station, create custom reports and graphs, and even contribute to larger weather monitoring networks.\r\n\r\nRemember to check the specific documentation for each weather monitoring container, as they may have additional configuration options and requirements based on your specific weather station and sensors.\r\n\r\n## Time Tracking Containers\r\n\r\nDocker containers offer excellent solutions for tracking and managing your time. Let's explore some of the most popular options:\r\n\r\n### Kimai\r\n\r\n[Kimai](https://www.kimai.org/) is a free, open-source time-tracking application that can be used for project management, freelance billing, or personal time management.\r\n\r\nKey features:\r\n\r\n- User-friendly interface with responsive design\r\n- Detailed reporting and export options\r\n- Project and customer management\r\n- Customizable through plugins\r\n- Multi-user support with role-based permissions\r\n\r\nDocker image: `kimai/kimai2`\r\n\r\n### TimeTagger\r\n\r\n[TimeTagger](https://timetagger.app/) is a simple, open-source time tracking application with a focus on ease of use and flexibility.\r\n\r\nKey features:\r\n\r\n- Intuitive timeline-based interface\r\n- Keyboard shortcuts for quick entry\r\n- Customizable tags and projects\r\n- Data export in various formats\r\n- Self-hosted option for privacy\r\n\r\nDocker image: `almarklein/timetagger`\r\n\r\n### Comparison Table\r\n\r\n| Feature            | Kimai                      | TimeTagger             |\r\n| ------------------ | -------------------------- | ---------------------- |\r\n| Interface          | Feature-rich, customizable | Simple, timeline-based |\r\n| Reporting          | Extensive                  | Basic                  |\r\n| Multi-user support | Yes                        | Limited                |\r\n| Project management | Yes                        | Basic                  |\r\n| Ease of use        | Good                       | Excellent              |\r\n\r\nThese time tracking containers can help you manage your time more effectively:\r\n\r\n- Use Kimai for a comprehensive time tracking solution with extensive project management and reporting features.\r\n- Implement TimeTagger for a simpler, more intuitive approach to personal time tracking.\r\n\r\nWhen setting up your time tracking solution, consider the following:\r\n\r\n- Choose a solution that fits your specific needs (personal use, team management, or client billing).\r\n- Ensure proper security measures, such as using HTTPS and implementing authentication.\r\n- Regularly backup your time tracking data.\r\n- Keep your containers updated to benefit from the latest features and security patches.\r\n\r\nExample docker-compose snippet for setting up a Kimai container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  kimai:\r\n    image: kimai/kimai2:latest\r\n    container_name: kimai\r\n    environment:\r\n      - TRUSTED_HOSTS=localhost,127.0.0.1\r\n      - DATABASE_URL=mysql://kimaiuser:kimaipassword@db:3306/kimai\r\n      - APP_ENV=prod\r\n    volumes:\r\n      - ./kimai_data:/opt/kimai/var\r\n    ports:\r\n      - \"8001:8001\"\r\n    depends_on:\r\n      - db\r\n    restart: unless-stopped\r\n\r\n  db:\r\n    image: mysql:5.7\r\n    container_name: kimai-db\r\n    environment:\r\n      - MYSQL_DATABASE=kimai\r\n      - MYSQL_USER=kimaiuser\r\n      - MYSQL_PASSWORD=kimaipassword\r\n      - MYSQL_ROOT_PASSWORD=rootpassword\r\n    volumes:\r\n      - ./mysql_data:/var/lib/mysql\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage a time tracking system on your home server. This allows you to monitor your work hours, manage projects, and generate reports, all while maintaining control over your data and ensuring your privacy.\r\n\r\nRemember to check the specific documentation for each time tracking container, as they may have additional configuration options and requirements based on your specific needs and use case.\r\n\r\n## Password Management Containers\r\n\r\nDocker containers offer secure and convenient solutions for managing your passwords and sensitive information. Let's explore some of the most popular options:\r\n\r\n### Passbolt\r\n\r\n[Passbolt](https://www.passbolt.com/) is an open-source password manager designed for team collaboration, with a focus on security and ease of use.\r\n\r\nKey features:\r\n\r\n- End-to-end encryption\r\n- User-friendly web interface\r\n- Browser extensions for easy access\r\n- Granular access control and user management\r\n- API for integration with other tools\r\n\r\nDocker image: `passbolt/passbolt`\r\n\r\n### Vaultwarden (Bitwarden RS)\r\n\r\n[Vaultwarden](https://github.com/dani-garcia/vaultwarden) (formerly known as Bitwarden RS) is an unofficial Bitwarden server implementation written in Rust, designed to be lighter and more efficient than the official server.\r\n\r\nKey features:\r\n\r\n- Compatible with official Bitwarden clients\r\n- End-to-end encryption\r\n- Self-hosted for complete control over your data\r\n- Supports all core Bitwarden features\r\n- Low resource requirements\r\n\r\nDocker image: `vaultwarden/server`\r\n\r\n### Comparison Table\r\n\r\n| Feature              | Passbolt                                    | Vaultwarden                 |\r\n| -------------------- | ------------------------------------------- | --------------------------- |\r\n| Focus                | Team collaboration                          | Personal and small team use |\r\n| Client compatibility | Custom web interface and browser extensions | Official Bitwarden clients  |\r\n| Resource usage       | Moderate                                    | Low                         |\r\n| Setup complexity     | Moderate                                    | Easy                        |\r\n| API availability     | Yes                                         | Limited                     |\r\n\r\nThese password management containers can help you securely store and manage your passwords:\r\n\r\n- Use Passbolt for team-oriented password management with granular access control.\r\n- Implement Vaultwarden for a lightweight, Bitwarden-compatible solution suitable for personal or small team use.\r\n\r\nWhen setting up your password management solution, consider the following best practices:\r\n\r\n- Always use HTTPS with valid SSL certificates to encrypt communications.\r\n- Implement strong authentication methods, such as two-factor authentication.\r\n- Regularly backup your password database and encryption keys.\r\n- Keep your containers and host system updated with the latest security patches.\r\n- Use a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to your password manager.\r\n\r\nExample docker-compose snippet for setting up a Vaultwarden container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  vaultwarden:\r\n    image: vaultwarden/server:latest\r\n    container_name: vaultwarden\r\n    environment:\r\n      - WEBSOCKET_ENABLED=true\r\n      - SIGNUPS_ALLOWED=false\r\n    volumes:\r\n      - ./vw-data:/data\r\n    ports:\r\n      - \"80:80\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage a secure password management system on your home server. This allows you to maintain control over your sensitive information while providing convenient access across your devices.\r\n\r\nRemember to choose a solution that best fits your needs, whether you're looking for personal password management or a team-oriented solution with advanced access controls. Always prioritize security when dealing with password managers, as they are a critical component of your overall digital security strategy.\r\n\r\n## VoIP Containers\r\n\r\nDocker containers offer excellent solutions for setting up your own Voice over IP (VoIP) system. Let's explore some of the most popular options:\r\n\r\n### Asterisk\r\n\r\n[Asterisk](https://www.asterisk.org/) is a powerful, open-source framework for building communications applications, including VoIP systems.\r\n\r\nKey features:\r\n\r\n- Supports various VoIP protocols (SIP, IAX, etc.)\r\n- Highly customizable and extensible\r\n- Can function as a PBX, VoIP gateway, or conference bridge\r\n- Supports interactive voice response (IVR) systems\r\n\r\nDocker image: `andrius/asterisk`\r\n\r\n### FreePBX\r\n\r\n[FreePBX](https://www.freepbx.org/) is a web-based open-source GUI that controls and manages Asterisk, making it easier to set up and manage a VoIP system.\r\n\r\nKey features:\r\n\r\n- User-friendly web interface for managing Asterisk\r\n- Extensive module system for adding functionality\r\n- Built-in reporting and call logging\r\n- Supports various VoIP phones and softphones\r\n\r\nDocker image: `tiredofit/freepbx`\r\n\r\n### Comparison Table\r\n\r\n| Feature        | Asterisk                            | FreePBX                                          |\r\n| -------------- | ----------------------------------- | ------------------------------------------------ |\r\n| User Interface | Command-line                        | Web-based GUI                                    |\r\n| Customization  | High (requires technical knowledge) | High (through modules)                           |\r\n| Ease of Use    | Complex                             | User-friendly                                    |\r\n| Resource Usage | Low                                 | Moderate                                         |\r\n| Best For       | Advanced users, custom solutions    | Small to medium businesses, less technical users |\r\n\r\nThese VoIP containers can help you set up your own phone system:\r\n\r\n- Use Asterisk for a highly customizable, lightweight VoIP solution if you have technical expertise.\r\n- Implement FreePBX for a more user-friendly approach to managing an Asterisk-based system.\r\n\r\nWhen setting up your VoIP solution, consider the following best practices:\r\n\r\n- Ensure your network is properly configured for VoIP traffic (QoS, port forwarding, etc.).\r\n- Implement strong security measures, including firewalls and encryption for SIP traffic.\r\n- Regularly backup your VoIP system configuration and data.\r\n- Keep your containers and host system updated with the latest security patches.\r\n- Consider using a reverse proxy (like Traefik or Nginx Proxy Manager) for secure access to the management interface.\r\n\r\nExample docker-compose snippet for setting up a FreePBX container:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  freepbx:\r\n    image: tiredofit/freepbx:latest\r\n    container_name: freepbx\r\n    environment:\r\n      - TIMEZONE=America/New_York\r\n      - ENABLE_CRON=TRUE\r\n      - ENABLE_SMTP=FALSE\r\n    volumes:\r\n      - ./freepbx-data:/data\r\n      - ./freepbx-logs:/var/log\r\n    ports:\r\n      - \"80:80\"\r\n      - \"5060:5060/udp\"\r\n      - \"5160:5160/udp\"\r\n      - \"16384-16484:16384-16484/udp\"\r\n    restart: unless-stopped\r\n```\r\n\r\nBy using these Docker containers, you can easily set up and manage a VoIP system on your home server. This allows you to create your own phone system, potentially saving on communication costs and gaining more control over your telecommunications infrastructure.\r\n\r\nRemember to carefully consider your specific needs and technical expertise when choosing between Asterisk and FreePBX. While Asterisk offers more flexibility and customization options, it requires more technical knowledge to set up and maintain. FreePBX, on the other hand, provides a more user-friendly interface for managing Asterisk, making it a good choice for those who prefer a graphical interface and easier management.\r\n\r\n## Best Practices for Managing Docker Containers\r\n\r\nWhen running multiple Docker containers on your home server, it's important to follow best practices to ensure optimal performance, security, and maintainability. Let's explore some key areas to focus on:\r\n\r\n### Container organization\r\n\r\nProperly organizing your Docker containers can make management and troubleshooting much easier:\r\n\r\n- Use meaningful container names and labels for easy identification\r\n- Group related containers using Docker Compose\r\n- Implement a consistent naming convention for volumes and networks\r\n- Use Docker networks to isolate container groups\r\n\r\nExample of organizing containers with Docker Compose:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  web:\r\n    image: nginx:latest\r\n    container_name: myapp-web\r\n    labels:\r\n      - \"com.example.description=Web server for MyApp\"\r\n    networks:\r\n      - myapp-network\r\n\r\n  db:\r\n    image: mysql:5.7\r\n    container_name: myapp-db\r\n    labels:\r\n      - \"com.example.description=Database for MyApp\"\r\n    networks:\r\n      - myapp-network\r\n\r\nnetworks:\r\n  myapp-network:\r\n    name: myapp-network\r\n```\r\n\r\n### Resource allocation\r\n\r\nProperly allocating resources to your containers ensures stable performance and prevents one container from impacting others:\r\n\r\n- Set memory limits for each container to prevent memory exhaustion\r\n- Use CPU quotas to prevent a single container from monopolizing CPU resources\r\n- Monitor resource usage and adjust limits as needed\r\n- Consider using Docker Swarm or Kubernetes for more advanced resource management in larger setups\r\n\r\nExample of setting resource limits in Docker Compose:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  myapp:\r\n    image: myapp:latest\r\n    deploy:\r\n      resources:\r\n        limits:\r\n          cpus: \"0.50\"\r\n          memory: 512M\r\n        reservations:\r\n          cpus: \"0.25\"\r\n          memory: 256M\r\n```\r\n\r\n### Security considerations\r\n\r\nSecuring your Docker containers is crucial to protect your home server and data:\r\n\r\n- Keep Docker and container images up to date\r\n- Use official images from trusted sources\r\n- Implement the principle of least privilege (run containers as non-root users when possible)\r\n- Use secrets management for sensitive data (e.g., [Docker secrets](https://www.bitdoze.com/docker-compose-secrets/) or environment variables)\r\n- Regularly scan your containers for vulnerabilities (e.g., using tools like Trivy or Clair)\r\n- Implement network segmentation using Docker networks\r\n\r\nExample of running a container as a non-root user:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  myapp:\r\n    image: myapp:latest\r\n    user: 1000:1000\r\n```\r\n\r\n### Updating and maintenance\r\n\r\nRegular updates and maintenance are essential for keeping your Docker environment healthy and secure:\r\n\r\n- Set up automated updates for your containers (e.g., using Watchtower)\r\n- Implement a backup strategy for your container data and configurations\r\n- Regularly prune unused images, containers, and volumes to free up disk space\r\n- Monitor container logs for errors and issues\r\n- Use Docker Compose for easier updates and rollbacks\r\n\r\nExample of using Watchtower for automated updates:\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  watchtower:\r\n    image: containrrr/watchtower\r\n    volumes:\r\n      - /var/run/docker.sock:/var/run/docker.sock\r\n    command: --interval 86400 --cleanup\r\n    restart: unless-stopped\r\n```\r\n\r\nBy following these best practices, you can ensure that your Docker-based home server remains organized, efficient, secure, and easy to maintain. Remember to regularly review and update your Docker setup to take advantage of new features and security improvements.\r\n\r\n## Troubleshooting Common Issues\r\n\r\nWhen running Docker containers on your home server, you may encounter various issues. Here are some common problems and their solutions:\r\n\r\n### Network connectivity problems\r\n\r\nNetwork issues can prevent containers from communicating with each other or the outside world. Here are some troubleshooting steps:\r\n\r\n1. Check container network settings:\r\n\r\n   - Ensure containers are on the correct network\r\n   - Verify port mappings are correct\r\n\r\n2. Inspect Docker networks:\r\n\r\n   ```\r\n   docker network ls\r\n   docker network inspect <network_name>\r\n   ```\r\n\r\n3. Use tools like `ping` or `curl` from within containers to test connectivity:\r\n\r\n   ```\r\n   docker exec -it <container_name> ping <destination>\r\n   ```\r\n\r\n4. Check host firewall settings to ensure necessary ports are open\r\n\r\n5. Verify that the Docker daemon is running and responsive:\r\n   ```\r\n   systemctl status docker\r\n   ```\r\n\r\n### Container conflicts\r\n\r\nConflicts between containers can cause various issues. Here's how to address them:\r\n\r\n1. Check for port conflicts:\r\n\r\n   - Use `docker ps` to view running containers and their port mappings\r\n   - Modify port mappings in your Docker Compose file or run command if conflicts exist\r\n\r\n2. Resolve name conflicts:\r\n\r\n   - Ensure container names are unique\r\n   - Use `docker rename` to change conflicting names\r\n\r\n3. Address volume mount conflicts:\r\n\r\n   - Check volume definitions in your Docker Compose file\r\n   - Use `docker volume ls` and `docker volume inspect` to investigate volume issues\r\n\r\n4. Resolve network conflicts:\r\n   - Ensure containers are on the correct networks\r\n   - Use `docker network prune` to remove unused networks\r\n\r\n### Resource constraints\r\n\r\nResource limitations can cause containers to perform poorly or crash. Here's how to identify and resolve these issues:\r\n\r\n1. Monitor resource usage:\r\n\r\n   - Use `docker stats` to view container resource usage in real-time\r\n   - Implement monitoring solutions like Prometheus and Grafana for more detailed insights\r\n\r\n2. Adjust resource limits:\r\n\r\n   - Modify memory and CPU limits in your Docker Compose file or run command\r\n   - Example:\r\n     ```yaml\r\n     services:\r\n       myapp:\r\n         image: myapp:latest\r\n         deploy:\r\n           resources:\r\n             limits:\r\n               cpus: \"0.5\"\r\n               memory: 512M\r\n     ```\r\n\r\n3. Check for memory leaks:\r\n\r\n   - Monitor container memory usage over time\r\n   - Update or replace containers with memory leak issues\r\n\r\n4. Optimize container performance:\r\n\r\n   - Use multi-stage builds to create smaller images\r\n   - Implement proper caching strategies in Dockerfiles\r\n\r\n5. Consider upgrading hardware if resource constraints persist\r\n\r\nBy following these troubleshooting steps, you can resolve many common issues that arise when running Docker containers on your home server. Remember to keep your Docker installation and container images up to date, as newer versions often include bug fixes and performance improvements.\r\n\r\n## Conclusions\r\n\r\n### Recap of the best Docker containers for home servers\r\n\r\nThroughout this article, we've explored a wide range of Docker containers that can enhance your home server setup. Here's a quick recap of some of the best options across various categories:\r\n\r\n- Media Management: Plex, Jellyfin, Sonarr, Radarr\r\n- File Sharing: Nextcloud, Syncthing\r\n- Home Automation: Home Assistant, Node-RED\r\n- Network Management: Pi-hole, Traefik\r\n- Monitoring: Grafana, Prometheus\r\n- Security: Bitwarden, OpenVPN\r\n- Productivity: Bookstack, Kanboard\r\n- Development: GitLab, Jenkins\r\n\r\nThese containers offer powerful, customizable solutions for various home server needs, from media streaming and file sharing to home automation and network management.\r\n\r\n### Future trends in home server containerization\r\n\r\nAs Docker and container technology continue to evolve, we can expect several trends to shape the future of home server containerization:\r\n\r\n1. Increased focus on security: With growing concerns about data privacy and security, future container solutions will likely place even greater emphasis on built-in security features and best practices.\r\n\r\n2. Improved resource management: As home servers become more powerful, we can expect more sophisticated resource allocation and management tools to optimize container performance.\r\n\r\n3. Integration with edge computing: The rise of edge computing may lead to new container solutions that better leverage distributed computing resources in home environments.\r\n\r\n4. AI and machine learning integration: More containers may incorporate AI and machine learning capabilities to enhance automation, data analysis, and user experience.\r\n\r\n5. Simplified management interfaces: As containerization becomes more mainstream, we can expect more user-friendly interfaces and management tools to emerge, making it easier for non-technical users to leverage container technology.\r\n\r\n### Final thoughts and recommendations\r\n\r\nDocker containers offer an excellent way to enhance your home server's capabilities while maintaining flexibility and ease of management. Here are some final recommendations:\r\n\r\n1. Start small: Begin with a few essential containers and gradually expand your setup as you become more comfortable with Docker.\r\n\r\n2. Prioritize security: Always follow security best practices, including keeping your containers updated, using secure networks, and implementing proper authentication.\r\n\r\n3. Monitor and maintain: Regularly monitor your containers' performance and resource usage, and perform maintenance tasks like updates and backups.\r\n\r\n4. Join the community: Engage with the Docker and home server communities to stay informed about new developments, share experiences, and get help when needed.\r\n\r\n5. Experiment and customize: Don't be afraid to experiment with different containers and configurations to find the perfect setup for your needs.\r\n\r\nBy leveraging Docker containers, you can create a powerful, flexible, and customized home server environment that caters to your specific needs while providing valuable learning opportunities in containerization and server management.\r\n\r\nRemember that the world of Docker containers is constantly evolving, with new and improved solutions emerging regularly. Stay curious, keep learning, and enjoy the process of building and maintaining your ideal home server setup!","src/content/posts/docker-containers-home-server.mdx",[888],"../../assets/images/24/07/docker-containers.jpeg","da83dfd13248363a","docker-containers-home-server.mdx","dokploy-docker-compose-app",{id:891,data:893,body:903,filePath:904,assetImports:905,digest:907,legacyId:908,deferredRender:32},{title:894,description:895,date:896,image:897,authors:898,categories:899,tags:900,canonical:902},"How To Deploy A Docker Compose App in Dokploy","Learn how you can deploy a docker compose self hosted app in dokploy and host any application you want.",["Date","2024-08-29T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/08/dokploy-docker-compose-app.jpeg",[19],[98],[901,242],"dokploy","https://www.bitdoze.com/dokploy-docker-compose-app/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/08/dns-a-rec.png\";\r\nimport img2 from \"../../assets/images/24/08/dokploy-compose.png\";\r\nimport img3 from \"../../assets/images/24/08/dokploy-env.png\";\r\nimport img4 from \"../../assets/images/24/08/dokploy-domain-add.png\";\r\n\r\n[Dokploy](https://dokploy.com/) is an open-source, self-hostable Platform as a Service (PaaS) that simplifies the deployment and management of applications and databases using Docker and Traefik. It serves as a free alternative to popular platforms like Vercel, Heroku, and Netlify, offering robust features for developers who prefer to manage their own infrastructure.\r\n\r\n## Dokploy Features\r\n\r\n- **Applications**: Deploy any type of application (Node.js, PHP, Python, Go, Ruby, etc.) with ease.\r\n- **Databases**: Create and manage databases with support for MySQL, PostgreSQL, MongoDB, MariaDB, Redis, and more.\r\n- **Docker Management**: Easily deploy and manage Docker containers.\r\n- **Traefik Integration**: Automatically integrates with Traefik for routing and load balancing.\r\n- **Real-time Monitoring**: Monitor CPU, memory, storage, and network usage.\r\n- **Database Backups**: Automate backups with support for multiple storage destinations.\r\n\r\nI have already made a complete article of how you can [install and configure Dokploy on your own VPS server](https://www.bitdoze.com/dokploy-install/) you can check it for more details. Now we are going to see how you can use Dokploy to deploy any self-hosted app with the help of docker compose. Dokploy has some templates but maybe you don't find all the apps in there and that's why you may want to deploy an app with docker compose.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## How To Deploy A Docker Compose App in Dokploy\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/mJY4lXbXsPM\"\r\n  label=\"How To Deploy A Docker Compose App in Dokploy\"\r\n/>\r\n\r\nThings are not complicated at all as it is using Traefik under the hood which makes things easey to do. If you want to learn more about Traefik you can check:\r\n\r\n- [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n- [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/)\r\n\r\nNow let's get started and see what's needs to be done, this is the Dokploy official doc if you need something more: [Docker Compose Setup](https://docs.dokploy.com/en/docs/core/domain/docker-compose-setup), [Example](https://docs.dokploy.com/en/docs/core/docker-compose/example)\r\n\r\n### 1. Point The Domain to Server IP\r\n\r\nThe first thing is to be sure that the domain or subdomain is pointing to your server IP address where you have Dokploy set up:\r\n\r\n- Add an A record to your DNS settings:\r\n- - Name: Enter the route you want to point to (e.g., app for app.yourdomain.com).\r\n- - Value: Type in the IP address of your server, such as 1.2.3.4.\r\n\r\n<Picture src={img1} alt=\"DNS A Record\" />\r\n\r\n### 2. Prepare the Docker Compose File For Dokploy\r\n\r\nThe docker compose file needs to be modified to work with dokploy, you need to add the `dokploy-network`, add the volumes in case something is used and traefik labels to work with Dokploy. Below is an complete file for Flowise AI which I instaled several times before with CloudFlare Tunnels or with Traefik.\r\n\r\n```yml\r\nservices:\r\n  flowise-db:\r\n    image: postgres:16-alpine\r\n    networks:\r\n      - dokploy-network\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    volumes:\r\n      - flowise-db-data:/var/lib/postgresql/data\r\n    restart: unless-stopped\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n\r\n  flowise:\r\n    image: flowiseai/flowise:latest\r\n    healthcheck:\r\n      test: wget --no-verbose --tries=1 --spider http://localhost:${PORT}\r\n    volumes:\r\n      - flowiseai:/root/.flowise\r\n    environment:\r\n      DEBUG: false\r\n      PORT: ${PORT}\r\n      FLOWISE_USERNAME: ${FLOWISE_USERNAME}\r\n      FLOWISE_PASSWORD: ${FLOWISE_PASSWORD}\r\n      APIKEY_PATH: /root/.flowise\r\n      SECRETKEY_PATH: /root/.flowise\r\n      LOG_LEVEL: info\r\n      LOG_PATH: /root/.flowise/logs\r\n      DATABASE_TYPE: postgres\r\n      DATABASE_PORT: 5432\r\n      DATABASE_HOST: flowise-db\r\n      DATABASE_NAME: ${POSTGRES_DB}\r\n      DATABASE_USER: ${POSTGRES_USER}\r\n      DATABASE_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\n    networks:\r\n      - dokploy-network\r\n    depends_on:\r\n      flowise-db:\r\n        condition: service_healthy\r\n    entrypoint: /bin/sh -c \"sleep 3; flowise start\"\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.http.routers.flowiseai.rule=Host(`flowise.domain.com`)\"\r\n      - \"traefik.http.routers.flowiseai.entrypoints=websecure\"\r\n      - \"traefik.http.routers.flowiseai.tls.certResolver=letsencrypt\"\r\n      - \"traefik.http.services.flowiseai.loadbalancer.server.port=3000\"\r\n\r\nvolumes:\r\n  flowiseai:\r\n    driver: local\r\n  flowise-db-data:\r\n    driver: local\r\n\r\nnetworks:\r\n  dokploy-network:\r\n    external: true\r\n```\r\n\r\nIn here you see that I have aded the `dokploy-network` my vars and everything. Also the labes are per their specifications:\r\n\r\n```yml\r\nlabels:\r\n  - \"traefik.enable=true\"\r\n  - \"traefik.http.routers.<unique-name>.entrypoints=websecure\"\r\n  - \"traefik.http.routers.<unique-name>.tls.certResolver=letsencrypt\"\r\n  - \"traefik.http.routers.<unique-name>.rule=Host(`app.yourdomain.com`)\"\r\n  - \"traefik.http.services.<unique-name>.loadbalancer.server.port=3000\"\r\n```\r\n\r\nYou need to add the Host and Port for your app, in my case the app is using internaly the 3000 port.\r\n\r\nWith this file modified with your details you create a new project in Dokploy if you didn't already and hit Create Service - Compose\r\n\r\n<Picture src={img2} alt=\"Dokploy Compose\" />\r\n\r\nAfter you go and select the project abd you go to General - Raw and paste your compose file and save it.\r\n\r\n> Don't add any container name as it can cause issues with logs as per Dokploy doc\r\n\r\n### 3. Add Env Variables\r\n\r\nIn the above example I am using `.env` vars to store some users and passwords, Dokploy can easely be used to add variables in environment tab.\r\n\r\n```sh\r\nPORT=3000\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='flowise'\r\nFLOWISE_USERNAME=bitdoze\r\nFLOWISE_PASSWORD=bitdoze\r\n```\r\n\r\n<Picture src={img3} alt=\"Dokploy env\" />\r\n\r\n### 4. (Optional) Use Domains to deploy the domain\r\n\r\nIf you don't want to use the labels in the docker-compose file you can use the domain tab and add the domain in there, you can refresh the service name and add the domain details with port in there. Dokploy will add the labels for you. It's app to you.\r\n\r\n<Picture src={img4} alt=\"Dokploy Domain add\" />\r\n\r\n### 5. Deploy your APP,\r\n\r\nAfter the only thing remaining is to deploy your app, You move to General and hit Deploy, after you can check the logs in deployments to see the details and if successful.\r\n\r\n## Conclusions\r\n\r\nDokploy makes easy to deploy any app with docker compose, in this way you can take advantage of dokploy features and use docker compose.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).","src/content/posts/dokploy-docker-compose-app.mdx",[906],"../../assets/images/24/08/dokploy-docker-compose-app.jpeg","f98c24ebf5701992","dokploy-docker-compose-app.mdx","docmost-docker-install",{id:909,data:911,body:920,filePath:921,assetImports:922,digest:924,legacyId:925,deferredRender:32},{title:912,description:913,date:914,image:915,authors:916,categories:917,tags:918,canonical:919},"Docmost Docker Compose Install: Self-Hosted Note App for Teams","Learn how you can install Docmost and self host you note taking app for your team with docker and docker compose.",["Date","2024-07-29T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/docmost-docker-install.jpeg",[19],[98],[242],"https://www.bitdoze.com/docmost-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\nimport imag2 from \"../../assets/images/24/07/docmost-ui.png\";\r\n\r\n[Docmost](https://docmost.com/) is an open-source collaborative wiki and documentation software that offers a powerful alternative to popular platforms like Confluence and Notion. Designed for teams and organizations seeking a self-hosted solution for knowledge management, Docmost provides a robust set of features to create, organize, and share information efficiently.\r\n\r\nAt its core, Docmost offers a user-friendly interface for creating and editing documents, with support for rich text formatting, markdown, and collaborative editing. The platform allows users to structure their content hierarchically, making it easy to navigate through complex information architectures. One of Docmost's standout features is its ability to create interconnected documents, enabling users to build a comprehensive knowledge base with cross-referencing capabilities.\r\n\r\nDocmost supports team collaboration through features like real-time editing, commenting, and version history. This makes it an ideal tool for project documentation, internal wikis, and knowledge sharing within organizations. The platform also offers customizable permissions and access controls, allowing administrators to manage user roles and content visibility effectively.\r\n\r\nAnother significant advantage of Docmost is its search functionality, which enables users to quickly find relevant information across the entire knowledge base. This is particularly useful for large organizations with extensive documentation.\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nDocmost offers several key features:\r\n\r\n1. **Real-time collaborative rich-text editor:** Multiple users can simultaneously edit pages in real-time, supporting tables, math (LaTeX), and callouts.\r\n\r\n2. **Spaces:** Content can be organized into distinct spaces dedicated to different teams, projects, or departments, providing a structured environment for collaboration.\r\n\r\n3. **Permissions system:** Robust controls allow administrators to manage who can view, edit, and manage content, ensuring information security and proper access.\r\n\r\n4. **Groups:** Users can be organized into groups for unified permissions management, simplifying access control.\r\n\r\n5. **Comment system:** An integrated inline commenting feature allows for meaningful discussions directly on pages.\r\n\r\n6. **Page History:** Users can view the history of changes made to each page, track edits over time, and revert to previous versions if needed.\r\n\r\n7. **Search functionality:** Powered by Postgres full-text search, allowing quick location of information across all pages.\r\n\r\n8. **Nested pages:** Supports page nesting to any level, with drag-and-drop functionality for easy reordering and management.\r\n\r\n9. **Attachments:** Images and videos can be easily attached to pages by pasting from the clipboard, with support for both S3 and local storage drivers.\r\n\r\n10. **File attachment: **Users can attach files to pages for easy reference and sharing.\r\n\r\nDocmost is designed as an alternative to platforms like Confluence and Notion, offering these features in an open-source package that can be self-hosted. It's particularly useful for managing wikis, knowledge bases, and documentation for teams and organizations.\r\n\r\nIf you are looking for other apps for your note taking needs you can check:\r\n\r\n- [How to Install Outline Wiki on Docker](https://www.bitdoze.com/outline-install/)\r\n- [How to Install Memos with Docker Compose](https://www.bitdoze.com/memos-install/)\r\n\r\n## Setting Up Docmost with Docker Compose\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/jFxf4dFKh9s\"\r\n  label=\"Docmost Installation\"\r\n/>\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host Docmost, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n- OR reverse proxy with CloudPanel you can check: [Setup CloudPanel As Reverse Proxy with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\n### 2. Docker Compose File\r\n\r\n```yaml\r\nversion: \"3\"\r\n\r\nservices:\r\n  docmost:\r\n    image: docmost/docmost:latest\r\n    container_name: docmost\r\n    hostname: docmost\r\n    user: root\r\n    healthcheck:\r\n      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/3000' || exit 1\r\n      interval: 10s\r\n      timeout: 5s\r\n      retries: 3\r\n      start_period: 90s\r\n    depends_on:\r\n      - docmost-db\r\n      - docmost-redis\r\n    environment:\r\n      APP_URL: \"${APP_URL}\"\r\n      APP_SECRET: \"${APP_SECRET}\"\r\n      DATABASE_URL: \"postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@docmost-db:5432/${POSTGRES_DB}?schema=public\"\r\n      REDIS_URL: \"redis://docmost-redis:6379\"\r\n    ports:\r\n      - \"5031:3000\"\r\n    restart: unless-stopped\r\n    volumes:\r\n      - ./docmost:/app/data/storage\r\n\r\n  docmost-db:\r\n    image: postgres:16-alpine\r\n    container_name: docmost-DB\r\n    hostname: docmost-db\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    volumes:\r\n      - ./docmost-db:/var/lib/postgresql/data:rw\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\n\r\n  docmost-redis:\r\n    image: redis:7.2-alpine\r\n    container_name: docmost-redis\r\n    hostname: docmost-redis\r\n    restart: unless-stopped\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"redis-cli ping || exit 1\"]\r\n    volumes:\r\n      - ./redis_data:/data\r\n```\r\n\r\nThis Docker Compose file defines three services: docmost (the main application), docmost-db (PostgreSQL database), and docmost-redis (Redis cache).\r\n\r\n1. `docmost` service:\r\n\r\n- Uses the latest Docmost image\r\n- Sets up a healthcheck to ensure the application is running\r\n- Depends on the database and Redis services\r\n- Configures environment variables for the application\r\n- Maps port 5031 on the host to port 3000 in the container\r\n- Mounts a volume for data storage\r\n\r\n2. `docmost-db` service:\r\n\r\n- Uses PostgreSQL 16 Alpine image\r\n- Sets up a healthcheck for the database\r\n- Configures environment variables for the database\r\n- Mounts a volume for persistent data storage\r\n\r\n3. `docmost-redis` service:\r\n\r\n- Uses Redis 7.2 Alpine image\r\n- Sets up a healthcheck for Redis\r\n- Mounts a volume for data persistence\r\n\r\n### 3. Docmost `.env` file\r\n\r\nThe `.env` file contains environment variables used by the Docker Compose configuration:\r\n\r\n```sh\r\nAPP_URL=docs.bitdoze.com\r\nPOSTGRES_DB=docmost\r\nPOSTGRES_USER=docmost\r\nPOSTGRES_PASSWORD=docmost\r\nAPP_SECRET='9Qyodnm1R7IYjVi7kKKL57pdlkDwkNK+dut3i8xRAZI='\r\n```\r\n\r\n- `APP_URL:` The URL where Docmost will be accessible\r\n- `POSTGRES_DB`, POSTGRES_USER, POSTGRES_PASSWORD: Database configuration\r\n- `APP_SECRET`: A secret key for the application (should be randomly generated)\r\n\r\nTo generate a secure APP_SECRET, use the command:\r\n\r\n```sh\r\nopenssl rand -base64\r\n```\r\n\r\n### 4. Enhancing Docmost with SMTP\r\n\r\nYou can add SMTP tou your Docmost instalation you just need to add your `environment` for docker compose file and add the passwords in `.env` file, [Docmost doc](https://docmost.com/docs/self-hosting/environment-variables) can be checked for more.\r\n\r\n```sh\r\n# Email Configuration\r\nMAIL_DRIVER=smtp\r\nSMTP_HOST=smtp.example.com\r\nSMTP_PORT=587\r\nSMTP_USERNAME=your_username\r\nSMTP_PASSWORD=your_password\r\nMAIL_FROM_ADDRESS=hello@example.com\r\nMAIL_FROM_NAME=Docmost\r\n```\r\n\r\n### 5. Deploying the Docker Compose File for Docmost\r\n\r\nOnce you have your Docker Compose and .env files set up, deploying Docmost is straightforward. Simply run the following command in the directory containing your Docker Compose file:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nThis command will:\r\n\r\n1. Pull the necessary Docker images if they're not already present on your system.\r\n2. Create and start the containers defined in your Docker Compose file.\r\n3. Run the containers in detached mode (-d), allowing them to run in the background.\r\n\r\nAfter running this command, you should see output indicating that the containers are being created and started. Once complete, you can verify that the containers are running with:\r\n\r\n```sh\r\ndocker-compose ps\r\n```\r\n\r\nThis will show you the status of your Docmost and PostgreSQL containers.\r\n\r\n### 6. Implementing SSL with CloudFlare Tunnels for Docmost\r\n\r\n[CloudFlare Tunnels](https://www.cloudflare.com/products/tunnel/) offer a innovative solution for securely connecting your web applications to the internet without the need for public IP addresses or opening inbound ports on your firewall. This service, part of CloudFlare's suite of security and performance tools, provides a secure tunnel between your origin server and CloudFlare's edge network.\r\n\r\nCloudFlare Tunnels operate on a simple yet powerful principle:\r\n\r\n1. **Outbound Connection**: Your server initiates an outbound connection to CloudFlare's network using the CloudFlare daemon (cloudflared).\r\n2. **Tunnel Creation**: This connection establishes a secure tunnel between your origin and CloudFlare's edge.\r\n3. **Traffic Routing**: Incoming requests to your domain are routed through this tunnel to your origin server.\r\n4. **Response Delivery**: Responses from your server are sent back through the tunnel and delivered to the user.\r\n\r\nThis process effectively eliminates the need for traditional port forwarding or firewall configuration, as all traffic flows through the secure tunnel.\r\n\r\nGo in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 7. Accessing The DocMost UI\r\n\r\nThe you can go and access Docmost with your `APP_URL` that yu have set, you will be promted to create a username and password at beginning.\r\n\r\n<Picture src={imag2} alt=\"Docmost UI\" />\r\n\r\n## Conclusion\r\n\r\nDeploying Docmost using Docker Compose offers a robust and efficient way to set up a self-hosted documentation and collaboration platform.\r\n\r\nBy following this guide, you've successfully set up a powerful, self-hosted documentation platform that offers the benefits of popular commercial solutions while maintaining full control over your data and infrastructure. Docmost, deployed through Docker Compose, provides a solid foundation for efficient knowledge management and collaboration within your organization.\r\n\r\nFor more Docker containers that can enhance your self-hosted setup, including other productivity and collaboration tools, don't forget to check out our guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This comprehensive list can help you discover additional applications to complement your Docmost installation and further improve your team's productivity.","src/content/posts/docmost-docker-install.mdx",[923],"../../assets/images/24/07/docmost-docker-install.jpeg","eae588a5fc17f3e1","docmost-docker-install.mdx","dokploy-install",{id:926,data:928,body:937,filePath:938,assetImports:939,digest:941,legacyId:942,deferredRender:32},{title:929,description:930,date:931,image:932,authors:933,categories:934,tags:935,canonical:936},"Dokploy Install - Ditch Vercel, Heroku and Self-Host Your SaaS","Dokploy install and presentation an alternative to serverless like Vercel, Heroku, etc",["Date","2024-06-11T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/05/dokploy-install.jpeg",[19],[98],[901,242],"https://www.bitdoze.com/dokploy-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/05/dokploy-app.png\";\r\n\r\n[Dokploy](https://dokploy.com/) is an open-source, self-hostable Platform as a Service (PaaS) that simplifies the deployment and management of applications and databases using Docker and Traefik. It serves as a free alternative to popular platforms like Vercel, Heroku, and Netlify, offering robust features for developers who prefer to manage their own infrastructure.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n## Dokploy Features\r\n\r\n- **Applications**: Deploy any type of application (Node.js, PHP, Python, Go, Ruby, etc.) with ease.\r\n- **Databases**: Create and manage databases with support for MySQL, PostgreSQL, MongoDB, MariaDB, Redis, and more.\r\n- **Docker Management**: Easily deploy and manage Docker containers.\r\n- **Traefik Integration**: Automatically integrates with Traefik for routing and load balancing.\r\n- **Real-time Monitoring**: Monitor CPU, memory, storage, and network usage.\r\n- **Database Backups**: Automate backups with support for multiple storage destinations.\r\n\r\nYou can check [Dokploy Deploy Apps with Docker Compose](https://www.bitdoze.com/dokploy-docker-compose-app/) if you want to see how you can deploy any application with Docker Compose in Dokploy.\r\n\r\n## Install Dokploy\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/XohTt3lh9qg\"\r\n  label=\"Dokploy Installation\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### Setup A VPS\r\n\r\nTo get started with Dokploy, you need a Virtual Private Server (VPS).\r\nIn the video, we go into detail about how you can do that on Hetzner. You can check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/) for more details if you are not aware of Hetzner and what it can do.\r\n\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n```sh\r\nssh username@your_vps_ip\r\n```\r\n\r\n### Add SWAP\r\n\r\nAdding swap space can help improve the performance of your VPS by providing additional virtual memory. Follow these steps to add a 2GB swap file:\r\n\r\n```sh\r\nsudo fallocate -l 2G /swapfile\r\nsudo chmod 600 /swapfile\r\nsudo mkswap /swapfile\r\nsudo swapon /swapfile\r\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\r\n```\r\n\r\n### Install Dokploy\r\n\r\nOnce your VPS is set up and swap space is added, you can install Dokploy using the following command:\r\n\r\n```sh\r\ncurl -sSL https://dokploy.com/install.sh | sh\r\n```\r\n\r\nThis command will download and run the Dokploy installation script, setting up Dokploy and its dependencies on your server.\r\n\r\n### Point Your Domain or Subdomain to Dokploy\r\n\r\nTo access your Dokploy instance via a custom domain or subdomain, you need to configure your DNS settings:\r\n\r\n1. **Log in to your DNS provider**: Access the DNS management console of your domain registrar.\r\n2. **Create an A Record**: Point your domain or subdomain to the IP address of your VPS.\r\n\r\nFor example, to point `app.yourdomain.com` to your VPS:\r\n\r\n```\r\nType: A\r\nName: app\r\nValue: your_vps_ip\r\nTTL: 3600\r\n```\r\n\r\n## Start Deploying Apps\r\n\r\n<Picture src={img1} alt=\"Dokploy App\" />\r\n\r\nWith Dokploy installed and your domain configured, you can start deploying applications. Dokploy supports a wide range of applications and databases, making it easy to manage your projects from a single platform.\r\n\r\n1. **Access Dokploy Dashboard**: Open your web browser and navigate to `http://your-ip-from-your-vps:3000`.\r\n2. **Create an Admin Account**: Follow the on-screen instructions to set up your administrative account.\r\n3. **Deploy Applications**: Use the Dokploy dashboard to deploy and manage your applications and databases.\r\n\r\n### Conclusions\r\n\r\nDokploy offers a powerful and flexible solution for developers looking to self-host their applications. By leveraging Docker and Traefik, Dokploy simplifies the deployment process while providing robust features for application and database management. Whether you're working on personal projects or managing large-scale applications, Dokploy provides the tools you need to streamline your deployment workflow.","src/content/posts/dokploy-install.mdx",[940],"../../assets/images/24/05/dokploy-install.jpeg","ae9f04e190a0b6e0","dokploy-install.mdx","easypanel-modern-server-control-panel",{id:943,data:945,body:954,filePath:955,assetImports:956,digest:958,legacyId:959,deferredRender:32},{title:946,description:947,date:948,image:949,authors:950,categories:951,tags:952,canonical:953},"Easypanel.io: A Modern Hosting Panel for Applications and Databases","Check out Easypanel.io, a web-based server control panel that helps you deploy and manage applications, databases, and SSL certificates with a simple and powerful interface. It supports various languages, frameworks, templates, and integrations. It also offers zero-downtime deployments and web consoles.",["Date","2023-11-09T06:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/easeypanel-overview.jpeg",[19],[98],[662,24],"https://www.bitdoze.com/easypanel-modern-server-control-panel/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/11/add-domain.jpeg\";\r\nimport img2 from \"../../assets/images/23/11/Easypanel-app-deploy.jpeg\";\r\n\r\n[Easypanel.io](https://easypanel.io/) is a web-based server control panel that allows you to deploy and manage applications, databases, and SSL certificates with ease. It supports various programming languages and frameworks, and uses Docker and Cloud Native Buildpacks to create and run containers. It also provides web consoles, logs, templates, backups, and more features to help you manage your server without fighting the terminal. Easypanel.io is simple, powerful, and reliable. It has received positive feedback from many users who appreciate its flexibility and convenience.\r\n\r\nEasyPanel can be used for free and functionalities can be extended with the pro plans.\r\n\r\n## Easypanel.io Top Features\r\n\r\n- **Web-based server control panel:** You can deploy and manage applications, databases, and SSL certificates with a user-friendly interface.\r\n- **Supports various languages and frameworks:** You can run any application using Docker and Cloud Native Buildpacks. It supports Node.js, Ruby, Python, PHP, Go, Java, and more.\r\n- **Integrates with Github:** You can push your code to Github and Easypanel.io will automatically build and deploy it for you.\r\n- **Provides Let’s Encrypt certificates:** You can secure your websites with free and auto-renewable SSL certificates from Let’s Encrypt.\r\n- **Offers easy templates:** You can use ready-made templates to deploy popular applications such as WordPress, Laravel, Django, etc. in seconds.\r\n- **Enables zero-downtime deployments:** You can update your applications without any interruptions for your users.\r\n- **Includes web consoles and logs:** You can check the logs or run shell commands for your applications and databases from the web interface.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n## What Easypanel.io Can Help You Deploy\r\n\r\nThere are a lot of things that can be deployed to easypanel.io, this panel can be an alternative to Netlify or Heroku that can help you deploy your projects and websites. Besides that, it has a lot of templates that can help with deploying apps which makes it a great alternative to services like [elsestio](https://elest.io/)\r\n\r\n### Databases\r\n\r\nEasypanel.io can help you deploy various database services with a few clicks. Some of the databases that you can use with Easypanel.io are:\r\n\r\n- **MySQL**: A popular open-source relational database management system that supports SQL queries and transactions.\r\n- **MMariaDB:** A fork of MySQL that offers enhanced performance, security, and compatibility.\r\n- **Postgres:** A powerful and scalable relational database system that supports advanced features such as JSON, full-text search, and GIS.\r\n- **MongoDB:** A document-oriented database that stores data in flexible JSON-like format and supports dynamic schemas and aggregation.\r\n- **Redis:** An in-memory data structure store that can be used as a database, cache, or message broker. It supports various data types such as strings, lists, sets, hashes, and streams.\r\n\r\n### Templates\r\n\r\n[Easypanel.io has a collection](https://easypanel.io/templates) of templates that can help you deploy popular applications in seconds, the list contains more then 200+ templates that you can use. Some of the most known templates are:\r\n\r\n- **WordPress:** A widely used content management system that powers millions of websites and blogs. It offers a rich set of features, themes, and plugins to create and customize your online presence.\r\n- **Plausible:** A lightweight and open-source web analytics tool that respects user privacy and does not use cookies. It helps you to track website traffic and conversions without compromising on performance or security\r\n- **n8n:** A powerful workflow automation tool that allows you to connect and integrate various apps and services. You can use a graphical interface to create custom workflows or use the REST API to send and receive data\r\n- **Listmonk:** A free and open-source newsletter and mailing list manager that supports multiple lists, campaigns, templates, and subscribers. It uses PostgreSQL as its data store and offers a web interface and an API for managing your email marketing\r\n- **Kelia:** A baby girl name that means “bright-headed” or “descendant of Ceallach”. It is of Celtic origin and has several variations such as Keelia, Keely, or Kelly\r\n- **Grafana:** A popular open-source platform for data visualization and monitoring. It allows you to query, analyze, and alert on metrics from various sources and create beautiful dashboards and alerts. It supports over 350 integrations and has a vibrant community\r\n- **SuiteCRM:** An enterprise-ready open-source CRM software that provides a comprehensive solution for sales, marketing, and service. It is a fork of SugarCRM and offers many features such as modules, workflows, reports, security, and customization\r\n- **Uptime Kuma:** A fancy self-hosted monitoring tool that lets you check the uptime and performance of your websites and applications. It supports various protocols and methods such as HTTP, TCP, Ping, DNS, and more. It also provides notifications via various channels such as Telegram, Discord, Slack, and email\r\n\r\n[Easypanel.io complete list off templates](https://easypanel.io/templates)\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n### Apps\r\n\r\nEasypanel.io can help you deploy applications or static websites that are hosted on GitHub, it will automatically fetch the latest version if you commit something to keep everything up to date. You can deploy, static websites, node js applications, etc.\r\n\r\nYou can check the below EasyPanel tutorials:\r\n\r\n- [How to Deploy Astro on Your VPS with EasyPanel](https://www.bitdoze.com/deploy-astro-easypanel/)\r\n\r\n## Easypanel.io Installation\r\n\r\nIn the next part, we are going to see how we can deploy our Easypanel installation on a Hetzner VPS and start deploying apps and templates to the server.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/RxgKE8TkQyQ\"\r\n  label=\"Easypanel.io Installation\"\r\n/>\r\n\r\n### 1.Deploy a Hetzner VPS\r\n\r\nThe first part is to have a server ready where we can deploy Easypanel, my preferred choice is Easypanel. The recommendation for Easypanel is to have at least 2 CPUs and 2 GB of memory and you can use an Ubuntu installation. In the video, we go into detail about how you can do that on Hetzner. You can check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/) for more details if you are not aware of Hetzner and what it can do.\r\n\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n### 2. Update the VPS server\r\n\r\nThe server needs to be up to date with latest packages, you can do that by running the bellow and reboot after.\r\n\r\n```sh\r\napt update && apt -y upgrade\r\nreboot\r\n```\r\n\r\n### 3. Add Server Swap\r\n\r\nSome of the VPS providers will add swap but not Hezner, to do that you can follow the below commands, and adjust the size of the swap in function of the server memory.\r\n\r\n```sh\r\nsudo fallocate -l 2G /swapfile\r\nsudo chmod 600 /swapfile\r\nsudo mkswap /swapfile\r\nsudo swapon /swapfile\r\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\r\n```\r\n\r\n### 4. Install Easypanel.io\r\n\r\nTo do this you just need to run a simple command and Easypanel will be installed:\r\n\r\n```sh\r\ncurl -sSL https://get.easypanel.io | sh\r\n```\r\n\r\nBellow you have the output for this:\r\n\r\n```sh\r\n# Executing docker install script, commit: e5543d473431b782227f8908005543bb4389b8de\r\n+ sh -c apt-get update -qq >/dev/null\r\n+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apt-transport-https ca-certificates curl >/dev/null\r\n+ sh -c install -m 0755 -d /etc/apt/keyrings\r\n+ sh -c curl -fsSL \"https://download.docker.com/linux/ubuntu/gpg\" | gpg --dearmor --yes -o /etc/apt/keyrings/docker.gpg\r\n+ sh -c chmod a+r /etc/apt/keyrings/docker.gpg\r\n+ sh -c echo \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu jammy stable\" > /etc/apt/sources.list.d/docker.list\r\n+ sh -c apt-get update -qq >/dev/null\r\n+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-ce-rootless-extras docker-buildx-plugin >/dev/null\r\n+ sh -c docker version\r\nClient: Docker Engine - Community\r\n Version:           24.0.7\r\n API version:       1.43\r\n Go version:        go1.20.10\r\n Git commit:        afdd53b\r\n Built:             Thu Oct 26 09:07:41 2023\r\n OS/Arch:           linux/amd64\r\n Context:           default\r\n\r\nServer: Docker Engine - Community\r\n Engine:\r\n  Version:          24.0.7\r\n  API version:      1.43 (minimum version 1.12)\r\n  Go version:       go1.20.10\r\n  Git commit:       311b9ff\r\n  Built:            Thu Oct 26 09:07:41 2023\r\n  OS/Arch:          linux/amd64\r\n  Experimental:     false\r\n containerd:\r\n  Version:          1.6.24\r\n  GitCommit:        61f9fd88f79f081d64d6fa3bb1a0dc71ec870523\r\n runc:\r\n  Version:          1.1.9\r\n  GitCommit:        v1.1.9-0-gccaecfc\r\n docker-init:\r\n  Version:          0.19.0\r\n  GitCommit:        de40ad0\r\n\r\n================================================================================\r\n\r\nTo run Docker as a non-privileged user, consider setting up the\r\nDocker daemon in rootless mode for your user:\r\n\r\n    dockerd-rootless-setuptool.sh install\r\n\r\nVisit https://docs.docker.com/go/rootless/ to learn about rootless mode.\r\n\r\n\r\nTo run the Docker daemon as a fully privileged service, but granting non-root\r\nusers access, refer to https://docs.docker.com/go/daemon-access/\r\n\r\nWARNING: Access to the remote API on a privileged Docker daemon is equivalent\r\n         to root access on the host. Refer to the 'Docker daemon attack surface'\r\n         documentation for details: https://docs.docker.com/go/attack-surface/\r\n\r\n================================================================================\r\n\r\nlatest: Pulling from easypanel/easypanel\r\n59bf1c3509f3: Pull complete\r\n05f4bd9ef508: Pull complete\r\n503741069fa0: Pull complete\r\n88b2b4880461: Pull complete\r\n17cd543fa95f: Pull complete\r\n6309c344e1de: Pull complete\r\nba8d083f659d: Pull complete\r\ne3989f5f1389: Pull complete\r\n3898d07198d0: Pull complete\r\nc6d552c5896f: Pull complete\r\na3d68703d70f: Pull complete\r\nffe8787e0b8a: Pull complete\r\n74edfe4a5e03: Pull complete\r\n1c5b4073f6eb: Pull complete\r\n79e34cbeeb4c: Pull complete\r\nDigest: sha256:12cd84c47f726c120ea056389ebc92a043c68c5efa0175ef073fa304d799ea9b\r\nStatus: Downloaded newer image for easypanel/easypanel:latest\r\ndocker.io/easypanel/easypanel:latest\r\nSwarm was initilized\r\nNetwork was created\r\nDefault certificate was created\r\n2.8: Pulling from library/traefik\r\n9621f1afde84: Pulling fs layer\r\n0448c1674845: Pulling fs layer\r\n49fdf0222254: Pulling fs layer\r\n04987ac71521: Pulling fs layer\r\n04987ac71521: Waiting\r\n0448c1674845: Verifying Checksum\r\n0448c1674845: Download complete\r\n9621f1afde84: Verifying Checksum\r\n9621f1afde84: Download complete\r\n9621f1afde84: Pull complete\r\n49fdf0222254: Verifying Checksum\r\n49fdf0222254: Download complete\r\n04987ac71521: Download complete\r\n0448c1674845: Pull complete\r\n49fdf0222254: Pull complete\r\n04987ac71521: Pull complete\r\nDigest: sha256:72164cb63797a902ba7c33dcb75349644bbf762323a0fed961ae25f54635ac93\r\nStatus: Downloaded newer image for traefik:2.8\r\ndocker.io/library/traefik:2.8\r\nTraefik image was pulled\r\nTraefik config was created\r\nTraefik service was created\r\nlatest: Pulling from easypanel/easypanel\r\nDigest: sha256:12cd84c47f726c120ea056389ebc92a043c68c5efa0175ef073fa304d799ea9b\r\nStatus: Image is up to date for easypanel/easypanel:latest\r\ndocker.io/easypanel/easypanel:latest\r\nEasypanel image was pulled\r\nEasypanel service was created\r\n\r\n\r\nEasypanel was installed successfully on your server!\r\n\r\n    http://23.88.117.41:3000\r\n```\r\n\r\nAt the end, you will have the access URL that will have 3000 port.\r\n\r\nThe docker processes for Easypanel are as follows:\r\n\r\n```sh\r\nroot@ep:~# docker ps\r\nCONTAINER ID   IMAGE                          COMMAND                  CREATED              STATUS                        PORTS                                                                      NAMES\r\nc7295671c7c5   easypanel/error-pages:latest   \"/bin/error-pages se…\"   About a minute ago   Up About a minute (healthy)                                                                              error-pages.1.llrg782ufd0dwkn8jtxbbqp11\r\n1e5a1e16da83   easypanel/easypanel:latest     \"npm run --silent st…\"   About a minute ago   Up About a minute             0.0.0.0:3000->3000/tcp, :::3000->3000/tcp                                  easypanel.1.nnsen969oblty7n4iqzjcdgvm\r\n71a49438ab9a   traefik:2.8                    \"/entrypoint.sh --ap…\"   About a minute ago   Up About a minute             0.0.0.0:80->80/tcp, :::80->80/tcp, 0.0.0.0:443->443/tcp, :::443->443/tcp   traefik.1.hrpoo62y2y4d6auk7vke6ih50\r\n```\r\n\r\n### 5. Configure Easypanel.io\r\n\r\nNext, we are going to start making the configs for Easeypanel to have it fully functional\r\n\r\n#### 5.1 Point Your Domain or Subdomain to Easypanel IP\r\n\r\nTo access easypanel secure with an SSL certificate and with our own domain you will need to go into the DNS administration for your domain and create a A record that will point to the server IP, I am doing this in the video with CloudFlare and a subdomain.\r\n\r\n#### 5.2 Add the domain in Easeypanel\r\n\r\nAfter the A record is there you will need to access Easeypanel, first, you will be prompted to create a user and after you go to in Settings - General and add your domain or subdomain.\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"EasyPanel Admin Domain\"\r\n/>\r\n\r\n#### 5.3 Configure the 2FA\r\n\r\nTo secure access you should activate the 2FA for your account with Google Authenticator app. You Just go to Settings - Authentication - Configure Two Factor Authentication. You will be prompted to scan the GR code in the app.\r\n\r\n#### 5.4 Create a Github Token\r\n\r\nIf you are using EasyPanel to deploy GitHub projects that are private you will need to link it to GitHub with the token, you can visit [https://github.com/settings/tokens](https://github.com/settings/tokens) and create your token. After you just add it to Settings - GitHub\r\n\r\n### 6. Deploy First Project\r\n\r\nNow you are ready to start deploying your first project, in the free version you can have up to 3 projects but each project can have multiple sub apps that can be added.\r\nFor a Github static website you can just add a new service APP and input GitHub details:\r\n\r\n<Picture\r\n  src={img2}\r\n  alt=\"EasyPanel app deploy\"\r\n/>\r\n\r\nIn the Build section you have multiple options to choose from to build your app, for this I am using Nixpacks.\r\n\r\nAfter you can go to domains and add your domain, it needs to be pointed to the server before.\r\n\r\nIn the same project, you can add databases or other apps through their templates.\r\n\r\nThe video has more details about EasyPanel.io interface and other things so you can check that for a more in-depth overview of easypanel.io\r\n\r\n## Conclusions\r\n\r\nEasyPanel is a great resource if you need a modern hosting panel that can help you host applications or databases fast you can use it for free and in case you need more like backups or advanced monitoring, user management you can go with a paid plan.","src/content/posts/easypanel-modern-server-control-panel.mdx",[957],"../../assets/images/23/11/easeypanel-overview.jpeg","dff521c529b92400","easypanel-modern-server-control-panel.mdx","embed-youtube-videos-to-gatsby",{id:960,data:962,body:972,filePath:973,assetImports:974,digest:976,legacyId:977,deferredRender:32},{title:963,description:964,date:965,image:966,authors:967,categories:968,tags:969,canonical:971},"How To Embed Youtube Videos to Gatsby ","See how you can Embed Youtube Videos to Gatsby easily",["Date","2022-09-20T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/How_To_Embed_Youtube Videos_to_Gatsby.jpeg",[19],[21],[970,24],"gatsby","https://www.bitdoze.com/embed-youtube-videos-to-gatsby/","Gatsby, it’s fast and built on React, I have started using it on this website moving from WordPress where everything was very easy. You can check this article if you are new to Gatsby and what to see the [best Gatsby online courses](https://www.bitdoze.com/gatsby-js-online-courses/).\r\n\r\nWith Gatsby, I have learned that everything we take for granted in other CMS systems needs some work to function properly here. Being new to Gatsby I have searched online to see how you can easily embed YouTube videos and there were different approaches that confused me at the beginning. In this article, we will see exactly what you need to do to easily embed videos from YouTube and Vimeo in your blog posts or pages.\r\n\r\nThis technique works with Contentful CMS or other Headless CMS systems that exist out there. Currently, I am using it with Contentful and I can say is the easiest way that you can use to have your videos embedded.\r\n\r\n**All this will work only if the Contentful is as markdown and not rich text.**\r\n\r\n## How To Embed YouTube Videos to Gatsby\r\n\r\nIframes are not a default thing for React so you can’t just go and insert your iframe with the video. This needs to be wrapped in React code to work.\r\n\r\nGatsby has plugins that can be used to make things easier. One plugin that you can use to embed YouTube videos is [gatsby-remark-better-embed-video](https://www.gatsbyjs.com/plugins/gatsby-remark-better-embed-video/)\r\n\r\nThis plugin can easily help you embed videos from YouTube, Vimeo or Twitch with just a link, below are the details:\r\n\r\n    `video: https://www.youtube.com/embed/2Xc9gXyf2G4`\r\n    `youtube: https://www.youtube.com/watch?v=2Xc9gXyf2G4`\r\n    `youtube: 2Xc9gXyf2G4`\r\n\r\n    `vimeo: https://vimeo.com/5299404`\r\n    `vimeo: 5299404`\r\n\r\n    `videoPress: https://videopress.com/v/kUJmAcSf`\r\n    `videoPress: kUJmAcSf`\r\n\r\n    `twitch: https://player.twitch.tv/?channel=dakotaz`\r\n    `twitch: https://player.twitch.tv/?autoplay=false&video=v273436948`\r\n    `twitch: 273436948`\r\n    `twitchLive: dakotaz`\r\n\r\nNow let's see all the needed steps.\r\n\r\n### Step 1: Install gatsby-remark-better-embed-video\r\n\r\nYou need to navigate into your website location and add the plugin to your Gatsby installation:\r\n\r\n    npm i gatsby-remark-better-embed-video\r\n    npm -i gatsby-transformer-remark\r\n    #OR\r\n    yarn add gatsby-remark-better-embed-video\r\n    yarn add gatsby-transformer-remark\r\n\r\n### Step 2: Update gatsby-config.js\r\n\r\nFor every plugin that you use you need to add it to the to gatsby-config.js. For this to work with Contentful or other headless CMS systems you can add the below:\r\n\r\n```\r\n    {\r\n    resolve: \"gatsby-transformer-remark\",\r\n    options: {\r\n      plugins: [\r\n      {\r\nresolve: \"gatsby-remark-better-embed-video\",\r\noptions: {\r\n  width: 800,\r\n  ratio: 1.77, // Optional: Defaults to 16/9 = 1.77.\r\n  height: 400, // Optional: Overrides optional.ratio.\r\n  related: false, // Optional: Will remove related videos from the end of an embedded YouTube video.\r\n  noIframeBorder: true, // Optional: Disable insertion of <style> border: 0.\r\n  showInfo: false // Optional: Hides video title and player actions.\r\n}\r\n      }\r\n      ]\r\n    }\r\n   },\r\n```\r\n\r\nThis taken from the plugin documentation and you have couple of options that you can modify in here if you want. In case you are using **gatsby-remark-responsive-iframe** it shoud be used after the video plugin.\r\n\r\n### Step 3: Add the Video In Contentful\r\n\r\nAfter you clear the cache and push the changes to your repo you can add the videos to your headless CMS with the format they want, for instance for a youtube video you add on a new line:\r\n\r\n    `youtube: https://youtu.be/2Wmats7Q6ck`\r\n\r\nThe final product will be as below:\r\n![youtube video example](//images.ctfassets.net/l6qg42gls3p1/FFqinDaxqYtYt2Oz2G0M2/32c04d044a9a944f2611a7ed62c8a48f/youtube_video.jpeg)\r\n\r\nThat's all you need to do to have your video embedded with Gatsby and contentful.","src/content/posts/embed-youtube-videos-to-gatsby.mdx",[975],"../../assets/images/How_To_Embed_Youtube Videos_to_Gatsby.jpeg","620421f27f5b4e03","embed-youtube-videos-to-gatsby.mdx","enable-command-autocomplete-in-zsh",{id:978,data:980,body:989,filePath:990,assetImports:991,digest:993,legacyId:994,deferredRender:32},{title:981,description:982,date:983,image:984,authors:985,categories:986,tags:987,canonical:988},"How to Enable Command Autocomplete in ZSH","Learn how to enable command to autocomplete in ZSH to be faster and make your life easier.",["Date","2023-11-16T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/zsh-enable-autocomplete.jpeg",[19],[98],[224],"https://www.bitdoze.com/enable-command-autocomplete-in-zsh/","If you're tired of manually typing out commands in your ZSH terminal, then you've come to the right place. In this article, we'll show you how to enable command autocomplete in ZSH, making your life as a developer or power user much easier. With command autocomplete, you can save time and reduce typos by letting your shell suggest and complete commands for you. So let's dive in and discover how to unleash the full potential of auto-completion in ZSH!\r\n\r\nEspecially if you are using a Mac device you would want to enable autocomplete to make your life easier.\r\n\r\n\r\nFor a nice Mac terminal you can also check [Maximize Efficiency: Integrating Wezterm, Zoxide, and Tmux for the Perfect Mac Terminal](https://www.bitdoze.com/install-wezterm-mac/)\r\n\r\n## What is ZSH?\r\n\r\n- **ZSH** stands for Z Shell.\r\n- It is an interactive shell program for Unix-like operating systems.\r\n- ZSH is designed to be a powerful and efficient command-line interface (CLI) with enhanced features compared to the traditional Bourne shell (sh).\r\n- It provides various improvements over other shells, including advanced customization options, better auto-completion capabilities, and extensibility through plugins.\r\n- ZSH offers a user-friendly and intuitive experience for both beginners and advanced users.\r\n- ZSH comes by default in MacOS\r\n\r\nYou can check the bellow ZSH articles if you like to see more ZSH customizations:\r\n\r\n- [How to Enable Syntax Highlighting in Zsh](https://www.bitdoze.com/enable-syntax-highlighting-zsh//)\r\n- [Top 15 Oh My ZSH Plugins You Must Try](https://www.bitdoze.com/best-oh-my-zsh.plugins/)\r\n\r\n> You can integrate ZSH Autocomplete with Zoxide for a productivity bust you just need to install zoxide as will work out of the box follow: [Zoxide: The Smarter Way to Navigate Your Terminal](https://www.bitdoze.com/zoxide/)\r\n\r\n### Key Features of ZSH:\r\n\r\n1. **Powerful Autocompletion**: One of the standout features of ZSH is its comprehensive autocompletion system. It can complete commands, options, filenames, variables, and more with remarkable accuracy. This feature saves time by reducing typing effort.\r\n\r\n2. **Customization Options**: ZSH allows extensive customization to personalize your shell environment according to your preferences. You can modify prompt appearances, define aliases or shortcuts for frequently used commands, and tweak various settings as per your needs.\r\n\r\n3. **Plugin Support**: With plugin managers like Oh My Zsh or antigen, you can easily extend the functionality of ZSH by adding useful plugins created by the community. These plugins offer additional features such as syntax highlighting, git integration, improved directory navigation shortcuts, etc.\r\n\r\n4. **Advanced Tab Completion**: Apart from basic command completion mentioned earlier in this section's bullet points above; it also supports tab completion based on context-awareness within different programming languages like Python or Ruby codebases when configured properly.\r\n\r\n5. **Efficient Command Line Editing**: The line editing capabilities in ZSH are highly versatile compared to other shells thanks to its Vi mode support which allows users familiar with Vim keybindings faster text manipulation within their terminal sessions without needing any external tools like GNU readline library-based Bash does require if one wants similar functionality there too!\r\n\r\nIn summary:\r\n\r\nZ Shell (**ZSH**) is an interactive shell program designed to provide a powerful and efficient command-line interface. It offers advanced features such as comprehensive autocompletion, customization options, plugin support, advanced tab completion, and efficient command line editing. With ZSH, users can enhance their productivity and enjoy a more intuitive shell experience.\r\n\r\n## Enabling Command Autocomplete in ZSH\r\n\r\nTo enhance your command-line experience, you can enable command autocomplete in ZSH. This feature allows you to conveniently fill in commands and arguments by pressing the Tab key. Here's how you can set it up:\r\n\r\n1. **Install Oh My Zsh and zsh-autosuggestions**: Oh My Zsh is a popular framework for managing your ZSH configuration and plugins.[zsh-autosuggestions](https://github.com/zsh-users/zsh-autosuggestions) is the plugin that will help you have the autocomplete functionality in My Zsh or without.\r\n\r\nInstall them by running the following command.\r\n\r\n```shell\r\n#Installs OhMyZSH\r\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\r\n#Installs zsh-autosuggestions plugin\r\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\r\n#Grabs the zsh-autosuggestions and adds it in .zsh\r\ngit clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions\r\n```\r\n\r\nYou can have autocomplete without Oh My Zsh but is better to have it installed to enhance it even more.\r\n\r\n2. **Configure .zshrc**: Once Oh My Zsh is installed, open your terminal's configuration file `.zshrc` using a text editor of your choice:\r\n\r\n```shell\r\n$ nano ~/.zshrc\r\n```\r\n\r\n3. **Enable Autocompletion Plugin**: In the `.zshrc` file, locate the line that begins with `plugins=(`. Add `git` and `zsh-autosuggestions` to the list of plugins within parentheses like this:\r\n\r\n```shell\r\n#for OhMyZSH\r\nplugins=(git zsh-autosuggestions)\r\n#For Default zsh\r\nsource ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh\r\n```\r\n\r\nSave and exit the file.\r\n\r\n4. **Source Changes**: To apply the changes made to `.zshrc`, either restart your terminal or run this command:\r\n\r\n```shell\r\n$ source ~/.zshrc\r\n```\r\n\r\n5. **Test Autocomplete**: Verify that autocompletion is working correctly by typing a partial command or argument and pressing Tab.\r\n\r\nYou can do an echo to see if second time you will have all the autocomplete.\r\n\r\n```sh\r\necho \"something\"\r\n```\r\n\r\nWith these steps completed, you should now have command autocomplete enabled in ZSH! Enjoy faster and more efficient command-line navigation with this handy feature.\r\n\r\n> Note: If you encounter any issues or prefer different customization options, refer to [Oh My Zsh documentation](https://github.com/ohmyzsh/ohmyzsh) for further guidance.\r\n\r\n## Customizing Command Autocomplete in ZSH\r\n\r\nCustomizing the command autocomplete feature in ZSH can greatly enhance your productivity and make your terminal experience more efficient. Here are some tips on how to customize command autocomplete to suit your needs:\r\n\r\n1. **Aliases** - ZSH allows you to create aliases for frequently used commands. This can be especially helpful when you have long or complex commands that you use often. To set up an alias, use the following syntax:\r\n\r\n   ```\r\n   alias name='command'\r\n   ```\r\n\r\n2. **Functions** - In addition to aliases, you can also define functions for more advanced customization of command autocomplete. Functions allow you to create custom logic and combine multiple commands into a single function. Here's an example:\r\n\r\n   ```bash\r\n   myfunc() {\r\n       echo \"Hello World\"\r\n       ls\r\n   }\r\n   ```\r\n\r\n3. **Path Expansion** - By default, ZSH expands file paths during autocompletion based on the current directory context. However, if you want to customize this behavior, you can modify the `$fpath` variable in your `.zshrc` file.\r\n\r\n4. **Completion Options** - You can fine-tune various completion options by modifying the `$compoptions` array in your `.zshrc` file as per your preferences and requirements.\r\n\r\n5. **Plugins** - There are several plugins available for customizing command autocomplete in ZSH such as `oh-my-zsh`, `zplug`, and `antigen`. These plugins provide additional functionality and customization options out of the box.\r\n\r\n6. **Themes** - Changing themes is another way to customize the appearance of command autocomplete results in ZSH. You can choose from a wide range of themes provided by popular frameworks like oh-my-zsh or even create your own theme.\r\n\r\nBy exploring these customization options, you will be able to personalize and optimize the command autocomplete feature in ZSH according to your specific workflow and preferences.\r\n\r\n## Conclusion\r\n\r\nIn conclusion, enabling command autocomplete in ZSH can greatly enhance your productivity and efficiency when working with the command line. By leveraging the power of autocompletion, you can save time by quickly accessing commands and options without having to type them out entirely.\r\n\r\nWith the easy-to-follow steps outlined in this guide, you can configure ZSH to provide intelligent suggestions as you type, making it easier than ever to navigate your system and execute commands. Whether you are a seasoned developer or just starting with the command line, enabling command autocomplete is a valuable skill that will streamline your workflow.\r\n\r\nBy taking advantage of ZSH's robust autocompletion features, you'll be able to speed up your work process, reduce errors caused by typos or misspellings, and ultimately become more proficient at using the terminal. So why wait? Start harnessing the power of command autocomplete in ZSH today and take your command line experience to new heights!","src/content/posts/enable-command-autocomplete-in-zsh.mdx",[992],"../../assets/images/23/11/zsh-enable-autocomplete.jpeg","b5274eb7648eb948","enable-command-autocomplete-in-zsh.mdx","exclude-directories-files-copy-remote-machine",{id:995,data:997,body:1006,filePath:1007,assetImports:1008,digest:1010,legacyId:1011,deferredRender:32},{title:998,description:999,date:1000,image:1001,authors:1002,categories:1003,tags:1004,canonical:1005},"How to Exclude Directories or Files When Copying to a Remote Machine","Let's see exactly what needs to be done when we want to use scp or rsync to exclude files or folders when coppyng to remote machine.\"",["Date","2023-07-07T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/07/linux-scp-rsync-exclude-files.jpeg",[19],[98],[449],"https://www.bitdoze.com/exclude-directories-files-copy-remote-machine/","In Linux we can use 2 commands to move files or directories to a remote server or from a remote server\r\n\r\n- **scp** - secure copy\r\n- **rsync** - remote sync\r\n\r\nFrom the scp and rsync the better choice is to use rsync as it offers more options and a straight forward way to exclude what you want easily. So without making this longer let's see the way to use both rsync and scp to exclude files.\r\n\r\n## Excluding Files or Directories with Rsync\r\n\r\nWhen using Rsync, you can easily exclude specific directories or files from the synchronization process. This allows you to fine-tune your sync to only include the data that is relevant to you. Here's how you can exclude files with Rsync:\r\n\r\n1. Use the `--exclude` option: Rsync provides the `--exclude` option, followed by the path of the file or directory you want to exclude. You can use this option multiple times to exclude multiple files or directories. For example:\r\n\r\n   ```shell\r\n   rsync --exclude 'dir1' --exclude 'dir2' source/ destination/\r\n   ```\r\n\r\n   This command will exclude `dir1` and `dir2` from the synchronization process.\r\n\r\n2. Exclude files based on patterns: Rsync also allows you to exclude files based on specific patterns. For instance, you can exclude all files with a certain extension or exclude files with a particular naming pattern. Here's an example:\r\n\r\n   ```shell\r\n   rsync --exclude '*.txt' --exclude 'backup*' source/ destination/\r\n   ```\r\n\r\n   In this case, all files with the `.txt` extension and files starting with `backup` will be excluded.\r\n\r\n3. Exclude directories recursively: If you want to exclude directories and their subdirectories, you can use the `--exclude-dir` option. This option works similarly to `--exclude` but applies to directories instead. Here's an example:\r\n\r\n   ```shell\r\n   rsync --exclude-dir 'dir1' --exclude-dir 'dir2' source/ destination/\r\n   ```\r\n\r\n   This command will exclude `dir1` and `dir2` along with their subdirectories.\r\n\r\n4. To exclude multiple directories or files, you can specify them using a comma-separated list. For example, to exclude both \"docs\" and \"tmp\" directories, you can use the command:\r\n   ```shell\r\n   rsync -av --exclude=\"docs,tmp\" source_directory destination_directory\r\n   ```\r\n\r\n> Note: Remember, when using Rsync, the order of the `--exclude` or `--exclude-dir` options matters. Rsync processes them from left to right, so place more specific exclusions before more general ones.\r\n\r\n## Excluding Files with SCP\r\n\r\nSCP (Secure Copy) is a popular choice but is not the best one if you would like to exclude files. SCP does not offer a default command that you can use and exclude any file or directory you want.\r\n\r\nHowever you can try excluding files or directories with SCP with extglob, this works in bash so enable extglob you do:\r\n\r\n```shell\r\nshopt -s extglob\r\n```\r\n\r\nAfter you can run your SCP command:\r\n\r\n```shell\r\nscp !(*.txt) root@centos7:/tmp/\r\n```\r\n\r\nThis will copy everything in the current directory except the txt files to the remote server /tmp locations.\r\n\r\nThe extglob extended pattern matching list is below one:\r\n\r\n| Pattern           | Description                                             |\r\n| ----------------- | ------------------------------------------------------- |\r\n| `?(pattern-list)` | Matches zero or one occurrence of the given patterns.   |\r\n| `*(pattern-list)` | Matches zero or more occurrences of the given patterns. |\r\n| `+(pattern-list)` | Matches one or more occurrences of the given patterns.  |\r\n| `@(pattern-list)` | Matches one of the given patterns.                      |\r\n| `!(pattern-list)` | Matches anything except the given patterns.             |\r\n\r\nIf you want more linux related articles you should check:\r\n\r\n- [How to Benchmark Cloud Servers (VPS)](https://www.bitdoze.com/benchmark-cloud-servers/)\r\n- [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)","src/content/posts/exclude-directories-files-copy-remote-machine.mdx",[1009],"../../assets/images/23/07/linux-scp-rsync-exclude-files.jpeg","c1828dd8e6faae41","exclude-directories-files-copy-remote-machine.mdx","enable-syntax-highlighting-zsh",{id:1012,data:1014,body:1023,filePath:1024,assetImports:1025,digest:1027,legacyId:1028,deferredRender:32},{title:1015,description:1016,date:1017,image:1018,authors:1019,categories:1020,tags:1021,canonical:1022},"How to Enable Syntax Highlighting in Zsh","Learn how to enable Syntax Highlighting in ZSH to enhance codding experience and make your life easier.",["Date","2023-11-16T01:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/enable-syntax-highlighting-zsh.jpeg",[19],[98],[224],"https://www.bitdoze.com/enable-syntax-highlighting-zsh/","Are you tired of staring at a plain, monotonous terminal screen? Do you want to make your coding experience more visually appealing and efficient? Look no further! In this article, we will guide you through the process of enabling syntax highlighting in Zsh – a powerful shell with enhanced customization options. By the end, you'll be able to effortlessly differentiate between keywords, variables, and commands in your code snippets. So let's dive right in and unlock the full potential of your Zsh terminal!\r\n\r\nFor a nice Mac terminal you can also check [Maximize Efficiency: Integrating Wezterm, Zoxide, and Tmux for the Perfect Mac Terminal](https://www.bitdoze.com/install-wezterm-mac/)\r\n\r\n## What is Syntax Highlighting?\r\n\r\nSyntax highlighting is a feature in text editors and terminal shells that helps developers visually distinguish different elements of code. It uses colors and formatting to make the code more readable and easier to understand.\r\n\r\nHere are some key points about syntax highlighting:\r\n\r\n- **Improved Readability**: Syntax highlighting makes it easier for programmers to identify various components of their code, such as keywords, variables, strings, comments, etc. By using different colors or styles for each element, it enhances readability and reduces eye strain.\r\n\r\n- **Error Detection**: Syntax highlighting can also help detect errors in the code by highlighting any syntax mistakes or missing elements. This allows developers to catch potential issues before running the program.\r\n\r\n- **Language-Specific Support**: Different programming languages have their own set of rules and syntax structures. Many text editors support language-specific syntax highlighting, meaning they automatically apply appropriate color schemes based on the chosen language.\r\n\r\n- **Customization Options**: Users often have the flexibility to customize the colors used for each syntactic element according to their personal preferences or specific requirements. This allows programmers to create an environment that suits their needs while maintaining readability.\r\n\r\nTo enable syntax highlighting in Zsh (Z shell), you can use plugins like [zsh-syntax-highlighting](https://github.com/zsh-users/zsh-syntax-highlighting). This plugin provides advanced syntax coloring capabilities specifically designed for Zsh users. Once installed and configured correctly, your Zsh shell will display commands and arguments with distinct colors based on their types.\r\n\r\nBy enabling syntax highlighting in Zsh, you'll not only enhance your coding experience but also make it easier to spot errors or inconsistencies within your command lines.\r\n\r\nYou can check the bellow ZSH articles if you like to see more ZSH customizations:\r\n\r\n- [How to Enable Command Autocomplete in ZSH](https://www.bitdoze.com/enable-command-autocomplete-in-zsh/)\r\n- [Top 15 Oh My ZSH Plugins You Must Try](https://www.bitdoze.com/best-oh-my-zsh.plugins/)\r\n\r\n## Enabling Syntax Highlighting in Zsh Using An Extension\r\n\r\nTo enhance your experience with the Zsh shell, enabling syntax highlighting can make it easier to read and understand your commands. Syntax highlighting adds color to different parts of your command line input, making it more visually appealing and helping you spot any errors or inconsistencies. Here's how you can enable syntax highlighting in Zsh:\r\n\r\n1. **Install Git**: You will need git to fetch the repo with the extension so you need to install it, in function if you are on Debian/Ubuntu or MacOS you can run the below commands:\r\n\r\nMacOS:\r\n\r\n```shell\r\nbrew install git\r\n```\r\n\r\nDebian/Ubuntu:\r\n\r\n```shell\r\nsudo apt install git\r\n```\r\n\r\n2. **Clone zsh-syntax-highlighting**:\r\n\r\n```shell\r\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.zsh/zsh-syntax-highlighting\r\n```\r\n\r\n3. **Add this syntax highlighting extension to the .zshrc**: You need to add the extension into `.zshrc` so it will be loaded every time you start the terminal.\r\n\r\n```shell\r\necho \"source ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\" >> ${ZDOTDIR:-$HOME}/.zshrc\r\n```\r\n\r\n4. **Source Changes**: To apply the changes made to `.zshrc`, either restart your terminal or run this command:\r\n\r\n```shell\r\n$ source ~/.zshrc\r\n```\r\n\r\n## Enabling Syntax Highlighting in Oh My ZSH\r\n\r\nOh My Zsh is a popular framework for managing your ZSH configuration and plugins. It's recommended to use it to enhance your terminal.\r\n\r\n1. **Install Oh My Zsh**:\r\n\r\n```shell\r\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\r\n```\r\n\r\n2. **Clone zsh-syntax-highlighting into plugins**:\r\n\r\n```shell\r\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\r\n```\r\n\r\n3. **Configure .zshrc**: Once Oh My Zsh is installed, open your terminal's configuration file `.zshrc` using a text editor of your choice:\r\n\r\n```shell\r\n$ nano ~/.zshrc\r\n```\r\n\r\n4. **Enable Syntax Highlighting**: In the `.zshrc` file, locate the line that begins with `plugins=(`. Add `git` and `zsh-autosuggestions` to the list of plugins within parentheses like this:\r\n\r\n```shell\r\nplugins=(git zsh-syntax-highlighting)\r\n```\r\n\r\nSave and exit the file.\r\n\r\n5. **Source Changes**: To apply the changes made to `.zshrc`, either restart your terminal or run this command:\r\n\r\n```shell\r\n$ source ~/.zshrc\r\n```\r\n\r\nSyntax highlighting is now enabled in Zsh! You should start seeing colors applied to various elements of your commands when typing them into the terminal.\r\n\r\nIf you want further customization options for syntax highlighting, you can modify settings by editing the `.zsrc` file mentioned earlier. For example, you can change colors assigned to specific elements or adjust other visual aspects according to personal preference.\r\n\r\nKeep in mind that syntax highlighting relies on having appropriate themes installed on your system; otherwise, default colors will be used instead.\r\n\r\nBy enabling syntax highlighting in Zsh, working with complex commands becomes more convenient as important keywords are highlighted distinctly from other text elements within each command line input. This feature improves overall readability and helps prevent mistakes while writing code or executing powerful shell commands.\r\n\r\nRemember that this configuration applies specifically to Z Bash (ZSH) rather than other shells like Bash or Fish Shell where alternative methods may be required for achieving similar results\r\n\r\n## Customizing Syntax Highlighting Colors\r\n\r\nTo customize the syntax highlighting colors in Zsh, you can modify the `ZSH_HIGHLIGHT_STYLES` variable in your Zsh configuration file. This allows you to define specific colors for different types of elements such as keywords, strings, variables, and more.\r\n\r\nHere are the steps to customize syntax highlighting colors in Zsh:\r\n\r\n1. Open your Zsh configuration file. You can typically find it at `~/.zshrc`.\r\n\r\n2. Locate the section where `ZSH_HIGHLIGHT_STYLES` is defined or add it if it doesn't already exist.\r\n\r\n3. Modify the values of individual elements within `ZSH_HIGHLIGHT_STYLES`. Each element represents a different type of syntax highlight and has its own name.\r\n\r\n   For example:\r\n\r\n   ```shell\r\n   # Change color for keywords (e.g., if, else)\r\n   ZSH_HIGHLIGHT_STYLES[keyword]='fg=blue,bold'\r\n\r\n   # Change color for strings (e.g., \"Hello World\")\r\n   ZSH_HIGHLIGHT_STYLES[string]='fg=green'\r\n\r\n   # Change color for variables (e.g., $HOME)\r\n   ZSH_HIGHLIGHT_STYLES[variable]='fg=cyan'\r\n\r\n    ...\r\n\r\n   ```\r\n\r\n4. Save the changes to your configuration file.\r\n\r\n5. Restart your terminal or run `source ~/.zshrc` to apply the new syntax highlighting colors immediately.\r\n\r\nBy customizing these values according to your preferences, you can create a personalized and visually appealing syntax highlighting scheme that suits your needs while working with Zsh.\r\n\r\n> **Note:** The above examples demonstrate how to change foreground color using ANSI escape codes (`fg=color`). However, you can also modify other attributes like background color (`bg=color`) or text formatting by adding additional arguments separated by commas.\r\n\r\n## Conclusion\r\n\r\nIn conclusion, enabling syntax highlighting in Zsh is a simple and effective way to enhance your command-line experience. By following the step-by-step guide mentioned in this article, you can easily configure syntax highlighting and take advantage of its benefits.\r\n\r\nSyntax highlighting not only makes your code more readable but also helps identify errors and inconsistencies quickly. With Zsh's extensive customization options, you have the flexibility to choose from various color schemes that suit your preferences.\r\n\r\nBy enabling syntax highlighting in Zsh, you can improve productivity and reduce coding errors. So why wait? Start using this powerful feature today and elevate your command-line skills to the next level!","src/content/posts/enable-syntax-highlighting-zsh.mdx",[1026],"../../assets/images/23/11/enable-syntax-highlighting-zsh.jpeg","27f36ff5e1cad512","enable-syntax-highlighting-zsh.mdx","fasthtml-start",{id:1029,data:1031,body:1041,filePath:1042,assetImports:1043,digest:1045,legacyId:1046,deferredRender:32},{title:1032,description:1033,date:1034,image:1035,authors:1036,categories:1037,tags:1038,canonical:1040},"FastHTML For Beginners: Build An UI to Python App in 5 Minutes","Master FastHTML quickly! Learn to add a user interface to your Python app in just 5 minutes with our beginner-friendly guide.",["Date","2025-02-26T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/02/fasthtml-start.jpg",[19],[98],[1039,294],"fasthtml","https://www.bitdoze.com/fasthtml-start/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\n[FastHTML](https://fastht.ml/) is an innovative Python-based web framework designed to make web development accessible and enjoyable for beginners while providing robust tools for seasoned developers. By blending Python's simplicity with HTML-like syntax, FastHTML allows you to create dynamic and responsive web applications without wrestling with complex setups or unfamiliar languages.\r\n\r\n## Why FastHTML?\r\n\r\nUnlike traditional web development that requires knowledge of HTML, CSS, JavaScript and possibly frameworks like React or Vue, FastHTML lets you build complete web applications using just Python. Here's why it's particularly valuable for beginners:\r\n\r\n- **Single Language**: Build both backend and frontend with just Python\r\n- **Simpler Than Alternatives**: More approachable than Django or Flask for UI development\r\n- **Hypermedia-Driven**: Built-in support for HTMX allows interactivity without JavaScript\r\n- **Python-Native Syntax**: Use familiar Python functions instead of learning template languages\r\n- **No Build Tools**: No need for npm, webpack, or other JavaScript build tools\r\n\r\nThis article serves as your entry point into FastHTML, guiding you through installation, basic syntax, and the use of various components with practical examples. By the end, you'll be equipped to build your first FastHTML project with confidence.\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/fqzTg5CrlGY\"\r\n  label=\"FastHTML For Beginners\"\r\n/>\r\n\r\n## FastHTML Series\r\n\r\nBelow are the articles on FastHTML to help you get started:\r\n\r\n- [FastHTML Get Started](https://www.bitdoze.com/fasthtml-start/)\r\n- [FastHTML Multiple Pages](https://www.bitdoze.com/fasthtml-multiple-pages/)\r\n\r\n## Installing FastHTML\r\n\r\nBefore diving into coding, you need to set up FastHTML on your machine. The first prerequisite is Python, version 3.7 or higher. If you don't have Python installed, head to the [Python install on MAC](https://www.bitdoze.com/install-upgrade-python-mac/) and download the latest version compatible with your operating macOS. Follow the installation prompts, ensuring you check the option to add Python to your system's PATH, which makes running Python commands easier from the terminal.\r\n\r\nIf you have other OS than Mac you should check the python website for the tutorial.\r\n\r\nWith Python ready, installing FastHTML is a breeze. Open your terminal (Command Prompt on Windows, Terminal on macOS/Linux) and follow these steps:\r\n\r\n**1. Create a virtual environment for the project and activate it**\r\n\r\n```sh\r\npython3 -m venv fhenv\r\nsource fhenv/bin/activate\r\n# On Windows:\r\n# fhenv\\Scripts\\activate\r\n```\r\n\r\nThis creates an isolated environment for your project, preventing package conflicts. A virtual environment is like a separate, clean installation of Python where you can install packages without affecting your system Python installation.\r\n\r\n**2. Install FastHTML**\r\n\r\n```bash\r\npip install python-fasthtml\r\n```\r\n\r\nThis fetches the FastHTML package and its dependencies. You can also check the official [FastHTML documentation](https://docs.fastht.ml/) for more details.\r\n\r\n## Understanding FastHTML Syntax\r\n\r\nFastHTML's standout feature is its ability to let you write web pages using Python functions that mimic HTML tags. Instead of juggling separate HTML files, you define your page structure directly in Python:\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\n# Creating a paragraph element\r\nparagraph = P(\"Hello, World!\")\r\n```\r\n\r\nIn this snippet, `P` is a FastHTML function that generates an HTML `<p>` tag with \"Hello, World!\" as its content. This gets converted to `<p>Hello, World!</p>` when rendered. FastHTML provides similar functions for all standard HTML elements—`H1` for headings, `Div` for divisions, `Ul` and `Li` for lists, and so on.\r\n\r\nHere's a quick mapping of some common HTML elements to FastHTML functions:\r\n\r\n| HTML | FastHTML | Example |\r\n|------|----------|---------|\r\n| `<p>` | `P()` | `P(\"Text\")` |\r\n| `<h1>` | `H1()` | `H1(\"Heading\")` |\r\n| `<div>` | `Div()` | `Div(P(\"Child element\"))` |\r\n| `<a>` | `A()` | `A(\"Link text\", href=\"https://example.com\")` |\r\n| `<input>` | `Input()` | `Input(type=\"text\", name=\"username\")` |\r\n\r\nTo build a complete webpage, you combine these functions into a structure:\r\n\r\n```python\r\npage = Html(\r\n    Head(\r\n        Title(\"My First Page\")\r\n    ),\r\n    Body(\r\n        H1(\"Welcome to FastHTML\"),\r\n        P(\"This is a simple page built with FastHTML.\")\r\n    )\r\n)\r\n```\r\n\r\nThis code produces a complete HTML document with a title, heading, and paragraph. The nested structure mirrors HTML's hierarchy, making it easy to visualize how components fit together.\r\n\r\n**How FastHTML Functions Work**\r\n\r\nEach FastHTML function takes:\r\n- Positional arguments for child elements or content\r\n- Keyword arguments for HTML attributes\r\n\r\nFor example, in `P(\"Hello\", cls=\"greeting\")`:\r\n- `\"Hello\"` is the content of the paragraph\r\n- `cls=\"greeting\"` becomes the HTML attribute `class=\"greeting\"`\r\n\r\nNote: We use `cls` instead of `class` because `class` is a reserved keyword in Python.\r\n\r\n## Building Your First Web Page\r\n\r\nLet's put this into action by creating a simple web page. Create a file called `main.py` and add the following code:\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\n# Create a FastHTML application\r\napp = FastHTML()\r\n\r\n# Define a route for the root URL \"/\"\r\n@app.get(\"/\")\r\ndef home():\r\n    page = Html(\r\n        Head(\r\n            Title(\"Getting Started with FastHTML\"),\r\n            Script(src=\"https://cdn.tailwindcss.com\")  # Including Tailwind CSS for styling\r\n        ),\r\n        Body(\r\n            H1(\"FastHTML Basics\", cls=\"text-2xl font-bold mb-4\"),\r\n            P(\"Below is a list of features:\", cls=\"mb-2\"),\r\n            Ul(\r\n                Li(\"Easy to learn\"),\r\n                Li(\"Python-based\"),\r\n                Li(\"Dynamic and responsive\")\r\n            )\r\n        )\r\n    )\r\n    return page\r\n\r\n# Start the FastHTML server\r\nif __name__ == \"__main__\":\r\n    serve()\r\n```\r\n\r\nLet's break this down:\r\n\r\n1. We import all FastHTML components with `from fasthtml.common import *`\r\n2. We create a FastHTML application with `app = FastHTML()`\r\n3. We define a route handler for the root URL (`/`) using the `@app.get(\"/\")` decorator\r\n4. Our `home()` function returns a complete HTML page structure\r\n5. The `serve()` function starts the FastHTML server\r\n\r\nSave the file, then run it from your terminal:\r\n\r\n```bash\r\npython main.py\r\n```\r\n\r\nYou should see output similar to:\r\n\r\n```\r\nLink: http://localhost:5001\r\nINFO:     Will watch for changes in these directories: ['/path/to/your/project']\r\nINFO:     Uvicorn running on http://0.0.0.0:5001 (Press CTRL+C to quit)\r\n```\r\n\r\nOpen your browser and navigate to `http://localhost:5001`. You'll see a page with a heading, paragraph, and bullet list. The `@app.get(\"/\")` decorator tells FastHTML to serve this page at the root URL.\r\n\r\n**What's Happening Behind the Scenes**\r\n\r\nWhen you visit `http://localhost:5001`:\r\n1. FastHTML receives the request for the root URL (`/`)\r\n2. It calls the `home()` function\r\n3. The function returns a page structure built with FastHTML components\r\n4. FastHTML converts these components to HTML\r\n5. The HTML is sent to your browser\r\n\r\nFastHTML handles all the HTTP server details so you can focus on building your UI.\r\n\r\n## Basic UI Components\r\n\r\nLet's explore the fundamental FastHTML components you can use to build interfaces. We'll start with the basics and progressively build more complex UIs.\r\n\r\n### Text Elements\r\n\r\nText elements are the foundation of any interface. FastHTML makes creating them intuitive:\r\n\r\n```python\r\n# Headings\r\nheading1 = H1(\"Main Heading\", cls=\"text-2xl font-bold\")\r\nheading2 = H2(\"Subheading\", cls=\"text-xl font-semibold\")\r\n\r\n# Paragraphs\r\nparagraph = P(\"This is a paragraph of text.\", cls=\"mb-4\")\r\n\r\n# Formatted text\r\nbold_text = Strong(\"This text is bold\")\r\nitalic_text = Em(\"This text is italicized\")\r\n```\r\n\r\nHere's a complete example showing various text elements:\r\n\r\n```python\r\n@app.get(\"/text-elements\")\r\ndef text_elements():\r\n    return Html(\r\n        Head(Title(\"Text Elements\")),\r\n        Body(\r\n            H1(\"Heading Level 1\", cls=\"text-3xl font-bold\"),\r\n            H2(\"Heading Level 2\", cls=\"text-2xl font-semibold\"),\r\n            H3(\"Heading Level 3\", cls=\"text-xl font-medium\"),\r\n            P(\"This is a regular paragraph with some \",\r\n              Strong(\"bold text\"), \" and some \",\r\n              Em(\"italicized text\"), \" mixed in.\"),\r\n            P(\"You can also use \", Code(\"code snippets\"), \" inline.\")\r\n        )\r\n    )\r\n```\r\n\r\nWhen rendered, this creates a hierarchy of text elements with different sizes and styles. The `cls` attribute sets CSS classes that style the elements. In this example, we're using Tailwind CSS classes like `text-3xl` (for font size) and `font-bold` (for font weight).\r\n\r\n### Containers and Layout\r\n\r\nOrganizing content is key to good UI design. FastHTML provides container elements for structuring your page:\r\n\r\n```python\r\n# Basic container\r\ncontainer = Div(\r\n    H2(\"Section Title\"),\r\n    P(\"Content inside a container\"),\r\n    cls=\"p-4 bg-gray-100 rounded\"\r\n)\r\n\r\n# Grid layout (with Tailwind CSS)\r\ngrid = Div(\r\n    Div(P(\"Column 1\"), cls=\"p-2\"),\r\n    Div(P(\"Column 2\"), cls=\"p-2\"),\r\n    Div(P(\"Column 3\"), cls=\"p-2\"),\r\n    cls=\"grid grid-cols-3 gap-4\"\r\n)\r\n```\r\n\r\nThe `Div` component is extremely versatile for creating containers and layout structures. When combined with CSS frameworks like Tailwind, you can create responsive layouts with minimal effort.\r\n\r\nHere's a layout example using nested containers:\r\n\r\n```python\r\n@app.get(\"/layout\")\r\ndef layout_demo():\r\n    return Html(\r\n        Head(\r\n            Title(\"Layout Demo\"),\r\n            Script(src=\"https://cdn.tailwindcss.com\")\r\n        ),\r\n        Body(\r\n            Div(\r\n                H1(\"Page Layout Example\", cls=\"text-2xl font-bold mb-4\"),\r\n\r\n                # Main layout grid with sidebar and content\r\n                Div(\r\n                    # Sidebar\r\n                    Div(\r\n                        H2(\"Sidebar\", cls=\"text-xl mb-2\"),\r\n                        Ul(\r\n                            Li(\"Home\"),\r\n                            Li(\"About\"),\r\n                            Li(\"Services\"),\r\n                            Li(\"Contact\"),\r\n                            cls=\"space-y-2\"\r\n                        ),\r\n                        cls=\"bg-gray-100 p-4 rounded\"\r\n                    ),\r\n\r\n                    # Main content\r\n                    Div(\r\n                        H2(\"Main Content\", cls=\"text-xl mb-2\"),\r\n                        P(\"This is the main content area of our layout example.\"),\r\n                        P(\"You can structure complex layouts using nested Div elements.\"),\r\n                        cls=\"bg-white p-4 rounded\"\r\n                    ),\r\n\r\n                    # Grid with 1 column on mobile, 4 columns on medium screens and up\r\n                    cls=\"grid grid-cols-1 md:grid-cols-4 gap-4\"\r\n                ),\r\n                cls=\"container mx-auto p-4\"\r\n            )\r\n        )\r\n    )\r\n```\r\n\r\nThis creates a responsive layout with:\r\n- A sidebar that contains navigation links\r\n- A main content area\r\n- A layout that adapts to different screen sizes (1 column on mobile, 4 columns on larger screens)\r\n\r\n### Links and Buttons\r\n\r\nInteractive elements like links and buttons allow users to navigate and take actions:\r\n\r\n```python\r\n# Basic link\r\nlink = A(\"Visit Google\", href=\"https://google.com\", cls=\"text-blue-500 hover:underline\")\r\n\r\n# Button\r\nbutton = Button(\"Click Me\", cls=\"bg-blue-500 text-white px-4 py-2 rounded\")\r\n```\r\n\r\nThe `A` component creates HTML anchor tags (`<a>`) for links, while the `Button` component creates HTML button elements (`<button>`).\r\n\r\nLet's create a navigation bar with links and buttons:\r\n\r\n```python\r\n@app.get(\"/navigation\")\r\ndef navigation_demo():\r\n    return Html(\r\n        Head(\r\n            Title(\"Navigation Demo\"),\r\n            Script(src=\"https://cdn.tailwindcss.com\")\r\n        ),\r\n        Body(\r\n            # Navigation bar\r\n            Div(\r\n                Div(\r\n                    # Logo/site name\r\n                    A(\"FastHTML Demo\", href=\"/\", cls=\"text-xl font-bold text-white\"),\r\n\r\n                    # Navigation links and login button\r\n                    Div(\r\n                        A(\"Home\", href=\"/\", cls=\"text-white hover:text-gray-200 mx-2\"),\r\n                        A(\"Features\", href=\"/features\", cls=\"text-white hover:text-gray-200 mx-2\"),\r\n                        A(\"Docs\", href=\"/docs\", cls=\"text-white hover:text-gray-200 mx-2\"),\r\n                        Button(\"Login\", cls=\"bg-white text-blue-600 px-3 py-1 rounded ml-4\"),\r\n                        cls=\"flex items-center\"\r\n                    ),\r\n                    cls=\"flex justify-between items-center\"\r\n                ),\r\n                cls=\"bg-blue-600 p-4\"\r\n            ),\r\n\r\n            # Page content\r\n            Div(\r\n                H1(\"Welcome to FastHTML\", cls=\"text-3xl font-bold mb-4\"),\r\n                P(\"This example shows a navigation bar with links and a button.\"),\r\n                cls=\"container mx-auto p-4\"\r\n            )\r\n        )\r\n    )\r\n```\r\n\r\nThis creates a navigation bar with:\r\n- A site logo/name on the left\r\n- Navigation links in the center\r\n- A login button on the right\r\n- Hover effects on the links\r\n- A clean, modern appearance thanks to Tailwind CSS\r\n\r\n### Forms and Inputs\r\n\r\nForms allow users to input data. FastHTML makes creating forms straightforward:\r\n\r\n```python\r\n# Text input\r\ntext_input = Input(type=\"text\", name=\"username\", placeholder=\"Enter username\")\r\n\r\n# Password input\r\npassword_input = Input(type=\"password\", name=\"password\", placeholder=\"Enter password\")\r\n\r\n# Complete form\r\nlogin_form = Form(\r\n    Label(\"Username:\", Input(type=\"text\", name=\"username\")),\r\n    Label(\"Password:\", Input(type=\"password\", name=\"password\")),\r\n    Button(\"Submit\", type=\"submit\"),\r\n    action=\"/submit\",\r\n    method=\"post\"\r\n)\r\n```\r\n\r\nThe `Form` component creates HTML form elements, while `Input` creates various input types based on the `type` attribute. The `action` attribute specifies where the form data will be sent, and the `method` attribute specifies the HTTP method (GET or POST).\r\n\r\nLet's create a complete contact form:\r\n\r\n```python\r\n@app.get(\"/contact\")\r\ndef contact_form():\r\n    return Html(\r\n        Head(\r\n            Title(\"Contact Form\"),\r\n            Script(src=\"https://cdn.tailwindcss.com\")\r\n        ),\r\n        Body(\r\n            Div(\r\n                H1(\"Contact Us\", cls=\"text-2xl font-bold mb-4\"),\r\n\r\n                # Contact form\r\n                Form(\r\n                    # Name field\r\n                    Div(\r\n                        Label(\"Name:\", For=\"name\", cls=\"block mb-1\"),\r\n                        Input(type=\"text\", id=\"name\", name=\"name\", placeholder=\"Your name\",\r\n                              cls=\"w-full p-2 border rounded mb-3\"),\r\n                        cls=\"mb-4\"\r\n                    ),\r\n\r\n                    # Email field\r\n                    Div(\r\n                        Label(\"Email:\", For=\"email\", cls=\"block mb-1\"),\r\n                        Input(type=\"email\", id=\"email\", name=\"email\", placeholder=\"Your email\",\r\n                              cls=\"w-full p-2 border rounded mb-3\"),\r\n                        cls=\"mb-4\"\r\n                    ),\r\n\r\n                    # Message field\r\n                    Div(\r\n                        Label(\"Message:\", For=\"message\", cls=\"block mb-1\"),\r\n                        Textarea(id=\"message\", name=\"message\", placeholder=\"Your message\", rows=5,\r\n                                cls=\"w-full p-2 border rounded mb-3\"),\r\n                        cls=\"mb-4\"\r\n                    ),\r\n\r\n                    # Submit button\r\n                    Button(\"Send Message\", type=\"submit\",\r\n                           cls=\"bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600\"),\r\n\r\n                    # Form attributes\r\n                    action=\"/submit-contact\",\r\n                    method=\"post\",\r\n                    cls=\"max-w-md mx-auto bg-gray-50 p-6 rounded shadow\"\r\n                ),\r\n                cls=\"container mx-auto p-4\"\r\n            )\r\n        )\r\n    )\r\n```\r\n\r\nThis creates a styled contact form with:\r\n- Text input for name\r\n- Email input for email address\r\n- Textarea for the message\r\n- Submit button\r\n- Form submission handling to \"/submit-contact\"\r\n- Proper styling and layout for all elements\r\n\r\n### Lists and Tables\r\n\r\nOrganizing data with lists and tables is common in web applications:\r\n\r\n```python\r\n# Unordered list\r\nunordered_list = Ul(\r\n    Li(\"Item 1\"),\r\n    Li(\"Item 2\"),\r\n    Li(\"Item 3\")\r\n)\r\n\r\n# Ordered list\r\nordered_list = Ol(\r\n    Li(\"First item\"),\r\n    Li(\"Second item\"),\r\n    Li(\"Third item\")\r\n)\r\n\r\n# Basic table\r\ntable = Table(\r\n    Thead(\r\n        Tr(\r\n            Th(\"Name\"),\r\n            Th(\"Email\"),\r\n            Th(\"Role\")\r\n        )\r\n    ),\r\n    Tbody(\r\n        Tr(\r\n            Td(\"John Doe\"),\r\n            Td(\"john@example.com\"),\r\n            Td(\"Admin\")\r\n        ),\r\n        Tr(\r\n            Td(\"Jane Smith\"),\r\n            Td(\"jane@example.com\"),\r\n            Td(\"User\")\r\n        )\r\n    )\r\n)\r\n```\r\n\r\nThe `Ul` and `Ol` components create unordered and ordered lists, while `Li` creates list items. The `Table`, `Thead`, `Tbody`, `Tr`, `Th`, and `Td` components create HTML table elements.\r\n\r\nHere's a data table example:\r\n\r\n```python\r\n@app.get(\"/data-table\")\r\ndef data_table():\r\n    return Html(\r\n        Head(\r\n            Title(\"Data Table\"),\r\n            Script(src=\"https://cdn.tailwindcss.com\")\r\n        ),\r\n        Body(\r\n            Div(\r\n                H1(\"User Data\", cls=\"text-2xl font-bold mb-4\"),\r\n\r\n                # User data table\r\n                Table(\r\n                    # Table header\r\n                    Thead(\r\n                        Tr(\r\n                            Th(\"ID\", cls=\"p-2 border\"),\r\n                            Th(\"Name\", cls=\"p-2 border\"),\r\n                            Th(\"Email\", cls=\"p-2 border\"),\r\n                            Th(\"Role\", cls=\"p-2 border\"),\r\n                            Th(\"Actions\", cls=\"p-2 border\"),\r\n                            cls=\"bg-gray-100\"\r\n                        )\r\n                    ),\r\n\r\n                    # Table body\r\n                    Tbody(\r\n                        # Row 1\r\n                        Tr(\r\n                            Td(\"1\", cls=\"p-2 border\"),\r\n                            Td(\"John Doe\", cls=\"p-2 border\"),\r\n                            Td(\"john@example.com\", cls=\"p-2 border\"),\r\n                            Td(\"Admin\", cls=\"p-2 border\"),\r\n                            Td(Button(\"Edit\", cls=\"bg-blue-500 text-white px-2 py-1 rounded mr-2\"),\r\n                               Button(\"Delete\", cls=\"bg-red-500 text-white px-2 py-1 rounded\"),\r\n                               cls=\"p-2 border\"),\r\n                        ),\r\n                        # Row 2\r\n                        Tr(\r\n                            Td(\"2\", cls=\"p-2 border\"),\r\n                            Td(\"Jane Smith\", cls=\"p-2 border\"),\r\n                            Td(\"jane@example.com\", cls=\"p-2 border\"),\r\n                            Td(\"User\", cls=\"p-2 border\"),\r\n                            Td(Button(\"Edit\", cls=\"bg-blue-500 text-white px-2 py-1 rounded mr-2\"),\r\n                               Button(\"Delete\", cls=\"bg-red-500 text-white px-2 py-1 rounded\"),\r\n                               cls=\"p-2 border\"),\r\n                        ),\r\n                        # Row 3\r\n                        Tr(\r\n                            Td(\"3\", cls=\"p-2 border\"),\r\n                            Td(\"Robert Johnson\", cls=\"p-2 border\"),\r\n                            Td(\"robert@example.com\", cls=\"p-2 border\"),\r\n                            Td(\"Editor\", cls=\"p-2 border\"),\r\n                            Td(Button(\"Edit\", cls=\"bg-blue-500 text-white px-2 py-1 rounded mr-2\"),\r\n                               Button(\"Delete\", cls=\"bg-red-500 text-white px-2 py-1 rounded\"),\r\n                               cls=\"p-2 border\"),\r\n                        )\r\n                    ),\r\n                    cls=\"w-full border-collapse\"\r\n                ),\r\n                cls=\"container mx-auto p-4 overflow-x-auto\"\r\n            )\r\n        )\r\n    )\r\n```\r\n\r\nThis creates a styled data table with:\r\n- Column headers (ID, Name, Email, Role, Actions)\r\n- Multiple rows of data\r\n- Action buttons in the last column\r\n- Proper styling for all elements\r\n- Horizontal scrolling for small screens\r\n\r\n## Adding Interactivity with HTMX\r\n\r\nFastHTML seamlessly integrates with HTMX, a library that allows you to access AJAX, CSS Transitions, WebSockets and Server Sent Events directly in HTML, without writing JavaScript. FastHTML includes HTMX by default, so there's no need to import it separately in most cases.\r\n\r\nHere's a simple counter example:\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\napp = FastHTML()\r\n\r\n# A simple counter variable to demonstrate state\r\ncounter = 0\r\n\r\n@app.get(\"/\")\r\ndef home():\r\n    return Titled(\"HTMX Counter Example\",\r\n        Div(\r\n            H1(\"HTMX Counter\", cls=\"text-2xl font-bold mb-4\"),\r\n            Div(\r\n                # Counter display with unique ID for targeting\r\n                P(f\"Current count: {counter}\", id=\"counter\", cls=\"text-xl mb-4\"),\r\n\r\n                # Increment button with HTMX attributes\r\n                Button(\"Increment\",\r\n                      hx_post=\"/increment\",\r\n                      hx_target=\"#counter\",\r\n                      cls=\"bg-blue-500 text-white px-4 py-2 rounded mr-2\"),\r\n\r\n                # Decrement button with HTMX attributes\r\n                Button(\"Decrement\",\r\n                      hx_post=\"/decrement\",\r\n                      hx_target=\"#counter\",\r\n                      cls=\"bg-red-500 text-white px-4 py-2 rounded\"),\r\n                cls=\"p-4 bg-gray-100 rounded\"\r\n            ),\r\n            cls=\"container mx-auto p-4\"\r\n        ),\r\n        Script(src=\"https://cdn.tailwindcss.com\")\r\n    )\r\n\r\n# Handler for increment button\r\n@app.post(\"/increment\")\r\ndef increment():\r\n    global counter\r\n    counter += 1\r\n    # Return just the counter element, not the whole page\r\n    return P(f\"Current count: {counter}\", id=\"counter\", cls=\"text-xl mb-4\")\r\n\r\n# Handler for decrement button\r\n@app.post(\"/decrement\")\r\ndef decrement():\r\n    global counter\r\n    counter -= 1\r\n    # Return just the counter element, not the whole page\r\n    return P(f\"Current count: {counter}\", id=\"counter\", cls=\"text-xl mb-4\")\r\n\r\nserve()\r\n```\r\n\r\n### Understanding HTMX Attributes\r\n\r\nFastHTML provides special attributes for HTMX integration:\r\n\r\n1. `hx_post` - Sends a POST request to the specified URL when the element is clicked\r\n2. `hx_get` - Sends a GET request to the specified URL when the element is clicked\r\n3. `hx_target` - Specifies which element to update with the response (using CSS selector syntax)\r\n4. `hx_swap` - Controls how the response is swapped in (e.g., \"innerHTML\", \"outerHTML\", \"beforeend\")\r\n5. `hx_trigger` - Specifies when to trigger the request (e.g., \"click\", \"change\", etc.)\r\n\r\nIn FastHTML, these attributes are provided as Python parameters, with underscores replacing hyphens (e.g., `hx_post` instead of `hx-post`).\r\n\r\n### How the Counter Works\r\n\r\n1. When you click the \"Increment\" button, HTMX sends a POST request to `/increment`\r\n2. The server runs the `increment()` function, which increases the counter value\r\n3. The function returns just the updated paragraph element\r\n4. HTMX replaces the content of the element with id=\"counter\" with the response\r\n5. The page updates without a full refresh\r\n\r\nThis pattern is powerful for creating interactive web applications without writing JavaScript.\r\n\r\n## Building a Todo Application\r\n\r\nLet's combine everything we've learned to build a simple todo application:\r\n\r\n```python\r\nfrom fasthtml.common import *\r\nfrom dataclasses import dataclass\r\n\r\napp = FastHTML()\r\n\r\n# Our simple data store\r\ntodos = []\r\ntodo_id_counter = 0\r\n\r\n# Define the data structure for a todo item\r\n@dataclass\r\nclass Todo:\r\n    id: int\r\n    title: str\r\n    completed: bool = False\r\n\r\n@app.get(\"/\")\r\ndef home():\r\n    return Titled(\"FastHTML Todo App\",\r\n        Div(\r\n            H1(\"Todo Application\", cls=\"text-2xl font-bold mb-4\"),\r\n\r\n            # Add new todo form\r\n            Form(\r\n                Div(\r\n                    Input(type=\"text\", name=\"title\", placeholder=\"Add a new todo\",\r\n                          cls=\"p-2 border rounded w-full md:w-80\"),\r\n                    Button(\"Add\", type=\"submit\",\r\n                           cls=\"bg-blue-500 text-white px-4 py-2 rounded ml-2\"),\r\n                    cls=\"flex items-center mb-4\"\r\n                ),\r\n                # When the form is submitted, send a POST request to /add-todo\r\n                hx_post=\"/add-todo\",\r\n                # Update the element with id=\"todo-list\"\r\n                hx_target=\"#todo-list\",\r\n                # Add the new todo at the end of the list\r\n                hx_swap=\"beforeend\"\r\n            ),\r\n\r\n            # Todo list container\r\n            Div(\r\n                id=\"todo-list\",\r\n                cls=\"space-y-2\"\r\n            ),\r\n            cls=\"container mx-auto p-4 max-w-md\"\r\n        ),\r\n        Script(src=\"https://cdn.tailwindcss.com\")\r\n    )\r\n\r\n# Handler for adding a new todo\r\n@app.post(\"/add-todo\")\r\ndef add_todo(title: str):\r\n    global todo_id_counter\r\n    # Skip if the title is empty\r\n    if not title.strip():\r\n        return \"\"\r\n\r\n    # Create a new todo and add it to the list\r\n    todo_id_counter += 1\r\n    new_todo = Todo(id=todo_id_counter, title=title)\r\n    todos.append(new_todo)\r\n\r\n    # Return the HTML for the new todo item\r\n    return create_todo_item(new_todo)\r\n\r\n# Handler for toggling a todo's completed status\r\n@app.post(\"/toggle-todo/{id}\")\r\ndef toggle_todo(id: int):\r\n    for todo in todos:\r\n        if todo.id == id:\r\n            # Toggle the completed status\r\n            todo.completed = not todo.completed\r\n            # Return the updated todo item HTML\r\n            return create_todo_item(todo)\r\n    return \"\"\r\n\r\n# Handler for deleting a todo\r\n@app.delete(\"/delete-todo/{id}\")\r\ndef delete_todo(id: int):\r\n    global todos\r\n    # Remove the todo with the specified id\r\n    todos = [todo for todo in todos if todo.id != id]\r\n    # Return an empty string since we're removing the element\r\n    return \"\"\r\n\r\n# Helper function to create the HTML for a todo item\r\ndef create_todo_item(todo: Todo):\r\n    # Add strikethrough style if the todo is completed\r\n    completed_class = \"line-through text-gray-500\" if todo.completed else \"\"\r\n\r\n    return Div(\r\n        Div(\r\n            # Checkbox for marking the todo as completed\r\n            Input(type=\"checkbox\",\r\n                  checked=todo.completed,\r\n                  hx_post=f\"/toggle-todo/{todo.id}\",\r\n                  hx_target=f\"#todo-{todo.id}\",\r\n                  hx_swap=\"outerHTML\",\r\n                  cls=\"mr-2\"),\r\n            # Todo title\r\n            Span(todo.title, cls=completed_class),\r\n            cls=\"flex-grow\"\r\n        ),\r\n        # Delete button\r\n        Button(\"×\",\r\n               hx_delete=f\"/delete-todo/{todo.id}\",\r\n               hx_target=f\"#todo-{todo.id}\",\r\n               hx_swap=\"outerHTML\",\r\n               cls=\"text-red-500 font-bold\"),\r\n        # Unique ID for targeting this todo item\r\n        id=f\"todo-{todo.id}\",\r\n        cls=\"flex items-center p-2 border rounded\"\r\n    )\r\n\r\nserve()\r\n```\r\n\r\n**How the Todo App Works**\r\n\r\n1. **Data Structure**: We use a Python dataclass to define the structure of a todo item\r\n2. **UI Structure**: The main page has a form for adding todos and a container for displaying them\r\n3. **Adding Todos**:\r\n   - The form sends a POST request to `/add-todo` when submitted\r\n   - The server creates a new todo and returns the HTML for it\r\n   - HTMX adds the new todo to the end of the list\r\n4. **Toggling Todos**:\r\n   - The checkbox sends a POST request to `/toggle-todo/{id}` when clicked\r\n   - The server toggles the todo's completed status and returns the updated HTML\r\n   - HTMX replaces the todo item with the updated version\r\n5. **Deleting Todos**:\r\n   - The delete button sends a DELETE request to `/delete-todo/{id}` when clicked\r\n   - The server removes the todo from the list\r\n   - HTMX removes the todo item from the page\r\n\r\nThis demonstrates how FastHTML can be used to build a complete interactive application with minimal code.\r\n\r\n## The FastHTML Advantage\r\n\r\nNow that you've seen FastHTML in action, let's summarize its key advantages:\r\n\r\n1. **Python-Powered UI**: Write both your backend and frontend in Python\r\n2. **Declarative Syntax**: Create UIs by composing functions rather than writing HTML templates\r\n3. **Integrated Interactivity**: Built-in support for HTMX makes adding interactivity simple\r\n4. **No Context Switching**: Stay in Python throughout your development workflow\r\n5. **Minimal Dependencies**: No need for a complex JavaScript stack\r\n6. **Quick Development**: Build functional UIs in minutes rather than hours\r\n\r\nFastHTML is particularly well-suited for:\r\n- Internal tools and dashboards\r\n- Prototypes and MVPs\r\n- Data visualization applications\r\n- Admin interfaces\r\n- Any application where development speed is prioritized over complex UI interactions\r\n\r\n## Conclusion\r\n\r\nFastHTML empowers beginners to craft web applications using Python's familiar syntax, sidestepping the complexities of traditional web development. In this guide, we've walked through installing FastHTML, mastering its HTML-like syntax, and building various UI components from simple text elements to complete interactive applications.\r\n\r\nYou've seen how to:\r\n- Create basic HTML elements using Python functions\r\n- Structure layouts with containers and grids\r\n- Build forms for user input\r\n- Create interactive UIs with HTMX integration\r\n\r\nWith FastHTML, you can focus on your application's functionality rather than wrestling with multiple languages and frameworks. Its intuitive approach makes web development more accessible while providing the power and flexibility needed for real-world applications.","src/content/posts/fasthtml-start.mdx",[1044],"../../assets/images/25/02/fasthtml-start.jpg","cbfcfef11cfe5688","fasthtml-start.mdx","fix-kernel-panic-unable-mount-root-fs",{id:1047,data:1049,body:1058,filePath:1059,assetImports:1060,digest:1062,legacyId:1063,deferredRender:32},{title:1050,description:1051,date:1052,image:1053,authors:1054,categories:1055,tags:1056,canonical:1057},"Fix Kernel Panic - Not Syncing: VFS: Unable to Mount Root FS on Unknown-Block(0,0)","Learn how to fix the common Ubuntu error Kernel Panic - Not Syncing: VFS: Unable to Mount Root FS on Unknown-Block(0,0) with our step-by-step guide.",["Date","2023-05-11T06:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/05/fix-kernel-panic.jpeg",[19],[98],[135],"https://www.bitdoze.com/fix-kernel-panic-unable-mount-root-fs/","import { Picture } from \"astro:assets\";\r\nimport ubuntukernal from \"../../assets/images/23/05/ubuntu-boot.jpeg\";\r\nimport dockerup from \"../../assets/images/23/05/docker-up.jpeg\";\r\n\r\nI have an Ubuntu 22.04 on an [Hetzner VPS](https://go.bitdoze.com/hetzner) and I have an update of the packages to the latest version. During the update the Ubuntu Kernal updated also to the latest version which was: **_5.15.0-71-generic_**\r\n\r\nAll went good and I have rebooted the server but it never went up. I have logged in to the console and I have seen that it was getting stuck during boot at the error:\r\n\r\n```\r\nKernel Panic - Not Syncing: VFS: Unable to Mount Root FS on Unknown-Block(0,0)\r\n```\r\n\r\nKernel Panic - Not Syncing: VFS: Unable to Mount Root FS on Unknown-Block(0,0) is a common error that can occur in Ubuntu when the system is unable to mount the root file system during boot. This error message indicates that the kernel is unable to find the root file system, which is necessary for the operating system to start up properly.\r\n\r\n## How to Fix Kernel Panic on Ubuntu\r\n\r\nThe steps below helped me move past the error and achieve a clean boot in the end. I will detail the steps so that you can follow along and effectively resolve your issue.\r\n\r\n### Login to Console and Choose Advanced Options For Ubuntu\r\n\r\nIf you are not on a laptop and you are using a VPS server the provider needs to have a console that will allow you to see what is happening during boot and choose the **Advanced Options For Ubuntu** in there you will have the latest Kernels to choose from and you need not to choose the latest one like in bellow picture:\r\n\r\n<Picture\r\n  src={ubuntukernal}\r\n  widths={[200, 400, 900]}\r\n  sizes=\"(max-width: 900px) 100vw, 900px\"\r\n  alt=\"Ubuntu Boot Options\"\r\n/>\r\n\r\nThis will allow your system to boot.\r\n\r\n### Check The File System\r\n\r\nLogin to your VPS and runn the bellow command:\r\n\r\n```bash\r\nsudo fdisk -l\r\n```\r\n\r\nOutput:\r\n\r\n```bash\r\nsudo fdisk -l\r\nDisk /dev/sda: 76.3 GiB, 81923145728 bytes, 160006144 sectors\r\nDisk model: QEMU HARDDISK\r\nUnits: sectors of 1 * 512 = 512 bytes\r\nSector size (logical/physical): 512 bytes / 512 bytes\r\nI/O size (minimum/optimal): 512 bytes / 512 bytes\r\nDisklabel type: gpt\r\nDisk identifier: 6288F8C3-0B08-4091-9A39-F8940D8E5D62\r\n\r\nDevice      Start       End   Sectors  Size Type\r\n/dev/sda1  528384 160006110 159477727   76G Linux filesystem\r\n/dev/sda14   2048      4095      2048    1M BIOS boot\r\n/dev/sda15   4096    528383    524288  256M EFI System\r\n\r\nPartition table entries are not in disk order.\r\n```\r\n\r\nIn here you will see the device for Linux filesystem in my case is **_/dev/sda1_**\r\n\r\n### Update the root FS\r\n\r\nNext you need to mount and update the FS:\r\n\r\n```bash\r\nsudo mount /dev/sda1 /mnt\r\nsudo mount --bind /dev /mnt/dev\r\nsudo mount --bind /dev/pts /mnt/dev/pts\r\nsudo mount --bind /proc /mnt/proc\r\nsudo mount --bind /sys /mnt/sys\r\nsudo chroot /mnt\r\n```\r\n\r\n### Update The Temporary File System\r\n\r\nNext run the bellow command:\r\n\r\n```bash\r\nroot@cloud:/# update-initramfs -u -k 5.15.0-71-generic\r\noutput:\r\nupdate-initramfs: Generating /boot/initrd.img-5.15.0-71-generic\r\n```\r\n\r\nIn my case the problem was on **_5.15.0-71-generic_** Kernal that was installed with the upgrade, you need to use your kernal, which should be in the Ubuntu Advanced options.\r\n\r\nThe command update-initramfs -u -k 5.15.0-71-generic is used to update the initial RAM file system (initramfs) for the kernel version 5.15.0-71-generic in Ubuntu.\r\n\r\nThe initramfs is a temporary file system that is loaded into memory during the boot process before the root file system is mounted. It contains the necessary files and drivers to initialize the hardware and load the root file system.\r\n\r\nBy running this command, you are updating the initramfs for the specified kernel version and ensuring that the necessary files and drivers are available during the boot process. This can be useful if you have recently installed new hardware or made changes to the system that require updated drivers.\r\n\r\nThe -u option tells the command to update the initramfs, and the -k option specifies the kernel version to update. The 5.15.0-71-generic part of the command specifies the specific kernel version to update.\r\n\r\n### Update Your GRUB\r\n\r\nNext run:\r\n\r\n```sh\r\nroot@cloud:/# update-grub\r\n\r\nOutput:\r\nSourcing file `/etc/default/grub'\r\nSourcing file `/etc/default/grub.d/init-select.cfg'\r\nGenerating grub configuration file ...\r\nFound linux image: /boot/vmlinuz-5.15.0-71-generic\r\nFound initrd image: /boot/initrd.img-5.15.0-71-generic\r\nFound linux image: /boot/vmlinuz-5.15.0-56-generic\r\nFound initrd image: /boot/initrd.img-5.15.0-56-generic\r\nFound linux image: /boot/vmlinuz-5.15.0-53-generic\r\nFound initrd image: /boot/initrd.img-5.15.0-53-generic\r\nFound linux image: /boot/vmlinuz-5.15.0-46-generic\r\nFound initrd image: /boot/initrd.img-5.15.0-46-generic\r\nFound linux image: /boot/vmlinuz-5.15.0-41-generic\r\nFound initrd image: /boot/initrd.img-5.15.0-41-generic\r\nWarning: os-prober will not be executed to detect other bootable partitions.\r\nSystems on them will not be added to the GRUB boot configuration.\r\nCheck GRUB_DISABLE_OS_PROBER documentation entry.\r\ndone\r\n```\r\n\r\nThe command update-grub is used to update the GRUB bootloader configuration in Ubuntu.\r\n\r\nGRUB (Grand Unified Bootloader) is a bootloader that is used to load the Linux kernel and start the boot process for Ubuntu. The GRUB configuration file is located at /boot/grub/grub.cfg and contains information about the available kernels and boot options.\r\n\r\nWhen you run the update-grub command, it scans your system and detects any changes to the available kernels and boot options. It then updates the GRUB configuration file with the new information.\r\n\r\nNow you can reboot your server and should start successfully.","src/content/posts/fix-kernel-panic-unable-mount-root-fs.mdx",[1061],"../../assets/images/23/05/fix-kernel-panic.jpeg","daf9a8040c9215ea","fix-kernel-panic-unable-mount-root-fs.mdx","fasthtml-multiple-pages",{id:1064,data:1066,body:1075,filePath:1076,assetImports:1077,digest:1079,legacyId:1080,deferredRender:32},{title:1067,description:1068,date:1069,image:1070,authors:1071,categories:1072,tags:1073,canonical:1074},"Create a Multi-Page Website with FastHTML: Complete Structure Tutorial","Learn how to build a structured multi-page website with FastHTML using reusable components, shared layouts, and organized directories. Perfect for Python developers wanting to create maintainable web applications.",["Date","2025-02-26T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/02/fasthtml-multiple-pages.jpg",[19],[98],[1039,294],"https://www.bitdoze.com/fasthtml-multiple-pages/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\nWelcome back to our FastHTML series! In the first article, [FastHTML - Getting Started](https://www.bitdoze.com/fasthtml-start/) we introduced the basics of FastHTML—a Python-based web framework that lets you build dynamic, responsive websites with minimal effort. We covered how to install FastHTML, create a simple page with components like headings and paragraphs, and add styling using the Tailwind CSS CDN script. If you haven't read it yet, don't worry—this article will bring you up to speed while guiding you through the next step: building a multi-page website with a consistent header and footer across all pages.\r\n\r\nIn this article, we'll show you how to structure a FastHTML project to support multiple pages—like \"Home,\" \"About,\" and \"Contact\"—and ensure they share a cohesive design. By the end, you'll have a fully functional multi-page website that you can run locally and expand as needed. Let's dive in!\r\n\r\n## Why Multiple Pages?\r\n\r\nA multi-page website allows you to organize content logically and improve the user experience. While single-page applications have their place, most websites benefit from distinct pages for different purposes:\r\n\r\n- **Organization**: Separate pages keep your content structured—think of a \"Home\" page for an overview, \"About\" for your story, and \"Contact\" for communication details.\r\n- **Navigation**: Users expect to click links to explore different sections, making your site intuitive.\r\n- **Scalability**: As your site grows, adding new pages becomes a breeze.\r\n- **SEO Benefits**: Search engines can index individual pages, improving discoverability.\r\n- **User Experience**: Users can bookmark specific pages and use browser navigation (back/forward) naturally.\r\n\r\nFastHTML makes this process easy with its routing system and reusable components. Let's see how to set it up.\r\n\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/Zc8APrgknug\"\r\n  label=\"FastHTML Multiple Pages\"\r\n/>\r\n\r\n\r\n## FastHTML Series\r\n\r\nBelow are the articles on FastHTML to help you get started:\r\n\r\n- [FastHTML Get Started](https://www.bitdoze.com/fasthtml-start/)\r\n- [FastHTML Multiple Pages](https://www.bitdoze.com/fasthtml-multiple-pages/)\r\n\r\n## Creating a Multi-Page Website with FastHTML\r\n\r\n### Step 1: Setting Up Your Project Structure\r\n\r\nA well-organized project structure is crucial for maintaining a multi-page website, especially as it grows. Here's a recommended directory structure that keeps your code modular and maintainable:\r\n\r\n```\r\nmywebsite/\r\n├── main.py              # Main application entry point\r\n├── components.py        # Reusable UI components\r\n└── pages/               # Individual page content\r\n    ├── __init__.py      # Makes pages a proper Python package\r\n    ├── home.py          # Home page content\r\n    ├── about.py         # About page content\r\n    └── contact.py       # Contact page content\r\n```\r\n\r\nLet's understand each component:\r\n\r\n- **`main.py`**: This is your application's entry point that handles routing (URL mapping) and server setup. It connects pages to URLs and starts the FastHTML server.\r\n\r\n- **`components.py`**: Contains reusable components like headers, footers, and navigation menus that appear on multiple pages. This approach follows the DRY (Don't Repeat Yourself) principle.\r\n\r\n- **`pages/`**: A directory containing Python modules for each page on your website. Each file defines the content specific to that page.\r\n\r\n- **`pages/__init__.py`**: An empty file that makes the `pages` directory a proper Python package, allowing for cleaner imports.\r\n\r\nThis modular structure gives you several advantages:\r\n\r\n1. **Separation of concerns**: Each file has a specific purpose\r\n2. **Maintainability**: Easy to find and update specific components\r\n3. **Scalability**: Simply add new files to the `pages/` directory as your site grows\r\n4. **Organization**: Logical grouping of related functionality\r\n\r\nLet's create these files one by one.\r\n\r\n### **Step 2: Creating Reusable Header and Footer Components**\r\n\r\nTo ensure consistency across your website, we'll create reusable header and footer components. This approach ensures that navigation and branding remain uniform throughout the site, and any updates need to be made in just one place.\r\n\r\nFirst, let's create the `components.py` file:\r\n\r\n**File: `mywebsite/components.py`**\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\ndef header(current_page=\"/\"):\r\n    \"\"\"\r\n    Creates a consistent header with navigation.\r\n\r\n    Args:\r\n        current_page: The current page path, used to highlight the active link\r\n\r\n    Returns:\r\n        A Header component with navigation\r\n    \"\"\"\r\n    # Define the navigation links\r\n    nav_items = [\r\n        (\"Home\", \"/\"),\r\n        (\"About\", \"/about\"),\r\n        (\"Contact\", \"/contact\")\r\n    ]\r\n\r\n    # Create navigation items with appropriate styling\r\n    nav_links = []\r\n    for title, path in nav_items:\r\n        # Apply special styling to the current page link\r\n        is_current = current_page == path\r\n        link_class = \"text-white hover:text-gray-300 px-3 py-2\"\r\n        if is_current:\r\n            link_class += \" font-bold underline\"\r\n\r\n        nav_links.append(\r\n            Li(\r\n                A(title, href=path, cls=link_class)\r\n            )\r\n        )\r\n\r\n    return Header(\r\n        Div(\r\n            # Website logo/name\r\n            A(\"MyWebsite\", href=\"/\", cls=\"text-xl font-bold text-white\"),\r\n\r\n            # Navigation menu\r\n            Nav(\r\n                Ul(\r\n                    *nav_links,\r\n                    cls=\"flex space-x-2\"\r\n                ),\r\n                cls=\"ml-auto\"\r\n            ),\r\n            cls=\"container mx-auto flex items-center justify-between px-4 py-3\"\r\n        ),\r\n        cls=\"bg-blue-600 shadow-md\"\r\n    )\r\n\r\ndef footer():\r\n    \"\"\"\r\n    Creates a consistent footer for all pages.\r\n\r\n    Returns:\r\n        A Footer component with copyright and links\r\n    \"\"\"\r\n    current_year = 2025  # In a real app, use datetime to get current year\r\n\r\n    return Footer(\r\n        Div(\r\n            Div(\r\n                P(f\"© {current_year} MyWebsite. All rights reserved.\",\r\n                  cls=\"text-gray-500\"),\r\n                cls=\"mb-4\"\r\n            ),\r\n            Div(\r\n                A(\"Privacy Policy\", href=\"#\", cls=\"text-blue-500 hover:underline mr-4\"),\r\n                A(\"Terms of Service\", href=\"#\", cls=\"text-blue-500 hover:underline\"),\r\n                cls=\"text-sm\"\r\n            ),\r\n            cls=\"container mx-auto px-4 py-6 text-center\"\r\n        ),\r\n        cls=\"bg-gray-100 mt-8\"\r\n    )\r\n\r\ndef page_layout(title, content, current_page=\"/\"):\r\n    \"\"\"\r\n    Wraps page content with consistent header, footer, and styling.\r\n\r\n    Args:\r\n        title: The page title (appears in browser tab)\r\n        content: The main content of the page\r\n        current_page: The current page path for navigation highlighting\r\n\r\n    Returns:\r\n        A complete HTML page with header, content, and footer\r\n    \"\"\"\r\n    return Html(\r\n        Head(\r\n            Title(title),\r\n            # Meta tags for better SEO and mobile display\r\n            Meta(name=\"viewport\", content=\"width=device-width, initial-scale=1.0\"),\r\n            Meta(name=\"description\", content=f\"{title} - MyWebsite built with FastHTML\"),\r\n            # Include Tailwind CSS for styling\r\n            Script(src=\"https://cdn.tailwindcss.com\")\r\n        ),\r\n        Body(\r\n            # Include header with current page highlighted\r\n            header(current_page),\r\n\r\n            # Main content area\r\n            Main(\r\n                Div(\r\n                    content,\r\n                    cls=\"container mx-auto px-4 py-8\"\r\n                ),\r\n                cls=\"min-h-screen\"  # Ensures footer stays at bottom\r\n            ),\r\n\r\n            # Include footer\r\n            footer()\r\n        )\r\n    )\r\n```\r\n\r\nLet's break down what we've created:\r\n\r\n1. **`header(current_page)`**:\r\n   - Creates a navigation bar with links to all pages\r\n   - Takes a `current_page` parameter to highlight the active page\r\n   - Uses Tailwind CSS classes for styling (background color, text color, spacing)\r\n   - Includes a website name/logo that links to the home page\r\n\r\n2. **`footer()`**:\r\n   - Creates a simple footer with copyright information and links\r\n   - Includes the current year (hardcoded for simplicity, but you could use `datetime.now().year`)\r\n   - Uses Tailwind CSS for styling and positioning\r\n\r\n3. **`page_layout(title, content, current_page)`**:\r\n   - Integrates the header and footer with the page-specific content\r\n   - Adds a page title that appears in the browser tab\r\n   - Includes meta tags for better SEO and mobile compatibility\r\n   - Adds the Tailwind CSS script for styling\r\n   - Ensures the main content has a minimum height to push the footer to the bottom\r\n\r\nThese components ensure your website has a consistent look and feel across all pages. The `current_page` parameter in the header function allows for visual indication of which page the user is currently viewing.\r\n\r\n### **Step 3: Creating Individual Page Content**\r\n\r\nNow, let's create the content for each page. We'll store these in separate files within the `pages/` directory.\r\n\r\nFirst, create the `pages/__init__.py` file (it can be empty) to make the directory a proper Python package.\r\n\r\n**File: `mywebsite/pages/home.py`**\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\ndef home():\r\n    \"\"\"\r\n    Defines the home page content.\r\n\r\n    Returns:\r\n        Components representing the home page content\r\n    \"\"\"\r\n    return Div(\r\n        # Hero section\r\n        Div(\r\n            H1(\"Welcome to MyWebsite\",\r\n               cls=\"text-4xl font-bold text-gray-800 mb-4\"),\r\n            P(\"Build beautiful web applications with FastHTML and Python.\",\r\n              cls=\"text-xl text-gray-600 mb-6\"),\r\n            Div(\r\n                A(\"Get Started\",\r\n                  href=\"/about\",\r\n                  cls=\"bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded mr-4\"),\r\n                A(\"Learn More\",\r\n                  href=\"/contact\",\r\n                  cls=\"bg-gray-200 hover:bg-gray-300 text-gray-800 font-bold py-2 px-4 rounded\"),\r\n                cls=\"flex\"\r\n            ),\r\n            cls=\"py-12 text-center\"\r\n        ),\r\n\r\n        # Features section\r\n        Div(\r\n            H2(\"Key Features\", cls=\"text-3xl font-bold text-center mb-8\"),\r\n            Div(\r\n                # Feature 1\r\n                Div(\r\n                    H3(\"Easy to Learn\", cls=\"text-xl font-semibold mb-2\"),\r\n                    P(\"Built on Python, making web development accessible to everyone.\",\r\n                      cls=\"text-gray-600\"),\r\n                    cls=\"bg-white p-6 rounded-lg shadow-md\"\r\n                ),\r\n                # Feature 2\r\n                Div(\r\n                    H3(\"Highly Productive\", cls=\"text-xl font-semibold mb-2\"),\r\n                    P(\"Create web applications faster with fewer lines of code.\",\r\n                      cls=\"text-gray-600\"),\r\n                    cls=\"bg-white p-6 rounded-lg shadow-md\"\r\n                ),\r\n                # Feature 3\r\n                Div(\r\n                    H3(\"Scalable\", cls=\"text-xl font-semibold mb-2\"),\r\n                    P(\"Easily expand your application as your needs grow.\",\r\n                      cls=\"text-gray-600\"),\r\n                    cls=\"bg-white p-6 rounded-lg shadow-md\"\r\n                ),\r\n                cls=\"grid grid-cols-1 md:grid-cols-3 gap-6 mb-12\"\r\n            ),\r\n            cls=\"py-8\"\r\n        )\r\n    )\r\n```\r\n\r\n**File: `mywebsite/pages/about.py`**\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\ndef about():\r\n    \"\"\"\r\n    Defines the about page content.\r\n\r\n    Returns:\r\n        Components representing the about page content\r\n    \"\"\"\r\n    return Div(\r\n        # Page header\r\n        H1(\"About Us\",\r\n           cls=\"text-3xl font-bold text-gray-800 mb-6 text-center\"),\r\n\r\n        # Main content\r\n        Div(\r\n            # Company description\r\n            Div(\r\n                H2(\"Our Story\", cls=\"text-2xl font-semibold mb-4\"),\r\n                P(\"Founded in 2025, MyWebsite was created to help developers build \"\r\n                  \"beautiful web applications using Python. Our mission is to make \"\r\n                  \"web development accessible, enjoyable, and productive.\",\r\n                  cls=\"text-gray-600 mb-4\"),\r\n                P(\"We believe that Python developers should be able to create \"\r\n                  \"stunning web applications without having to learn multiple \"\r\n                  \"languages and frameworks.\",\r\n                  cls=\"text-gray-600 mb-4\"),\r\n                cls=\"mb-8\"\r\n            ),\r\n\r\n            # Team section\r\n            Div(\r\n                H2(\"Our Team\", cls=\"text-2xl font-semibold mb-4\"),\r\n                Div(\r\n                    # Team member 1\r\n                    Div(\r\n                        H3(\"Jane Doe\", cls=\"text-xl font-semibold\"),\r\n                        P(\"Founder & CEO\", cls=\"text-gray-500 italic mb-2\"),\r\n                        P(\"Python enthusiast with 15 years of experience in web development.\",\r\n                          cls=\"text-gray-600\"),\r\n                        cls=\"bg-white p-4 rounded shadow-md\"\r\n                    ),\r\n                    # Team member 2\r\n                    Div(\r\n                        H3(\"John Smith\", cls=\"text-xl font-semibold\"),\r\n                        P(\"CTO\", cls=\"text-gray-500 italic mb-2\"),\r\n                        P(\"Full-stack developer with a passion for clean, maintainable code.\",\r\n                          cls=\"text-gray-600\"),\r\n                        cls=\"bg-white p-4 rounded shadow-md\"\r\n                    ),\r\n                    cls=\"grid grid-cols-1 md:grid-cols-2 gap-6\"\r\n                )\r\n            )\r\n        )\r\n    )\r\n```\r\n\r\n**File: `mywebsite/pages/contact.py`**\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\ndef contact():\r\n    \"\"\"\r\n    Defines the contact page with a form.\r\n\r\n    Returns:\r\n        Components representing the contact page content\r\n    \"\"\"\r\n    return Div(\r\n        # Page header\r\n        H1(\"Contact Us\",\r\n           cls=\"text-3xl font-bold text-gray-800 mb-6 text-center\"),\r\n\r\n        # Contact information and form\r\n        Div(\r\n            # Contact info\r\n            Div(\r\n                H2(\"Get in Touch\", cls=\"text-2xl font-semibold mb-4\"),\r\n                P(\"We'd love to hear from you! Please use the form or contact \"\r\n                  \"information below to reach out.\",\r\n                  cls=\"text-gray-600 mb-4\"),\r\n                Div(\r\n                    P(Strong(\"Email: \"), \"info@mywebsite.com\", cls=\"mb-2\"),\r\n                    P(Strong(\"Phone: \"), \"+1 (555) 123-4567\", cls=\"mb-2\"),\r\n                    P(Strong(\"Address: \"), \"123 Web Street, Internet City, 10101\", cls=\"mb-2\"),\r\n                    cls=\"mb-6\"\r\n                ),\r\n                cls=\"mb-8 md:pr-8\"\r\n            ),\r\n\r\n            # Contact form\r\n            Div(\r\n                H2(\"Send a Message\", cls=\"text-2xl font-semibold mb-4\"),\r\n                Form(\r\n                    # Name field\r\n                    Div(\r\n                        Label(\"Name\", For=\"name\", cls=\"block text-gray-700 mb-1\"),\r\n                        Input(type=\"text\", id=\"name\", name=\"name\",\r\n                              placeholder=\"Your name\",\r\n                              cls=\"w-full px-3 py-2 border rounded focus:outline-none focus:ring focus:border-blue-500\"),\r\n                        cls=\"mb-4\"\r\n                    ),\r\n                    # Email field\r\n                    Div(\r\n                        Label(\"Email\", For=\"email\", cls=\"block text-gray-700 mb-1\"),\r\n                        Input(type=\"email\", id=\"email\", name=\"email\",\r\n                              placeholder=\"Your email\",\r\n                              cls=\"w-full px-3 py-2 border rounded focus:outline-none focus:ring focus:border-blue-500\"),\r\n                        cls=\"mb-4\"\r\n                    ),\r\n                    # Message field\r\n                    Div(\r\n                        Label(\"Message\", For=\"message\", cls=\"block text-gray-700 mb-1\"),\r\n                        Textarea(id=\"message\", name=\"message\",\r\n                                placeholder=\"Your message\",\r\n                                rows=5,\r\n                                cls=\"w-full px-3 py-2 border rounded focus:outline-none focus:ring focus:border-blue-500\"),\r\n                        cls=\"mb-6\"\r\n                    ),\r\n                    # Submit button\r\n                    Button(\"Send Message\",\r\n                           type=\"submit\",\r\n                           cls=\"bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\"),\r\n                    action=\"/submit-contact\",\r\n                    method=\"post\",\r\n                    cls=\"bg-white p-6 rounded-lg shadow-md\"\r\n                )\r\n            ),\r\n            cls=\"md:flex\"\r\n        )\r\n    )\r\n```\r\n\r\nEach page file defines a function that returns the content specific to that page:\r\n\r\n1. **`home.py`**:\r\n   - Features a hero section with a welcome message and call-to-action buttons\r\n   - Includes a features section highlighting key benefits\r\n   - Uses a responsive grid layout for the features\r\n\r\n2. **`about.py`**:\r\n   - Contains the company story and team information\r\n   - Uses a responsive grid for team members\r\n   - Maintains consistent styling with the rest of the site\r\n\r\n3. **`contact.py`**:\r\n   - Provides contact information\r\n   - Includes a contact form with name, email, and message fields\r\n   - Uses form validation and focus states for better user experience\r\n\r\nThese pages demonstrate how to create rich, styled content while keeping each page's code separate and maintainable.\r\n\r\n### **Step 4: Setting Up Routing in the Main Application**\r\n\r\nFinally, let's create the `main.py` file, which will serve as the entry point for our application. This file handles routing (mapping URLs to page content) and starts the FastHTML server.\r\n\r\n**File: `mywebsite/main.py`**\r\n\r\n```python\r\nfrom fasthtml.common import *\r\n\r\n# Import page content from the pages directory\r\nfrom pages.home import home as home_page\r\nfrom pages.about import about as about_page\r\nfrom pages.contact import contact as contact_page\r\n\r\n# Import the page layout component\r\nfrom components import page_layout\r\n\r\n# Initialize the FastHTML application\r\napp = FastHTML()\r\n\r\n# Define route for the home page\r\n@app.get(\"/\")\r\ndef home():\r\n    \"\"\"Handler for the home page route.\"\"\"\r\n    return page_layout(\r\n        title=\"Home - MyWebsite\",\r\n        content=home_page(),\r\n        current_page=\"/\"\r\n    )\r\n\r\n# Define route for the about page\r\n@app.get(\"/about\")\r\ndef about():\r\n    \"\"\"Handler for the about page route.\"\"\"\r\n    return page_layout(\r\n        title=\"About Us - MyWebsite\",\r\n        content=about_page(),\r\n        current_page=\"/about\"\r\n    )\r\n\r\n# Define route for the contact page\r\n@app.get(\"/contact\")\r\ndef contact():\r\n    \"\"\"Handler for the contact page route.\"\"\"\r\n    return page_layout(\r\n        title=\"Contact Us - MyWebsite\",\r\n        content=contact_page(),\r\n        current_page=\"/contact\"\r\n    )\r\n\r\n# Handle form submission (for the contact form)\r\n@app.post(\"/submit-contact\")\r\ndef submit_contact(name: str, email: str, message: str):\r\n    \"\"\"\r\n    Handler for contact form submission.\r\n\r\n    In a real application, you might store this data or send an email.\r\n    \"\"\"\r\n    # Simple acknowledgment page\r\n    acknowledgment = Div(\r\n        H1(\"Thank You!\", cls=\"text-3xl font-bold text-gray-800 mb-4\"),\r\n        P(f\"Hello {name}, we've received your message and will respond to {email} soon.\",\r\n          cls=\"text-xl text-gray-600 mb-6\"),\r\n        A(\"Return Home\", href=\"/\", cls=\"bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\"),\r\n        cls=\"text-center py-12\"\r\n    )\r\n\r\n    return page_layout(\r\n        title=\"Thank You - MyWebsite\",\r\n        content=acknowledgment,\r\n        current_page=\"/contact\"\r\n    )\r\n\r\n# Handle 404 Not Found errors\r\n@app.get(\"/{path:path}\")\r\ndef not_found(path: str):\r\n    \"\"\"Handler for any routes that don't match the defined routes.\"\"\"\r\n    error_content = Div(\r\n        H1(\"404 - Page Not Found\", cls=\"text-3xl font-bold text-gray-800 mb-4\"),\r\n        P(f\"Sorry, the page '/{path}' does not exist.\",\r\n          cls=\"text-xl text-gray-600 mb-6\"),\r\n        A(\"Return Home\", href=\"/\", cls=\"bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\"),\r\n        cls=\"text-center py-12\"\r\n    )\r\n\r\n    return page_layout(\r\n        title=\"404 Not Found - MyWebsite\",\r\n        content=error_content,\r\n        current_page=\"/\"\r\n    )\r\n\r\n# Run the application\r\nif __name__ == \"__main__\":\r\n    # Using FastHTML's built-in serve() function\r\n    serve()\r\n```\r\n\r\nLet's understand what's happening in the main application:\r\n\r\n1. **Imports**:\r\n   - We import the FastHTML framework\r\n   - We import each page's content function\r\n   - We import the `page_layout` function from our components file\r\n\r\n2. **Application Initialization**:\r\n   - We create a FastHTML application instance with `app = FastHTML()`\r\n\r\n3. **Route Definitions**:\r\n   - Each route (`/`, `/about`, `/contact`) is mapped to a handler function\r\n   - The handler functions use `page_layout` to wrap the page-specific content with our header and footer\r\n   - We pass the `current_page` parameter to highlight the active link in the navigation\r\n\r\n4. **Form Handling**:\r\n   - We define a handler for the contact form submission\r\n   - This demonstrates how to process POST requests and form data\r\n   - In a real application, you might store this data in a database or send an email\r\n\r\n5. **404 Error Handling**:\r\n   - We create a catch-all route to handle any URLs that don't match our defined routes\r\n   - This provides a user-friendly error page instead of the default 404 response\r\n\r\n6. **Server Startup**:\r\n   - We use FastHTML's built-in `serve()` function to start the application server\r\n   - This is simpler than manually configuring Uvicorn and includes auto-reload functionality\r\n\r\nWith this setup, our application can handle multiple pages, form submissions, and even 404 errors, all while maintaining a consistent user interface.\r\n\r\n### **Step 5: Running and Testing Your Website**\r\n\r\nNow that we've created all the necessary files, let's run the application and test our multi-page website.\r\n\r\n1. **Navigate to your project directory**:\r\n   ```bash\r\n   cd mywebsite\r\n   ```\r\n\r\n2. **Run the application**:\r\n   ```bash\r\n   python main.py\r\n   ```\r\n\r\n3. **Open your browser** and visit:\r\n   - `http://localhost:5001/` (Home)\r\n   - `http://localhost:5001/about` (About)\r\n   - `http://localhost:5001/contact` (Contact)\r\n\r\nYou should see a fully-functional multi-page website with consistent navigation, styling, and structure. Try clicking the links in the header to navigate between pages, and notice how the current page is highlighted in the navigation.\r\n\r\nIf you encounter any issues, here are some troubleshooting tips:\r\n\r\n- **404 Errors**: Make sure the route paths in `main.py` match the `href` values in the header function.\r\n- **Import Errors**: Check that your directory structure is correct and that `pages/__init__.py` exists.\r\n- **Styling Issues**: Ensure the Tailwind CSS script is included in the `page_layout` function.\r\n- **Server Won't Start**: Verify that all required packages are installed and imports are correct.\r\n\r\n## **Extending Your Multi-Page Website**\r\n\r\nNow that you have a solid foundation for your multi-page website, you can extend it in various ways:\r\n\r\n### **Adding More Pages**\r\n\r\nTo add a new page (e.g., \"Services\"):\r\n\r\n1. Create a new file `pages/services.py` with the page content function\r\n2. Import the function in `main.py`\r\n3. Add a new route handler in `main.py`\r\n4. Add a link to the page in the `header` function in `components.py`\r\n\r\n### **Adding Dynamic Content**\r\n\r\nYou can make your pages more dynamic by:\r\n\r\n- Fetching data from external APIs\r\n- Reading content from a database\r\n- Generating content based on user input or URL parameters\r\n\r\n### **Implementing User Authentication**\r\n\r\nFor pages that require authentication:\r\n\r\n- Add login/register pages\r\n- Implement session management\r\n- Create protected routes that redirect unauthenticated users\r\n\r\n### **Enhancing the User Interface**\r\n\r\nImprove the user experience with:\r\n\r\n- More sophisticated Tailwind CSS styling\r\n- Interactive components using HTMX\r\n- Client-side validation for forms\r\n- Animated transitions between pages\r\n\r\n## **Conclusion**\r\n\r\nCongratulations! You've successfully created a multi-page website with FastHTML that features consistent navigation, styling, and structure. This approach demonstrates the power and flexibility of FastHTML for building web applications quickly and efficiently.\r\n\r\nBy organizing your project into reusable components and separate page files, you've created a maintainable codebase that's easy to extend as your website grows. The shared layout ensures a consistent user experience across all pages, while the modular structure keeps your code clean and organized.\r\n\r\nHere's a summary of what we've covered:\r\n\r\n1. **Project Structure**: Creating a clean, modular organization for your code\r\n2. **Reusable Components**: Building header, footer, and layout functions\r\n3. **Page Content**: Defining separate content for each page\r\n4. **Routing**: Mapping URLs to page content\r\n5. **Form Handling**: Processing user input from forms\r\n6. **Error Handling**: Creating a custom 404 page\r\n\r\nThis foundation gives you everything you need to continue building and expanding your FastHTML website. In the next article, we'll explore advanced styling options and interactive components to make your website even more impressive.\r\n\r\nHappy coding!","src/content/posts/fasthtml-multiple-pages.mdx",[1078],"../../assets/images/25/02/fasthtml-multiple-pages.jpg","6ab8649399ca57ac","fasthtml-multiple-pages.mdx","fix-rpmdb-error-bdb0087-db_runrecovery",{id:1081,data:1083,body:1092,filePath:1093,assetImports:1094,digest:1096,legacyId:1097,deferredRender:32},{title:1084,description:1085,date:1086,image:1087,authors:1088,categories:1089,tags:1090,canonical:1091},"Fix Cannot Open Packages Database In /var/lib/rpm DB_RUNRECOVERY: Fatal error","Learn how to fix the common Cannot Open Packages Database In /var/lib/rpm DB_RUNRECOVERY: Fatal error and install packages.",["Date","2024-05-24T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/05/fix-rpmdb-error-bdb0087-db_runrecovery.jpeg",[19],[98],[135],"https://www.bitdoze.com/fix-rpmdb-error-bdb0087-db_runrecovery/","If you are encountering the below error message that is not allowing you to install `rpm` or `yum` packages:\r\n\r\n```sh\r\nerror: rpmdb: BDB0113 Thread/process 21929/140612494501952 failed: BDB1507 Thread died in Berkeley DB library\r\nerror: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery\r\nerror: cannot open Packages index using db5 - (-30973)\r\nerror: cannot open Packages database in /var/lib/rpm\r\nerror: rpmdb: BDB0113 Thread/process 21929/140612494501952 failed: BDB1507 Thread died in Berkeley DB library\r\nerror: db5 error(-30973) from dbenv->failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery\r\nerror: cannot open Packages database in /var/lib/rpm\r\n```\r\n\r\nindicates that the RPM database (rpmdb) is corrupted. This corruption can occur due to several reasons, including interrupted package installations, updates, or removals, power failures, or other system interruptions.\r\n\r\n## Causes of RPM Database Corruption\r\n\r\n1. **Interrupted Operations**: The most common cause is the interruption of package management operations such as installation, update, or removal. This can happen if the process is manually stopped or if the system loses power during the operation.\r\n2. **Stale Lock Files**: If `rpm`, `yum`, or `dnf` commands do not exit cleanly, they can leave behind lock files in `/var/lib/rpm`, which can cause subsequent operations to fail.\r\n3. **Hardware Issues**: Failing memory or disk can also lead to database corruption. This is less common but should be considered if the problem persists.\r\n4. **Software Bugs**: There may be bugs in the RPM or Berkeley DB software that cause corruption under certain conditions.\r\n\r\n## Steps to Resolve the Issue\r\n\r\nTo fix the corrupted RPM database, you can follow these steps:\r\n\r\n1. **Backup the Current RPM Database**:\r\n\r\n   ```bash\r\n   mkdir /var/lib/rpm/backup\r\n   cp -a /var/lib/rpm/__db* /var/lib/rpm/backup/\r\n   ```\r\n\r\n2. **Remove the Corrupted Database Files**:\r\n\r\n   ```bash\r\n   rm -f /var/lib/rpm/__db.[0-9][0-9]*\r\n   ```\r\n\r\n3. **Rebuild the RPM Database**:\r\n\r\n   ```bash\r\n   rpm --rebuilddb\r\n   ```\r\n\r\n4. **Clean the Yum Cache**:\r\n\r\n   ```bash\r\n   yum clean all\r\n   ```\r\n\r\n5. **Verify the Integrity of the Packages File** (Optional):\r\n\r\n   ```bash\r\n   db_verify /var/lib/rpm/Packages\r\n   ```\r\n\r\n6. **Check for Hardware Issues** (Optional):\r\n   Review system logs for any hardware-related errors that might indicate failing memory or disk.\r\n\r\n## Example Commands\r\n\r\nHere is a consolidated set of commands to perform the above steps:\r\n\r\n```bash\r\n# Backup the current RPM database\r\nmkdir /var/lib/rpm/backup\r\ncp -a /var/lib/rpm/__db* /var/lib/rpm/backup/\r\n\r\n# Remove the corrupted database files\r\nrm -f /var/lib/rpm/__db.[0-9][0-9]*\r\n\r\n# Rebuild the RPM database\r\nrpm --rebuilddb\r\n\r\n# Clean the Yum cache\r\nyum clean all\r\n```\r\n\r\n## Additional Considerations\r\n\r\n- **Ensure Uninterrupted Operations**: Make sure that package management operations are not interrupted. Use a UPS to prevent power failures during critical operations.\r\n- **Check for Automation Issues**: If you are using automation tools, ensure they are not terminating package management processes abruptly.\r\n- **Monitor System Logs**: Regularly check system logs for any signs of hardware issues or other anomalies that could lead to database corruption.\r\n\r\nBy following these steps, you should be able to resolve the RPM database corruption and prevent it from recurring. If the problem persists, consider investigating deeper into hardware issues or potential software bugs.","src/content/posts/fix-rpmdb-error-bdb0087-db_runrecovery.mdx",[1095],"../../assets/images/24/05/fix-rpmdb-error-bdb0087-db_runrecovery.jpeg","139442230c6f66d0","fix-rpmdb-error-bdb0087-db_runrecovery.mdx","flowiseai-install",{id:1098,data:1100,body:1109,filePath:1110,assetImports:1111,digest:1113,legacyId:1114,deferredRender:32},{title:1101,description:1102,date:1103,image:1104,authors:1105,categories:1106,tags:1107,canonical:1108},"How to Install FlowiseAI with Docker Compose","Learn how you can FlowiseAI with docker compose  and Postgres DB and take advantage of no-code AI flows.",["Date","2024-03-27T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/flowise-ai.jpg",[19],[98],[242],"https://www.bitdoze.com/flowiseai-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\nimport imag2 from \"../../assets/images/24/03/flowise_flow.png\";\r\nimport imag3 from \"../../assets/images/24/03/flowise-usechat.png\";\r\n\r\n[FlowiseAI](https://flowiseai.com/) is an open-source platform that allows you to build and deploy custom AI workflows using a drag-and-drop interface. It provides a user-friendly way to integrate various AI models, APIs, and data sources into your applications without writing complex code. FlowiseAI is built on top of Node.js and React, making it easy to extend and customize.\r\n\r\nIn this tutorial, we are going to see how easy is to host FlowiseAI on your VPS server with docker-compose and have an SSL certificate. I will include also an option to backup the database with [Docker DB Backup](https://github.com/tiredofit/docker-db-backup) to have the SQL dumps in case something goes wrong and you can't use the volumes.\r\n\r\nWe are going to use [Dockge](https://www.bitdoze.com/dockge-install/) to administrate the docker-compose file and as reverse proxy CloudFlare Tunnels. You can also use docker-compose to deploy as it will work the same on whatever reverse proxy you prefer.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n## How to Install FlowiseAI with Docker and Docker Compose\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/ZJvl1_DVy_g\"\r\n  label=\"How to Install FlowiseAI with Docker Compose\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host FlowiseAI, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n- OR reverse proxy with CloudPanel you can check: [Setup CloudPanel As Reverse Proxy with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\nHaving all of this you will be ready to move to next step and add the containers in Dockge.\r\n\r\n### 2. Create Docker Compose File\r\n\r\nThe first step is to create a Docker Compose file that defines the services required to run FlowiseAI. Here's an example `docker-compose.yml` file:\r\n\r\n```yaml\r\nversion: \"3.9\"\r\nservices:\r\n  flowise-db:\r\n    image: postgres:16-alpine\r\n    hostname: flowise-db\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    volumes:\r\n      - ./flowise-db-data:/var/lib/postgresql/data\r\n    restart: unless-stopped\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n\r\n  flowise:\r\n    image: flowiseai/flowise:latest\r\n    container_name: flowiseai\r\n    hostname: flowise\r\n    healthcheck:\r\n      test: wget --no-verbose --tries=1 --spider http://localhost:${PORT}\r\n    ports:\r\n      - 5023:${PORT}\r\n    volumes:\r\n      - ./flowiseai:/root/.flowise\r\n    environment:\r\n      DEBUG: false\r\n      PORT: ${PORT}\r\n      FLOWISE_USERNAME: ${FLOWISE_USERNAME}\r\n      FLOWISE_PASSWORD: ${FLOWISE_PASSWORD}\r\n      APIKEY_PATH: /root/.flowise\r\n      SECRETKEY_PATH: /root/.flowise\r\n      LOG_LEVEL: info\r\n      LOG_PATH: /root/.flowise/logs\r\n      DATABASE_TYPE: postgres\r\n      DATABASE_PORT: 5432\r\n      DATABASE_HOST: flowise-db\r\n      DATABASE_NAME: ${POSTGRES_DB}\r\n      DATABASE_USER: ${POSTGRES_USER}\r\n      DATABASE_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\n    depends_on:\r\n      flowise-db:\r\n        condition: service_healthy\r\n    entrypoint: /bin/sh -c \"sleep 3; flowise start\"\r\n```\r\n\r\nThis Compose file defines two services:\r\n\r\n1. `flowise-db`: A PostgreSQL database service used by FlowiseAI to store data.\r\n2. `flowise`: The FlowiseAI service itself, which depends on the `flowise-db` service.\r\n\r\nThe `flowise` service uses the official `flowiseai/flowise:latest` Docker image and exposes port 3000 (configurable via the `PORT` environment variable). It also mounts a volume at `/root/.flowise` to persist data across container restarts.\r\n\r\nThe Compose file also includes a healthcheck for the `flowise` service, which checks if the FlowiseAI application is running and accessible on `http://localhost:3000`.\r\n\r\nIf you want to include a backup solution for the PostgreSQL database, you can add a third service to the Compose file:\r\n\r\n```yaml\r\nflowise-db-backup:\r\n  container_name: flowise-db-backup\r\n  image: tiredofit/db-backup\r\n  volumes:\r\n    - ./backups:/backup\r\n  environment:\r\n    DB_TYPE: postgres\r\n    DB_HOST: flowise-db\r\n    DB_NAME: ${POSTGRES_DB}\r\n    DB_USER: ${POSTGRES_USER}\r\n    DB_PASS: ${POSTGRES_PASSWORD}\r\n    DB_BACKUP_INTERVAL: 720\r\n    DB_CLEANUP_TIME: 72000\r\n    CHECKSUM: SHA1\r\n    COMPRESSION: GZ\r\n    CONTAINER_ENABLE_MONITORING: false\r\n  depends_on:\r\n    flowise-db:\r\n      condition: service_healthy\r\n  restart: unless-stopped\r\n```\r\n\r\nThis service uses the `tiredofit/db-backup` image to create regular backups of the PostgreSQL database. The backups are stored in the `./backups` directory on the host machine.\r\n\r\n### 3. Create .env file with credentials\r\n\r\nNext, create a `.env` file in the same directory as your `docker-compose.yml` file and add the required environment variables:\r\n\r\n```sh\r\nPORT=3000\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='flowise'\r\nFLOWISE_USERNAME=bitdoze\r\nFLOWISE_PASSWORD=bitdoze\r\n```\r\n\r\nReplace the values with your desired credentials and database name.\r\n\r\n### 4. Deploy FlowiseAI\r\n\r\nWith the Docker Compose file and the `.env` file in place, you can now deploy FlowiseAI using Docker Compose:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nThis command will start the services defined in the Compose file in detached mode (running in the background).\r\n\r\nTo check if the containers are running, use the following command:\r\n\r\n```sh\r\ndocker ps\r\n```\r\n\r\nYou should see the `flowise` and `flowise-db` containers listed as running.\r\n\r\n### 5. Configure the CloudFlare Tunnels for SSL and Domain access\r\n\r\nTo access FlowiseAI securely over the internet, you can set up CloudFlare Tunnels. CloudFlare Tunnels provide a secure way to expose your FlowiseAI instance to the internet without exposing your server's IP address.\r\n\r\nGo in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 6. Create your first FlowiseAI flow\r\n\r\nOnce your FlowiseAI instance is up and running, you can access the web interface by navigating to `http://localhost:3000` (or the domain you configured with CloudFlare Tunnels).\r\n\r\nFrom the FlowiseAI interface, you can start creating your first AI workflow by dragging and dropping nodes, connecting them, and configuring their settings.\r\n\r\n<Picture src={imag2} alt=\"FlowiseAI add flow\" />\r\n\r\nYou have also a marketplace with flows that can be used.\r\n\r\n### 7. Use the FlowiseAI Chatflow Externally\r\n\r\nAfter you finish designing your flow, you can go and use it externally with few options like Embed, Python, JavaScript, CURL or just share it.\r\n\r\n<Picture src={imag3} alt=\"Use the FlowiseAI Chatflow Externally\" />\r\n\r\n## Conclusions\r\n\r\nBy following this guide, you've learned how to deploy FlowiseAI to Docker using Docker Compose. You've created a Docker Compose file that defines the required services, set up environment variables, and started the FlowiseAI instance. Additionally, you've learned how to configure CloudFlare Tunnels to access your FlowiseAI instance securely over the internet.\r\n\r\nWith FlowiseAI running in Docker, you can easily manage and scale your AI workflows, while taking advantage of Docker's portability and isolation features. The Docker Compose setup also makes it easy to include additional services, such as a database backup solution, in your deployment.\r\n\r\nIf you're interested in exploring more Docker containers for your home server or self-hosted setup, including other AI and productivity tools, check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you build a powerful and versatile self-hosted environment.","src/content/posts/flowiseai-install.mdx",[1112],"../../assets/images/24/03/flowise-ai.jpg","1e8166b52bea3e08","flowiseai-install.mdx","fix-ssh-too-many-authentication-failures",{id:1115,data:1117,body:1126,filePath:1127,assetImports:1128,digest:1130,legacyId:1131,deferredRender:32},{title:1118,description:1119,date:1120,image:1121,authors:1122,categories:1123,tags:1124,canonical:1125},"Fix - SSH Too Many Authentication Failures","Fix Too Many Authentication Failures in SSH with our guide. Learn to activate SSH agent, use options, and modify settings",["Date","2023-08-03T06:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/08/too_many_authentication_failures.jpeg",[19],[98],[135],"https://www.bitdoze.com/fix-ssh-too-many-authentication-failures/","Let's face it, the world of servers and hosting can be as exciting as it is frustrating. Just when you think you have everything running smoothly, an error like \"Received disconnect from UNKNOWN port 65535:2: Too Many Authentication Failures\" pops up and you're left scratching your head. This usually happens for me because I need to switch between servers with different keys and SSH configs.\r\n\r\nSound familiar? Don't fret! We've all been there. And I'm here to walk you through a solution, that can help you fix the error, they helped me.\r\n\r\n## What's the Issue?\r\n\r\nThis particular error is often linked to SSH (Secure Shell) connections. SSH is a network protocol that allows users to manage their servers remotely, and it's a powerful tool that many of us rely on daily. However, it can occasionally throw a curveball like this one.\r\n\r\nThe error message essentially means that there have been too many failed attempts to authenticate the SSH connection. The server gets suspicious and shuts down the connection for security reasons. Makes sense, right? But what's the fix?\r\n\r\n## The Fixes\r\n\r\n### **1. Activate SSH Agent**\r\n\r\nThe first thing to do is to activate the SSH agent. This is like a manager for your SSH keys, and it can help streamline the authentication process. Just run:\r\n\r\n```bash\r\neval `ssh-agent`\r\n```\r\n\r\nThis command will initialize the SSH agent in the background, and you're good to go.\r\n\r\n### **2. Use the -o IdentitiesOnly=yes Option (if not using an SSH key)**\r\n\r\nIf you're not using an SSH key, then this option can be a lifesaver. It tells the SSH client to only use the authentication identity files that are configured in the SSH configuration files or passed on the command line. Run:\r\n\r\n```bash\r\nssh -o IdentitiesOnly=yes user@host\r\n```\r\n\r\nThis way, you're narrowing down the authentication methods, and that can clear up the issue.\r\n\r\n### **3. Increase MaxAuthTries in /etc/ssh/sshd_config**\r\n\r\nSometimes, the issue is that the server's threshold for failed attempts is just too low. You can fix this by increasing the MaxAuthTries value in the SSH daemon configuration file. Here's how:\r\n\r\n```bash\r\nsudo nano /etc/ssh/sshd_config\r\n```\r\n\r\nFind the line that says MaxAuthTries and increase the value. If it doesn't exist, add:\r\n\r\n```bash\r\nMaxAuthTries 10\r\n```\r\n\r\nDon't forget to restart the SSH service:\r\n\r\n```bash\r\nsudo systemctl restart sshd\r\n```\r\n\r\n## Conclusion\r\n\r\nThese steps should help you tackle the \"Received disconnect from UNKNOWN port 65535:2: Too Many Authentication Failures\" error.It's all about understanding what's happening under the hood and applying the right solution.\r\n\r\nRemember, servers can be quirky, but they don't have to be a mystery. Happy hosting!","src/content/posts/fix-ssh-too-many-authentication-failures.mdx",[1129],"../../assets/images/23/08/too_many_authentication_failures.jpeg","d01c20e8f0ebfaa8","fix-ssh-too-many-authentication-failures.mdx","ghostty-terminal",{id:1132,data:1134,body:1146,filePath:1147,assetImports:1148,digest:1150,legacyId:1151,deferredRender:32},{title:1135,description:1136,date:1137,image:1138,authors:1139,categories:1140,tags:1141,canonical:1145},"Ghostty Terminal: A Complete Setup Guide for Modern Mac Development","Learn how to install and configure Ghostty terminal on Mac, integrated with powerful tools like Powerlevel10k, zsh plugins, and tmux. Create a modern, GPU-accelerated terminal environment for enhanced development workflow..",["Date","2025-01-08T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/01/ghostty-yt.jpg",[19],[98],[1142,1143,1144],"zoxide","tmux","ghostty","https://www.bitdoze.com/ghostty-terminal/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[Ghostty](https://ghostty.org/) is a cutting-edge terminal emulator that's rapidly gaining popularity among developers and power users. Built with performance and customizability in mind, Ghostty offers a compelling alternative to other terminal emulators like Wezterm.\r\n\r\n## Key Features of Ghostty\r\n\r\n### GPU Acceleration\r\n\r\nGhostty leverages your computer's GPU for rendering, resulting in smooth scrolling and excellent performance, even with complex terminal outputs. This GPU acceleration ensures a responsive and fluid experience, especially when dealing with large amounts of text or running resource-intensive tasks.\r\n\r\n### Native UI\r\n\r\nUnlike some cross-platform terminals that use custom or web-based UIs, Ghostty focuses on a platform-native GUI. On Linux, it uses GTK4/libadwaita, while on macOS, it employs Swift, AppKit, and SwiftUI. This approach ensures that Ghostty feels integrated with your operating system, providing a seamless user experience.\r\n\r\n### Lua Configuration\r\n\r\nSimilar to Wezterm, Ghostty allows you to configure it using a simple key-value pair syntax in a configuration file. This flexibility enables users to customize their terminal environment to suit their specific needs and preferences.\r\n\r\n### Built-in Multiplexer\r\n\r\nGhostty includes built-in multiplexing capabilities, allowing you to manage multiple terminal sessions within a single window. This feature eliminates the need for external tools like tmux for many users.\r\n\r\n### Rich Color Support\r\n\r\nThe terminal supports 24-bit true color and offers a wide range of color schemes out of the box. With over 100 built-in themes and support for custom ones, you can easily personalize your terminal's appearance.\r\n\r\n### Image Support\r\n\r\nGhostty supports the Kitty Graphics Protocol, allowing you to display images directly in the terminal. This can be particularly useful for certain workflows or for adding visual elements to your terminal environment.\r\n\r\n## Performance\r\n\r\nGhostty is built using the Zig programming language, which contributes to its impressive speed. Recent benchmarks place Ghostty as one of the fastest terminals for reading and displaying large Unicode files. In a comparison of popular terminal emulators, Ghostty ranked second in speed, only slightly behind Alacritty:\r\n\r\n| Terminal | Version | Speed |\r\n|----------|---------|-------|\r\n| Ghostty  | 1 | 73ms  |\r\n| Alacritty| 0.13.1  | 66ms  |\r\n| WezTerm  | 20240203| 140ms |\r\n\r\n## Unique Features\r\n\r\n### Terminal Inspector\r\n\r\nOne of Ghostty's standout features is its Terminal Inspector. This real-time debugging tool allows you to peer into every detail of your terminal's activity, from keystrokes to render timings. It's an invaluable tool for understanding and optimizing your terminal usage.\r\n\r\n### Shaders\r\n\r\nGhostty supports custom shaders, allowing you to apply visual effects to your terminal. Whether you want a subtle glow or a retro CRT effect, shaders let you customize the visual atmosphere of your terminal environment.\r\n\r\n## Installation and Configuration\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/NR25BdXR6mE\"\r\n  label=\"Ghostty Terminal: A Complete Setup Guide for Modern Mac Development\"\r\n/>\r\n\r\n> If you are interested to see some free cool Mac Apps you can check [toolhunt.net mac apps section](https://toolhunt.net/mac/).\r\n\r\n\r\n\r\n### Ghostty Install\r\n\r\nYou can [download Ghostty](https://ghostty.org/download) as a package on your Mac or you can install it with homebrew with:\r\n\r\n```sh\r\nbrew install --cask ghostty\r\n```\r\n\r\n### Install Meslo Nerd Font\r\n\r\nNerd Fonts are a collection of fonts that have been patched with a high number of glyphs (icons). They're particularly useful for developers as they include many programming-related icons that can enhance your terminal and text editor experience.\r\n\r\nTo install the Meslo Nerd Font using Homebrew, run the following command:\r\n\r\n```sh\r\nbrew install font-meslo-lg-nerd-font\r\n```\r\n\r\nThis command does the following:\r\n1. It uses Homebrew to install the Meslo LG Nerd Font.\r\n2. The font will be downloaded and installed in your system's font directory.\r\n\r\nAfter running this command, the font should be available for use in Ghostty and other applications on your Mac.\r\n\r\nTo verify that the font was installed correctly, you can check your Font Book application or use it in an application that allows font selection.\r\n\r\nIt's worth noting that you may need to restart Ghostty or any other applications where you want to use this font after installation.\r\n\r\nThe Meslo Nerd Font is an excellent choice for terminal use as it's clear, easily readable, and includes a wide range of glyphs that can be useful in command-line interfaces and programming environments.\r\n\r\nIn the next section, we'll configure Ghostty to use this newly installed font.\r\n\r\n### Setup Ghostty Config File\r\n\r\nTo set up the Ghostty config file with the specified settings, follow these steps:\r\n\r\n1. Create or open the Ghostty configuration file:\r\n\r\n```bash\r\ntouch ~/.config/ghostty/config\r\nvim ~/.config/ghostty/config\r\n```\r\n\r\n2. Add the following configuration to the file:\r\n\r\n```\r\nfont-family = MesloLGS Nerd Font Mono\r\nfont-size = 19\r\nbackground-opacity = 0.9\r\ntheme = Argonaut\r\n```\r\n\r\nThis configuration does the following:\r\n\r\n- Sets the font family to MesloLGS Nerd Font Mono. To list the fonts that are available in ghostty you can run the bellow command:\r\n  ```sh\r\n  ghostty +list-fonts\r\n  ```\r\n- Changes the font size to 19\r\n- Sets the background opacity to 0.9 (90% opaque)\r\n- Applies the Argonaut color theme. To list the themes that are available in ghostty you can run the bellow command:\r\n  ```sh\r\n  ghostty +list-themes\r\n  ```\r\n\r\nAfter saving this configuration and restarting Ghostty, you should see the changes take effect. The terminal will now have a slightly transparent background, larger text, and the Argonaut color scheme.\r\n\r\nNote that Ghostty uses a simple key-value pair syntax for its configuration, making it easy to understand and modify. You can further customize your Ghostty environment by adding more options to this config file as needed.\r\n\r\n### Install powerlevel10k theme\r\n\r\nPowerlevel10k is a highly customizable theme for Zsh that emphasizes speed, flexibility, and out-of-the-box experience. It's designed to make your command line informative and visually appealing without sacrificing performance.\r\n\r\nTo install Powerlevel10k using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install powerlevel10k\r\necho \"source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\" >> ~/.zshrc\r\n```\r\n\r\nLet's break down what these commands do:\r\n\r\n1. The first command installs Powerlevel10k using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the Powerlevel10k theme. This ensures that the theme is loaded every time you start a new Zsh session.\r\n\r\nAfter running these commands, you need to apply the changes to your current session:\r\n\r\n```sh\r\nsource ~/.zshrc\r\n```\r\n\r\nThis command reloads your Zsh configuration, applying the Powerlevel10k theme.\r\n\r\nWhen you first run this, you'll likely be greeted with the Powerlevel10k configuration wizard. This interactive process allows you to customize various aspects of your prompt, including:\r\n\r\n- The style of the prompt\r\n- Which segments to display (git status, time, etc.)\r\n- Color scheme\r\n- Icons and glyphs\r\n\r\nFollow the on-screen instructions to set up Powerlevel10k according to your preferences. Don't worry if you're not sure about some options – you can always reconfigure later by running `p10k configure`.\r\n\r\nOnce configured, you'll have a highly informative and visually appealing prompt that can show git status, execution time of commands, and much more, all while maintaining excellent performance.\r\n\r\nRemember, you can always fine-tune your Powerlevel10k configuration by editing the `~/.p10k.zsh` file that was created during the configuration process.\r\n\r\n\r\n### Setup zsh-autosuggestions plugin\r\n\r\nZsh-autosuggestions is a powerful plugin that suggests commands as you type based on your command history and completions. It can significantly speed up your command-line workflow by reducing the amount of typing needed for frequently used commands.\r\n\r\nFor a more detailed guide on enabling command autocomplete in Zsh, check out this article: [Enable Command Autocomplete in Zsh](https://www.bitdoze.com/enable-command-autocomplete-in-zsh/)\r\n\r\nTo install zsh-autosuggestions using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install zsh-autosuggestions\r\necho \"source $(brew --prefix)/share/zsh-autosuggestions/zsh-autosuggestions.zsh\" >> ~/.zshrc\r\nsource ~/.zshrc\r\n```\r\n\r\nLet's break down these commands:\r\n\r\n1. The first command installs the zsh-autosuggestions plugin using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the zsh-autosuggestions plugin. This ensures that the plugin is loaded every time you start a new Zsh session.\r\n3. The third command reloads your Zsh configuration, applying the changes immediately.\r\n\r\nAfter running these commands, you should see autosuggestions appear as you type in your terminal. The suggestions will be shown in a faded gray color. To accept a suggestion, you can typically press the right arrow key or End key.\r\n\r\nYou can customize the behavior of zsh-autosuggestions by adding configuration options to your `.zshrc` file. For example, you can change the color of the suggestions or modify the key bindings used to accept suggestions.\r\n\r\nWith zsh-autosuggestions set up, you'll find that entering commands becomes faster and more efficient, especially for long or complex commands that you use frequently.\r\n\r\n### Setup zsh-syntax-highlighting\r\n\r\nZsh-syntax-highlighting is a plugin that provides syntax highlighting for the shell zsh. It enables highlighting of commands while they are typed at a zsh prompt into an interactive terminal. This helps in catching syntax errors, missing quotes, and other common mistakes before executing a command.\r\n\r\nFor a more comprehensive guide on enabling syntax highlighting in Zsh, check out this article: [Enable Syntax Highlighting in Zsh](https://www.bitdoze.com/enable-syntax-highlighting-zsh/)\r\n\r\nTo install zsh-syntax-highlighting using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install zsh-syntax-highlighting\r\necho \"source $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\" >> ~/.zshrc\r\nsource ~/.zshrc\r\n```\r\n\r\nLet's break down these commands:\r\n\r\n1. The first command installs the zsh-syntax-highlighting plugin using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the zsh-syntax-highlighting plugin. This ensures that the plugin is loaded every time you start a new Zsh session.\r\n3. The third command reloads your Zsh configuration, applying the changes immediately.\r\n\r\nAfter running these commands, you should see syntax highlighting in your terminal as you type commands. Typically, correctly typed commands will appear in green, while errors or unknown commands will appear in red.\r\n\r\nSome key features of zsh-syntax-highlighting include:\r\n\r\n- Command highlighting: Valid commands are highlighted differently from invalid ones.\r\n- Option highlighting: Valid options for commands are highlighted.\r\n- Path highlighting: Existing file paths are underlined.\r\n- Bracket matching: Brackets are highlighted in pairs.\r\n\r\nYou can customize the colors and styles used by zsh-syntax-highlighting by adding configuration options to your `.zshrc` file. This allows you to tailor the appearance to your preferences or to match your terminal's color scheme.\r\n\r\nWith zsh-syntax-highlighting set up, you'll find it easier to spot and correct mistakes in your commands before executing them, leading to a more efficient and error-free command-line experience.\r\n\r\n## Enhance Ghostty with tmux and zoxide\r\n\r\nWhile Ghostty is already a powerful terminal emulator, we can further enhance its capabilities by integrating it with tmux for advanced session management and zoxide for smarter directory navigation.\r\n\r\n### What is tmux and how can it help\r\n\r\nTmux (Terminal Multiplexer) is a powerful tool that allows you to create multiple terminal sessions within a single window. It's particularly useful for:\r\n\r\n- Managing multiple terminal sessions\r\n- Running long processes in the background\r\n- Sharing terminal sessions with other users\r\n- Preserving your terminal setup across system reboots\r\n\r\nTo learn more about tmux and its basic usage, check out this guide: [Tmux Basics](https://www.bitdoze.com/tmux-basics/)\r\n\r\nTo install tmux using Homebrew, run:\r\n\r\n```sh\r\nbrew install tmux\r\n```\r\n\r\nAfter installation, you can start a new tmux session by simply typing `tmux` in your terminal.\r\n\r\n### What is zoxide and how can it help\r\n\r\nZoxide is a smarter cd command that helps you navigate your filesystem more efficiently. It remembers which directories you use most frequently, so you can \"jump\" to them in just a few keystrokes.\r\n\r\nKey features of zoxide include:\r\n\r\n- Faster navigation to frequently-used directories\r\n- Fuzzy matching for directory names\r\n- Integration with common shells and file managers\r\n\r\nFor a detailed guide on using zoxide, refer to this article: [Zoxide Guide](https://www.bitdoze.com/zoxide/)\r\n\r\nTo install zoxide using Homebrew, run:\r\n\r\n```sh\r\nbrew install zoxide\r\n```\r\n\r\nThen, add the following line to your `~/.zshrc` file to initialize zoxide:\r\n\r\n```sh\r\neval \"$(zoxide init zsh)\"\r\n```\r\n\r\nAfter restarting your shell or running `source ~/.zshrc`, you can start using zoxide. For example, use `z` instead of `cd` to navigate to directories.\r\n\r\nBy integrating tmux and zoxide with Ghostty, you're creating a powerful, efficient terminal environment. Tmux allows you to manage complex workflows with multiple panes and windows, while zoxide speeds up your navigation between projects and directories. Together with Ghostty's GPU acceleration and customizability, you'll have a terminal setup that significantly boosts your productivity.\r\n\r\n## Known Ghostty Erros\r\n\r\n\r\n###  'xterm-ghostty': unknown terminal type\r\n\r\nWhen I try to run `top` on a bash Linux 6 environment I get the error: `'xterm-ghostty': unknown terminal type.` To fix that I just do in the session:\r\n\r\n```sh\r\nexport TERM=xterm-256color\r\n```\r\n\r\nYou can add that in your `.bashrc` if you have only a couple of servers I have thousands :)\r\n\r\n```sh\r\necho \"export TERM=xterm-256color\" >> ~/.bashrc\r\nsource ~/.bashrc\r\n```\r\n\r\n\r\n## Conclusion\r\n\r\nGhostty represents a new generation of terminal emulators, combining performance, customizability, and modern features. Its focus on native UI, GPU acceleration, and innovative tools like the Terminal Inspector set it apart from other options. While it's still in active development, Ghostty is already proving to be a powerful and flexible choice for users who demand more from their terminal emulator.\r\n\r\nWhether you're a developer, system administrator, or power user, Ghostty offers a compelling package that's worth exploring. As it continues to evolve, it may well become the go-to terminal for those seeking the perfect blend of speed, features, and customization.","src/content/posts/ghostty-terminal.mdx",[1149],"../../assets/images/25/01/ghostty-yt.jpg","40b0234d924bb504","ghostty-terminal.mdx","gatsby-js-online-courses",{id:1152,data:1154,body:1163,filePath:1164,assetImports:1165,digest:1167,legacyId:1168,deferredRender:32},{title:1155,description:1156,date:1157,image:1158,authors:1159,categories:1160,tags:1161,canonical:1162},"Best Gatsby.js Online Courses","Check out some of the best online courses that you can take for Gatsby JS",["Date","2022-09-20T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/Best_Gatsby_Courses.jpeg",[19],[21],[970,188],"https://www.bitdoze.com/gatsby-js-online-courses/","[Gatsby](https://www.gatsbyjs.com/)it's a react framework that can help you build headless websites. It's built on react and can quickly help you spin website.\r\nBitdoze.com is built on Gatsby and in this article, I would like to show you some Gatsby courses that can help you get started with Gatsby as your framework.\r\n\r\nGatsby is more complicated then WordPress and if you are new it can be scary at the beginning, that’s why a good course can help you understand what is all about and build your website with modern technologies.\r\n\r\nThere are some videos out there on YouTube that can get you started but they are basic and will not help you understand Gatsby if you are new. For beginners, the best would be to have a dedicated course done that will bring you thru all of the Gatsby and React functionality.\r\n\r\nAs a start to get you going with node.js and GitHub you can check: [Install Node.js using NVM](https://www.bitdoze.com/install-nodejs-using-nvm-macos-ubuntu/) and [Link GitHub with A SSH Key](https://www.bitdoze.com/link-github-with-ssh-maco-linux/)\r\n\r\n## Best Gatsby.js Online Courses for Beginners\r\n\r\nIn this part, we will see some of the courses that are out there and can help you better understand Gatsby, I am doing the first and the second one currently to better understand Gatsby.\r\n\r\n### [1. Gatsby.js Tutorial and Projects Course](https://go.bitdoze.com/gatsby-c1)\r\n\r\n![Gatsby.js Tutorial and Projects Course](//images.ctfassets.net/l6qg42gls3p1/5eI1DWFJekpwhOnIWiGG9M/497cbba83ba06fb8bcc9030d2cb97a27/gatsby_course_1.jpeg)\r\n\r\nThis is the first course I have taken for Gatsby and I can say that is very well explained and it brings you from all the Gatsby features.\r\nJohn Smilga the author of the course is explaining very well all the things you need to do to have a blog or website build with Gatsby.\r\n\r\nThe contentment has 22 hours and you have modules like building a recipes website with contentful or building a blog or portofolio website. With this course you have everything you need to start working with Gatsby.\r\n\r\nBesides the large content that you have what I like about the course is that John Smilga is explaining it very well and slow so everyone to understand all and be able to follow.\r\n\r\n** [Get The Course](https://go.bitdoze.com/gatsby-c1)\r\n**\r\n\r\n### [2. Gatsby JS: Build Gatsby static sites with React & WordPress](https://go.bitdoze.com/gatsby-c2)\r\n\r\n![Gatsby JS: Build Gatsby static sites with React & WordPress](//images.ctfassets.net/l6qg42gls3p1/5Kk8sbJrXbL6qc0qmw6kNV/037485bbaa4b72bc2e5f7ea5f02168d7/gatsby_course_2.jpeg)\r\n\r\nGatsby needs a CMS system to help you publish content faster. If you have a blog then WordPress would be to used as Headless CMS to publish the articles and assets.\r\n\r\nThis course will guide you thru how you can use WordPress as a Headless CMS and have the content in there and use Gatsby as a static site generator.\r\n\r\nThe course has about 3.5 hours and will guide you threw all you need to do to set everything up and build your blog on Gatsby for the front end.\r\n\r\n** [Get The Course](https://go.bitdoze.com/gatsby-c2)\r\n**\r\n\r\n### [3. Gatsby JS, Contentful & Gatsby Cloud (Gatsby JS V3 2022)](https://go.bitdoze.com/gatsby-c3)\r\n\r\n![Gatsby JS, Contentful & Gatsby Cloud (Gatsby JS V3 2022)](//images.ctfassets.net/l6qg42gls3p1/2ie2mU7TJP2VqyxZ79ttXW/b96bc7df8a30abf07fc887b7c8ff4c0a/gatsby_course_3.jpeg)\r\n\r\nIf you want to use contentful and Gatsby cloud to host your blog then this course will only focus on that.\r\n\r\nIn this course, you have 3.5 hours of videos that can help you build your blog on Contentful and host it on Gatsby Cloud.\r\n\r\nAll that you need to do to have a headless blog set up and styled you will have it in this tutorial.\r\n\r\n** [Get The Course](https://go.bitdoze.com/gatsby-c3)\r\n**\r\n\r\n### 4. Gatsby on Linkedin Learning\r\n\r\nIf you already have a subscription to Linkedin Learning you will find some Gatsby courses also there. There are 2 that I am recommanding and have everything you need:\r\n\r\n**[Learning Gatsby](https://www.linkedin.com/learning/learning-gatsby-14442452)**\r\n\r\nAbout 3 hours of video content, will help you start with gatsby and understand all is about. If you already have a subscription you will be happy with what it has to offer.\r\n\r\n**[Building a Headless WordPress Site with Gatsby](https://www.linkedin.com/learning/building-a-headless-wordpress-site-with-gatsby-15801541)**\r\nThis course has about 2 hours and 30 minutes. As Udemy course will help you set up a blog with WordPress as a headless CMS. If you have a Linked","src/content/posts/gatsby-js-online-courses.mdx",[1166],"../../assets/images/Best_Gatsby_Courses.jpeg","ac2249060592d016","gatsby-js-online-courses.mdx","groq-api-mistral-streamlit",{id:1169,data:1171,body:1182,filePath:1183,assetImports:1184,digest:1186,legacyId:1187,deferredRender:32},{title:1172,description:1173,date:1174,image:1175,authors:1176,categories:1177,tags:1178,canonical:1181},"How to Integrate FREE Groq API and Mistral LLM into Your Streamlit App","Learn How to Integrate Groq API and Mistral LLM into Your Streamlit App to ",["Date","2024-03-01T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/groq-api-mistral-streamlit.jpeg",[19],[98],[1179,1180],"groq","streamlit","https://www.bitdoze.com/groq-api-mistral-streamlit/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/02/groq-mistral-streamlit.png\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nThe Groq LPU™ Inference Engine is a specialized processing system developed by Groq to handle computationally intensive applications, particularly Large Language Models (LLMs). The LPU stands for Language Processing Unit™ and is designed to provide exceptional sequential performance, high accuracy, and instant memory access.\r\n\r\nIt aims to overcome bottlenecks in LLMs related to compute density and memory bandwidth, offering faster text generation compared to traditional processors like GPUs\r\n\r\n[Groq](https://groq.com/) allows you to use for free the LLMs : Llama-2 and Mistral, the latest one started to be known after Microsoft invested in them.\r\n\r\nGroq allows you to chat with both LLMs for FREE and see how fast things are. Lately, Groq released the API that will allow you to build apps on top of it and harvest the speed that Groq has.\r\n\r\nGroq has also a playground similar to OpenAI that will allow you to chat with the LLMs and add system roles.\r\n\r\n## Video with Groq API, Mistral and Streamlit\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/uika9hrAdro\"\r\n  label=\"Integrate FREE Groq API and Mistral LLM into Your Streamlit App\"\r\n/>\r\n\r\n## How To Use Groq API with Python\r\n\r\nGroq has a Python package that can be used, you just need to install it with:\r\n\r\n```python\r\npip install groq\r\n```\r\n\r\nAfter you can use it as below:\r\n\r\n**Imports**\r\n\r\n- **import os:** This imports the standard `os` library, which provides ways to interact with the operating system. It's used here to access environment variables.\r\n- **from groq import Groq:** Imports the necessary `Groq` class from the Groq Python library for interacting with the Groq API.\r\n\r\n**API Key Setup**\r\n\r\n```python\r\nclient = Groq(\r\n api_key=os.environ.get(\"GROQ_API_KEY\"),\r\n)\r\n```\r\n\r\n- **`api_key=os.environ.get(\"GROQ_API_KEY\")`**: This retrieves the Groq API key stored in an environment variable named \"GROQ_API_KEY\". It's a more secure way of managing the API key than including it directly in the code.\r\n- **`client = Groq(...)`**: This creates a Groq client object, which is your primary way of interacting with the Groq API.\r\n\r\n**Chat Completion Function**\r\n\r\n```python\r\ncompletion = client.chat.completions.create(\r\n model=\"mixtral-8x7b-32768\",\r\n messages=[ ... ],\r\n temperature=0.5,\r\n max_tokens=5640,\r\n top_p=1,\r\n stream=True,\r\n stop=None,\r\n)\r\n```\r\n\r\n- **client.chat.completions.create()**: This function call initiates a chat completion task using the Groq API. It's designed for generating conversational text responses.\r\n- **model=\"mixtral-8x7b-32768\"**: Selects a specific Groq language model for the task.\r\n- **messages=[...]**: A list defining the initial conversation structure:\r\n  - `\"role\": \"system\"`: Designates a system message.\r\n  - `\"content\": \"...\"`: Provides instructions that tell the model to act like a YouTube expert focused on generating titles for a given keyword\r\n- **temperature=0.5**: Controls randomness in output. Lower values lead to more predictable results.\r\n- **max_tokens=5640**: Sets a maximum limit on the number of tokens (roughly words or word parts) in the generated response.\r\n- **top_p=1**: Influences the sampling of tokens during generation.\r\n- **stream=True**: Requests that the output is streamed as it's generated, rather than providing the entire output all at once.\r\n- **stop=None**: No specific stop sequence is indicated.\r\n\r\n**Output Handling**\r\n\r\n```python\r\nfor chunk in completion:\r\n  print(chunk.choices[0].delta.content or \"\", end=\"\")\r\n```\r\n\r\n- **`for chunk in completion`**: Iterates over the streamed results from the chat completion task.\r\n- **`print(chunk.choices[0].delta.content or \"\", end=\"\")`**:\r\n  - Prints the content of the first suggested title (`chunk.choices[0]`).\r\n  - The `or \"\"` handles cases where there might not be content.\r\n  - `end=\"\"` prevents adding a newline after each title, keeping the output in a single line.\r\n\r\nAn example of complete code would be:\r\n\r\n```python\r\nimport os\r\nfrom groq import Groq\r\n\r\nclient = Groq(\r\n  api_key=os.environ.get(\"GROQ_API_KEY\"),\r\n)\r\ncompletion = client.chat.completions.create(\r\n  model=\"mixtral-8x7b-32768\",\r\n  messages=[\r\n    {\r\n      \"role\": \"system\",\r\n      \"content\": \"You are a YouTube expert creator who likes to write engaging titles for a keyword. \\nYou will provide 10 attention Grabbing Youtube titles on keywords specified by the user.\"\r\n    },\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"Install WordPress on Docker\"\r\n    }\r\n  ],\r\n  temperature=0.5,\r\n  max_tokens=5640,\r\n  top_p=1,\r\n  stream=True,\r\n  stop=None,\r\n)\r\n\r\nfor chunk in completion:\r\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\r\n```\r\n\r\nThe above will provide 10 YouTube video titles for the video `Install WordPress on Docker`. And if you run it you will see that results are provided very fast.\r\n\r\n## How To Use Groq API with Streamlit\r\n\r\nIf you want to add a UI to your Python code you can do that easily with Streamlit or any other [Python UI Library](https://www.bitdoze.com/best-python-web-frameworks/)\r\n\r\nFirst, thing will be to install Streamlit near Groq:\r\n\r\nTo do that for my previous example you will have a code like:\r\n\r\n```python\r\npip install streamlit\r\n```\r\n\r\nThen you will have the Python with Streamlit code:\r\n\r\n```python\r\nimport os\r\nimport streamlit as st\r\nfrom groq import Groq\r\n\r\n# Function to get Groq completions\r\ndef get_groq_completions(user_content):\r\n    client = Groq(\r\n        api_key=os.environ.get(\"GROQ_API_KEY\"),\r\n    )\r\n\r\n    completion = client.chat.completions.create(\r\n        model=\"mixtral-8x7b-32768\",\r\n        messages=[\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are a YouTube expert creator who likes to write engaging titles for a keyword. \\nYou will provide 10 attention-grabbing YouTube titles on keywords specified by the user.\"\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": user_content\r\n            }\r\n        ],\r\n        temperature=0.5,\r\n        max_tokens=5640,\r\n        top_p=1,\r\n        stream=True,\r\n        stop=None,\r\n    )\r\n\r\n    result = \"\"\r\n    for chunk in completion:\r\n        result += chunk.choices[0].delta.content or \"\"\r\n\r\n    return result\r\n\r\n# Streamlit interface\r\ndef main():\r\n    st.title(\"YouTube Title Generator\")\r\n    user_content = st.text_input(\"Enter the keyword for YouTube titles:\")\r\n\r\n    if st.button(\"Generate Titles\"):\r\n        if not user_content:\r\n                st.warning(\"Please enter a keyword before generating titles.\")\r\n                return\r\n        st.info(\"Generating titles... Please wait.\")\r\n        generated_titles = get_groq_completions(user_content)\r\n        st.success(\"Titles generated successfully!\")\r\n\r\n        # Display the generated titles\r\n        st.markdown(\"### Generated Titles:\")\r\n        st.text_area(\"\", value=generated_titles, height=200)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThe Streamlit-Mistral-Groq code was generated with the help of ChatGPT as I don't know that well either of the codes but is fully functional. You can use AI tools to help you build some things. At the end you can go and run the Streamlit App with\r\n\r\n```python\r\nstreamlit run app.py\r\n```\r\n\r\n<Picture src={img1} alt=\"Streamlit Groq App\" />\r\n\r\nWhen you are ready you can go and deploy the app for free in Streamlit Cloud or on your own VPS with: [Deploy Streamlit on a VPS and Proxy to Cloudflare Tunnels](https://www.bitdoze.com/streamlit-deploy-vps-cloudflare/)\r\n\r\n## Conclusions\r\n\r\nGroq can help you build very fast AI applications because of their faster LPU and Mistral is an LLM that is not for behind ChatGPT 4, you can use both FREE and see if they are useful to you.\r\n\r\nI hope this article motivated you to start and see what both have to offer.","src/content/posts/groq-api-mistral-streamlit.mdx",[1185],"../../assets/images/24/02/groq-api-mistral-streamlit.jpeg","42c4a64e4b5a4cd7","groq-api-mistral-streamlit.mdx","install-cloudpanel-host-nodejs",{id:1188,data:1190,body:1200,filePath:1201,assetImports:1202,digest:1204,legacyId:1205,deferredRender:32},{title:1191,description:1192,date:1193,image:1194,authors:1195,categories:1196,tags:1197,canonical:1199},"How To Install CloudPanel and Host Node.js Apps","Learn how to install CloudPanel and host Node.js apps in minutes! Follow our easy step-by-step guide for a quick set up with no coding experience required.",["Date","2023-02-02T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/instal-cloudpanel-host-nodejs.jpeg",[19],[98],[501,1198,24],"node","https://www.bitdoze.com/install-cloudpanel-host-nodejs/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport sanity from \"../../assets/images/23/sanity_node.jpeg\";\r\n\r\n[CloudPanel](https://www.cloudpanel.io/) is a free hosting panel that can be installed on VPS servers and can help with hosting websites including node.js applications.\r\n\r\nCloudpanel is a simple and nice hosting panel that runs on top of Nginx and MySql or Maria DB and can help you run various applications from PHP ones to Python or node.js.\r\n\r\n**Below is a list of what CloudPanel has to offer:**\r\n\r\n- File Manager\r\n- IP & Bot Blocking\r\n- Varnish Cache & Redis\r\n- SSH/FTP\r\n- Firewall\r\n- Cron Jobs\r\n- Vhost Editor\r\n- Remote Backup with Rclone\r\n- Free Let’s Encrypt Certificates\r\n- Cloudflare Integration\r\n- User Management\r\n- System resources usage graphs\r\n- Multiple PHP versions\r\n- MySQL and MariaDB support\r\n- Node.js, Python support\r\n- Nginx Web Server\r\n\r\n**Minimum requirements:**\r\n\r\n- Minimum 1 core\r\n- Minimum 2 GB RAM\r\n- 10 GB of disk space\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nCloudPanel has direct cloud integrations with [DigitalOcean](https://go.bitdoze.com/do), [Vultr](https://go.bitdoze.com/vultr), [Hetzner](https://go.bitdoze.com/hetzner), AWS or Google Cloud. These integrations can help you create snapshots and enable automatic snapshot creations from CloudPanel UI.\r\n\r\nIn this article and video I will use Hetzner to create the VPS and Ubuntu 22.04 to host my node.js application, I have made a review and benchmark off [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/) so you can check that to see the exact differences.\r\n\r\nIf you are interested on how you can monitor your CPU and have an automatic email sent when load is too big you should check: [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)\r\n\r\n## Video With CloudPanel Install and Node.js Configs\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/5KndMFz-VKQ\"\r\n  label=\"How To Install CloudPanel and Host Node.js Apps\"\r\n/>\r\n\r\nIn case you are interested to have a web panel that can help you manage your applications and be used as a reverse proxy you can check the bellow course:\r\n\r\n<Button\r\n  link=\"https://webdoze.net/courses/cloudpanel-setup/\"\r\n  text=\"CloudPanel Setup Course\"\r\n/>\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n> You can also check [Setup CloudPanel with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers and [CloudPanel Remote Backups](https://www.bitdoze.com/cloudpanel-remote-backups/).\r\n\r\n## 1. How To Install Cloudpanel\r\n\r\nAfter you have your VPS created you need to SSH to it and run the below command:\r\n\r\n### 1.1 Update the OS:\r\n\r\n```bash\r\napt update && apt -y upgrade && apt -y install curl wget sudo\r\n```\r\n\r\n### 1.2 Install CloudPanel:\r\n\r\nI use MariaDB 10.9 for this install and Hetzner as a cloud provider you can check [Cloudpanel Install Doc](https://www.cloudpanel.io/docs/v2/getting-started/hetzner-cloud/installation/installer/) to check the options:\r\n\r\n```bash\r\ncurl -sS https://installer.cloudpanel.io/ce/v2/install.sh -o install.sh; \\\r\necho \"2d3812327d8229c372f599156515c4639d18badd5c6a972616affbf86960c24f  install.sh\" | \\\r\nsha256sum -c && sudo CLOUD=hetzner DB_ENGINE=MARIADB_10.9 bash install.sh\r\n```\r\n\r\n### 1.3 Access CloudPanel Admin\r\n\r\nYou can access the CloudPanel admin with the **https://serverIpAddress:8443** but in the next steps, we are going to go and create a subdomain and use it to access CloudPanel in the future.\r\n\r\n### 1.4 Create an Admin Subdomain:\r\n\r\nTo access CloudPanel securely and have an SSL certificate we need to create a subdomain and point it to the CloudPanel VPS server. I am using CloudFlare for this and I just go and add an A record under DNS on the domain I want to host.\r\n\r\nNext, we need to go and add it to the CloudPanel Admin area in Settings. And add it there to secure the admin area.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n## 2. Run Your Node.js App on CloudPanel\r\n\r\nNext, we will deploy a Node.js app that we are going to host with CloudPanel. To do this we are going to add the application to CloudPanel.\r\n\r\n### 2.1 Add a Node.JS Webiste in CloudPanel\r\n\r\nYou just need to click add a site and add your domain with node.js version with username and passwords as in the picture below:\r\n\r\n<Picture\r\n  src={sanity}\r\n  alt=\"CloudPanel Add Sanity\"\r\n/>\r\n\r\n### 2.2 Point the Domain to CloudPanel\r\n\r\nI will use CloudFlare for this and I will add an A record to my domain or subdomain that will point to my VPS server. I will also enable CloudFlare on it to benefit from the speed improvements.\r\n\r\nFrom the SSL/TLS tab in CloudPanel, I will generate a Let's Encrypt certificate so the website works over HTTPS.\r\n\r\n### 2.3 Generate the SSL certificate\r\n\r\nNext, you need to go under SSL/TLS and generate an SSL Let's Encrypt certificate for your domain.\r\n\r\n### 2.4 Install Strapi to CloudPanel\r\n\r\n```bash\r\n sudo su - <username created when website is added>\r\n cd htdocs && rm -rf www.domain.com\r\n npx create-strapi-app@latest www.domain.com\r\n```\r\n\r\nThen you will be asked to configure the admin panel you will be able to do that at https://www.domain.com/admin\r\n\r\n### 2.5 Build Strapi Admin Panel\r\n\r\n```bash\r\ncd htdocs/www.domain.com/\r\nNODE_ENV=production npm run build\r\n```\r\n\r\n### 2.6 Run Strapi\r\n\r\n```bash\r\nNODE_ENV=production npm start\r\n\r\n> strapi start\r\n\r\n\r\n Project information\r\n\r\n┌────────────────────┬──────────────────────────────────────────────────┐\r\n│ Time               │ Thu Feb 02 2023 10:04:46 GMT+0000 (Coordinated … │\r\n│ Launched in        │ 1047 ms                                          │\r\n│ Environment        │ production                                       │\r\n│ Process PID        │ 121469                                           │\r\n│ Version            │ 4.6.0 (node v14.21.2)                            │\r\n│ Edition            │ Community                                        │\r\n│ Database           │ sqlite                                           │\r\n└────────────────────┴──────────────────────────────────────────────────┘\r\n\r\n Actions available\r\n\r\nOne more thing...\r\nCreate your first administrator 💻 by going to the administration panel at:\r\n\r\n┌───────────────────────────┐\r\n│ http://0.0.0.0:1337/admin │\r\n└───────────────────────────┘\r\n\r\n```\r\n\r\n### 2.7 Create The admin User\r\n\r\nNow you just need to visit the website you chose with Admin like: https://www.domain.com/admin\r\n\r\nHere just add your details with the password\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n## 3.Enable Auto Start With PM2\r\n\r\nThe node.js app will not start by itself in case the server reboots, so if you don't want to go and manually start it we need to enable pm2 to auto-restart it.\r\n\r\n### 3.1 Install the latest pm2 via npm\r\n\r\nFor a complete guide on PM2, check [Manage Applications with PM2](https://www.bitdoze.com/pm2-manage-apps/)\r\n\r\n```bash\r\nnpm install pm2@latest -g\r\n```\r\n\r\n### 3.2 Start The App and save the config\r\n\r\n```bash\r\npm2 start npm --name strapi-app -- start\r\npm2 save\r\n```\r\n\r\n### 3.3 Add Cron To Start Strapi On Reboot\r\n\r\nFind the current path:\r\n\r\n```bash\r\necho $PATH\r\n```\r\n\r\nEdit the user crontab and add the action:\r\n\r\n```bash\r\ncrontab -e\r\nPATH=<output of path>\r\n@reboot pm2 resurrect &> /dev/null\r\n\r\neg:\r\nPATH=/home/bitdoze-strapi/.nvm/versions/node/v14.21.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\r\n@reboot pm2 resurrect &> /dev/null\r\n```\r\n\r\nYou can check with crontab -l to see if crontab for the user was added ok.\r\nAfter you can reboot the server and see if the process is up with pm2 status\r\n\r\n```bash\r\nbitdoze-strapi@ubuntu-2gb-hil-2:~$ pm2 status\r\n┌─────┬───────────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐\r\n│ id  │ name          │ namespace   │ version │ mode    │ pid      │ uptime │ ↺    │ status    │ cpu      │ mem      │ user     │ watching │\r\n├─────┼───────────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤\r\n│ 0   │ strapi-app    │ default     │ 0.39.1  │ fork    │ 1521     │ 50s    │ 0    │ online    │ 0%       │ 56.7mb   │ bit… │ disabled │\r\n└─────┴───────────────┴─────────────┴─────────┴─────────┴──────────┴────────┴──────┴───────────┴──────────┴──────────┴──────────┴──────────┘\r\n```\r\n\r\n## Conlcusions\r\n\r\nThis is how you can easily have CloudPanel installed on any VPS and you can host your node.js apps or other types of apps you want. I have been using CloudPanel for more than a year now and I can say it didn't disappointed with anything till now.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />","src/content/posts/install-cloudpanel-host-nodejs.mdx",[1203],"../../assets/images/23/instal-cloudpanel-host-nodejs.jpeg","5d9fd6dd6a103310","install-cloudpanel-host-nodejs.mdx","git-commands",{id:1206,data:1208,body:1218,filePath:1219,assetImports:1220,digest:1222,legacyId:1223,deferredRender:32},{title:1209,description:1210,date:1211,image:1212,authors:1213,categories:1214,tags:1215,canonical:1217},"Top 100+ GIT Commands You MUST Know","Discover the top 100+ GIT commands every developer must know to streamline version control and boost productivity. ",["Date","2024-07-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/git-commands.jpeg",[19],[98],[1216],"git","https://www.bitdoze.com/git-commands/","Git, a distributed version control system, has become an indispensable tool for developers and DevOps professionals worldwide. Its ability to track changes, manage code versions, and facilitate collaboration on projects makes it a cornerstone of modern software development. Whether you're working on a solo project or contributing to a large team, mastering Git commands is essential for efficient and effective version control.\r\n\r\nIn this article, we will cover 100+ essential Git commands that every developer should know. These commands range from basic operations to advanced techniques, providing a comprehensive guide to navigating and utilizing Git. Each command will be explained with examples to help you understand its usage and practical applications.\r\n\r\n## Section 1: Basic Git Commands\r\n\r\n### 1.1 git init\r\n\r\nThe `git init` command is used to initialize a new Git repository. This command sets up all the necessary files and directories that Git needs to track changes in your project.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ mkdir my_project\r\n$ cd my_project\r\n$ git init\r\nInitialized empty Git repository in /path/to/my_project/.git/\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `mkdir my_project`: Creates a new directory named `my_project`.\r\n- `cd my_project`: Changes the current directory to `my_project`.\r\n- `git init`: Initializes a new Git repository in the current directory. After running this command, a `.git` directory is created, which contains all the metadata and object database for the repository.\r\n\r\n### 1.2 git clone\r\n\r\nThe `git clone` command is used to create a copy of an existing repository. This is particularly useful when you want to contribute to a project or use an existing project as a starting point.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git clone https://github.com/user/repository.git\r\nCloning into 'repository'...\r\nremote: Enumerating objects: 10, done.\r\nremote: Counting objects: 100% (10/10), done.\r\nremote: Compressing objects: 100% (8/8), done.\r\nremote: Total 10 (delta 2), reused 10 (delta 2), pack-reused 0\r\nUnpacking objects: 100% (10/10), done.\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git clone https://github.com/user/repository.git`: Clones the repository located at the specified URL into a new directory named `repository`. The command downloads all the files, branches, and commits from the remote repository.\r\n\r\n### 1.3 git status\r\n\r\nThe `git status` command shows the current state of the working directory and the staging area. It lets you see which changes have been staged, which haven't, and which files aren't being tracked by Git.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git status\r\nOn branch main\r\nYour branch is up to date with 'origin/main'.\r\n\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n    newfile.txt\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git status`: Displays the status of the working directory and staging area. In this example, it shows that there is an untracked file named `newfile.txt`.\r\n\r\n### 1.4 git add\r\n\r\nThe `git add` command adds file contents to the staging area. This command is used to prepare changes for the next commit.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git add newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git add newfile.txt`: Adds the file `newfile.txt` to the staging area. This means that the changes in `newfile.txt` will be included in the next commit.\r\n\r\n### 1.5 git commit\r\n\r\nThe `git commit` command records changes to the repository. It takes a snapshot of the changes in the staging area and saves them in the repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git commit -m \"Add newfile.txt\"\r\n[main 1a2b3c4] Add newfile.txt\r\n 1 file changed, 1 insertion(+)\r\n create mode 100644 newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git commit -m \"Add newfile.txt\"`: Creates a new commit with the message \"Add newfile.txt\". The commit includes the changes that were staged with `git add`.\r\n\r\n### 1.6 git config\r\n\r\nThe `git config` command is used to configure Git settings. It can set user information, such as name and email, and other preferences.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git config --global user.name \"John Doe\"\r\n$ git config --global user.email \"johndoe@example.com\"\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git config --global user.name \"John Doe\"`: Sets the global Git user name to \"John Doe\".\r\n- `git config --global user.email \"johndoe@example.com\"`: Sets the global Git user email to \"johndoe@example.com\".\r\n\r\nBy mastering these basic Git commands, you can start managing your projects more effectively. These commands form the foundation of Git usage, enabling you to initialize repositories, track changes, and commit updates. In the next section, we will explore branching and merging, which are crucial for managing different versions of your code.\r\n\r\n## Section 2: Branching and Merging\r\n\r\nBranching and merging are fundamental concepts in Git that allow developers to work on different features or fixes simultaneously without interfering with the main codebase. This section will cover essential commands for managing branches and merging changes.\r\n\r\n### 2.1 git branch\r\n\r\nThe `git branch` command is used to list, create, or delete branches. Branches are pointers to specific commits, allowing you to isolate your work.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# List all branches\r\n$ git branch\r\n* main\r\n  feature-branch\r\n\r\n# Create a new branch\r\n$ git branch new-feature\r\n\r\n# Delete a branch\r\n$ git branch -d feature-branch\r\nDeleted branch feature-branch (was 1a2b3c4).\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git branch`: Lists all the branches in the repository. The `*` indicates the current branch.\r\n- `git branch new-feature`: Creates a new branch named `new-feature`.\r\n- `git branch -d feature-branch`: Deletes the branch named `feature-branch`.\r\n\r\n### 2.2 git checkout\r\n\r\nThe `git checkout` command is used to switch branches or restore working tree files. This command changes the current branch to the specified branch.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Switch to an existing branch\r\n$ git checkout new-feature\r\nSwitched to branch 'new-feature'\r\n\r\n# Create and switch to a new branch\r\n$ git checkout -b another-feature\r\nSwitched to a new branch 'another-feature'\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git checkout new-feature`: Switches the current branch to `new-feature`.\r\n- `git checkout -b another-feature`: Creates a new branch named `another-feature` and switches to it.\r\n\r\n### 2.3 git merge\r\n\r\nThe `git merge` command is used to merge changes from one branch into another. This is typically done to integrate changes from a feature branch into the main branch.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Switch to the main branch\r\n$ git checkout main\r\nSwitched to branch 'main'\r\n\r\n# Merge changes from 'new-feature' into 'main'\r\n$ git merge new-feature\r\nUpdating 1a2b3c4..5d6e7f8\r\nFast-forward\r\n newfile.txt | 1 +\r\n 1 file changed, 1 insertion(+)\r\n create mode 100644 newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git checkout main`: Switches to the `main` branch.\r\n- `git merge new-feature`: Merges the changes from the `new-feature` branch into the `main` branch. The output shows the details of the merge, including the commit range and the files affected.\r\n\r\n### 2.4 git rebase\r\n\r\nThe `git rebase` command is used to reapply commits on top of another base tip. This command is useful for maintaining a linear project history.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Switch to the feature branch\r\n$ git checkout feature-branch\r\nSwitched to branch 'feature-branch'\r\n\r\n# Rebase onto the main branch\r\n$ git rebase main\r\nFirst, rewinding head to replay your work on top of it...\r\nApplying: Add new feature\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git checkout feature-branch`: Switches to the `feature-branch`.\r\n- `git rebase main`: Reapplies the commits from `feature-branch` on top of the `main` branch. This process rewinds the commits in `feature-branch`, applies the changes from `main`, and then replays the commits from `feature-branch`.\r\n\r\n### 2.5 git branch -d\r\n\r\nThe `git branch -d` command is used to delete a branch. This command ensures that the branch being deleted has been fully merged before deletion.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Delete a fully merged branch\r\n$ git branch -d feature-branch\r\nDeleted branch feature-branch (was 1a2b3c4).\r\n\r\n# Force delete an unmerged branch\r\n$ git branch -D unmerged-branch\r\nDeleted branch unmerged-branch (was 5d6e7f8).\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git branch -d feature-branch`: Deletes the `feature-branch` if it has been fully merged.\r\n- `git branch -D unmerged-branch`: Force deletes the `unmerged-branch` even if it hasn't been merged.\r\n\r\nBy understanding and utilizing these branching and merging commands, you can effectively manage different versions of your code and collaborate with other developers. These commands provide the flexibility to work on multiple features simultaneously and integrate changes seamlessly.\r\n\r\n## Section 3: Remote Repositories\r\n\r\nRemote repositories are versions of your project that are hosted on the internet or another network. They allow you to collaborate with other developers, share your code, and keep your work synchronized across multiple locations. This section covers essential commands for managing remote repositories.\r\n\r\n### 3.1 git remote\r\n\r\nThe `git remote` command is used to manage the set of tracked repositories. It allows you to add, remove, and view remote repositories.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# List all remote repositories\r\n$ git remote -v\r\norigin  https://github.com/user/repository.git (fetch)\r\norigin  https://github.com/user/repository.git (push)\r\n\r\n# Add a new remote repository\r\n$ git remote add upstream https://github.com/another-user/repository.git\r\n\r\n# Remove a remote repository\r\n$ git remote remove upstream\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git remote -v`: Lists all the remote repositories along with their URLs. The output shows the remote name (`origin`) and the URL for fetching and pushing.\r\n- `git remote add upstream https://github.com/another-user/repository.git`: Adds a new remote repository named `upstream` with the specified URL.\r\n- `git remote remove upstream`: Removes the remote repository named `upstream`.\r\n\r\n### 3.2 git fetch\r\n\r\nThe `git fetch` command downloads objects and refs from another repository. It updates your local copy with the latest changes from the remote repository without merging them into your working directory.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git fetch origin\r\nremote: Enumerating objects: 5, done.\r\nremote: Counting objects: 100% (5/5), done.\r\nremote: Compressing objects: 100% (3/3), done.\r\nremote: Total 5 (delta 2), reused 5 (delta 2), pack-reused 0\r\nUnpacking objects: 100% (5/5), done.\r\nFrom https://github.com/user/repository\r\n   1a2b3c4..5d6e7f8  main       -> origin/main\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git fetch origin`: Fetches the latest changes from the remote repository named `origin`. The output shows the objects that were downloaded and the branches that were updated.\r\n\r\n### 3.3 git pull\r\n\r\nThe `git pull` command fetches from and integrates with another repository or a local branch. It is a combination of `git fetch` and `git merge`, allowing you to update your local branch with the latest changes from the remote repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git pull origin main\r\nremote: Enumerating objects: 5, done.\r\nremote: Counting objects: 100% (5/5), done.\r\nremote: Compressing objects: 100% (3/3), done.\r\nremote: Total 5 (delta 2), reused 5 (delta 2), pack-reused 0\r\nUnpacking objects: 100% (5/5), done.\r\nFrom https://github.com/user/repository\r\n * branch            main       -> FETCH_HEAD\r\nUpdating 1a2b3c4..5d6e7f8\r\nFast-forward\r\n newfile.txt | 1 +\r\n 1 file changed, 1 insertion(+)\r\n create mode 100644 newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git pull origin main`: Fetches the latest changes from the `main` branch of the remote repository named `origin` and merges them into the current branch. The output shows the objects that were downloaded and the details of the merge.\r\n\r\n### 3.4 git push\r\n\r\nThe `git push` command updates remote refs along with associated objects. It is used to upload your local changes to a remote repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git push origin main\r\nEnumerating objects: 5, done.\r\nCounting objects: 100% (5/5), done.\r\nDelta compression using up to 4 threads\r\nCompressing objects: 100% (3/3), done.\r\nWriting objects: 100% (3/3), 300 bytes | 300.00 KiB/s, done.\r\nTotal 3 (delta 2), reused 0 (delta 0)\r\nTo https://github.com/user/repository.git\r\n   1a2b3c4..5d6e7f8  main -> main\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git push origin main`: Pushes the changes from the local `main` branch to the `main` branch of the remote repository named `origin`. The output shows the objects that were uploaded and the branches that were updated.\r\n\r\n### 3.5 git remote -v\r\n\r\nThe `git remote -v` command shows the URLs of the remote repositories. It is useful for verifying the remote repositories that are configured for your project.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git remote -v\r\norigin  https://github.com/user/repository.git (fetch)\r\norigin  https://github.com/user/repository.git (push)\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git remote -v`: Lists all the remote repositories along with their URLs for fetching and pushing.\r\n\r\nBy mastering these commands for managing remote repositories, you can efficiently collaborate with other developers and keep your work synchronized across multiple locations. These commands allow you to fetch updates, push your changes, and manage remote repositories with ease.\r\n\r\n## Section 4: Viewing and Exploring History\r\n\r\nUnderstanding the history of your project is crucial for tracking changes, debugging issues, and collaborating with team members. Git provides several commands to view and explore the commit history, which can help you gain insights into the evolution of your codebase.\r\n\r\n### 4.1 git log\r\n\r\nThe `git log` command shows the commit logs, providing a detailed history of changes made to the repository. It displays the commit hash, author, date, and commit message.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git log\r\ncommit 5d6e7f8 (HEAD -> main, origin/main)\r\nAuthor: John Doe <johndoe@example.com>\r\nDate:   Mon Oct 18 14:00:00 2021 -0400\r\n\r\n    Add new feature\r\n\r\ncommit 1a2b3c4\r\nAuthor: Jane Smith <janesmith@example.com>\r\nDate:   Sun Oct 17 10:00:00 2021 -0400\r\n\r\n    Fix bug in feature\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git log`: Displays the commit history in reverse chronological order. Each commit includes the commit hash, author, date, and message.\r\n\r\n### 4.2 git diff\r\n\r\nThe `git diff` command shows the differences between commits, the working directory, and the staging area. It highlights the changes made to files, making it easier to review modifications.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Show changes between working directory and staging area\r\n$ git diff\r\n\r\n# Show changes between two commits\r\n$ git diff 1a2b3c4 5d6e7f8\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git diff`: Displays the changes between the working directory and the staging area.\r\n- `git diff 1a2b3c4 5d6e7f8`: Shows the differences between the commits with hashes `1a2b3c4` and `5d6e7f8`.\r\n\r\n### 4.3 git show\r\n\r\nThe `git show` command displays various types of objects, including commits, trees, and blobs. It is commonly used to view the details of a specific commit.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git show 5d6e7f8\r\ncommit 5d6e7f8 (HEAD -> main, origin/main)\r\nAuthor: John Doe <johndoe@example.com>\r\nDate:   Mon Oct 18 14:00:00 2021 -0400\r\n\r\n    Add new feature\r\n\r\ndiff --git a/newfile.txt b/newfile.txt\r\nnew file mode 100644\r\nindex 0000000..e69de29\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git show 5d6e7f8`: Displays the details of the commit with hash `5d6e7f8`, including the commit message and the changes made to files.\r\n\r\n### 4.4 git blame\r\n\r\nThe `git blame` command shows what revision and author last modified each line of a file. This command is useful for identifying the origin of changes and understanding the history of specific lines in a file.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git blame newfile.txt\r\n5d6e7f8e (John Doe 2021-10-18 14:00:00 -0400 1) Line 1 of newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git blame newfile.txt`: Displays the commit hash, author, date, and content of each line in `newfile.txt`. This information helps track the origin of changes.\r\n\r\n### 4.5 git shortlog\r\n\r\nThe `git shortlog` command summarizes the `git log` output, grouping commits by author and providing a count of commits per author. It is useful for generating a summary of contributions.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git shortlog -sn\r\n    10  John Doe\r\n     5  Jane Smith\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git shortlog -sn`: Displays a summary of commits, grouped by author, with the number of commits made by each author. The `-s` option shows the commit count, and the `-n` option sorts the output by the number of commits.\r\n\r\nBy mastering these commands for viewing and exploring history, you can gain valuable insights into your project's development and make informed decisions based on the commit history. These commands help you track changes, understand the evolution of your codebase, and collaborate more effectively with your team.\r\n\r\n## Section 5: Undoing Changes\r\n\r\nIn the course of development, it's common to make mistakes or need to revert changes. Git provides several commands to undo changes, whether you need to reset your working directory, revert commits, or clean up untracked files. This section covers essential commands for undoing changes in your repository.\r\n\r\n### 5.1 git reset\r\n\r\nThe `git reset` command is used to reset the current HEAD to a specified state. It can modify the staging area and working directory to match the specified commit.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Soft reset: move HEAD to the specified commit, but leave the changes in the working directory and staging area\r\n$ git reset --soft HEAD~1\r\n\r\n# Mixed reset: move HEAD to the specified commit and update the staging area, but leave the working directory unchanged\r\n$ git reset --mixed HEAD~1\r\n\r\n# Hard reset: move HEAD to the specified commit and update both the staging area and working directory\r\n$ git reset --hard HEAD~1\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git reset --soft HEAD~1`: Moves HEAD to the previous commit (`HEAD~1`), but keeps the changes in the working directory and staging area.\r\n- `git reset --mixed HEAD~1`: Moves HEAD to the previous commit and updates the staging area to match, but leaves the working directory unchanged.\r\n- `git reset --hard HEAD~1`: Moves HEAD to the previous commit and updates both the staging area and working directory to match, effectively discarding any changes.\r\n\r\n### 5.2 git revert\r\n\r\nThe `git revert` command creates a new commit that undoes the changes introduced by a previous commit. This is useful for safely undoing changes without altering the commit history.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git revert 5d6e7f8\r\n[main 9a8b7c6] Revert \"Add new feature\"\r\n 1 file changed, 1 deletion(-)\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git revert 5d6e7f8`: Creates a new commit that undoes the changes introduced by the commit with hash `5d6e7f8`. The output shows the details of the revert commit.\r\n\r\n### 5.3 git clean\r\n\r\nThe `git clean` command removes untracked files from the working directory. This is useful for cleaning up files that are not tracked by Git and not needed in the repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Show which files would be removed\r\n$ git clean -n\r\nWould remove untracked_file.txt\r\n\r\n# Remove untracked files\r\n$ git clean -f\r\nRemoving untracked_file.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git clean -n`: Displays which untracked files would be removed without actually removing them.\r\n- `git clean -f`: Removes untracked files from the working directory.\r\n\r\n### 5.4 git stash\r\n\r\nThe `git stash` command temporarily saves changes in a dirty working directory without committing them. This is useful for quickly switching branches or pulling updates without losing your current work.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Stash changes\r\n$ git stash\r\nSaved working directory and index state WIP on main: 1a2b3c4 Add new feature\r\n\r\n# List stashed changes\r\n$ git stash list\r\nstash@{0}: WIP on main: 1a2b3c4 Add new feature\r\n\r\n# Apply stashed changes\r\n$ git stash apply\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git stash`: Saves the current changes in the working directory and staging area to a new stash.\r\n- `git stash list`: Lists all stashed changes.\r\n- `git stash apply`: Applies the most recent stash to the working directory.\r\n\r\n### 5.5 git cherry-pick\r\n\r\nThe `git cherry-pick` command applies the changes introduced by an existing commit to the current branch. This is useful for selectively applying specific commits.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git cherry-pick 5d6e7f8\r\n[main 9a8b7c6] Add new feature\r\n 1 file changed, 1 insertion(+)\r\n create mode 100644 newfile.txt\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git cherry-pick 5d6e7f8`: Applies the changes introduced by the commit with hash `5d6e7f8` to the current branch. The output shows the details of the cherry-picked commit.\r\n\r\nBy mastering these commands for undoing changes, you can effectively manage and correct mistakes in your repository. These commands provide the flexibility to reset, revert, clean, stash, and cherry-pick changes, ensuring that your project remains in a stable and consistent state.\r\n\r\n## Section 6: Advanced Git Commands\r\n\r\nAdvanced Git commands provide powerful tools for more complex tasks such as debugging, managing submodules, and creating archives. Mastering these commands can significantly enhance your ability to manage large projects and troubleshoot issues efficiently.\r\n\r\n### 6.1 git bisect\r\n\r\nThe `git bisect` command uses a binary search algorithm to find the commit that introduced a bug. This command is invaluable for debugging and pinpointing the exact commit where things went wrong.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Start the bisect process\r\n$ git bisect start\r\n\r\n# Mark the current commit as bad\r\n$ git bisect bad\r\n\r\n# Mark an earlier commit as good\r\n$ git bisect good 1a2b3c4\r\n\r\n# Git will now check out a commit halfway between the good and bad commits\r\n# Test the code and mark the commit as bad or good\r\n$ git bisect bad\r\n\r\n# Repeat the process until the offending commit is found\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git bisect start`: Starts the bisect process.\r\n- `git bisect bad`: Marks the current commit as bad.\r\n- `git bisect good 1a2b3c4`: Marks the commit with hash `1a2b3c4` as good.\r\n- Git will then check out a commit halfway between the good and bad commits. You need to test the code and mark the commit as bad or good until the offending commit is found.\r\n\r\n### 6.2 git tag\r\n\r\nThe `git tag` command is used to create, list, delete, or verify a tag object signed with GPG. Tags are useful for marking specific points in the project’s history, such as releases.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Create a lightweight tag\r\n$ git tag v1.0\r\n\r\n# Create an annotated tag\r\n$ git tag -a v1.0 -m \"Version 1.0 release\"\r\n\r\n# List all tags\r\n$ git tag\r\n\r\n# Delete a tag\r\n$ git tag -d v1.0\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git tag v1.0`: Creates a lightweight tag named `v1.0`.\r\n- `git tag -a v1.0 -m \"Version 1.0 release\"`: Creates an annotated tag named `v1.0` with a message.\r\n- `git tag`: Lists all tags in the repository.\r\n- `git tag -d v1.0`: Deletes the tag named `v1.0`.\r\n\r\n### 6.3 git submodule\r\n\r\nThe `git submodule` command is used to initialize, update, or inspect submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Add a submodule\r\n$ git submodule add https://github.com/user/repository.git path/to/submodule\r\n\r\n# Initialize submodules\r\n$ git submodule init\r\n\r\n# Update submodules\r\n$ git submodule update\r\n\r\n# Remove a submodule\r\n$ git submodule deinit -f path/to/submodule\r\n$ rm -rf .git/modules/path/to/submodule\r\n$ git rm -f path/to/submodule\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git submodule add https://github.com/user/repository.git path/to/submodule`: Adds a submodule located at the specified URL to the specified path.\r\n- `git submodule init`: Initializes the submodules.\r\n- `git submodule update`: Updates the submodules to match the commit specified in the superproject.\r\n- `git submodule deinit -f path/to/submodule`: Deinitializes the submodule.\r\n- `rm -rf .git/modules/path/to/submodule`: Removes the submodule's directory.\r\n- `git rm -f path/to/submodule`: Removes the submodule from the repository.\r\n\r\n### 6.4 git archive\r\n\r\nThe `git archive` command is used to create an archive of files from a named tree. This can be useful for creating backups or distributing a specific version of the project.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Create a tar archive of the current branch\r\n$ git archive --format=tar --output=archive.tar HEAD\r\n\r\n# Create a zip archive of a specific commit\r\n$ git archive --format=zip --output=archive.zip 1a2b3c4\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git archive --format=tar --output=archive.tar HEAD`: Creates a tar archive of the current branch and saves it as `archive.tar`.\r\n- `git archive --format=zip --output=archive.zip 1a2b3c4`: Creates a zip archive of the commit with hash `1a2b3c4` and saves it as `archive.zip`.\r\n\r\n### 6.5 git reflog\r\n\r\nThe `git reflog` command is used to manage the reference logs, which record when the tips of branches and other references were updated in the local repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Show the reflog for the current branch\r\n$ git reflog\r\n\r\n# Show the reflog for a specific branch\r\n$ git reflog show main\r\n\r\n# Delete the reflog entries older than a specified time\r\n$ git reflog expire --expire=90.days.ago --all\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git reflog`: Displays the reflog for the current branch.\r\n- `git reflog show main`: Displays the reflog for the `main` branch.\r\n- `git reflog expire --expire=90.days.ago --all`: Deletes the reflog entries older than 90 days for all references.\r\n\r\nBy mastering these advanced Git commands, you can handle more complex tasks and troubleshoot issues more effectively. These commands provide powerful tools for debugging, managing submodules, creating archives, and maintaining a clean and organized repository.\r\n\r\n## Section 7: Collaboration and Contribution\r\n\r\nCollaboration and contribution are key aspects of using Git, especially in team environments. Git provides several commands that facilitate working with others, applying patches, and generating summaries of changes. This section covers essential commands for collaboration and contribution.\r\n\r\n### 7.1 git cherry\r\n\r\nThe `git cherry` command is used to find commits that have not yet been applied upstream. This is useful for identifying changes that need to be pushed or reviewed.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git cherry -v\r\n+ 5d6e7f8 Add new feature\r\n- 1a2b3c4 Fix bug in feature\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git cherry -v`: Lists commits that are in the current branch but not in the upstream branch. The `+` indicates commits that are unique to the current branch, while `-` indicates commits that are already in the upstream branch.\r\n\r\n### 7.2 git apply\r\n\r\nThe `git apply` command applies a patch to files and/or to the index. This is useful for applying changes that have been shared as patches.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Apply a patch file\r\n$ git apply patch.diff\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git apply patch.diff`: Applies the changes from the patch file `patch.diff` to the working directory.\r\n\r\n### 7.3 git format-patch\r\n\r\nThe `git format-patch` command prepares patches for email submission. It generates patch files from commits, which can be shared and applied using `git apply`.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Generate patch files for the last 3 commits\r\n$ git format-patch -3\r\n0001-Add-new-feature.patch\r\n0002-Fix-bug-in-feature.patch\r\n0003-Update-documentation.patch\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git format-patch -3`: Creates patch files for the last 3 commits. Each patch file contains the changes from a single commit.\r\n\r\n### 7.4 git send-email\r\n\r\nThe `git send-email` command sends a collection of patches as emails. This is useful for submitting patches to a mailing list or a project maintainer.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n# Send patch files via email\r\n$ git send-email *.patch\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git send-email *.patch`: Sends all patch files in the current directory as emails. The command prompts for email configuration and recipient information.\r\n\r\n### 7.5 git request-pull\r\n\r\nThe `git request-pull` command generates a summary of pending changes, which can be sent to a project maintainer to request a pull. This is useful for formally requesting that your changes be merged into the upstream repository.\r\n\r\n**Example Usage:**\r\n\r\n```bash\r\n$ git request-pull origin/main https://github.com/user/repository.git main\r\nThe following changes since commit 1a2b3c4:\r\n  Fix bug in feature (2021-10-17 10:00:00 -0400)\r\n\r\nare available in the Git repository at:\r\n  https://github.com/user/repository.git main\r\n\r\nfor you to fetch changes up to 5d6e7f8:\r\n  Add new feature (2021-10-18 14:00:00 -0400)\r\n\r\n----------------------------------------------------------------\r\nAdd new feature\r\n```\r\n\r\n**Explanation:**\r\n\r\n- `git request-pull origin/main https://github.com/user/repository.git main`: Generates a summary of changes from the `main` branch in the local repository that are not in the `origin/main` branch. The summary includes a description of the changes and a link to the repository.\r\n\r\nBy mastering these commands for collaboration and contribution, you can effectively work with other developers, share your changes, and request that your contributions be reviewed and merged. These commands facilitate a smooth workflow for team projects and open-source contributions.","src/content/posts/git-commands.mdx",[1221],"../../assets/images/24/07/git-commands.jpeg","6ca9f83fe8fe0a32","git-commands.mdx","install-nodejs-using-nvm-macos-ubuntu",{id:1224,data:1226,body:1235,filePath:1236,assetImports:1237,digest:1239,legacyId:1240,deferredRender:32},{title:1227,description:1228,date:1229,image:1230,authors:1231,categories:1232,tags:1233,canonical:1234},"How to Install Node.js using NVM on MacOS and Ubuntu","Tutorial of how Node.js can be installed on MacOS and Ubuntu to start your node projects.",["Date","2022-10-07T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/2210/nodejs_install.jpeg",[19],[21],[1198,24],"https://www.bitdoze.com/install-nodejs-using-nvm-macos-ubuntu/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nNode.js can be installed in different ways, in this article we are going to leave NVM and see how it can be installed on our Mac or Ubuntu machines. M1 or M2 are also supported with these.\r\n\r\n## 1. NVM Installation\r\n\r\n### 1.1 Mac NVM Installation\r\n\r\n#### Install Homebrew\r\n\r\n[Homebrew](https://brew.sh/) is the package manager that will be used to install the node. If you are already using it you should skip this step.\r\n\r\n```bash\r\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\r\n```\r\n\r\n#### Install NVM with Homebrew on Mac\r\n\r\nTo do this you just need to follow the below steps:\r\n\r\n```bash\r\nbrew install nvm\r\n```\r\n\r\nThen create a directory in the user home\r\n\r\n```bash\r\nmkdir ~/.nvm\r\n```\r\n\r\nWhen using the zsh shell, add the following config inside ~/.zshrc:\r\n\r\n```bash\r\nexport NVM_DIR=\"$HOME/.nvm\"\r\n[ -s \"/opt/homebrew/opt/nvm/nvm.sh\" ] && \\. \"/opt/homebrew/opt/nvm/nvm.sh\"  # This loads nvm\r\n[ -s \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\" ] && \\. \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\"\r\n```\r\n\r\nTo activate the changes you either login logout or do:\r\n\r\n```bash\r\nsource ~/.zshrc\r\n```\r\n\r\nAt the end you can check to see the nvm version with the below command:\r\n\r\n```bash\r\nnvm --version\r\n```\r\n\r\n## Youtube Video With Details\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/XXrZHWKTJfg\"\r\n  label=\"How to Install Node.js using NVM on MacOS and Ubuntu\"\r\n/>\r\n\r\n### 1.2 Ubuntu NVM Installation\r\n\r\nHomebrew can be used also on Ubuntu but it's recommended to use the default repo. In this part, we are going to see how we can install NVM on Ubuntu to use it for the last part Node.JS install.\r\n\r\nInstall NVM:\r\n\r\n```bash\r\nsudo apt install curl\r\ncurl https://raw.githubusercontent.com/creationix/nvm/master/install.sh | bash\r\n```\r\n\r\nThe script will do the rest and activate everything needed on the user profile.\r\nYou just need to source it to have the changes, you can login/logout or do:\r\n\r\n```bash\r\nsource ~/.bashrc\r\n```\r\n\r\nAt the end you can check to see the nvm version with the below command:\r\n\r\n```bash\r\nnvm --version\r\n```\r\n\r\n## 2. Install Node.js with NVM\r\n\r\nThis part can be done on both Mac and Ubuntu as it uses the NVM to install node.\r\n\r\nThere are multiple versions of node out there, in function of what your application is supporting you should install the right version, you can also install multiple versions and tell what to use.\r\nIn this case, we are going to go with LTS one which is 16.\r\n\r\nTo install the current LTS Node.js version, execute:\r\n\r\n```bash\r\nnvm install --lts\r\n```\r\n\r\nSee the version used:\r\n\r\n```bash\r\nnode --version\r\n```\r\n\r\nYou can install multiple nodejs versions and choose what to use.\r\n\r\nInstall the second version:\r\n\r\n```bash\r\nnvm install 18\r\n```\r\n\r\nUse a specific version:\r\n\r\n```bash\r\nnvm use 16\r\n```","src/content/posts/install-nodejs-using-nvm-macos-ubuntu.mdx",[1238],"../../assets/images/2210/nodejs_install.jpeg","dd8504fdbf55e923","install-nodejs-using-nvm-macos-ubuntu.mdx","install-docker-ubuntu-arm",{id:1241,data:1243,body:1252,filePath:1253,assetImports:1254,digest:1256,legacyId:1257,deferredRender:32},{title:1244,description:1245,date:1246,image:1247,authors:1248,categories:1249,tags:1250,canonical:1251},"How To Install Docker & Docker-compose for Ubuntu ARM Systems","Learn how to install docker and docker compose on Ubuntu ARM system to host your own apps",["Date","2023-09-28T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/10/install-docker-ubuntu-arm.jpeg",[19],[98],[100],"https://www.bitdoze.com/install-docker-ubuntu-arm/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\n\r\nDocker has revolutionized the way we think about software development and deployment. With Docker, you can package your application and its dependencies into a container, making it easier to move and manage. Docker-Compose further simplifies multi-container Docker applications. In this guide, we'll walk you through how to install Docker and Docker-Compose on Ubuntu ARM systems.\r\n\r\nARM and x86 are two major types of CPU architectures that have been ruling different domains of computing. ARM, which stands for Advanced RISC Machine, is known for its power efficiency and is predominantly used in mobile devices, IoT gadgets, and increasingly in servers. On the other hand, x86 architecture, developed by Intel, has long been the standard for desktop and server computing.\r\n\r\nThe key difference lies in their design philosophy. ARM relies on a simpler set of instructions, allowing for lower power consumption, which is crucial for battery-powered devices. x86 architectures offer a broad set of instructions, optimized for high performance in complex computing tasks. Both have their pros and cons, but with ARM's growing presence in the server and desktop markets, it's becoming more important to understand how to work with both.\r\n\r\nThe [ARM vs x86: A Benchmark Comparison You Need to See](https://www.wpdoze.com/arm-vs-x86-vps-server-benchmarks/) article will provide more details about the benchmarks and where you can create an ARM VPS.\r\n\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Get an ARM Server\" />\r\n\r\nSome other docker articles that can help you in your docker journey:\r\n\r\n- [Add Users to a Docker Container](https://www.bitdoze.com/add-users-to-docker-container/)\r\n- [Copy Multiple Files in One Layer Using a Dockerfile](https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/)\r\n- [Redirect Docker Logs to a Single File](https://www.bitdoze.com/redirect-docker-logs-to-a-single-file/)\r\n- [Environment Variables ARG and ENV in Docker](https://www.bitdoze.com/docker-env-vars/)\r\n\r\n## Steps to Install Docker & Docker-compose for Ubuntu ARM Systems\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n### Step 1: Update Package List\r\n\r\nStart by updating the package list to make sure you have the latest version of the packages.\r\n\r\n```bash\r\nsudo apt-get update\r\n```\r\n\r\n- **sudo:** Run the command as a superuser.\r\n- **apt-get:** Ubuntu package management utility.\r\n- **update:** Fetches the package list from the repository.\r\n\r\n### Step 2: Install Required Packages\r\n\r\nBefore installing Docker, you'll need to install some essential packages.\r\n\r\n```bash\r\nsudo apt-get install ca-certificates curl gnupg lsb-release\r\n```\r\n\r\n- **ca-certificates:** For secure web communication.\r\n- **curl:** Command-line tool for transferring data.\r\n- **gnupg:** For key management.\r\n- **lsb-release:** Provides info about the Linux distribution.\r\n\r\n### Step 3: Create Keyring Directory\r\n\r\nCreate a directory to store the Docker GPG key.\r\n\r\n```bash\r\nsudo mkdir -p /etc/apt/keyrings\r\n```\r\n\r\n- **mkdir:** Make directory command.\r\n- **\"-p\":** Create parent directories as needed.\r\n\r\n### Step 4: Add Docker GPG Key\r\n\r\nDownload and add the Docker GPG key to the keyring directory.\r\n\r\n```bash\r\ncurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\r\n```\r\n\r\n### Step 5: Add Docker Repository\r\n\r\nAdd the Docker repository to your sources list.\r\n\r\n```bash\r\necho \\\r\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\r\n  jammy stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\r\n```\r\n\r\n- **\"dpkg --print-architecture\":** Prints the system architecture.\r\n- **tee:** Reads from standard input and writes to standard output and files.\r\n\r\n### Step 6: Update Package List Again\r\n\r\nRun an update again to fetch packages from the newly added Docker repository.\r\n\r\n```bash\r\nsudo apt-get update\r\n```\r\n\r\n### Step 7: Install Docker and Docker-Compose\r\n\r\nFinally, install Docker and Docker-Compose.\r\n\r\n```bash\r\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-compose\r\n```\r\n\r\n- **docker-ce:** Docker Community Edition.\r\n- **docker-ce-cli:** Docker CLI.\r\n- **containerd.io:** Container runtime.\r\n- **docker-compose-plugin:** Compose CLI plugin.\r\n- **docker-compose:** To define and run multi-container Docker applications.\r\n\r\nIn case you are interested to have a web panel that can help you manage your applications and be used as a reverse proxy you can check the bellow course:\r\n\r\n<Button\r\n  link=\"https://webdoze.net/courses/cloudpanel-setup/\"\r\n  text=\"CloudPanel Setup Course\"\r\n/>\r\n\r\n## Conclusion\r\n\r\nAnd there you have it! You've successfully installed Docker and Docker-Compose on your Ubuntu ARM system. With these tools at your disposal, you're now ready to begin containerizing applications and taking full advantage of what Docker has to offer. Happy Docking!","src/content/posts/install-docker-ubuntu-arm.mdx",[1255],"../../assets/images/23/10/install-docker-ubuntu-arm.jpeg","8617a2793b30852e","install-docker-ubuntu-arm.mdx","install-upgrade-python-mac",{id:1258,data:1260,body:1269,filePath:1270,assetImports:1271,digest:1273,legacyId:1274,deferredRender:32},{title:1261,description:1262,date:1263,image:1264,authors:1265,categories:1266,tags:1267,canonical:1268},"How To Install, Upgrade Python and Run VENV on MAC","Learn how install, upgrade Python on MAC and use VENV for your projects with this easy steps.",["Date","2024-03-26T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/install-upgrade-python-mac.jpeg",[19],[98],[294],"https://www.bitdoze.com/install-upgrade-python-mac/","Virtual environments in Python are a crucial tool for managing project-specific dependencies and avoiding conflicts with system-wide installations. They allow developers to work with isolated Python environments, ensuring that each project has its own set of dependencies that do not interfere with others. This article will guide you through the process of installing, upgrading, and running Python virtual environments (venv) on macOS using Homebrew, a popular package manager for macOS.\r\n\r\n## Install Brew on MAC\r\n\r\nHomebrew is an open-source package manager for macOS that simplifies the installation of software on Apple's operating system. To install Homebrew, open the Terminal app and enter the following command:\r\n\r\n```bash\r\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\r\n```\r\n\r\nThis command downloads and runs the Homebrew installation script.\r\n\r\n## Install Latest Python on MAC with BREW\r\n\r\nOnce Homebrew is installed, you can easily install the latest version of Python. To do so, run the following command in your terminal:\r\n\r\n```bash\r\nbrew install python\r\n```\r\n\r\nThis command installs the latest stable version of Python, along with pip, setuptools, and wheel, which are tools for managing Python packages.\r\n\r\nCheck the version that is in use:\r\n\r\n```sh\r\n❯ python3 -V\r\nPython 3.11.6\r\n```\r\n\r\n## Install Specific Version of Python on MAC with BREW\r\n\r\nIf you need a specific version of Python, Homebrew can help you install it. First, search for the available Python versions using:\r\n\r\n```bash\r\nbrew search python\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n❯ brew search python\r\n==> Formulae\r\napp-engine-python          python-argcomplete         python-gdbm@3.12           python-packaging           python-tabulate            python-yq                  wxpython\r\nboost-python3              python-build               python-idna                python-ply                 python-tk@3.10             python@3.10 ✔              pythran\r\nbpython                    python-chardet             python-kiwisolver          python-psutil              python-tk@3.11             python@3.11 ✔              jython\r\ncyclonedx-python           python-charset-normalizer  python-launcher            python-pyparsing           python-tk@3.12             python@3.12 ✔              cython\r\nipython                    python-cycler              python-lsp-server          python-pytz ✔              python-tk@3.9              python@3.7\r\nmeson-python               python-dateutil            python-lxml                python-requests            python-trove-classifiers   python@3.8\r\nmicropython                python-flit-core           python-markdown            python-setuptools ✔        python-typing-extensions   python@3.9 ✔\r\nptpython                   python-gdbm@3.11           python-matplotlib          python-setuptools-scm      python-urllib3             reorder-python-imports\r\n```\r\n\r\nThen, install the desired version, for example, Python 3.10:\r\n\r\n```bash\r\nbrew install python@3.10\r\n```\r\n\r\nAfter installation, you may need to link the Python version if it's not already in your PATH:\r\n\r\n```bash\r\nbrew link python@3.10\r\n```\r\n\r\nThen you can use the version to run your python app with:\r\n\r\n```sh\r\n❯  python3.10 -V\r\nPython 3.10.14\r\n```\r\n\r\n## Upgrade Python to Latest Version on MAC\r\n\r\nTo upgrade Python to the latest version available in Homebrew, first update Homebrew itself:\r\n\r\n```bash\r\nbrew update\r\n```\r\n\r\nThen, upgrade Python:\r\n\r\n```bash\r\nbrew upgrade python\r\n```\r\n\r\nOutput:\r\n\r\n```sh\r\n❯ python3 -V\r\nPython 3.12.2\r\n```\r\n\r\nThis will upgrade Python to the latest version provided by Homebrew.\r\n\r\nIf you are interested in creating some Python projects you can check the below article to get started:\r\n\r\n- [NiceGUI For Beginners: Build An UI to Python App in 5 Minutes](https://www.bitdoze.com/nicegui-get-started/)\r\n- [How To Add Multiple Pages to NiceGUI](https://www.bitdoze.com/nicegui-pages/)\r\n- [How To Run Any Python App in Docker with Docker Compose](https://www.bitdoze.com/docker-run-python/)\r\n\r\n## Run Python in VENV on MAC\r\n\r\nTo create a virtual environment in Python, navigate to your project directory and run the following command:\r\n\r\n```bash\r\npython3 -m venv myenv\r\n```\r\n\r\nReplace `myenv` with your preferred environment name. This command creates a new virtual environment directory `myenv` within your project[6].\r\n\r\nTo activate the virtual environment, use:\r\n\r\n```bash\r\nsource myenv/bin/activate\r\n```\r\n\r\nOnce activated, your terminal prompt will change to indicate that you are working inside the virtual environment. You can now install packages locally within this environment without affecting the global Python installation.\r\n\r\nTo deactivate the virtual environment and return to the global Python context, simply run:\r\n\r\n```bash\r\ndeactivate\r\n```\r\n\r\n## Conclusions\r\n\r\nManaging Python environments with venv on macOS is straightforward, especially with the help of Homebrew. By following the steps outlined in this article, you can install and manage multiple versions of Python, create isolated environments for your projects, and ensure a clean and conflict-free development experience. Remember to activate the appropriate virtual environment before working on a project to keep dependencies organized and projects running smoothly.","src/content/posts/install-upgrade-python-mac.mdx",[1272],"../../assets/images/24/03/install-upgrade-python-mac.jpeg","bdb613e08d78ef24","install-upgrade-python-mac.mdx","install-plausible-analytics",{id:1275,data:1277,body:1286,filePath:1287,assetImports:1288,digest:1290,legacyId:1291,deferredRender:32},{title:1278,description:1279,date:1280,image:1281,authors:1282,categories:1283,tags:1284,canonical:1285},"How To Install Plausible  With One Click","Learn how to install Plausible Analytics with 1 click in docker via Coolify on your own server.",["Date","2023-03-17T07:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/03/plausible-install.jpeg",[19],[98],[554,24],"https://www.bitdoze.com/install-plausible-analytics/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/03/01createservice.png\";\r\nimport img2 from \"../../assets/images/23/03/choose_plausible_analytics.jpeg\";\r\nimport img3 from \"../../assets/images/23/03/plausible-configs.png\";\r\nimport img4 from \"../../assets/images/23/03/plausible-secrets.jpeg\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[Plausible Analytics](https://www.bitdoze.com/plausible-tool/) can help you take control of your analytics and stop relying on Google Analytics. In this tutorial, we will see everything that needs to be done to deploy Plausible in Docker via Coolify on your own server. The process is easy to do and will help you to have your own analytics and not share the details with other providers like Google Analytics.\r\n\r\nIf you don't know, Coolify is a self-hosted Heroku or Netlify alternative that also allows you to deploy various Docker apps. For more details you can check out: [Coolify Install A Free Heroku and Netlify Self-Hosted Alternative](https://www.bitdoze.com/coolify-install-heroku-alternative/) where we go into more detail.\r\n\r\nPlausible is using a config file to enable some variables, you can do that in the Coolify interface in the secrets area, so in case you want to add an SMTP server or integrate it with Google Web Console if you want, we will take a look at that part when we reach the step for that.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n## 1. Deploy A VPS server\r\n\r\nTo host Plausible in a Docker container you need a VPS server, there are a lot of services that can help you with this, the most known are [DigitalOcean](https://go.bitdoze.com/do), [Vultr](https://go.bitdoze.com/vultr), [Hetzner](https://go.bitdoze.com/hetzner), I also wrote an article and made a video with the benchmarks here: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/) you can check it out.\r\n\r\nIn this tutorial we are going to use Hetzner for this where we have the VPS created and we can ssh to it to have Coolify installed and then deploy Plausible Analitycs.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n## 2. Installing Coolify\r\n\r\nCoolify can be installed with a simple command, you can also check my tutorial for complete steps including the SSL certificate: [Coolify Install](https://www.bitdoze.com/coolify-install-heroku-alternative/)\r\n\r\nWe will install Coolify on a Ubuntu 22.04 server. To do this, you just need to SSH to the VPS server and run the following command\r\n\r\n```bash\r\nwget -q https://get.coollabs.io/coolify/install.sh \\\r\n-O install.sh; sudo bash ./install.sh\r\n```\r\n\r\nIf you are interested in how to monitor your CPU and have an automatic email sent when the load is too high, you should have a look at [Monitor CPU Usage and Send Email Alerts in Linux](https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/)\r\n\r\n## 🎥 Plausible Install Video\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/RNnuXCUhHF4\"\r\n  label=\"How To Install Plausible  With One Click\"\r\n/>\r\n\r\n## 3. Deploy the Plausible service with Coolify\r\n\r\n### 3.1 Point Domain or Subdomain to VPS\r\n\r\nWe need a domain or subdomain that we can use to access Plausible, in this tutorial we are going to use CloudFlare and point a subdomain to the VPS server where we have Coolify installed. We will not be using the CloudFlare proxy and will leave it disabled. All you need to do is add an A record and point it to the server IP.\r\n\r\n### 3.2 Create a service\r\n\r\nYou need to go to Create New Resource and select **Service**. From there select the **Plausible Analytics** service as shown in the images below:\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"Coolify service create\"\r\n/>\r\n<Picture\r\n  src={img2}\r\n  alt=\"Coolify choose Plausible Analytics\"\r\n/>\r\n\r\n### 3.3 Configure Coolify Plausible Analytics\r\n\r\nThe next thing on the list is to configure the Coolify service, in there you need to add the below:\r\n\r\n- **Name** - here you add the name of your deployment\r\n- **Version / Tag** - you choose the Plausible Analytics version you want to install\r\n- **URL (FQDN)** - you ad the domain or sudomain that you want to use\r\n- **Admin Email Address** - enter the admin email address\r\n- **Admin User Name** - enter admin user name\r\n- **Authentication** - should be enabled if you choose to have it internet facing and more secure.\r\n- **Registration** - you can allow them or not allow registration in function of needs. the secure way is to not allow them or invite only.\r\n\r\n<Picture\r\n  src={img3}\r\n  alt=\"Coolify Plausible Configs\"\r\n/>\r\n\r\nAfter what needs to be done is to hit Save to save all the configs.\r\n\r\n### 3.4 Add Extra Configs in Secrets\r\n\r\nPlausible has various configs that can be activated, for the list check [Plausible Config](https://plausible.io/docs/self-hosting-configuration). For instance with the secrets you can change the passwords and add SMTP details if you want to link Plausible to an email server. Below are the configs for adding the smtp server with an image.\r\n\r\n```bash\r\nSMTP_USER_PWD=<smtp pass>\r\nSMTP_USER_NAME=<smtp user>\r\nSMTP_HOST_PORT=<smtp port>\r\nSMTP_HOST_ADDR=<smtp server>\r\nMAILER_EMAIL=<from email>\r\n```\r\n\r\n<Picture\r\n  src={img4}\r\n  widths={[200, 400, 900]}\r\n  sizes=\"(max-width: 900px) 100vw, 900px\"\r\n  alt=\"Plausible Secrets\"\r\n/>\r\n\r\nAfter you just need to hit Deploy and Coolify will start deploy the docker containers.\r\n\r\n### 3.5 Access Plausible Analytics\r\n\r\nAfter deploy is finished you should go and access Plausible dashboard with the URL you have added in the configs. The first time you will be prompted to create a user and a password.\r\n\r\nAfter you create the account you can add your websites. The configs will be stored on the disks with docker volumes and in case you redeploy it you will not lose any data.\r\n\r\n## Conclusions\r\n\r\nThis is how you can easily deploy Plausible with the help of Coolify, the config has everything that it needs and you can access it securely with an SSL certificate. In case you want to update it you can do it easily from Coolify interface.","src/content/posts/install-plausible-analytics.mdx",[1289],"../../assets/images/23/03/plausible-install.jpeg","02e0faac787f450b","install-plausible-analytics.mdx","install-wordpress-docker",{id:1292,data:1294,body:1303,filePath:1304,assetImports:1305,digest:1307,legacyId:1308,deferredRender:32},{title:1295,description:1296,date:1297,image:1298,authors:1299,categories:1300,tags:1301,canonical:1302},"Install WordPress in a Docker Container with Docker Compose","Use docker to install WordPress in a container with Docker Compose with phpMyAdmin and Database backups.",["Date","2024-02-21T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/install-wordpress-docker.jpeg",[19],[98],[242],"https://www.bitdoze.com/install-wordpress-docker/","import { Picture } from \"astro:assets\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\nimport imag2 from \"../../assets/images/24/02/docker-wp-access.png\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nDocker is a powerful containerization platform that packages applications and their dependencies into containers. These containers can then be deployed across different environments, ensuring consistency and simplifying the deployment process.\r\n\r\nIn this article, we are going to cover everything you need to do to have WordPress running in a docker container with the help of docker-compose. Here is what you will get by following the WordPress Docker Installation in this article:\r\n\r\n- **WordPress Container**: we are going to install the latest WordPress version with volums for WordPress installation and php.ini file that will allow you to modify the PHP settings like `memory_limit`, `upload_max_filesize`, `max_execution_time`, etc\r\n- **Database Container**: MYSQL latest version will be used with a volume to mysql data that will allow you to backup WordPress if needed.\r\n- **phpMyAdmin**: container with phpMyAdmin app that will allow you to connect to the database and do various things in there directly, `UPLOAD_LIMIT` can be set to accommodate big databases, access to phpMyAdmin config files to modify parameters in case you have custom things or databases are too big and you need custom PHP parameters\r\n- **Database Backups**: a container that will use `sqldump` to create periodic backups to your WordPress database and do a cleanup of older backups in case something goes wrong to restore the backup. Backups will be stored in a local volume.\r\n- **SSL/Reverse Proxy**: CloudFlare Tunels will be used to set up a domain to the container and have SSL certificates + protection through CloudFlare free plan with WAF.\r\n- **Dockge for Manage Docker**: Dockge will be used to add our docker compose files and manage the containers, this can be done also with docker-compose commands but Dockge offers a better way to work with the containers thru a UI.\r\n- **Redis**: you can cache requests to your Database via Redis, you will see the configs needed to add Redis with WordPress to take advantage of Redis.\r\n\r\n> This being said we will get started and install WordPress in a Docker with docker-compose, I will use the `latest` for the image tags but you can use the exact version if you don't want surprises when a container moves to a next version.\r\n\r\n## Steps to Install WordPress in Docker with Docker Compose\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/m3FNd_7MSGQ\"\r\n  label=\"Install WordPress in a Docker Container with Docker Compose\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host WordPress, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\nHaving all of this you will be ready to move to next step and add the docker-compose file in Dockge.\r\n\r\n### 2. Docker Compose File\r\n\r\nThe first thing is to have a docker-compose file with all the services we need: WordPress, Database, phpMyAdmin, DB backup.\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  wp:\r\n    image: wordpress:latest\r\n    restart: unless-stopped\r\n    ports:\r\n      - 5010:80\r\n    volumes:\r\n      - ./config/wp_php.ini:/usr/local/etc/php/conf.d/conf.ini\r\n      - ./wp-app:/var/www/html # Full wordpress project\r\n    environment:\r\n      WORDPRESS_DB_HOST: wp-db:3306\r\n      WORDPRESS_DB_NAME: \"${DB_NAME}\"\r\n      WORDPRESS_DB_USER: \"${DB_USER}\"\r\n      WORDPRESS_DB_PASSWORD: \"${DB_PASSWORD}\"\r\n    depends_on:\r\n      - wp-db\r\n  wp-db:\r\n    image: mysql:latest\r\n    volumes:\r\n      - ./db_data:/var/lib/mysql\r\n    restart: unless-stopped\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD: \"${DB_ROOT_PASSWORD}\"\r\n      MYSQL_DATABASE: \"${DB_NAME}\"\r\n      MYSQL_USER: \"${DB_USER}\"\r\n      MYSQL_PASSWORD: \"${DB_PASSWORD}\"\r\n  pma:\r\n    image: phpmyadmin:latest\r\n    ports:\r\n      - 5011:80\r\n    volumes:\r\n      - ./config/pma_php.ini:/usr/local/etc/php/conf.d/conf.ini\r\n      - ./config/pma_config.php:/etc/phpmyadmin/config.user.inc.php\r\n    restart: unless-stopped\r\n    environment:\r\n      # https://docs.phpmyadmin.net/en/latest/setup.html#docker-environment-variables\r\n      PMA_HOST: wp-db\r\n      PMA_PORT: 3306\r\n      MYSQL_ROOT_PASSWORD: \"${DB_ROOT_PASSWORD}\"\r\n      UPLOAD_LIMIT: 50M\r\n    depends_on:\r\n      - wp-db\r\n  wp-db-backup:\r\n    container_name: wp-db-backup\r\n    image: tiredofit/db-backup\r\n    volumes:\r\n      - ./backups:/backup\r\n    restart: unless-stopped\r\n    environment:\r\n      DB_TYPE: mysql\r\n      DB_HOST: wp-db\r\n      DB_NAME: \"${DB_NAME}\"\r\n      DB_USER: \"${DB_USER}\"\r\n      DB_PASS: \"${DB_PASSWORD}\"\r\n      DB_BACKUP_INTERVAL: 720\r\n      DB_CLEANUP_TIME: 72000\r\n      #DB_BACKUP_BEGIN: 1\r\n      CHECKSUM: SHA1\r\n      COMPRESSION: GZ\r\n      CONTAINER_ENABLE_MONITORING: \"false\"\r\n    depends_on:\r\n      - wp-db\r\n```\r\n\r\nThis file defines four services: `wp` (WordPress), `wp-db`(MySQL database for WordPress), `pma` (phpMyAdmin for database management), and `wp-db-backup` (for database backups). Each service is configured with specific settings, such as image versions, ports, volumes for persistent storage, and environment variables for configuration.\r\n\r\nPort `5010` will be used for WordPress to get access to the application and `5011` for phpMyAdmin, they can be changed and used as per requirements. The DB users and password are set as environment variables to not keep them in the compose-file.\r\n\r\n`unless-stopped` is added to each container to restart automatically in case something goes wrong with the server and reboots.\r\n\r\n`DB_BACKUP_INTERVAL` and `DB_CLEANUP_TIME` can be altered as per needs to take backups and clean old ones, the time is in minutes.\r\n\r\n### 3. Setup .env File\r\n\r\nVariables with DB details that will be used by all containers will be stored in `.env` file, you can create one in the same location of your docker-compose file or add them in Dockge and save the config.\r\n\r\n```sh\r\nDB_NAME='wordpress'\r\nDB_USER='wp'\r\nDB_PASSWORD='password'\r\nDB_ROOT_PASSWORD=d1155bc02c5bfd6b2a4\r\n```\r\n\r\nYou replace the values with the things you want for your installation, to make them as secure as possible.\r\n\r\n### 4. Create the Path to Config Files\r\n\r\nDocker it doesn't play well if you are trying to mount a file directly from host to container it will create directories instead of a file that can be altered. To be able to change the configs for your WordPress PHP settings and phpMyAdmin we are going to need to set up the files before, we can create some empty ones if we don't have values and alter them when we need.\r\n\r\n```sh\r\n#navigate where app stack will be set\r\ncd /opt/staks/wordpress\r\n#create the config directory\r\nmkdir config\r\n#Create the empty files\r\ntouch ./config/wp_php.ini\r\ntouch ./config/pma_php.ini\r\ntouch ./config/pma_config.php\r\n```\r\n\r\n### 5. Start WordPress on Docker\r\n\r\nYou can use the below command if you are using plain docker-compose and not Dockge:\r\n\r\n```sh\r\ndocker compose up -d\r\n```\r\n\r\nIf Dockge is used you just need to start the stack you created with the docker compose file after saving.\r\n\r\n### 6. Configure CloudFlare Tunnels\r\n\r\nYou need to let CloudFlare Tunel know which port to use, you just need to go in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\nAs we are using port `5010` in the compose file we should use that in CloudFlare Tunnels. We can add also another config to a subdomain for phpMyAdmin with `5011` port but we will be able to access it via port also.\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 7. Configure WordPress\r\n\r\nAfter the steps are done you will be able to access WordPress and set up the admin account. You should use the domain you set up in CloudFlare Tunells in the browser and you will be asked to choose a language and create a WordPress admin user as in the picture below:\r\n\r\n<Picture src={imag2} alt=\"WordPress Docker Setup\" />\r\n\r\nAfter you add the details you will be able to access the admin area with the user and password that was set.\r\n\r\nIf you don't want to add a domain you can access the installation with `IP:5011` or the port you have set up.\r\n\r\nAfter you can go and configure the permalinks and add themes and plugins to WordPress.\r\n\r\n### 8. Alter PHP Variables for WordPress in Docker\r\n\r\n**Modify WordPress PHP Variables in Docker**\r\n\r\nEdit the `./config/wp_php.ini`` file to customize PHP settings for WordPress:\r\n\r\n```ini\r\nfile_uploads = On\r\nmemory_limit = 256M\r\nupload_max_filesize = 64M\r\npost_max_size = 64M\r\nmax_execution_time = 300\r\nmax_input_time = 1000\r\n```\r\n\r\nYou can add the PHP parameters you want to accommodate a big app if needed.\r\n\r\n**Restart the Container**\r\n\r\nAfter modifying the PHP configuration, restart the WordPress container to apply the changes:\r\n\r\n```sh\r\ndocker compose restart wp\r\n```\r\n\r\nYou can use also Dockge to restart the stack there is no way to restart only a container for now in the stack.\r\n\r\n### 9. Access phpMyAdmin\r\n\r\nIf you want to check the database you can access phpMyAdmin directly with: `IP:5011` and input the user and password you have set in the point point 3 in `.env` file.\r\n\r\nIf you set up phpMyAdmin with a CloudFlare Tunel you can go and access it from there.\r\n\r\n### 10. Verify Backups\r\n\r\nIn the `backup` directory where you have the stack or docker compose file you should have a backup directory with database backups you should check and see if they were generated:\r\n\r\nBelow is an example:\r\n\r\n```sh\r\n/opt/stacks/wordpress# cd backups/\r\n/opt/stacks/wordpress/backups# ls -ltr\r\ntotal 16\r\n-rw------- 1 10000 10000 495 Feb 21 09:26 mysql_wordpress_wp-db_20240221-092619.sql.gz\r\n-rw------- 1 10000 10000  87 Feb 21 09:26 mysql_wordpress_wp-db_20240221-092619.sql.gz.sha1\r\n-rw------- 1 10000 10000 495 Feb 21 09:32 mysql_wordpress_wp-db_20240221-093228.sql.gz\r\n-rw------- 1 10000 10000  87 Feb 21 09:32 mysql_wordpress_wp-db_20240221-093228.sql.gz.sha1\r\nlrwxrwxrwx 1 10000 10000  44 Feb 21 09:32 latest-mysql_wordpress_wp-db -> mysql_wordpress_wp-db_20240221-093228.sql.gz\r\n```\r\n\r\n### 11. Add Redis to WordPress Docker Compose file\r\n\r\nYou can also add Redis to your docker compose file for better db performance, all the request that will be made to the mysql database will be cached in memory and will be a lot faster, to do so you just need to add:\r\n\r\n```yaml\r\nredis-wp:\r\n  image: redis\r\n  restart: unless-stopped\r\n```\r\n\r\n**Full YAML file with Redis:**\r\n\r\n```yaml\r\nversion: \"3\"\r\nservices:\r\n  wp:\r\n    image: wordpress:latest\r\n    restart: unless-stopped\r\n    ports:\r\n      - 5010:80\r\n    volumes:\r\n      - ./config/wp_php.ini:/usr/local/etc/php/conf.d/conf.ini\r\n      - ./wp-app:/var/www/html # Full wordpress project\r\n    environment:\r\n      WORDPRESS_DB_HOST: wp-db:3306\r\n      WORDPRESS_DB_NAME: ${DB_NAME}\r\n      WORDPRESS_DB_USER: ${DB_USER}\r\n      WORDPRESS_DB_PASSWORD: ${DB_PASSWORD}\r\n    depends_on:\r\n      - wp-db\r\n  wp-db:\r\n    image: mysql:latest\r\n    volumes:\r\n      - ./db_data:/var/lib/mysql\r\n    restart: unless-stopped\r\n    environment:\r\n      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}\r\n      MYSQL_DATABASE: ${DB_NAME}\r\n      MYSQL_USER: ${DB_USER}\r\n      MYSQL_PASSWORD: ${DB_PASSWORD}\r\n  pma:\r\n    image: phpmyadmin:latest\r\n    ports:\r\n      - 5011:80\r\n    volumes:\r\n      - ./config/pma_php.ini:/usr/local/etc/php/conf.d/conf.ini\r\n      - ./config/pma_config.php:/etc/phpmyadmin/config.user.inc.php\r\n    restart: unless-stopped\r\n    environment:\r\n      # https://docs.phpmyadmin.net/en/latest/setup.html#docker-environment-variables\r\n      PMA_HOST: wp-db\r\n      PMA_PORT: 3306\r\n      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}\r\n      UPLOAD_LIMIT: 50M\r\n    depends_on:\r\n      - wp-db\r\n  wp-db-backup:\r\n    container_name: wp-db-backup\r\n    image: tiredofit/db-backup\r\n    volumes:\r\n      - ./backups:/backup\r\n    restart: unless-stopped\r\n    environment:\r\n      DB_TYPE: mysql\r\n      DB_HOST: wp-db\r\n      DB_NAME: ${DB_NAME}\r\n      DB_USER: ${DB_USER}\r\n      DB_PASS: ${DB_PASSWORD}\r\n      DB_BACKUP_INTERVAL: 720\r\n      DB_CLEANUP_TIME: 72000\r\n      #DB_BACKUP_BEGIN: 1\r\n      CHECKSUM: SHA1\r\n      COMPRESSION: GZ\r\n      CONTAINER_ENABLE_MONITORING: false\r\n    depends_on:\r\n      - wp-db\r\n  redis-wp:\r\n    image: redis\r\n    restart: unless-stopped\r\n```\r\n\r\nAfter you edit the `wp-config.php` and add the host and port so request to be cached by Redis:\r\n\r\n`wp-config.php` is located into the volume you created for WordPress under `./wp-app`\r\n\r\n```php\r\ndefine('WP_REDIS_HOST', 'redis-wp');\r\ndefine('WP_REDIS_PORT', '6379');\r\n```\r\n\r\nThis has the name of the service which is `redis-db` and the default port `6379`. After you just need to install the [WordPress Redis plugin](https://wordpress.org/plugins/redis-cache/) after you enable it you should have Redis set up for your WordPress installation on Docker.\r\n\r\n### 12. What's Next\r\n\r\nNow you should have everything up and running and you should be able to use the WordPress installation for your projects. You can go and see if everything is working as expected and install some themes and plugins.\r\n\r\nYou can also close access to the ports through a firewall if you are using CloudFlare Tunels or a reverse proxy so only the domain can be used for access.\r\n\r\n## Conclusions\r\n\r\nThis is how you are setting up WordPress on top of docker, I hope the article helped and will get you started with WordPress.","src/content/posts/install-wordpress-docker.mdx",[1306],"../../assets/images/24/02/install-wordpress-docker.jpeg","7e634db5447e4909","install-wordpress-docker.mdx","install-wezterm-mac",{id:1309,data:1311,body:1321,filePath:1322,assetImports:1323,digest:1325,legacyId:1326,deferredRender:32},{title:1312,description:1313,date:1314,image:1315,authors:1316,categories:1317,tags:1318,canonical:1320},"Maximize Efficiency: Integrating Wezterm, Zoxide, and Tmux for the Perfect Mac Terminal","Learn how to create a powerful and visually stunning terminal setup using Wezterm on Mac. This comprehensive guide covers installation, configuration, and integration with essential tools like Powerlevel10k, zsh plugins, and tmux for maximum productivity.",["Date","2024-10-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/10/wezterm-install.jpeg",[19],[98],[1142,1143,1319],"wezterm","https://www.bitdoze.com/install-wezterm-mac/","[Wezterm](https://wezfurlong.org/wezterm/) is a modern, feature-rich terminal emulator that's quickly gaining popularity among developers and power users. Developed by Wez Furlong and written in Rust, Wezterm offers a compelling blend of performance, customizability, and cross-platform compatibility.\r\n\r\nKey features of Wezterm include:\r\n\r\n1. **GPU Acceleration:** Wezterm leverages your computer's GPU for rendering, resulting in smooth scrolling and excellent performance, even with complex terminal outputs.\r\n\r\n2. **Lua Configuration:** Unlike many terminal emulators that use static config files, Wezterm allows you to configure it using Lua scripts, providing immense flexibility and programmability.\r\n\r\n3. **Built-in Multiplexer:** Similar to tmux, Wezterm has built-in multiplexing capabilities, allowing you to manage multiple terminal sessions within a single window.\r\n\r\n4. **Ligature Support:** For those using programming fonts with ligatures, Wezterm renders them beautifully, enhancing code readability.\r\n\r\n5. **Cross-Platform:** Whether you're on macOS, Windows, or Linux, Wezterm provides a consistent experience across all major operating systems.\r\n\r\n6. **Rich Color Support:** Wezterm supports 24-bit true color and offers a wide range of color schemes out of the box.\r\n\r\n7. **Image Support:** You can display images directly in the terminal, which can be particularly useful for certain workflows or just for customization.\r\n\r\nBy combining these features with its modern, sleek interface, Wezterm offers a powerful alternative to traditional terminal emulators, making it an excellent choice for users looking to enhance their command-line experience.\r\n\r\n> If you are interested to see some free cool Mac Apps you can check [toolhunt.net mac apps section](https://toolhunt.net/mac/).\r\n\r\n\r\n\r\n\r\n## Install and configure Wezterm on Mac\r\n\r\nIn this section we are going to see all the things we need to do to have Wezterm configured on Mac with everything needed.\r\n\r\n### Install homebrew\r\n\r\nHomebrew is a package manager for macOS that simplifies the installation of software on Apple's operating system. It's an essential tool for developers and power users, making it easy to install and manage various command-line tools and applications.\r\n\r\nTo install Homebrew, open your terminal and run the following command:\r\n\r\n```sh\r\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\r\n```\r\n\r\nThis command does the following:\r\n1. It uses `curl` to download the Homebrew installation script from GitHub.\r\n2. The `-fsSL` flags ensure a quiet, fail-fast download with automatic redirection.\r\n3. The downloaded script is then piped directly to bash for execution.\r\n\r\nAfter running this command, follow any on-screen prompts to complete the installation. You may need to enter your system password to grant the necessary permissions.\r\n\r\nOnce Homebrew is installed, you can verify the installation by running:\r\n\r\n```sh\r\nbrew --version\r\n```\r\n\r\nThis should display the version of Homebrew installed on your system.\r\n\r\nWith Homebrew set up, you're now ready to easily install Wezterm and other necessary components for your terminal setup.\r\n\r\n### Install Wezterm\r\n\r\nNow that we have Homebrew installed, we can easily install Wezterm. Run the following command in your terminal:\r\n\r\n```sh\r\nbrew install --cask wezterm\r\n```\r\n\r\nThis command uses Homebrew's `cask` functionality, which is designed for installing macOS applications and large binaries. The `--cask` flag tells Homebrew to use the Cask version of the formula.\r\n\r\nAfter the installation is complete, you can launch Wezterm from your Applications folder or by using Spotlight (Cmd + Space, then type \"Wezterm\").\r\n\r\n### Install Git\r\n\r\nGit is a distributed version control system that's essential for many development workflows. While macOS comes with Git pre-installed, it's often an older version. To ensure you have the latest version, let's install Git using Homebrew:\r\n\r\n```sh\r\nbrew install git\r\n```\r\n\r\nThis command will install the latest version of Git available through Homebrew.\r\n\r\nAfter installation, you can verify the Git version by running:\r\n\r\n```sh\r\ngit --version\r\n```\r\n\r\nThis should display the version of Git you just installed, which should be more recent than the one that came with your Mac.\r\n\r\nInstalling Git through Homebrew also makes it easier to keep it updated in the future. You can update all Homebrew packages, including Git, with the command:\r\n\r\n```sh\r\nbrew upgrade\r\n```\r\n\r\nWith Wezterm and Git installed, you're now ready to move on to the next steps in customizing your terminal environment.\r\n\r\n### Install Meslo Nerd Font\r\n\r\nNerd Fonts are a collection of fonts that have been patched with a high number of glyphs (icons). They're particularly useful for developers as they include many programming-related icons that can enhance your terminal and text editor experience.\r\n\r\nTo install the Meslo Nerd Font using Homebrew, run the following command:\r\n\r\n```sh\r\nbrew install font-meslo-lg-nerd-font\r\n```\r\n\r\nThis command does the following:\r\n1. It uses Homebrew to install the Meslo LG Nerd Font.\r\n2. The font will be downloaded and installed in your system's font directory.\r\n\r\nAfter running this command, the font should be available for use in Wezterm and other applications on your Mac.\r\n\r\nTo verify that the font was installed correctly, you can check your Font Book application or use it in an application that allows font selection.\r\n\r\nIt's worth noting that you may need to restart Wezterm or any other applications where you want to use this font after installation.\r\n\r\nThe Meslo Nerd Font is an excellent choice for terminal use as it's clear, easily readable, and includes a wide range of glyphs that can be useful in command-line interfaces and programming environments.\r\n\r\nIn the next section, we'll configure Wezterm to use this newly installed font.\r\n\r\n### Setup Wezterm Config File\r\n\r\nWezterm uses a Lua configuration file to customize its behavior and appearance. Let's create and configure this file.\r\n\r\n#### Create the Config File\r\n\r\nFirst, we need to create the configuration file in the correct location. Run these commands in your terminal:\r\n\r\n```sh\r\ntouch ~/.wezterm.lua\r\nvim ~/.wezterm.lua\r\n```\r\n\r\nThese commands do the following:\r\n1. `touch` creates an empty file named `.wezterm.lua` in your home directory.\r\n2. `vim` opens this file in the Vim text editor. If you're not comfortable with Vim, you can use any text editor you prefer.\r\n\r\n#### Configure Wezterm\r\n\r\nNow that we've created the file, let's add some configuration. Copy and paste the following Lua code into your `.wezterm.lua` file:\r\n\r\n```lua\r\n-- Pull in the wezterm API\r\nlocal wezterm = require(\"wezterm\")\r\n\r\n-- This will hold the configuration.\r\nlocal config = wezterm.config_builder()\r\n\r\n-- This is where you actually apply your config choices\r\n\r\nconfig.font = wezterm.font(\"MesloLGS Nerd Font Mono\")\r\nconfig.font_size = 19\r\n\r\nconfig.window_decorations = \"RESIZE\"\r\nconfig.window_background_opacity = 0.8\r\nconfig.macos_window_background_blur = 10\r\n\r\n-- my coolnight colorscheme:\r\nconfig.colors = {\r\n    foreground = \"#CBE0F0\",\r\n    background = \"#011423\",\r\n    cursor_bg = \"#47FF9C\",\r\n    cursor_border = \"#47FF9C\",\r\n    cursor_fg = \"#011423\",\r\n    selection_bg = \"#033259\",\r\n    selection_fg = \"#CBE0F0\",\r\n    ansi = { \"#214969\", \"#E52E2E\", \"#44FFB1\", \"#FFE073\", \"#0FC5ED\", \"#a277ff\", \"#24EAF7\", \"#24EAF7\" },\r\n    brights = { \"#214969\", \"#E52E2E\", \"#44FFB1\", \"#FFE073\", \"#A277FF\", \"#a277ff\", \"#24EAF7\", \"#24EAF7\" },\r\n}\r\n\r\n-- and finally, return the configuration to wezterm\r\nreturn config\r\n```\r\n\r\nLet's break down what this configuration does:\r\n\r\n1. It sets the font to the Meslo Nerd Font we just installed.\r\n2. Sets the font size to 19.\r\n3. Configures the window to be resizable.\r\n4. Sets the background opacity to 80% and adds a blur effect (MacOS only).\r\n5. Defines a custom color scheme called \"coolnight\".\r\n\r\n`config.colors` description:\r\n\r\n1. `foreground = \"#CBE0F0\"`: This sets the default text color to a light blue-gray.\r\n\r\n2. `background = \"#011423\"`: This sets the background color to a very dark blue, almost black.\r\n\r\n3. `cursor_bg = \"#47FF9C\"`: This sets the background color of the cursor to a bright green.\r\n\r\n4. `cursor_border = \"#47FF9C\"`: This sets the border color of the cursor to the same bright green.\r\n\r\n5. `cursor_fg = \"#011423\"`: This sets the foreground color of the cursor (for block cursors) to the same dark blue as the background.\r\n\r\n6. `selection_bg = \"#033259\"`: This sets the background color for selected text to a dark blue.\r\n\r\n7. `selection_fg = \"#CBE0F0\"`: This sets the foreground color for selected text to the same light blue-gray as the default text.\r\n\r\n8. `ansi = {...}`: This array defines the colors for the first 8 ANSI color codes (0-7). These are used for basic color formatting in the terminal.\r\n\r\n9. `brights = {...}`: This array defines the colors for the bright versions of the ANSI color codes (8-15). These are often used for bold or highlighted text.\r\n\r\nIn both `ansi` and `brights` arrays, the colors represent:\r\n- Black\r\n- Red\r\n- Green\r\n- Yellow\r\n- Blue\r\n- Magenta\r\n- Cyan\r\n- White\r\n\r\nThis color scheme appears to be a custom \"coolnight\" theme, with a dark blue background and various bright, contrasting colors for text and highlights. It's designed to be easy on the eyes while providing good contrast for readability.\r\n\r\nAfter saving this file and restarting Wezterm, you should see the changes take effect. You can further customize this configuration to your liking by adjusting colors, key bindings, and other settings as needed.\r\n\r\nRemember, Wezterm's configuration is dynamic and in Lua, so you can add conditional logic, functions, and even pull in external data if you want to create a more complex setup.\r\n\r\n### Install powerlevel10k theme\r\n\r\nPowerlevel10k is a highly customizable theme for Zsh that emphasizes speed, flexibility, and out-of-the-box experience. It's designed to make your command line informative and visually appealing without sacrificing performance.\r\n\r\nTo install Powerlevel10k using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install powerlevel10k\r\necho \"source $(brew --prefix)/share/powerlevel10k/powerlevel10k.zsh-theme\" >> ~/.zshrc\r\n```\r\n\r\nLet's break down what these commands do:\r\n\r\n1. The first command installs Powerlevel10k using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the Powerlevel10k theme. This ensures that the theme is loaded every time you start a new Zsh session.\r\n\r\nAfter running these commands, you need to apply the changes to your current session:\r\n\r\n```sh\r\nsource ~/.zshrc\r\n```\r\n\r\nThis command reloads your Zsh configuration, applying the Powerlevel10k theme.\r\n\r\nWhen you first run this, you'll likely be greeted with the Powerlevel10k configuration wizard. This interactive process allows you to customize various aspects of your prompt, including:\r\n\r\n- The style of the prompt\r\n- Which segments to display (git status, time, etc.)\r\n- Color scheme\r\n- Icons and glyphs\r\n\r\nFollow the on-screen instructions to set up Powerlevel10k according to your preferences. Don't worry if you're not sure about some options – you can always reconfigure later by running `p10k configure`.\r\n\r\nOnce configured, you'll have a highly informative and visually appealing prompt that can show git status, execution time of commands, and much more, all while maintaining excellent performance.\r\n\r\nRemember, you can always fine-tune your Powerlevel10k configuration by editing the `~/.p10k.zsh` file that was created during the configuration process.\r\n\r\n\r\n### Setup zsh-autosuggestions plugin\r\n\r\nZsh-autosuggestions is a powerful plugin that suggests commands as you type based on your command history and completions. It can significantly speed up your command-line workflow by reducing the amount of typing needed for frequently used commands.\r\n\r\nFor a more detailed guide on enabling command autocomplete in Zsh, check out this article: [Enable Command Autocomplete in Zsh](https://www.bitdoze.com/enable-command-autocomplete-in-zsh/)\r\n\r\nTo install zsh-autosuggestions using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install zsh-autosuggestions\r\necho \"source $(brew --prefix)/share/zsh-autosuggestions/zsh-autosuggestions.zsh\" >> ~/.zshrc\r\nsource ~/.zshrc\r\n```\r\n\r\nLet's break down these commands:\r\n\r\n1. The first command installs the zsh-autosuggestions plugin using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the zsh-autosuggestions plugin. This ensures that the plugin is loaded every time you start a new Zsh session.\r\n3. The third command reloads your Zsh configuration, applying the changes immediately.\r\n\r\nAfter running these commands, you should see autosuggestions appear as you type in your terminal. The suggestions will be shown in a faded gray color. To accept a suggestion, you can typically press the right arrow key or End key.\r\n\r\nYou can customize the behavior of zsh-autosuggestions by adding configuration options to your `.zshrc` file. For example, you can change the color of the suggestions or modify the key bindings used to accept suggestions.\r\n\r\nWith zsh-autosuggestions set up, you'll find that entering commands becomes faster and more efficient, especially for long or complex commands that you use frequently.\r\n\r\n### Setup zsh-syntax-highlighting\r\n\r\nZsh-syntax-highlighting is a plugin that provides syntax highlighting for the shell zsh. It enables highlighting of commands while they are typed at a zsh prompt into an interactive terminal. This helps in catching syntax errors, missing quotes, and other common mistakes before executing a command.\r\n\r\nFor a more comprehensive guide on enabling syntax highlighting in Zsh, check out this article: [Enable Syntax Highlighting in Zsh](https://www.bitdoze.com/enable-syntax-highlighting-zsh/)\r\n\r\nTo install zsh-syntax-highlighting using Homebrew, run the following commands:\r\n\r\n```sh\r\nbrew install zsh-syntax-highlighting\r\necho \"source $(brew --prefix)/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\" >> ~/.zshrc\r\nsource ~/.zshrc\r\n```\r\n\r\nLet's break down these commands:\r\n\r\n1. The first command installs the zsh-syntax-highlighting plugin using Homebrew.\r\n2. The second command adds a line to your `.zshrc` file that sources the zsh-syntax-highlighting plugin. This ensures that the plugin is loaded every time you start a new Zsh session.\r\n3. The third command reloads your Zsh configuration, applying the changes immediately.\r\n\r\nAfter running these commands, you should see syntax highlighting in your terminal as you type commands. Typically, correctly typed commands will appear in green, while errors or unknown commands will appear in red.\r\n\r\nSome key features of zsh-syntax-highlighting include:\r\n\r\n- Command highlighting: Valid commands are highlighted differently from invalid ones.\r\n- Option highlighting: Valid options for commands are highlighted.\r\n- Path highlighting: Existing file paths are underlined.\r\n- Bracket matching: Brackets are highlighted in pairs.\r\n\r\nYou can customize the colors and styles used by zsh-syntax-highlighting by adding configuration options to your `.zshrc` file. This allows you to tailor the appearance to your preferences or to match your terminal's color scheme.\r\n\r\nWith zsh-syntax-highlighting set up, you'll find it easier to spot and correct mistakes in your commands before executing them, leading to a more efficient and error-free command-line experience.\r\n\r\n## Enhance Wezterm with tmux and zoxide\r\n\r\nWhile Wezterm is already a powerful terminal emulator, we can further enhance its capabilities by integrating it with tmux for advanced session management and zoxide for smarter directory navigation.\r\n\r\n### What is tmux and how can it help\r\n\r\nTmux (Terminal Multiplexer) is a powerful tool that allows you to create multiple terminal sessions within a single window. It's particularly useful for:\r\n\r\n- Managing multiple terminal sessions\r\n- Running long processes in the background\r\n- Sharing terminal sessions with other users\r\n- Preserving your terminal setup across system reboots\r\n\r\nTo learn more about tmux and its basic usage, check out this guide: [Tmux Basics](https://www.bitdoze.com/tmux-basics/)\r\n\r\nTo install tmux using Homebrew, run:\r\n\r\n```sh\r\nbrew install tmux\r\n```\r\n\r\nAfter installation, you can start a new tmux session by simply typing `tmux` in your terminal.\r\n\r\n### What is zoxide and how can it help\r\n\r\nZoxide is a smarter cd command that helps you navigate your filesystem more efficiently. It remembers which directories you use most frequently, so you can \"jump\" to them in just a few keystrokes.\r\n\r\nKey features of zoxide include:\r\n\r\n- Faster navigation to frequently-used directories\r\n- Fuzzy matching for directory names\r\n- Integration with common shells and file managers\r\n\r\nFor a detailed guide on using zoxide, refer to this article: [Zoxide Guide](https://www.bitdoze.com/zoxide/)\r\n\r\nTo install zoxide using Homebrew, run:\r\n\r\n```sh\r\nbrew install zoxide\r\n```\r\n\r\nThen, add the following line to your `~/.zshrc` file to initialize zoxide:\r\n\r\n```sh\r\neval \"$(zoxide init zsh)\"\r\n```\r\n\r\nAfter restarting your shell or running `source ~/.zshrc`, you can start using zoxide. For example, use `z` instead of `cd` to navigate to directories.\r\n\r\nBy integrating tmux and zoxide with Wezterm, you're creating a powerful, efficient terminal environment. Tmux allows you to manage complex workflows with multiple panes and windows, while zoxide speeds up your navigation between projects and directories. Together with Wezterm's GPU acceleration and customizability, you'll have a terminal setup that significantly boosts your productivity.\r\n\r\n## Conclusions\r\n\r\nBy following this guide, you've not only set up a visually appealing terminal but also created a powerful development environment that can significantly improve your workflow efficiency.\r\n\r\nRemember, the beauty of this setup lies in its flexibility. Feel free to further customize your Wezterm configuration, explore additional zsh plugins, or tweak your Powerlevel10k settings to perfectly match your working style.\r\n\r\nAs you become more comfortable with your new setup, you may find yourself discovering new ways to optimize your command-line experience. Embrace this process of continuous improvement – after all, one of the joys of being a developer is constantly refining our tools and workflows.\r\n\r\nWith your new Wezterm setup, you're well-equipped to tackle your development tasks with increased speed, clarity, and enjoyment. Happy coding!","src/content/posts/install-wezterm-mac.mdx",[1324],"../../assets/images/24/10/wezterm-install.jpeg","fb4096084a35371d","install-wezterm-mac.mdx","keila-setup",{id:1327,data:1329,body:1339,filePath:1340,assetImports:1341,digest:1343,legacyId:1344,deferredRender:32},{title:1330,description:1331,date:1332,image:1333,authors:1334,categories:1335,tags:1336,canonical:1338},"How to Launch Your Own Newsletter Platform with Keila and Docker","Deploy Keila self-hosted newsletter on your server with Docker and EasyPanel. ",["Date","2023-11-28T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/keila-setup.jpeg",[19],[77],[662,1337],"email","https://www.bitdoze.com/keila-setup/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/11/keila-project.png\";\r\nimport imag2 from \"../../assets/images/23/11/creae-keila.png\";\r\nimport imag3 from \"../../assets/images/23/11/keila-interface.png\";\r\nimport imag4 from \"../../assets/images/23/11/keila-stats.png\";\r\n\r\n[Keila](https://www.keila.io/) is an open source email newsletter tool that allows you to create and send engaging campaigns to your contacts.\r\n\r\nSome of the features of Keila are:\r\n\r\n- Easy import of contacts: You can upload a CSV file with your existing contacts and start sending newsletters right away.\r\n- Custom data and tags: You can add any information you want to your contacts and use it to filter them or personalize your campaigns.\r\n- Smart segments: You can create subsets of your contacts based on criteria you define with a visual editor or a query language.\r\n- Privacy and data ownership: You can host Keila on your servers and keep full control of your data. Keila also respects the privacy of your contacts and gives you useful statistics without tracking them.\r\n\r\nKeila can be deployed easily to your server with the help of Docker, you will need to use an SMTP to send the campaigns, if you don't have a SMTP service that you are using you can check: [Mail.Baby Review](https://www.bitdoze.com/mail-baby-review/) a service that can help you send emails on a budget.\r\n\r\nKeila can help you create forms to collect emails but does not have a double opt-in option like [ListMonk](https://listmonk.app/) yet, this is under work.\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n\r\n## How to Install Keila\r\n\r\nIn this article, we are going to see everything that needs to be done to set Keila and start sending newsletters, we are going to use EasyPanel for this as it offers easy control and setup.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/lpvAGQwO0RA\"\r\n  label=\"Keila Setup\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1.Deploy a Hetzner VPS\r\n\r\nThe first part is to have a server ready where we can deploy Easypanel, my preferred choice is Easypanel. The recommendation for Easypanel is to have at least 2 CPUs and 2 GB of memory and you can use an Ubuntu installation. In the video, we go into detail about how you can do that on Hetzner. You can check this [Hetzner Review](https://www.wpdoze.com/hetzner-cloud-review/) for more details if you are not aware of Hetzner and what it can do.\r\n\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n### 2. Install Easypanel.io\r\n\r\nTo do this you just need to run a simple command and Easypanel will be installed:\r\n\r\n```sh\r\ncurl -sSL https://get.easypanel.io | sh\r\n```\r\n\r\nThe complete [EasyPanel Setup tutorial](https://www.bitdoze.com/easypanel-modern-server-control-panel/) can be followed to properly set EasyPanel.\r\n\r\n### 3. Create an Easypanel Project\r\n\r\nAfter you install EasyPanel you will need to add your first project and go and choose the Keila from the templates they have, as in the below picture:\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"EasyPanel Keila Choose\"\r\n/>\r\n\r\n### 4. Create Keila in EaasyPanel\r\n\r\nNext, you will need to add the details for deploying Keila:\r\n\r\n- App Service Name: name of the app in easypanel\r\n- App Service Image: use `pentacent/keila:latest` to use the latest one.\r\n- Database Service Name: name for the DB container service\r\n- SMTP details: System From Email, SMTP Email Host,SMTP Email Port, SMTP Email Username, SMTP Email Password, if you don't have an SMTP provider you can check: [Mail.Baby Review](https://www.bitdoze.com/mail-baby-review/)\r\n\r\n<Picture\r\n  src={imag2}\r\n  alt=\"Create Keila\"\r\n/>\r\n\r\nAt the end you hit `create`.\r\n\r\nThen you will be prompted to use the generated user and password that can be changed later:\r\n\r\nLogin with changeme@easypanel.io:password123 - change this from the administration panel.\r\n\r\n### 5. Add your domain to the installation\r\n\r\nBy default, easypanel will use his domain for this, but you can add your domain or subdomain to access Keila, to do this you need to go under project `domains` in EasyPanel and add your own. \\\r\nYou need first to point the A record to the EasyPanel server IP for the domain for this to work.\r\n\r\n### 6. Configure Keila\r\n\r\nAfter you set up the domain you can access Keila and change the password for the admin user, also you can add your first project and senders you have. You can use different senders for different projects. The video has more details about this.\r\n\r\n<Picture\r\n  src={imag3}\r\n  alt=\"Keila Interface\"\r\n/>\r\n\r\nNow you can import your contacts, create forms and start sending newsletters.\r\n\r\nKeila has statistics for your campaigns and after you send one you will have the stats as below:\r\n\r\n<Picture\r\n  src={imag4}\r\n  alt=\"EasyPanel Keila Choose\"\r\n/>\r\n\r\n## Conclusion\r\n\r\n[Keila](https://www.keila.io/) is a light newsletter alternative that you can self-host easily and start sending emails fast, it has statistics about campaigns sent and can help you manage things easily. With the help of EasyPanel you can host it easily.","src/content/posts/keila-setup.mdx",[1342],"../../assets/images/23/11/keila-setup.jpeg","fa9c219b6ca39d3a","keila-setup.mdx","langfuse-docker-install",{id:1345,data:1347,body:1356,filePath:1357,assetImports:1358,digest:1360,legacyId:1361,deferredRender:32},{title:1348,description:1349,date:1350,image:1351,authors:1352,categories:1353,tags:1354,canonical:1355},"Langfuse Docker Install: Self Hosted LangSmith Alternative","Learn how you can install Langfuse with docker compose and Postgres DB and take advantage of the observability software for your AI apps.",["Date","2024-07-19T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/langfuse-docker-install.jpeg",[19],[98],[242],"https://www.bitdoze.com/langfuse-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\nimport imag2 from \"../../assets/images/24/07/langfuse-ui.png\";\r\n\r\n[Langfuse](https://langfuse.com/) is an open-source LLM engineering platform that offers a comprehensive suite of tools for observability, metrics, evaluations, prompt management, and dataset handling. As a self-hosted alternative to LangSmith, Langfuse provides developers with powerful capabilities for managing and optimizing their language model applications.\r\n\r\n**Key features of Langfuse include:**\r\n\r\n1. **Observability:** Langfuse allows you to instrument your application and ingest traces, providing detailed insights into your LLM's performance and behavior.\r\n\r\n2. **Analytics:** Track important metrics such as cost, latency, and quality through customizable dashboards and data exports.\r\n\r\n3. **Prompt Management:** Manage, version, and deploy prompts directly within the Langfuse platform, streamlining your workflow.\r\n\r\n4. **Evaluations:** Collect and calculate scores for your LLM completions, including model-based evaluations, user feedback, and manual scoring.\r\n\r\n5. **Experimentation:** Test and track application behavior before deploying new versions, using datasets to benchmark performance.\r\n\r\n6. **LLM Playground:** A built-in environment for testing and iterating on prompts.\r\n\r\n7. **Integrations:** Langfuse offers seamless integration with popular LLM frameworks like LlamaIndex and Langchain.\r\n\r\nFor those looking to self-host Langfuse using Docker, the platform provides a straightforward deployment process. Langfuse can be deployed as a single Docker container, making it simple to set up and maintain.\r\n\r\n## Step-by-Step Guide to Installing LangFuse on Docker\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/Gm4HBrK63AM\"\r\n  label=\"How to Install langfuse with Docker Compose\"\r\n/>\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host Langfusew, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n- OR reverse proxy with CloudPanel you can check: [Setup CloudPanel As Reverse Proxy with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\n### 2. Langfuse Docker Compose File\r\n\r\n```yaml\r\nversion: \"3.9\"\r\nservices:\r\n  langfuse-db:\r\n    image: postgres:16-alpine\r\n    container_name: LangFuse-DB\r\n    hostname: langfuse-db\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    volumes:\r\n      - ./langfuse-db:/var/lib/postgresql/data:rw\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\n\r\n  langfuse:\r\n    image: langfuse/langfuse:latest\r\n    container_name: LangFuse\r\n    ports:\r\n      - 5061:3000\r\n    restart: unless-stopped\r\n    depends_on:\r\n      - langfuse-db\r\n    environment:\r\n      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@langfuse-db:5432/${POSTGRES_DB}?sslmode=disable\r\n      NEXTAUTH_URL: https://longfuse.bitdoze.com\r\n      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET}\r\n      SALT: ${SALT}\r\n      LANGFUSE_CSP_ENFORCE_HTTPS: true\r\n      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: false\r\n      TELEMETRY_ENABLED: false\r\n      AUTH_DISABLE_SIGNUP: ${AUTH_DISABLE_SIGNUP}\r\n```\r\n\r\nThis Docker Compose file defines two services:\r\n\r\n1. **langfuse-db**: This service sets up a PostgreSQL database using the official PostgreSQL Alpine image. It includes:\r\n\r\n   - A healthcheck to ensure the database is ready before other services start\r\n   - A volume mount to persist data\r\n   - Environment variables for database configuration\r\n   - A restart policy to handle failures\r\n\r\n2. **langfuse**: This service runs the main LangFuse application. It:\r\n   - Uses the latest LangFuse Docker image\r\n   - Exposes port 5061 for web access\r\n   - Depends on the langfuse-db service\r\n   - Sets various environment variables for configuration\r\n\r\nKey points to note:\r\n\r\n- The use of environment variables `${VARIABLE_NAME}` allows for easy configuration without modifying the Docker Compose file directly.\r\n- The healthcheck in the database service ensures that the database is fully operational before LangFuse attempts to connect to it.\r\n- The volume mount for the database ensures that your data persists even if the container is stopped or removed.\r\n- The LANGFUSE_CSP_ENFORCE_HTTPS setting is set to true, enforcing HTTPS for enhanced security.\r\n- Experimental features and telemetry are disabled by default.\r\n\r\n### 3. .env File for LangFuse\r\n\r\nTo use the environment variables referenced in the Docker Compose file, you'll need to create a `.env` file in the same directory. Here's an example of what it should contain:\r\n\r\n```sh\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='langfuse'\r\nNEXTAUTH_SECRET=aOlY0UgIitolkrZUWoWyVRwuo2BpUPKB/t2l2ufbXSw=\r\nSALT=VBQk4V98zZ8L8xwpvI696Ixv88D5QfrciLU4fx/C4VQ=\r\nAUTH_DISABLE_SIGNUP=false\r\n```\r\n\r\nImportant security considerations:\r\n\r\n- Replace 'user' and 'pass' with a strong username and password for your PostgreSQL database.\r\n- The NEXTAUTH_SECRET and SALT values should be unique, randomly generated strings. You can use a tool like [OpenSSL](https://www.openssl.org/) to generate these securely.\r\n\r\nFor example, to generate a secure NEXTAUTH_SECRET, you could use:\r\n\r\n```sh\r\nopenssl rand -base64 32\r\n```\r\n\r\nRemember, never share these secrets or commit them to version control. Treat them with the same level of security as you would any other sensitive credentials.\r\n\r\nYou have all the variables that can be used in [Langfuse Self-Hosting Guide](https://langfuse.com/docs/deployment/self-host)\r\n\r\n### 4. Deploying the Docker Compose File for LangFuse\r\n\r\nOnce you have your Docker Compose and .env files set up, deploying LangFuse is straightforward. Simply run the following command in the directory containing your Docker Compose file:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nThis command will:\r\n\r\n1. Pull the necessary Docker images if they're not already present on your system.\r\n2. Create and start the containers defined in your Docker Compose file.\r\n3. Run the containers in detached mode (-d), allowing them to run in the background.\r\n\r\nAfter running this command, you should see output indicating that the containers are being created and started. Once complete, you can verify that the containers are running with:\r\n\r\n```sh\r\ndocker-compose ps\r\n```\r\n\r\nThis will show you the status of your LangFuse and PostgreSQL containers.\r\n\r\n### 5. Implementing SSL with CloudFlare Tunnels for Langfuse\r\n\r\n[CloudFlare Tunnels](https://www.cloudflare.com/products/tunnel/) offer a innovative solution for securely connecting your web applications to the internet without the need for public IP addresses or opening inbound ports on your firewall. This service, part of CloudFlare's suite of security and performance tools, provides a secure tunnel between your origin server and CloudFlare's edge network.\r\n\r\nCloudFlare Tunnels operate on a simple yet powerful principle:\r\n\r\n1. **Outbound Connection**: Your server initiates an outbound connection to CloudFlare's network using the CloudFlare daemon (cloudflared).\r\n2. **Tunnel Creation**: This connection establishes a secure tunnel between your origin and CloudFlare's edge.\r\n3. **Traffic Routing**: Incoming requests to your domain are routed through this tunnel to your origin server.\r\n4. **Response Delivery**: Responses from your server are sent back through the tunnel and delivered to the user.\r\n\r\nThis process effectively eliminates the need for traditional port forwarding or firewall configuration, as all traffic flows through the secure tunnel.\r\n\r\nGo in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 6. Access the Langfuse UI\r\n\r\nNow after you set the subdomain in Cloudflare tunnels you can go and access the aplication with the url. First you will be promted to create a username and a password and after you can access the apps.\r\n\r\nYou can create your first project and start tracking the AI apps you have. You need to create an API key so you can use it with Langfuse, in the video you will find all the details as well as integrating this with Flowise AI.\r\n\r\n<Picture src={imag2} alt=\"Langfuse UI\" />\r\n\r\n### 7. Disable Signups\r\n\r\nBy default anyone can sign up and create an account, after you create you account you can alter the `.env` and change the `AUTH_DISABLE_SIGNUP=false` to `AUTH_DISABLE_SIGNUP=true`\r\nthis will not allow for new accounts to be created via sign up.\r\n\r\nYou need to restart your container for this change to be activated:\r\n\r\n```sh\r\ndocker compose pull\r\ndocker compose up -d\r\n```\r\n\r\n## Conclusion\r\n\r\nIn conclusion, as AI continues to transform industries and drive innovation, the importance of robust observability tools cannot be overstated. LangFuse on Docker offers a powerful, flexible, and cost-effective solution for organizations looking to gain deeper insights into their AI models' performance while maintaining control over their data and infrastructure.\r\n\r\nBy implementing self-hosted observability solutions like LangFuse, AI developers and organizations can unlock new levels of performance, reliability, and innovation in their AI projects. The time to act is now – start exploring how LangFuse on Docker can revolutionize your AI development process and drive your projects to new heights of success.\r\n\r\nIf you're interested in exploring more Docker containers for your home server or self-hosted setup, including other AI and productivity tools, check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you build a powerful and versatile self-hosted environment that can complement your LangFuse installation and enhance your overall AI development ecosystem.","src/content/posts/langfuse-docker-install.mdx",[1359],"../../assets/images/24/07/langfuse-docker-install.jpeg","60d1712731d9c5e7","langfuse-docker-install.mdx","langflow-docker-install",{id:1362,data:1364,body:1373,filePath:1374,assetImports:1375,digest:1377,legacyId:1378,deferredRender:32},{title:1365,description:1366,date:1367,image:1368,authors:1369,categories:1370,tags:1371,canonical:1372},"How to Install LangFlow with Docker Compose and Add SSL Over CloudFlare Tunnels","Learn how you can install LangFlow with docker compose  and Postgres DB and take advantage of no-code AI flows. Add SSL over CloudFlare tunnels",["Date","2024-07-18T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/longflow-docker.jpeg",[19],[98],[242],"https://www.bitdoze.com/langflow-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/01/cloudflare-tunel-setup.png\";\r\nimport imag2 from \"../../assets/images/24/07/langflow-ui.png\";\r\n\r\n[LangFlow](https://www.langflow.org/) is an open-source platform that provides a visual and intuitive way to create, prototype, and deploy AI workflows. It's designed to make the process of working with large language models (LLMs) more accessible to developers, data scientists, and even non-technical users. By leveraging a drag-and-drop interface, LangFlow allows users to construct complex AI pipelines without diving deep into code.\r\n\r\n## Comparison with Flowise AI\r\n\r\nWhile LangFlow has gained significant popularity, it's worth comparing it to another player in the field: [Flowise AI](https://flowiseai.com/). Both platforms aim to simplify AI workflow creation. You can check [How to Install FlowiseAI with Docker Compose](https://www.bitdoze.com/flowiseai-install/) for more on Flowise\r\n\r\nWhile both platforms offer visual AI workflow creation, LangFlow tends to provide more flexibility and customization options. It's particularly well-suited for users who need to integrate a wide range of LLMs and tools into their workflows. On the other hand, Flowise AI might be a better choice for those looking for a more streamlined, out-of-the-box solution with a gentler learning curve.\r\n\r\nThe choice between LangFlow and Flowise AI often comes down to specific project requirements, team expertise, and the desired level of customization. Many organizations find value in experimenting with both platforms to determine which best suits their needs.\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## Setting Up Langflow with Docker Compose\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/mkSJV0UPDH8\"\r\n  label=\"How to Install LangFlow with Docker Compose\"\r\n/>\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host LangFlow, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n- CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n- OR reverse proxy with CloudPanel you can check: [Setup CloudPanel As Reverse Proxy with Docker and Dockge](https://www.bitdoze.com/cloudpanel-setup-dockge/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\n### 2. Docker Compose File\r\n\r\n```yml\r\nversion: \"3.9\"\r\nservices:\r\n  langflow-db:\r\n    image: postgres:16-alpine\r\n    container_name: Langflow-DB\r\n    hostname: langflow-db\r\n    healthcheck:\r\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\"]\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    volumes:\r\n      - ./langflow-db:/var/lib/postgresql/data:rw\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\n\r\n  langflow:\r\n    image: langflowai/langflow:latest\r\n    container_name: Langflow\r\n    user: root\r\n    ports:\r\n      - 5060:7860\r\n    healthcheck:\r\n      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/7860' || exit 1\r\n      interval: 10s\r\n      timeout: 5s\r\n      retries: 3\r\n      start_period: 90s\r\n    restart: on-failure:5\r\n    depends_on:\r\n      - langflow-db\r\n    environment:\r\n      LANGFLOW_DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@langflow-db:5432/${POSTGRES_DB}?sslmode=disable\r\n      LANGFLOW_CONFIG_DIR: /var/lib/langflow\r\n      LANGFLOW_SUPERUSER: ${LANGFLOW_SUPERUSER}\r\n      LANGFLOW_SUPERUSER_PASSWORD: ${LANGFLOW_SUPERUSER_PASSWORD}\r\n      LANGFLOW_AUTO_LOGIN: False\r\n    volumes:\r\n      - ./langflow:/var/lib/langflow:rw\r\n```\r\n\r\nLet's break down this Docker Compose file and explain what each section is doing:\r\n\r\n1. **Version**: The `version: \"3.9\"` specifies the version of the Docker Compose file format we're using.\r\n\r\n2. **Services**: We define two services - `langflow-db` and `langflow`.\r\n\r\n3. **langflow-db service**:\r\n\r\n   - Uses the `postgres:16-alpine` image, which is a lightweight PostgreSQL database.\r\n   - Sets up a health check to ensure the database is ready before other services start.\r\n   - Mounts a volume to persist database data.\r\n   - Uses environment variables for database configuration.\r\n   - Restarts on failure, with a maximum of 5 attempts.\r\n\r\n4. **langflow service**:\r\n   - Uses the latest LangFlow image.\r\n   - Exposes port 5060 on the host, mapping to port 7860 in the container.\r\n   - Implements a health check to verify the service is running correctly.\r\n   - Depends on the `langflow-db` service, ensuring the database is up before starting.\r\n   - Sets various environment variables for LangFlow configuration.\r\n   - Mounts a volume for persistent storage of LangFlow data.\r\n\r\nThis configuration allows for a robust and scalable LangFlow setup, with separate containers for the application and its database, health checks to ensure reliability, and persistent storage for both the database and LangFlow data.\r\n\r\n### 3 .env file for LangFlow\r\n\r\nTo keep our sensitive information secure and our configuration flexible, we'll use a `.env` file to store environment variables. Create a file named `.env` in the same directory as your `docker-compose.yml` with the following content:\r\n\r\n```sh\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='langflow'\r\nLANGFLOW_SUPERUSER=bitdoze\r\nLANGFLOW_SUPERUSER_PASSWORD=bitdoze\r\n```\r\n\r\nThis file sets up the necessary environment variables for our PostgreSQL database and LangFlow superuser. Remember to replace these placeholder values with secure, unique values for your production environment.\r\n\r\n### 4. Deploy The Docker Compose File for LangFlow\r\n\r\nWith our `docker-compose.yml` and `.env` files in place, we're ready to deploy LangFlow. Open a terminal, navigate to the directory containing these files, and run:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nThis command will start our LangFlow setup in detached mode, allowing it to run in the background.\r\n\r\n### 5. Implementing SSL with CloudFlare Tunnels\r\n\r\n[CloudFlare Tunnels](https://www.cloudflare.com/products/tunnel/) offer a innovative solution for securely connecting your web applications to the internet without the need for public IP addresses or opening inbound ports on your firewall. This service, part of CloudFlare's suite of security and performance tools, provides a secure tunnel between your origin server and CloudFlare's edge network.\r\n\r\nCloudFlare Tunnels operate on a simple yet powerful principle:\r\n\r\n1. **Outbound Connection**: Your server initiates an outbound connection to CloudFlare's network using the CloudFlare daemon (cloudflared).\r\n2. **Tunnel Creation**: This connection establishes a secure tunnel between your origin and CloudFlare's edge.\r\n3. **Traffic Routing**: Incoming requests to your domain are routed through this tunnel to your origin server.\r\n4. **Response Delivery**: Responses from your server are sent back through the tunnel and delivered to the user.\r\n\r\nThis process effectively eliminates the need for traditional port forwarding or firewall configuration, as all traffic flows through the secure tunnel.\r\n\r\nGo in **Access - Tunnels** and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port.\r\n\r\n<Picture src={imag1} alt=\"Cloudflare Tunnel setup\" />\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\n### 6. Access the LangFlow UI\r\n\r\nAfter you set the CloudFlare tunnels you can go and access you LangFlow UI and start building your first flow. You can logun with the user and password that you have set in the .env file.\r\n\r\n<Picture src={imag2} alt=\"LanfFlow UI\" />\r\n\r\n\r\nBy following this guide, you've successfully set up LangFlow using Docker Compose, configured it with a PostgreSQL database for persistence, and secured it with SSL through CloudFlare Tunnels. This setup provides you with a powerful, flexible, and secure environment for creating and managing AI workflows.\r\n\r\nLangFlow's visual interface makes it easier to experiment with different AI models and create complex workflows without deep coding knowledge. As you explore its capabilities, you'll find it to be a valuable tool for prototyping AI applications and streamlining your development process.\r\n\r\nIf you're interested in exploring more Docker containers for your home server or self-hosted setup, including other AI and productivity tools, check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you build a powerful and versatile self-hosted environment.","src/content/posts/langflow-docker-install.mdx",[1376],"../../assets/images/24/07/longflow-docker.jpeg","66105840827b60d8","langflow-docker-install.mdx","link-github-with-ssh-maco-linux",{id:1379,data:1381,body:1390,filePath:1391,assetImports:1392,digest:1394,legacyId:1395,deferredRender:32},{title:1382,description:1383,date:1384,image:1385,authors:1386,categories:1387,tags:1388,canonical:1389},"Link GitHub with A SSH Key to MacOS or Linux","A tutorial that you can follow to create your first GitHub repo and link it via SSH to your laptop.",["Date","2022-10-07T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/2210/link-github-with-ssh.jpeg",[19],[21],[1216,24],"https://www.bitdoze.com/link-github-with-ssh-maco-linux/","import { Picture } from \"astro:assets\";\r\nimport githubssh1 from \"../../assets/images/2210/github_ssh_key.jpeg\";\r\nimport githubssh2 from \"../../assets/images/2210/github_add_ssh_key.jpeg\";\r\nimport githubssh3 from \"../../assets/images/2210/create_repo.jpeg\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[GitHub](https://github.com/) is a public repository where you can store your git projects. Most of the services offer integrations with GitHub and you can use it to store your projects or to store your static websites like Gatsy or Astro.\r\n\r\nIn this article, we will see how you can create a private repo on GitHub and connect it to your laptop with an SSH key.\r\n\r\n## Link GitHub with A SSH Key to MacOS or Linux\r\n\r\nThe first thing should be to have a GitHub account, to do that you just register to them. After we can go and proceed with the SSH key creation and add it to GitHub.\r\n\r\n### Generate an SSH Key on Mac or Linux\r\n\r\nIn this step we are going to generate the Key the below command should work on all systems:\r\n\r\n```bash\r\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\r\n```\r\n\r\nWhen you will get the prompts just hit enter to place the key into the default ~/.ssh directory. I will not set up a passphrase.\r\n\r\n### Get The Public Key and add it to GitHub\r\n\r\nNow that the key is created we need to get the public key and add it to GitHub.\r\n\r\n```bash\r\ncat ~/.ssh/id_rsa.pub\r\n```\r\n\r\n### Add the SSH Key to GitHub\r\n\r\nThe output of the above command should be added to GitHub under **Profile - Settings - SSH and GPG Keys - New SSH key**\r\n\r\n<Picture\r\n  src={githubssh1}\r\n  alt=\"GitHub Profile add Key\"\r\n/>\r\n\r\nThere you set a Title and under Key, you add the output from above.\r\n\r\n<Picture\r\n  src={githubssh2}\r\n  alt=\"GitHub Profile add Key 2\"\r\n/>\r\n\r\nNow your GitHub account should be linked to the laptop and you can\r\n\r\n## Youtube Video With Details\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/aGWACCA2Kcg\"\r\n  label=\"Link GitHub with A SSH Key to MacOS or Linux\"\r\n/>\r\n\r\n## Create A GitHub Repo and Push From Your Laptop\r\n\r\nNow what remains to do is to create our GitHub repo and push it to GitHub. From your profile on the left side hit the **New Repo** and choose the below details:\r\n\r\n- **Repository name** - add the name you like your repo to have, I have used test-repo\r\n- **Description** - set a description for your repo, this is optional\r\n- **Private or Public** - chose if the repo should be only for you.\r\n\r\nAnd that's about it after you just hit the **Create repository**\r\n\r\n<Picture\r\n  src={githubssh3}\r\n  alt=\"GitHub create repo\"\r\n/>\r\n\r\nAfter we should go into our laptop and create a directory where the repo should be and initiate it.\r\n\r\nCreate the directory:\r\n\r\n```bash\r\nmkdir test-repo\r\n```\r\n\r\nGo and initiate it, install git if you don,t have it:\r\n\r\n```bash\r\n#mac install\r\nbrew install git\r\n#Ubuntu install\r\nsudo apt install git\r\n#go into the folder\r\ncd test-repo\r\n#initiate the repo\r\ngit init\r\n```\r\n\r\nAdd a file to the repo\"\r\n\r\n```bash\r\necho \"# test-repo for Bit Doze\" >> README.md\r\n```\r\n\r\nAfter a file is added you need to add it to the repo with, the below command:\r\n\r\n```bash\r\ngit add .\r\n```\r\n\r\nCommit the change:\r\n\r\n```bash\r\ngit commit -m \"added read me file\"\r\n```\r\n\r\nSet the branch to main:\r\n\r\n```bash\r\ngit branch -M main\r\n```\r\n\r\nLink the folder with the GitHub repo:\r\n\r\n```bash\r\ngit remote add origin git@github.com:bitdoze/test-repo.git\r\n```\r\n\r\nPush the changes:\r\n\r\n```bash\r\ngit push -u origin main\r\n```\r\n\r\nIn case you want to fetch an existing repo you can do it with:\r\n\r\n```bash\r\ngit clone git@github.com:<repo>\r\n```\r\n\r\nAfter you can do your modifications and push it back after.","src/content/posts/link-github-with-ssh-maco-linux.mdx",[1393],"../../assets/images/2210/link-github-with-ssh.jpeg","d3b545b13379f40e","link-github-with-ssh-maco-linux.mdx","litellm-docker-install",{id:1396,data:1398,body:1407,filePath:1408,assetImports:1409,digest:1411,legacyId:1412,deferredRender:32},{title:1399,description:1400,date:1401,image:1402,authors:1403,categories:1404,tags:1405,canonical:1406},"Install LiteLLM With Docker Compose and Simplify LLMs","Discover LiteLLM, the game-changing tool that simplifies LLM management, cuts costs, and boosts efficiency for developers and businesses alike.",["Date","2024-08-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/08/litellm-docker.jpeg",[19],[98],[242],"https://www.bitdoze.com/litellm-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/08/litellm-white.png\";\r\n\r\n[LiteLLM](https://www.litellm.ai/) is a tool designed to manage and interact with multiple large language models (LLMs) through a unified interface. It supports over 100 different LLMs, including those from HuggingFace, Bedrock, TogetherAI, and others, using the OpenAI format for API calls. This makes it a versatile solution for developers looking to integrate various LLMs into their applications.\r\n\r\n## Key Features of LiteLLM\r\n\r\n1. **Unified Interface**: LiteLLM offers a single, convenient interface to call over 100 different LLMs, such as those from HuggingFace, Bedrock, and TogetherAI, using the OpenAI API specification. This feature simplifies the integration and management of multiple models.\r\n\r\n2. **Cost Efficiency**: The model is optimized to reduce computational costs, making it a more affordable option for NLP tasks. This also contributes to lowering the environmental impact associated with running large-scale models.\r\n\r\n3. **Flexibility and Simplicity**: LiteLLM enables seamless transitions between various models with minimal code changes. Users can switch between models like GPT-3.5, O Lama, and Palm 2 effortlessly, which enhances the flexibility of application development.\r\n\r\n4. **Load Balancing**: LiteLLM can handle a high volume of requests, supporting up to 1,500 requests per second during load tests. This capability ensures efficient processing and distribution of requests across multiple models and deployments.\r\n\r\n5. **Compatibility**: It is compatible with several SDKs, including OpenAI, Anthropic, Mistral, LLamaIndex, and Langchain, allowing for diverse integration options across different platforms.\r\n\r\n<Picture src={imag1} alt=\"LiteLLM Diagram\" />\r\n\r\n## How LiteLLM Can Help You\r\n\r\nLiteLLM can significantly benefit developers and organizations by providing a streamlined and efficient approach to working with LLMs. By offering a unified interface, it reduces the complexity involved in managing multiple models, thereby saving time and resources. Its cost-effective nature makes it an attractive option for businesses looking to leverage NLP capabilities without incurring high expenses. Additionally, the tool's flexibility and compatibility with various SDKs and models make it a versatile solution for a wide range of applications, from chatbots to advanced data analysis.\r\n\r\nOverall, LiteLLM's features and capabilities make it a powerful tool for enhancing productivity and reducing costs in NLP projects.\r\n\r\n## LiteLLM Deploy Options\r\n\r\nWhen deploying LiteLLM using Docker Compose, there are notable differences between deploying with and without a database.\r\n\r\n### Deployment Without a Database\r\n\r\n- **Configuration Simplicity**: Deploying LiteLLM without a database involves fewer components, making the setup process simpler. You primarily need to configure the application using a configuration file (`litellm_config.yaml`) and run the Docker container with necessary environment variables and ports. We are going to see below.\r\n- **Use Cases**: This setup is suitable for scenarios where persistent data storage is not required, or the application can function with in-memory data or external APIs.\r\n\r\n### Deployment With a Database\r\n\r\n- **Database Requirement**: When deploying with a database, you need to set up a Postgres database and provide a `DATABASE_URL` in the environment variables. This setup is essential for applications that require persistent data storage.\r\n- **Additional Configuration**: You must configure the database connection details in the environment variables and ensure that the database service is up and running before the application starts. This might involve using Docker Compose to define the startup order.\r\n- **Data Persistence**: Using a database allows for persistent data storage, which is crucial for applications that handle significant amounts of data or require data integrity over time. It is important to use Docker volumes to ensure data is not lost when containers are stopped or removed.\r\n- **Complexity and Management**: Deploying with a database adds complexity, as you need to manage database backups, scaling, and performance tuning. It is recommended to use separate containers for the database and application to maintain a clean separation of concerns and facilitate easier management.\r\n\r\nIn summary, deploying LiteLLM with a database provides the advantage of data persistence and is suitable for production environments, while deploying without a database is simpler and more suited for development or scenarios where persistent storage is not critical.\r\n\r\n## Install LiteLLM with Docker Compose\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/j0IFFoCfihk\"\r\n  label=\"LiteLLM Docker Install\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### Prerequizites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host LiteLLM, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) You can use a VPS to have LiteLLM installed but performances will not be that good. In our test we are using a 8 CPUs 16 GB RAM and is bearly moving. Best will be to have a GPU powered system.\r\n- Traefic with Docker set up, you can check: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/) or [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n\r\nBelow we are going to check both options without a database and with a database so you can use the one that you need.\r\n\r\n### Install LiteLLM with Docker Compose - NO Database\r\n\r\nFirst let's create the LiteLLM config file, you can do so by checking the list [here](https://litellm.vercel.app/docs/providers)\r\n`litellm_config.yaml`\r\n\r\n```yaml\r\nmodel_list:\r\n  - model_name: gpt-4o\r\n    litellm_params:\r\n      model: gpt-4o\r\n  - model_name: claude-3-5-sonnet\r\n    litellm_params:\r\n      model: claude-3-5-sonnet-20240620\r\n```\r\n\r\nDocker Compose File\r\n\r\n```yaml\r\nlitellm:\r\n  image: ghcr.io/berriai/litellm:main-latest\r\n  restart: unless-stopped\r\n  command:\r\n    - \"--config=/litellm_config.yaml\"\r\n    - \"--detailed_debug\"\r\n  environment:\r\n    LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\r\n    OPENAI_API_KEY: ${OPENAI_API_KEY}\r\n    ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}\r\n  volumes:\r\n    - ./litellm_config.yaml:/litellm_config.yaml\r\n```\r\n\r\n- **Image**: Uses the Docker image `ghcr.io/berriai/litellm:main-latest`.\r\n- **Restart Policy**: Set to `unless-stopped`, ensuring automatic restarts unless manually stopped.\r\n- **Command**:\r\n  - `--config=/litellm_config.yaml`: Specifies the configuration file.\r\n  - `--detailed_debug`: Enables verbose logging for troubleshooting.\r\n- **Environment Variables**:\r\n  - `LITELLM_MASTER_KEY`: Master key for LiteLLM.\r\n  - `OPENAI_API_KEY`: API key for OpenAI.\r\n  - `ANTHROPIC_API_KEY`: API key for Anthropic.\r\n- **Volumes**: Mounts `./litellm_config.yaml` from the host to `/litellm_config.yaml` in the container for configuration access.\r\n\r\n`.env `file\r\n\r\n```sh\r\nLITELLM_MASTER_KEY=sk-1234\r\nOPENAI_API_KEY=<openaiapikey>\r\nANTHROPIC_API_KEY= <ANTHROPIC key>\r\n```\r\n\r\nFor a complete file with Open WebUI. I have created the [OpenWebUI deploy with Ollama](https://www.bitdoze.com/ollama-docker-install/) before and if you want to use Opem Web UI with LiteLLM below is the complete file:\r\n\r\n```yaml\r\nservices:\r\n  openWebUI:\r\n    image: ghcr.io/open-webui/open-webui:main\r\n    container_name: openwebui\r\n    hostname: openwebui\r\n    networks:\r\n      - traefik-net\r\n    restart: unless-stopped\r\n    volumes:\r\n      - ./open-webui-local:/app/backend/data\r\n    labels:\r\n      - traefik.enable=true\r\n      - traefik.http.routers.openwebui.rule=Host(`openwebui.domain.com`)\r\n      - traefik.http.routers.openwebui.entrypoints=https\r\n      - traefik.http.services.openwebui.loadbalancer.server.port=8080\r\n    environment:\r\n      OLLAMA_BASE_URLS: http://ollama:11434\r\n      OPENAI_API_KEY: ${LITELLM_MASTER_KEY}\r\n      OPENAI_API_BASE_URL: http://litellm:4000/v1\r\n  ollama:\r\n    image: ollama/ollama:latest\r\n    container_name: ollama\r\n    hostname: ollama\r\n    networks:\r\n      - traefik-net\r\n    volumes:\r\n      - ./ollama-local:/root/.ollama\r\n  litellm:\r\n    image: ghcr.io/berriai/litellm:main-latest\r\n    networks:\r\n      - traefik-net\r\n    restart: unless-stopped\r\n    command:\r\n      - --config=/litellm_config.yaml\r\n      - --detailed_debug\r\n    environment:\r\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\r\n      OPENAI_API_KEY: ${OPENAI_API_KEY}\r\n      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}\r\n    volumes:\r\n      - ./litellm_config.yaml:/litellm_config.yaml\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\n- OPENAI_API_BASE_URL is pointing to the container with Lite LLM\r\n\r\n### Install LiteLLM with Docker Compose - With Database\r\n\r\nTo have access to advanced features and save details to database you can install LiteLLM with Postgress and have access to the UI also.\r\n\r\n```yaml\r\nservices:\r\n  litellm:\r\n    image: ghcr.io/berriai/litellm:main-latest\r\n    networks:\r\n      - traefik-net\r\n    restart: unless-stopped\r\n    depends_on:\r\n      - litellm-db\r\n    environment:\r\n      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@litellm-db:5432/${POSTGRES_DB}\r\n      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}\r\n      UI_USERNAME: ${UI_USERNAME}\r\n      UI_PASSWORD: ${UI_PASSWORD}\r\n      STORE_MODEL_IN_DB: \"True\"\r\n    labels:\r\n      - traefik.enable=true\r\n      - traefik.http.routers.litellm.rule=Host(`litellm.domain.com`)\r\n      - traefik.http.routers.litellm.entrypoints=https\r\n      - traefik.http.services.litellm.loadbalancer.server.port=4000\r\n  litellm-db:\r\n    image: postgres:16-alpine\r\n    networks:\r\n      - traefik-net\r\n    healthcheck:\r\n      test:\r\n        - CMD-SHELL\r\n        - pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    volumes:\r\n      - ./litellm-db:/var/lib/postgresql/data:rw\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\n- **litellm**:\r\n\r\n  - **Image**: Uses the `litellm` image from GitHub Container Registry with the `main-latest` tag.\r\n  - **Networks**: Connects the `litellm` service to the `traefik-net` network, allowing it to communicate with Traefik.\r\n  - **Restart Policy**: Sets the restart policy to `unless-stopped`, meaning the container will always restart unless explicitly stopped.\r\n  - **Depends On**: Specifies that `litellm` depends on the `litellm-db` service, ensuring the database is available before starting the application.\r\n  - **Environment Variables**:\r\n    - `DATABASE_URL`: Constructs the PostgreSQL connection string using environment variables for the user, password, and database name.\r\n    - `LITELLM_MASTER_KEY`: A security key used for the application.\r\n    - `UI_USERNAME` and `UI_PASSWORD`: Credentials for accessing the application's user interface.\r\n    - `STORE_MODEL_IN_DB`: A flag set to `\"True\"` to indicate that models should be stored in the database.\r\n  - **Labels** (for Traefik):\r\n    - `traefik.enable=true`: Enables Traefik for the `litellm` service.\r\n    - `traefik.http.routers.litellm.rule=Host(litellm.domain.com)`: Specifies the domain name for routing requests to `litellm`.\r\n    - `traefik.http.routers.litellm.entrypoints=https`: Configures Traefik to use the HTTPS entry point for the `litellm` service.\r\n    - `traefik.http.services.litellm.loadbalancer.server.port=4000`: Sets the port for the `litellm` service to 4000.\r\n\r\n- **litellm-db**:\r\n  - **Image**: Uses the `postgres:16-alpine` image, which is a lightweight version of PostgreSQL based on Alpine Linux.\r\n  - **Networks**: Connects the `litellm-db` service to the `traefik-net` network for communication.\r\n  - **Health Check**:\r\n    - Command: `pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}` checks if the PostgreSQL server is ready to accept connections using the specified user and database.\r\n    - Interval: Runs the health check every 5 seconds.\r\n    - Timeout: Sets a timeout of 5 seconds for the health check.\r\n    - Retries: Specifies that the health check should retry up to 5 times before marking the service as unhealthy.\r\n  - **Volumes**: Maps a local directory (`./litellm-db`) to the PostgreSQL data directory (`/var/lib/postgresql/data`) using read-write permissions. This ensures data persistence for the database.\r\n  - **Environment Variables**:\r\n    - `POSTGRES_DB`: Specifies the name of the database to create.\r\n    - `POSTGRES_USER`: Sets the username for the PostgreSQL server.\r\n    - `POSTGRES_PASSWORD`: Defines the password for the PostgreSQL user.\r\n  - **Restart Policy**: Sets the restart policy to `on-failure:5`, meaning the container will restart up to 5 times if it fails.\r\n\r\n### Networks\r\n\r\n- **traefik-net**:\r\n  - Defines an external network named `traefik-net`, which is likely managed by the Traefik reverse proxy. This allows the `litellm` and `litellm-db` services to communicate with Traefik for routing and load balancing.\r\n\r\nYou can check: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/) or [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/) to see hwo to set up Traefik on your server.\r\n\r\n`.env` file:\r\nBelow are the configs files for you env file, you can change what you don't like:\r\n\r\n```sh\r\nLITELLM_MASTER_KEY=sk-1234\r\nPOSTGRES_DB=litellm\r\nPOSTGRES_USER=litellm\r\nPOSTGRES_PASSWORD=litellm\r\nUI_USERNAME=bitdoze\r\nUI_PASSWORD=bitdoze\r\n```\r\n\r\n## Conclusions\r\n\r\nThat's how you can install LiteLLM and use it in your projects. With LiteLLM, you will significantly simplify the process of integrating and managing multiple language models in your applications.\r\n\r\nLiteLLM's unified interface and support for over 100 different LLMs make it an invaluable tool for developers looking to leverage the power of various language models without the complexity of managing multiple APIs. Its cost efficiency, flexibility, and load balancing capabilities further enhance its value for both small-scale projects and large-scale deployments.\r\n\r\nBy following this guide, you've set up a powerful infrastructure that can serve as the backbone for your AI-driven applications. Whether you're building chatbots, content generation tools, or complex NLP systems, LiteLLM provides the flexibility and simplicity to streamline your development process.\r\n\r\nFor those interested in exploring more Docker containers to enhance your self-hosted setup or complement your LiteLLM installation, don't forget to check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource offers a wealth of options for various applications and services that can be seamlessly integrated into your Docker environment, helping you build a robust and versatile self-hosted ecosystem.","src/content/posts/litellm-docker-install.mdx",[1410],"../../assets/images/24/08/litellm-docker.jpeg","8b056fec2fb13968","litellm-docker-install.mdx",{id:449,data:1414,body:1423,filePath:1424,assetImports:1425,digest:1427,legacyId:1428,deferredRender:32},{title:1415,description:1416,date:1417,image:1418,authors:1419,categories:1420,tags:1421,canonical:1422},"Top 100+ Linux Commands You MUST Know","Unlock the full potential of Linux with this essential guide to the top 100+ commands. Enhance your command-line skills, improve productivity, and master the Linux environment with these must-know commands and tips.",["Date","2024-07-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/linux-commands.jpeg",[19],[98],[135],"https://www.bitdoze.com/linux-commands/","Linux, an open-source operating system, has become a cornerstone in the tech world, powering everything from servers and supercomputers to smartphones and home appliances. Its flexibility, security, and robustness make it a preferred choice for developers, system administrators, and tech enthusiasts alike. One of the most powerful aspects of Linux is its command-line interface (CLI), which allows users to interact directly with the system through text-based commands.\r\n\r\nThe CLI is not just a tool for the tech-savvy; it's an essential skill for anyone looking to harness the full potential of Linux. From managing files and processes to configuring networks and automating tasks, the command line offers unparalleled control and efficiency.\r\n\r\nThis article aims to provide a comprehensive guide to essential Linux commands, catering to both beginners and advanced users. Whether you're just starting your Linux journey or looking to deepen your command-line expertise, this guide will equip you with the knowledge you need to navigate and manage your Linux system effectively.\r\n\r\n## Section 1: Basic Commands\r\n\r\n### 1.1. Navigating the Filesystem\r\n\r\nNavigating the filesystem is fundamental to using Linux. Here are some basic commands to get you started:\r\n\r\n#### `pwd` - Print Working Directory\r\n\r\nThe `pwd` command displays the current directory you're working in. This is particularly useful when you need to confirm your location within the filesystem.\r\n\r\n```bash\r\n$ pwd\r\n/home/username\r\n```\r\n\r\n#### `ls` - List Directory Contents\r\n\r\nThe `ls` command lists the contents of a directory. It has several options to enhance its functionality:\r\n\r\n- `-l`: Long listing format, showing detailed information about each file.\r\n- `-a`: Include hidden files (those starting with a dot).\r\n- `-h`: Human-readable format, making file sizes easier to read.\r\n- `-R`: Recursively list subdirectories.\r\n\r\n```bash\r\n$ ls\r\nDesktop  Documents  Downloads  Music  Pictures  Videos\r\n\r\n$ ls -l\r\ntotal 0\r\ndrwxr-xr-x 2 username username 4096 Jan 1 12:34 Desktop\r\ndrwxr-xr-x 2 username username 4096 Jan 1 12:34 Documents\r\n\r\n$ ls -a\r\n.  ..  .bashrc  .profile  Desktop  Documents\r\n\r\n$ ls -h\r\ntotal 0\r\ndrwxr-xr-x 2 username username 4.0K Jan 1 12:34 Desktop\r\ndrwxr-xr-x 2 username username 4.0K Jan 1 12:34 Documents\r\n\r\n$ ls -R\r\n.:\r\nDesktop  Documents\r\n\r\n./Desktop:\r\nfile1.txt\r\n\r\n./Documents:\r\nfile2.txt\r\n```\r\n\r\n#### `cd` - Change Directory\r\n\r\nThe `cd` command changes the current directory. Here are some common usages:\r\n\r\n- `cd ..`: Move up one directory level.\r\n- `cd /`: Move to the root directory.\r\n- `cd ~/Documents`: Move to the Documents directory in your home folder (the tilde `~` represents the home directory).\r\n\r\n```bash\r\n$ cd ..\r\n$ pwd\r\n/home\r\n\r\n$ cd /var/log\r\n$ pwd\r\n/var/log\r\n\r\n$ cd ~/Documents\r\n$ pwd\r\n/home/username/Documents\r\n```\r\n\r\n### 1.2. File Operations\r\n\r\nManaging files is a core task in any operating system. Here are some essential commands for file operations:\r\n\r\n#### `touch` - Create an Empty File\r\n\r\nThe `touch` command creates an empty file or updates the timestamp of an existing file.\r\n\r\n```bash\r\n$ touch newfile.txt\r\n$ ls -l newfile.txt\r\n-rw-r--r-- 1 username username 0 Jan 1 12:34 newfile.txt\r\n```\r\n\r\n#### `cp` - Copy Files and Directories\r\n\r\nThe `cp` command copies files or directories. Key options include:\r\n\r\n- `-r`: Recursively copy directories.\r\n- `-i`: Prompt before overwriting files.\r\n- `-v`: Verbose mode, showing files being copied.\r\n\r\n```bash\r\n$ cp file1.txt file2.txt\r\n$ cp -r dir1/ dir2/\r\n$ cp -i file1.txt file2.txt\r\n$ cp -v file1.txt file2.txt\r\n```\r\n\r\n#### `mv` - Move or Rename Files and Directories\r\n\r\nThe `mv` command moves or renames files and directories.\r\n\r\n```bash\r\n$ mv file1.txt file2.txt\r\n$ mv file.txt /path/to/destination/\r\n```\r\n\r\n#### `rm` - Remove Files and Directories\r\n\r\nThe `rm` command removes files or directories. Important options include:\r\n\r\n- `-r`: Recursively remove directories.\r\n- `-f`: Force removal without prompting.\r\n\r\n```bash\r\n$ rm file1.txt\r\n$ rm -rf directory/\r\n```\r\n\r\n### 1.3. Viewing and Editing Files\r\n\r\nViewing and editing files is a frequent task. Here are some commands to help with that:\r\n\r\n#### `cat` - Concatenate and Display Files\r\n\r\nThe `cat` command displays the contents of a file.\r\n\r\n```bash\r\n$ cat file.txt\r\nHello, World!\r\n```\r\n\r\n#### `more` - View File Contents Page by Page\r\n\r\nThe `more` command allows you to view file contents one page at a time.\r\n\r\n```bash\r\n$ more file.txt\r\n```\r\n\r\n#### `less` - View File Contents with Backward Navigation\r\n\r\nThe `less` command is similar to `more` but allows backward navigation.\r\n\r\n```bash\r\n$ less file.txt\r\n```\r\n\r\n#### `head` - Display the First Part of a File\r\n\r\nThe `head` command shows the first 10 lines of a file by default.\r\n\r\n```bash\r\n$ head file.txt\r\n```\r\n\r\n#### `tail` - Display the Last Part of a File\r\n\r\nThe `tail` command shows the last 10 lines of a file by default. The `-f` option allows real-time updates.\r\n\r\n```bash\r\n$ tail file.txt\r\n$ tail -f logfile.txt\r\n```\r\n\r\n#### `nano` - Simple Text Editor\r\n\r\n`nano` is a straightforward text editor that's easy to use.\r\n\r\n```bash\r\n$ nano file.txt\r\n```\r\n\r\n#### `vim` - Advanced Text Editor\r\n\r\n`vim` is a powerful text editor with extensive features for advanced users.\r\n\r\n```bash\r\n$ vim file.txt\r\n```\r\n\r\nWith these basic commands, you can navigate the filesystem, manage files, and view or edit their contents efficiently. Mastering these commands is the first step towards becoming proficient in Linux.\r\n\r\n## Section 2: System Information and Management\r\n\r\nUnderstanding and managing system information is crucial for maintaining a healthy Linux environment. This section covers commands that provide insights into system status, user management, and process control.\r\n\r\n### 2.1. System Information\r\n\r\nThese commands allow you to gather detailed information about your Linux system.\r\n\r\n#### `uname` - Print System Information\r\n\r\nThe `uname` command displays system information. Useful options include:\r\n\r\n- `-a`: Print all system information.\r\n- `-r`: Print the kernel release.\r\n- `-s`: Print the kernel name.\r\n\r\n```bash\r\n$ uname -a\r\nLinux hostname 5.4.0-42-generic #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ uname -r\r\n5.4.0-42-generic\r\n\r\n$ uname -s\r\nLinux\r\n```\r\n\r\n#### `top` - Task Manager for Linux\r\n\r\nThe `top` command provides a dynamic, real-time view of running processes, including CPU and memory usage.\r\n\r\n```bash\r\n$ top\r\n```\r\n\r\n#### `htop` - Interactive Process Viewer\r\n\r\n`htop` is an enhanced version of `top`, offering a more user-friendly interface and additional features.\r\n\r\n```bash\r\n$ htop\r\n```\r\n\r\n#### `df` - Report File System Disk Space Usage\r\n\r\nThe `df` command reports the amount of disk space used and available on file systems. The `-h` option displays the output in a human-readable format.\r\n\r\n```bash\r\n$ df -h\r\nFilesystem      Size  Used Avail Use% Mounted on\r\nudev            1.9G     0  1.9G   0% /dev\r\ntmpfs           393M  1.3M  392M   1% /run\r\n/dev/sda1        20G   15G  4.5G  77% /\r\n```\r\n\r\n#### `du` - Estimate File Space Usage\r\n\r\nThe `du` command estimates the file space usage. Key options include:\r\n\r\n- `-h`: Human-readable format.\r\n- `-s`: Summarize the total.\r\n\r\n```bash\r\n$ du -h\r\n4.0K    ./Desktop\r\n8.0K    ./Documents\r\n12K     .\r\n\r\n$ du -sh\r\n12K     .\r\n```\r\n\r\n### 2.2. User and Group Management\r\n\r\nManaging users and groups is essential for system security and organization.\r\n\r\n#### `whoami` - Display the Current User\r\n\r\nThe `whoami` command shows the username of the current user.\r\n\r\n```bash\r\n$ whoami\r\nusername\r\n```\r\n\r\n#### `id` - Display User and Group Information\r\n\r\nThe `id` command displays user and group information for the current user or a specified user.\r\n\r\n```bash\r\n$ id\r\nuid=1000(username) gid=1000(username) groups=1000(username),27(sudo)\r\n\r\n$ id username\r\nuid=1000(username) gid=1000(username) groups=1000(username),27(sudo)\r\n```\r\n\r\n#### `useradd` - Add a New User\r\n\r\nThe `useradd` command creates a new user. Important options include:\r\n\r\n- `-m`: Create a home directory for the new user.\r\n- `-G`: Specify supplementary groups.\r\n\r\n```bash\r\n$ sudo useradd -m newuser\r\n$ sudo useradd -m -G sudo newuser\r\n```\r\n\r\n#### `usermod` - Modify a User Account\r\n\r\nThe `usermod` command modifies an existing user account. Key options include:\r\n\r\n- `-aG`: Add the user to supplementary groups.\r\n\r\n```bash\r\n$ sudo usermod -aG sudo newuser\r\n```\r\n\r\n#### `passwd` - Change User Password\r\n\r\nThe `passwd` command changes the password for a user.\r\n\r\n```bash\r\n$ sudo passwd newuser\r\nEnter new UNIX password:\r\nRetype new UNIX password:\r\npasswd: password updated successfully\r\n```\r\n\r\n#### `groupadd` - Add a New Group\r\n\r\nThe `groupadd` command creates a new group.\r\n\r\n```bash\r\n$ sudo groupadd newgroup\r\n```\r\n\r\n### 2.3. Process Management\r\n\r\nManaging processes is vital for maintaining system performance and stability.\r\n\r\n#### `ps` - Report a Snapshot of Current Processes\r\n\r\nThe `ps` command provides a snapshot of current processes. Useful options include:\r\n\r\n- `-e`: Display all processes.\r\n- `-f`: Full-format listing.\r\n- `-u`: Display processes for a specific user.\r\n\r\n```bash\r\n$ ps -e\r\nPID TTY          TIME CMD\r\n  1 ?        00:00:01 systemd\r\n  2 ?        00:00:00 kthreadd\r\n\r\n$ ps -ef\r\nUID        PID  PPID  C STIME TTY          TIME CMD\r\nroot         1     0  0 12:34 ?        00:00:01 /sbin/init\r\nroot         2     0  0 12:34 ?        00:00:00 [kthreadd]\r\n\r\n$ ps -u username\r\nUID        PID  PPID  C STIME TTY          TIME CMD\r\nusername  1234  5678  0 12:34 ?        00:00:00 /usr/bin/bash\r\n```\r\n\r\n#### `kill` - Terminate a Process\r\n\r\nThe `kill` command sends a signal to terminate a process. The most common signal is `-9` (SIGKILL), which forces termination.\r\n\r\n```bash\r\n$ kill -9 PID\r\n```\r\n\r\n#### `pkill` - Terminate Processes by Name\r\n\r\nThe `pkill` command terminates processes based on their name.\r\n\r\n```bash\r\n$ pkill processname\r\n```\r\n\r\n#### `bg` - Resume a Suspended Job in the Background\r\n\r\nThe `bg` command resumes a suspended job in the background.\r\n\r\n```bash\r\n$ bg %1\r\n```\r\n\r\n#### `fg` - Bring a Job to the Foreground\r\n\r\nThe `fg` command brings a background job to the foreground.\r\n\r\n```bash\r\n$ fg %1\r\n```\r\n\r\nThese commands provide essential tools for monitoring and managing your Linux system, ensuring you can keep your environment running smoothly and efficiently.\r\n\r\n## Section 3: Networking Commands\r\n\r\nNetworking is a critical aspect of any operating system, and Linux provides a robust set of commands to manage and troubleshoot network connections. This section covers essential networking commands for both basic and advanced network management.\r\n\r\n### 3.1. Basic Networking\r\n\r\nThese commands help you perform fundamental networking tasks such as checking connectivity and configuring network interfaces.\r\n\r\n#### `ping` - Send ICMP ECHO_REQUEST to Network Hosts\r\n\r\nThe `ping` command checks the network connectivity between your system and another host. It sends ICMP ECHO_REQUEST packets and waits for a response. The output shows the time it takes for each packet to travel to the host and back.\r\n\r\n```bash\r\n$ ping google.com\r\nPING google.com (172.217.164.110) 56(84) bytes of data.\r\n64 bytes from 172.217.164.110: icmp_seq=1 ttl=54 time=10.3 ms\r\n64 bytes from 172.217.164.110: icmp_seq=2 ttl=54 time=10.2 ms\r\n```\r\n\r\n#### `ifconfig` - Configure Network Interfaces (deprecated, use `ip` instead)\r\n\r\nThe `ifconfig` command is used to configure network interfaces. However, it is deprecated in favor of the `ip` command.\r\n\r\n```bash\r\n$ ifconfig\r\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\r\n        inet 192.168.1.10  netmask 255.255.255.0  broadcast 192.168.1.255\r\n        inet6 fe80::a00:27ff:fe4e:66a1  prefixlen 64  scopeid 0x20<link>\r\n        ether 08:00:27:4e:66:a1  txqueuelen 1000  (Ethernet)\r\n        RX packets 308  bytes 25624 (25.6 KB)\r\n        RX errors 0  dropped 0  overruns 0  frame 0\r\n        TX packets 308  bytes 25624 (25.6 KB)\r\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\r\n```\r\n\r\n#### `ip` - Show/Manipulate Routing, Devices, Policy Routing, and Tunnels\r\n\r\nThe `ip` command is the modern replacement for `ifconfig` and provides more functionality. Here are some common usages:\r\n\r\n- `ip a`: Show all IP addresses.\r\n- `ip link`: Show network interfaces.\r\n\r\n```bash\r\n$ ip a\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\r\n    inet 127.0.0.1/8 scope host lo\r\n       valid_lft forever preferred_lft forever\r\n    inet6 ::1/128 scope host\r\n       valid_lft forever preferred_lft forever\r\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\r\n    link/ether 08:00:27:4e:66:a1 brd ff:ff:ff:ff:ff:ff\r\n    inet 192.168.1.10/24 brd 192.168.1.255 scope global dynamic eth0\r\n       valid_lft 86397sec preferred_lft 86397sec\r\n    inet6 fe80::a00:27ff:fe4e:66a1/64 scope link\r\n       valid_lft forever preferred_lft forever\r\n\r\n$ ip link\r\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\r\n    link/loopback 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\r\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\r\n    link/ether 08:00:27:4e:66:a1 brd ff:ff:ff:ff:ff:ff\r\n```\r\n\r\n#### `netstat` - Network Statistics\r\n\r\nThe `netstat` command provides various network-related statistics such as network connections, routing tables, interface statistics, masquerade connections, and multicast memberships. Key options include:\r\n\r\n- `-tuln`: Show TCP and UDP listening ports.\r\n\r\n```bash\r\n$ netstat -tuln\r\nActive Internet connections (only servers)\r\nProto Recv-Q Send-Q Local Address           Foreign Address         State\r\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN\r\ntcp6       0      0 :::22                   :::*                    LISTEN\r\nudp        0      0 0.0.0.0:68              0.0.0.0:*\r\n```\r\n\r\n#### `ss` - Another Utility to Investigate Socket Statistics\r\n\r\nThe `ss` command is a modern replacement for `netstat` and provides more detailed information about network connections. Key options include:\r\n\r\n- `-tuln`: Show TCP and UDP listening ports.\r\n\r\n```bash\r\n$ ss -tuln\r\nNetid  State      Recv-Q Send-Q    Local Address:Port                 Peer Address:Port\r\ntcp    LISTEN     0      128                   *:22                              *:*\r\ntcp    LISTEN     0      128                  :::22                             :::*\r\nudp    UNCONN     0      0                     *:68                              *:*\r\n```\r\n\r\n### 3.2. Data Transfer\r\n\r\nThese commands are used for transferring data between hosts, whether for copying files or downloading content from the web.\r\n\r\n#### `scp` - Secure Copy (Remote File Copy)\r\n\r\nThe `scp` command securely copies files between hosts over SSH. Here are some common usages:\r\n\r\n```bash\r\n$ scp file.txt user@remote:/path/to/destination/\r\n$ scp user@remote:/path/to/source/file.txt /local/destination/\r\n```\r\n\r\n#### `rsync` - Remote File and Directory Synchronization\r\n\r\nThe `rsync` command synchronizes files and directories between two locations. Key options include:\r\n\r\n- `-a`: Archive mode, which preserves permissions, timestamps, and other attributes.\r\n- `-v`: Verbose mode, showing detailed output.\r\n- `-z`: Compress file data during the transfer.\r\n\r\n```bash\r\n$ rsync -avz /local/directory/ user@remote:/path/to/destination/\r\n$ rsync -avz user@remote:/path/to/source/ /local/destination/\r\n```\r\n\r\n#### `wget` - Non-Interactive Network Downloader\r\n\r\nThe `wget` command downloads files from the web non-interactively, making it ideal for scripts and automated tasks.\r\n\r\n```bash\r\n$ wget http://example.com/file.zip\r\n```\r\n\r\n#### `curl` - Transfer Data from or to a Server\r\n\r\nThe `curl` command transfers data to or from a server using various protocols, including HTTP, HTTPS, FTP, and more. Here are some common usages:\r\n\r\n- Download a file:\r\n\r\n```bash\r\n$ curl -O http://example.com/file.zip\r\n```\r\n\r\n- Send a POST request with data:\r\n\r\n```bash\r\n$ curl -d \"param1=value1&param2=value2\" -X POST http://example.com/resource\r\n```\r\n\r\n- Fetch headers from a URL:\r\n\r\n```bash\r\n$ curl -I http://example.com\r\n```\r\n\r\nThese networking commands provide essential tools for managing and troubleshooting network connections, as well as transferring data between hosts. Mastering these commands will enable you to handle a wide range of networking tasks efficiently.\r\n\r\n## Section 4: Advanced Commands\r\n\r\nAdvanced commands in Linux allow users to perform complex tasks such as package management, disk management, and system monitoring. These commands are essential for system administrators and power users who need to maintain and optimize their Linux systems.\r\n\r\n### 4.1. Package Management\r\n\r\nPackage management is crucial for installing, updating, and removing software on your Linux system. Different Linux distributions use different package management systems.\r\n\r\n#### `apt-get` - APT Package Handling Utility (Debian-based)\r\n\r\nThe `apt-get` command is used for handling packages in Debian-based distributions like Ubuntu. Common commands include:\r\n\r\n- `update`: Update the package list.\r\n- `upgrade`: Upgrade all installed packages.\r\n- `install`: Install a new package.\r\n- `remove`: Remove an installed package.\r\n\r\n```bash\r\n$ sudo apt-get update\r\n$ sudo apt-get upgrade\r\n$ sudo apt-get install package_name\r\n$ sudo apt-get remove package_name\r\n```\r\n\r\n#### `yum` - Package Manager for RPM-based Distributions\r\n\r\nThe `yum` command is used for managing packages in RPM-based distributions like CentOS and Red Hat. Common commands include:\r\n\r\n- `install`: Install a new package.\r\n- `update`: Update all installed packages.\r\n- `remove`: Remove an installed package.\r\n\r\n```bash\r\n$ sudo yum install package_name\r\n$ sudo yum update\r\n$ sudo yum remove package_name\r\n```\r\n\r\n#### `dnf` - Next Generation Package Management (Fedora)\r\n\r\nThe `dnf` command is the modern replacement for `yum` in Fedora and other RPM-based distributions. It offers improved performance and better dependency management.\r\n\r\n```bash\r\n$ sudo dnf install package_name\r\n$ sudo dnf update\r\n$ sudo dnf remove package_name\r\n```\r\n\r\n#### `snap` - Package Management System for Snaps\r\n\r\nThe `snap` command is used to manage snap packages, which are self-contained applications. Common commands include:\r\n\r\n- `install`: Install a snap package.\r\n- `remove`: Remove a snap package.\r\n- `list`: List installed snap packages.\r\n\r\n```bash\r\n$ sudo snap install package_name\r\n$ sudo snap remove package_name\r\n$ snap list\r\n```\r\n\r\n### 4.2. Disk Management\r\n\r\nDisk management involves creating, modifying, and managing disk partitions and filesystems.\r\n\r\n#### `fdisk` - Partition Table Manipulator\r\n\r\nThe `fdisk` command is used to create and manipulate disk partitions. Here is an example of creating a new partition:\r\n\r\n```bash\r\n$ sudo fdisk /dev/sda\r\n\r\nCommand (m for help): n\r\nPartition type:\r\n   p   primary (1 primary, 0 extended, 3 free)\r\n   e   extended\r\nSelect (default p): p\r\nPartition number (1-4, default 1): 1\r\nFirst sector (2048-20971519, default 2048): 2048\r\nLast sector, +sectors or +size{K,M,G,T,P} (2048-20971519, default 20971519): +1G\r\n\r\nCommand (m for help): w\r\nThe partition table has been altered!\r\n```\r\n\r\n#### `mkfs` - Build a Linux File System\r\n\r\nThe `mkfs` command is used to create a filesystem on a partition.\r\n\r\n```bash\r\n$ sudo mkfs.ext4 /dev/sda1\r\n```\r\n\r\n#### `mount` - Mount a File System\r\n\r\nThe `mount` command is used to mount a filesystem.\r\n\r\n```bash\r\n$ sudo mount /dev/sda1 /mnt\r\n```\r\n\r\n#### `umount` - Unmount a File System\r\n\r\nThe `umount` command is used to unmount a filesystem.\r\n\r\n```bash\r\n$ sudo umount /mnt\r\n```\r\n\r\n### 4.3. System Monitoring and Performance\r\n\r\nMonitoring system performance is essential for maintaining a healthy Linux environment. These commands help you track system resources and performance metrics.\r\n\r\n#### `vmstat` - Report Virtual Memory Statistics\r\n\r\nThe `vmstat` command reports virtual memory statistics, including processes, memory, paging, block IO, traps, and CPU activity.\r\n\r\n```bash\r\n$ vmstat\r\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\r\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\r\n 1  0      0 122104  13172  94456    0    0    12    10   35   45  1  0 98  1  0\r\n```\r\n\r\n#### `iostat` - Report CPU and I/O Statistics\r\n\r\nThe `iostat` command reports CPU and I/O statistics, helping you identify performance bottlenecks.\r\n\r\n```bash\r\n$ iostat\r\nLinux 5.4.0-42-generic (hostname) \t01/01/2021 \t_x86_64_\t(2 CPU)\r\n\r\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\r\n           1.00    0.00    0.50    0.10    0.00   98.40\r\n\r\nDevice             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\r\nsda               1.00         5.00         3.00       500       300\r\n```\r\n\r\n#### `sar` - Collect, Report, or Save System Activity Information\r\n\r\nThe `sar` command collects and reports system activity information, including CPU usage, memory usage, and network activity. Here are some common usages:\r\n\r\n- Report CPU usage every second for three iterations:\r\n\r\n```bash\r\n$ sar -u 1 3\r\nLinux 5.4.0-42-generic (hostname) \t01/01/2021 \t_x86_64_\t(2 CPU)\r\n\r\n01:00:01 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle\r\n01:00:02 AM     all      1.00      0.00      0.50      0.10      0.00     98.40\r\n01:00:03 AM     all      1.00      0.00      0.50      0.10      0.00     98.40\r\n01:00:04 AM     all      1.00      0.00      0.50      0.10      0.00     98.40\r\n```\r\n\r\n- Report memory usage:\r\n\r\n```bash\r\n$ sar -r 1 3\r\nLinux 5.4.0-42-generic (hostname) \t01/01/2021 \t_x86_64_\t(2 CPU)\r\n\r\n01:00:01 AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit\r\n01:00:02 AM    122104    94456     43.63     13172     94456    122104      12.3\r\n01:00:03 AM    122104    94456     43.63     13172     94456    122104      12.3\r\n01:00:04 AM    122104    94456     43.63     13172     94456    122104      12.3\r\n```\r\n\r\nThese advanced commands provide powerful tools for managing packages, disks, and system performance, enabling you to maintain and optimize your Linux environment effectively.\r\n\r\n## Section 5: Scripting and Automation\r\n\r\nScripting and automation are powerful aspects of Linux that allow users to automate repetitive tasks, manage system configurations, and perform complex operations with ease. This section covers the basics of shell scripting and task scheduling.\r\n\r\n### 5.1. Shell Scripting Basics\r\n\r\nShell scripting involves writing scripts using a shell language like Bash to automate tasks. Here are some fundamental concepts and commands to get you started.\r\n\r\n#### Creating and Running Shell Scripts\r\n\r\nA shell script is a text file containing a series of commands. To create a shell script, use a text editor to write your commands and save the file with a `.sh` extension. To run the script, you need to make it executable and then execute it.\r\n\r\n```bash\r\n$ nano myscript.sh\r\n```\r\n\r\nAdd the following content to the script:\r\n\r\n```bash\r\n#!/bin/bash\r\necho \"Hello, World!\"\r\n```\r\n\r\nMake the script executable and run it:\r\n\r\n```bash\r\n$ chmod +x myscript.sh\r\n$ ./myscript.sh\r\nHello, World!\r\n```\r\n\r\n#### `echo` - Display a Line of Text\r\n\r\nThe `echo` command prints text to the terminal.\r\n\r\n```bash\r\n$ echo \"Hello, World!\"\r\nHello, World!\r\n```\r\n\r\n#### `read` - Read a Line of Input\r\n\r\nThe `read` command reads a line of input from the user.\r\n\r\n```bash\r\n$ read -p \"Enter your name: \" name\r\n$ echo \"Hello, $name!\"\r\n```\r\n\r\n#### Variables - Using Variables in Scripts\r\n\r\nVariables store data that can be used and manipulated within a script.\r\n\r\n```bash\r\n$ name=\"John\"\r\n$ echo \"Hello, $name!\"\r\nHello, John!\r\n```\r\n\r\n#### Loops - For, While, and Until Loops\r\n\r\nLoops allow you to execute a block of code multiple times.\r\n\r\n- **For Loop**:\r\n\r\n```bash\r\nfor i in 1 2 3 4 5\r\ndo\r\n  echo \"Iteration $i\"\r\ndone\r\n```\r\n\r\n- **While Loop**:\r\n\r\n```bash\r\ncount=1\r\nwhile [ $count -le 5 ]\r\ndo\r\n  echo \"Count is $count\"\r\n  count=$((count + 1))\r\ndone\r\n```\r\n\r\n- **Until Loop**:\r\n\r\n```bash\r\ncount=1\r\nuntil [ $count -gt 5 ]\r\ndo\r\n  echo \"Count is $count\"\r\n  count=$((count + 1))\r\ndone\r\n```\r\n\r\n#### Conditionals - If, Else, Elif Statements\r\n\r\nConditionals execute different blocks of code based on certain conditions.\r\n\r\n```bash\r\nif [ $count -eq 5 ]\r\nthen\r\n  echo \"Count is 5\"\r\nelif [ $count -lt 5 ]\r\nthen\r\n  echo \"Count is less than 5\"\r\nelse\r\n  echo \"Count is greater than 5\"\r\nfi\r\n```\r\n\r\n### 5.2. Task Scheduling\r\n\r\nTask scheduling allows you to automate the execution of scripts and commands at specific times or intervals.\r\n\r\n#### `cron` - Schedule Commands to Run at Specific Times\r\n\r\nThe `cron` daemon runs scheduled tasks at specified times. You can use the `crontab` command to edit the cron table and schedule tasks.\r\n\r\n- **Crontab Syntax**:\r\n\r\n```plaintext\r\n* * * * * command_to_execute\r\n- - - - -\r\n| | | | |\r\n| | | | +----- Day of the week (0 - 7) (Sunday is both 0 and 7)\r\n| | | +------- Month (1 - 12)\r\n| | +--------- Day of the month (1 - 31)\r\n| +----------- Hour (0 - 23)\r\n+------------- Minute (0 - 59)\r\n```\r\n\r\n- **Examples**:\r\n\r\nRun a script every day at 2 AM:\r\n\r\n```bash\r\n$ crontab -e\r\n0 2 * * * /path/to/script.sh\r\n```\r\n\r\nRun a script every 5 minutes:\r\n\r\n```bash\r\n$ crontab -e\r\n*/5 * * * * /path/to/script.sh\r\n```\r\n\r\n#### `at` - Schedule Commands to Run Once\r\n\r\nThe `at` command schedules a command to run once at a specified time.\r\n\r\n```bash\r\n$ at 2:00 PM\r\nat> /path/to/script.sh\r\nat> <EOT>\r\n```\r\n\r\nThese scripting and automation commands provide powerful tools for managing and automating tasks on your Linux system, enabling you to increase efficiency and reduce manual effort.\r\n\r\n---\r\n\r\n## Conclusion\r\n\r\nMastering Linux commands is essential for anyone looking to harness the full potential of the operating system. From basic file operations and system management to advanced networking and scripting, these commands provide the tools you need to navigate, manage, and optimize your Linux environment effectively.\r\n\r\nRemember, practice is key to becoming proficient with these commands. Explore beyond the listed commands, experiment with different options, and refer to the man pages (`man command`) for detailed information.\r\n\r\nFor further learning and practice, consider exploring online tutorials, Linux forums, and communities. The more you practice, the more confident and efficient you will become in using the Linux command line.","src/content/posts/linux-commands.mdx",[1426],"../../assets/images/24/07/linux-commands.jpeg","505f55478484d603","linux-commands.mdx","mail-baby-review",{id:1429,data:1431,body:1441,filePath:1442,assetImports:1443,digest:1445,legacyId:1446,deferredRender:32},{title:1432,description:1433,date:1434,image:1435,authors:1436,categories:1437,tags:1438,canonical:1440},"Mail.Baby Review: Pros and Cons of the Budget-Friendly Email Service by Interserver","Mail.Baby Review: See if this SMTP email-sending solution is for you and your business.",["Date","2023-11-27T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/mail-baby-review.jpeg",[19],[98],[1439],"mail","https://www.bitdoze.com/mail-baby-review/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/23/11/mail-baby-order.png\";\r\n\r\nDo you want to send emails without breaking the bank? Do you want to avoid the hassle of setting up your own mail server? Do you want to enjoy high deliverability and reliability? If you answered yes to any of these questions, you might be interested in [Mail.Baby](https://mail.baby/), a low-cost outbound email service by Interserver.\r\n\r\nI have tried different email services in the past and I want to use something that is:\r\n\r\n- **not expensive**: I want to be able to send bulk emails and not pay a ton of money.\r\n- **reliable**: when an email is sent I want to have a high delivery rate, so my emails reach the inbox.\r\n- **use it for lists and transactional emails**: I have servers and applications and I want to be able to use the email server for both\r\n- **use multiple domains**: I have more than 10 domains and sometimes I want that domain to send the email not a central one.\r\n- **secure**: I need to connect securely to the email server.\r\n- **pay as you go**: I am not sending thousands of emails monthly I just need to be charged when I send them.\r\n\r\nBut before you sign up, there is one thing you should know: Mail.Baby might expose your server’s IP address to the public. Here’s why.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/MkbcnC8TIow\"\r\n  label=\"Mail.Baby Review\"\r\n/>\r\n\r\n\r\n> For configuring Postfix to use an external SMTP service on your server you can check: [Configure Postfix to Send Email Using External SMTP Servers](https://www.bitdoze.com/postfix-external-smtp/)\r\n## Mail.Baby and SPF records\r\n\r\nMail.Baby is a service that allows you to send emails from your own domain name using their SMTP relay servers. To do that, you need to configure your DNS records to authorize Mail.Baby to send emails on your behalf. One of these records is the SPF (Sender Policy Framework) record, which specifies which servers are allowed to send emails from your domain.\r\n\r\nMail.Baby requires that you add their servers and your own server’s IP address to your SPF record. For example, if your domain is example.com and your server’s IP is 123.456.789.0, your SPF record should look like this:\r\n\r\n```\r\nv=spf1 include:relay.mailbaby.net ip4:123.456.789.0 -all\r\n```\r\n\r\nThis means that only Mail.Baby’s servers and your server can send emails from example.com. However, it also means that anyone who looks up your SPF record can see your server’s IP address. This can be a security risk, especially if you are using a service like Cloudflare to hide your server’s IP behind a proxy. By revealing your server’s IP, you might expose yourself to attacks or abuse.\r\n\r\n### Why does Mail.Baby need your server’s IP?\r\n\r\nYou might wonder why Mail.Baby needs your server’s IP in the first place. After all, other email services like MXroute and MailChimp’s Mandrill don’t require this. The reason is that Mail.Baby checks your SPF record to verify that your server is authorized to send emails for your domain. This is a way to prevent spam and abuse of their service. However, this also means that they don’t accept emails from servers that are not listed in your SPF record, even if they are using your Mail.Baby credentials.\r\n\r\n### Is there a way around this?\r\n\r\nUnfortunately, there is no easy way to use Mail.Baby without revealing your server’s IP in your SPF record. You could try to use a different IP for sending emails, but that would require setting up another server or using a VPN. You could also try to use a subdomain for sending emails, but that would require changing your email addresses and configuring your DNS records accordingly. Neither of these options is very convenient or practical.\r\n\r\n## Mail.Baby Sending Limits, Price, Quality\r\n\r\n### Price\r\n\r\n[Mail.Baby](https://mail.baby/) charges $1 a month for the availability and then $0.20 for every 1000 emails sent. The price would be on top of the $1 a month one, so if you send 10.000 emails a month you will be charged $1 + $2 = $3. This is a very affordable service and Only Amazon SES comes close.\r\nCompared with other services the price is quite good, other services charge even 10 times more. So mail.baby has a plus from my side even if you need to pay the $1 a month.\r\n\r\n### Sending Limits\r\n\r\nSending limits are the following:\r\n\r\n- 6,000 emails per hour – per email address (not per Mail Baby account).\r\n- If you send over 6,000 emails in one hour, all the “excess” emails (beyond the 6,000th email) will be disregarded.\r\n\r\nThere is a problem if you need to send more than 6000 emails an hour as you need your tool to cache them and not send more otherwise they will be lost.\r\nThis could have been done better as they could have cached the emails and sent them the next hour. You need to be careful with this limit.\r\n\r\n### What Types of Emails are Supported\r\n\r\nMail.Baby lets you send transactional emails and marketing emails. So you can use the service for both and not need 2 services for this. I like this as not all the SMTP providers out here lets you do both, usually sending marketing emails can be quite expensive.\r\n\r\n### Security\r\n\r\nSTARTTLS it is used to connect to the email server but in case your server doesn't support TLS you can access it insecure. The ports that can be used and SMTP details can be found in the admin area.\r\nMail.Baby is scanning the email that is sent for spam so in case your server is compromised mail.baby will block the sending of the email.\r\n\r\n### Email Deliverability\r\n\r\nI am using Mail.Baby for a couple of weeks and I can say that I didn't have any issues all the emails that I have sent have been delivered. I have tested the service with bulk send and also to some of my inboxes on Gmail and live and I didn't see any issues.\r\n\r\n### How You Register\r\n\r\n- Go To Interserver site and create an account: [https://my.interserver.net/](https://my.interserver.net/)\r\n- Order Email: After you create an account you should go under **Mail - Order** and get a package\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"mail.baby order\"\r\n/>\r\n\r\n### My Experience With Mail.Baby\r\n\r\nI have been using them for a couple of weeks, and till now I am happy with the service they are providing, I am using them on multiple servers and I can say the emails are delivered without problems. paying the $1 a month is not a problem especially when you want the email to reach the other side.\r\nSetting up the account and connecting to the email server is not hard and all the details are on their interface.\r\n\r\nIn case of issues, I will update this section.\r\n\r\n## Should you use Mail.Baby?\r\n\r\nMail.Baby is a very attractive service for anyone who needs to send a lot of emails at a low price. Mail.baby is much cheaper than most other email services, and the quality and deliverability are reportedly very good. However, you should also be aware of the security risk of exposing your server’s IP address in your SPF record. If you are not comfortable with that, you might want to look for another email service.\r\n\r\nIn the end it's up to you if you want to use it or not.","src/content/posts/mail-baby-review.mdx",[1444],"../../assets/images/23/11/mail-baby-review.jpeg","78c280b0a190acaf","mail-baby-review.mdx","monitor-cpu-usage-and-send-email-alerts-in-linux",{id:1447,data:1449,body:1458,filePath:1459,assetImports:1460,digest:1462,legacyId:1463,deferredRender:32},{title:1450,description:1451,date:1452,image:1453,authors:1454,categories:1455,tags:1456,canonical:1457},"Monitor CPU Usage and Send Email Alerts in Linux","Let's see how we can monitor CPU usage on a server and receive emails.",["Date","2022-09-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/cpu_monitoring.jpeg",[19],[98],[135,24],"https://www.bitdoze.com/monitor-cpu-usage-and-send-email-alerts-in-linux/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport Button from \"../../layouts/components/widgets/Button.astro\";\r\n\r\nMonitoring the CPU is very important on a Linux server as there may be cases when your applications will need more CPU and can consume everything. You would want to be notified via email in case the CPU usage spikes and you need to do some checks.\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\nFor your server to be able to send emails you will need to have configured an SMTP Relay or you should have an email server hosted on the VPS that will send the alarm.\r\n\r\nIn case you are using an online VPS provider like Hetzner or DigitalOcean having such a script can be very useful as it can catch problems coming from your hosting provider or your app. For more details on Hetzner, you can check this review.\r\n\r\nIn this article, we will configure a script that will run in crontab every 5 minutes and check to see if the CPU usage is above 80% in case that happens you will be notified via email with the:\r\n\r\n- Current Usage\r\n- Top 20 processes that consume high CPU\r\n- Top 10 Processes that consume high CPU using the ps command\r\n- Memory Utilization on the server\r\n\r\nIn case you are interested to have a web panel that can help you manage your applications and be used as a reverse proxy you can check the bellow course:\r\n\r\n<Button\r\n  link=\"https://webdoze.net/courses/cloudpanel-setup/\"\r\n  text=\"CloudPanel Setup Course\"\r\n/>\r\n\r\nHaving all of this in an email will help us better understand what is happening on the server we have. To build the script we are going to use shell commands.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />\r\n\r\n## Youtube Video With Details\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/FdjECMq1N2U\"\r\n  label=\"Monitor CPU Usage and Send Email Alerts in Linux\"\r\n/>\r\n\r\n## Shell Script To Be Used\r\n\r\nBelow is the script that we are going to use to help us monitor the CPU usage, this will help you monitor Ubuntu versions like 20.04 or 22.04 or RedHat distros.\r\n\r\n```\r\n    #!/bin/bash\r\n    cpu_idle=`top -b -n 1 | grep Cpu | awk '{print $8}'|cut -f 1 -d \".\"`\r\n    cpuuse=`expr 100 - $cpu_idle`\r\n    date=$(date '+%Y-%m-%d %H:%M:%S')\r\n    if [ \"$cpuuse\" -ge 80 ]; then\r\n    SUBJECT=\"ATTENTION: CPU load is high on $(hostname) at $(date)\"\r\n    MESSAGE=\"/tmp/Mail.out\"\r\n    TO=\"youremail@domain.com\"\r\n      echo \"CPU current usage is: $cpuuse%\" >> $MESSAGE\r\n      echo \"\" >> $MESSAGE\r\n      echo \"+------------------------------------------------------------------+\" >> $MESSAGE\r\n      echo \"Top 20 processes that consume high CPU\" >> $MESSAGE\r\n      echo \"+------------------------------------------------------------------+\" >> $MESSAGE\r\n      echo \"$(top -bn1 | head -20)\" >> $MESSAGE\r\n      echo \"\" >> $MESSAGE\r\n      echo \"+------------------------------------------------------------------+\" >> $MESSAGE\r\n      echo \"Top 10 Processes which consuming high CPU using the ps command\" >> $MESSAGE\r\n      echo \"+------------------------------------------------------------------+\" >> $MESSAGE\r\n      echo \"$(ps -eo pcpu,pid,user,args | sort -k 1 -r | head -10)\" >> $MESSAGE\r\n      echo \"Memory Utilization on the server\" >> $MESSAGE\r\n      echo \"+------------------------------------------------------------------+\" >> $MESSAGE\r\n      echo \"$(ps_mem)\" >> $MESSAGE\r\n      mail -s \"$SUBJECT\" \"$TO\" < $MESSAGE\r\n      rm /tmp/Mail.out\r\n    else\r\n    echo \"$date: Server CPU usage is in under threshold.CPU current usage is: $cpuuse%\"\r\n      fi\r\n```\r\n\r\nIn this script, you need to change the TO with your email and in case you want to change the threshold you put the value you want in the -ge 80 (now is 80).\r\n\r\n## Activating the CPU Monitoring Script\r\n\r\n### Install ps_mem\r\n\r\nThis is a tool that will show you better memory usage per process, to install this tool you do:\r\n\r\nGet ps_men:\r\n\r\n```\r\n      sudo wget -qO /usr/local/bin/ps_mem https://raw.githubusercontent.com/pixelb/ps_mem/master/ps_mem.py\r\n```\r\n\r\nMake ps_mem executable:\r\n\r\n```\r\nsudo chmod a+x /usr/local/bin/ps_mem\r\n```\r\n\r\nCheck the version:\r\n\r\n```\r\n      ps_mem --version\r\n```\r\n\r\nIf you are receiving an error message /usr/bin/env: 'python': No such file or directory, you need to create a symbolic link for /usr/bin/python. In Ubuntu 20.04 or Ubuntu 22.04, only Python 3 is installed by default.\r\n\r\n```\r\n      sudo ln -s /usr/bin/python3 /usr/bin/python\r\n```\r\n\r\n### Put the Script in Crontab\r\n\r\nCreate a file with the script under /opt/scripts or in any location you want:\r\n\r\n```\r\nvi /opt/scripts/cpu-alert.sh\r\n```\r\n\r\nMake The File Executable:\r\n\r\n```\r\nsudo chmod a+x /opt/scripts/cpu-alert.sh\r\n```\r\n\r\nExecute the Script:\r\n\r\n```\r\n      /opt/scripts/cpu-alert.sh\r\n```\r\n\r\nThe output should be on email:\r\n\r\n```\r\n    CPU current usage is: 5%\r\n    +------------------------------------------------------------------+\r\n    Top 20 processes that consume high CPU\r\n    +------------------------------------------------------------------+\r\n    top - 11:27:12 up 6 days, 4:31, 1 user, load average: 0.08, 0.08, 0.07\r\n    Tasks: 204 total, 1 running, 202 sleeping, 0 stopped, 1 zombie\r\n    %Cpu(s): 4.1 us, 0.0 sy, 0.0 ni, 95.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r\n    MiB Mem : 3827.7 total, 556.1 free, 1427.2 used, 1844.4 buff/cache\r\n    MiB Swap: 2048.0 total, 1276.7 free, 771.3 used. 1454.0 avail Mem\r\n    PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\r\n    1959 clp 20 0 1355012 63248 21020 S 6.2 1.6 127:54.60 beam.smp\r\n    1 root 20 0 168072 9940 6204 S 0.0 0.3 2:35.24 systemd\r\n    2 root 20 0 0 0 0 S 0.0 0.0 0:00.11 kthreadd\r\n    3 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_gp\r\n    4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_par_gp\r\n    5 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 netns\r\n    7 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H-events_highpri\r\n    10 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq\r\n    11 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_rude_\r\n    12 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_tasks_trace\r\n    13 root 20 0 0 0 0 S 0.0 0.0 3:41.85 ksoftirqd/0\r\n    14 root 20 0 0 0 0 I 0.0 0.0 9:58.13 rcu_sched\r\n    15 root rt 0 0 0 0 S 0.0 0.0 0:01.65 migration/0\r\n    +------------------------------------------------------------------+\r\n    Top 10 Processes that consume high CPU using the ps command\r\n    +------------------------------------------------------------------+\r\n    %CPU PID USER COMMAND\r\n    5.0 596375 btdo php-fpm: pool btdo.uk\r\n    3.5 1039 mysql /usr/sbin/mariadbd\r\n    1.4 1959 clp /app/erts-12.0/bin/beam.smp -- -root /app -progname erl -- -home /app -- -noshell -s elixir start_cli -mode embedded -setcookie S4WHAF4LH5WHSPWUHPKIVMSFISQKQJVFWEGHSP7YW6MT5LKCNDCA==== -sname plausible -config /app/releases/0.0.1/sys -boot /app/releases/0.0.1/start -boot_var RELEASE_LIB /app/lib -- -extra --no-halt\r\n    1.2 772 redis /usr/bin/redis-server 127.0.0.1:6379\r\n    0.6 2677 systemd+ /usr/bin/clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n    0.4 579396 root nginx: worker process\r\n    0.3 2071 root node server/server.js\r\n    0.1 789 root /usr/bin/containerd\r\n    0.1 14 root [rcu_sched]\r\n    Memory Utilization on the server\r\n    +------------------------------------------------------------------+\r\n    Private + Shared = RAM used    Program\r\n    4.0 KiB + 25.5 KiB = 29.5 KiB    erl_child_setup\r\n    40.0 KiB + 24.5 KiB = 64.5 KiB    epmd\r\n    4.0 KiB + 82.5 KiB = 86.5 KiB    dumb-init\r\n    56.0 KiB + 108.5 KiB = 164.5 KiB    inet_gethost (3)\r\n    144.0 KiB + 56.5 KiB = 200.5 KiB    cron\r\n    148.0 KiB + 52.5 KiB = 200.5 KiB    atd\r\n    320.0 KiB + 0.5 KiB = 320.5 KiB    exim4\r\n    192.0 KiB + 136.0 KiB = 328.0 KiB    agetty (2)\r\n    268.0 KiB + 89.5 KiB = 357.5 KiB    qemu-ga\r\n    340.0 KiB + 104.5 KiB = 444.5 KiB    irqbalance\r\n    304.0 KiB + 361.5 KiB = 665.5 KiB    master\r\n    352.0 KiB + 361.0 KiB = 713.0 KiB    chronyd (2)\r\n    384.0 KiB + 358.5 KiB = 742.5 KiB    unattended-upgr\r\n    728.0 KiB + 111.5 KiB = 839.5 KiB    systemd-udevd\r\n    652.0 KiB + 339.5 KiB = 991.5 KiB    polkitd\r\n    732.0 KiB + 418.5 KiB = 1.1 MiB    systemd-logind\r\n    920.0 KiB + 243.5 KiB = 1.1 MiB    dbus-daemon\r\n    800.0 KiB + 391.5 KiB = 1.2 MiB    systemd-networkd\r\n    4.0 KiB + 1.3 MiB = 1.3 MiB    clckhouse-watch\r\n    820.0 KiB + 516.5 KiB = 1.3 MiB    systemd-resolved\r\n    1.4 MiB + 128.5 KiB = 1.5 MiB    rsyslogd\r\n    328.0 KiB + 1.2 MiB = 1.5 MiB    php-fpm7.1\r\n    660.0 KiB + 902.5 KiB = 1.5 MiB    qmgr\r\n    708.0 KiB + 878.5 KiB = 1.5 MiB    pickup\r\n    1.3 MiB + 458.5 KiB = 1.8 MiB    ModemManager\r\n    776.0 KiB + 1.2 MiB = 1.9 MiB    php-fpm7.2\r\n    2.0 MiB + 49.5 KiB = 2.0 MiB    proftpd\r\n    924.0 KiB + 1.2 MiB = 2.1 MiB    php-fpm7.3\r\n    1.6 MiB + 533.5 KiB = 2.2 MiB    udisksd\r\n    2.5 MiB + 1.1 MiB = 3.6 MiB    tlsmgr\r\n    2.3 MiB + 1.4 MiB = 3.7 MiB    bash (2)\r\n    3.3 MiB + 992.5 KiB = 4.3 MiB    networkd-dispat\r\n    4.3 MiB + 73.5 KiB = 4.4 MiB    memcached\r\n    4.2 MiB + 754.0 KiB = 4.9 MiB    docker-proxy (4)\r\n    3.6 MiB + 1.5 MiB = 5.1 MiB    sshd (2)\r\n    5.4 MiB + 5.0 MiB = 10.5 MiB    systemd (3)\r\n    11.1 MiB + 35.5 KiB = 11.1 MiB    clp-agent\r\n    12.3 MiB + 1.6 MiB = 13.9 MiB    containerd-shim-runc-v2 (5)\r\n    14.5 MiB + 2.6 MiB = 17.1 MiB    php-fpm8.0\r\n    17.4 MiB + 23.5 KiB = 17.4 MiB    containerd\r\n    18.9 MiB + 170.5 KiB = 19.0 MiB    redis-server\r\n    17.0 MiB + 3.8 MiB = 20.8 MiB    php-fpm8.1 (2)\r\n    21.4 MiB + 751.5 KiB = 22.2 MiB    multipathd\r\n    13.9 MiB + 10.2 MiB = 24.1 MiB    systemd-journald\r\n    28.5 MiB + 178.5 KiB = 28.6 MiB    dockerd\r\n    28.9 MiB + 1.3 MiB = 30.2 MiB    clickhouse\r\n    29.6 MiB + 15.6 MiB = 45.2 MiB    postgres (18)\r\n    40.1 MiB + 6.0 MiB = 46.0 MiB    php-fpm7.4 (2)\r\n    47.2 MiB + 8.0 MiB = 55.3 MiB    beam.smp\r\n    82.3 MiB + 82.5 KiB = 82.4 MiB    node\r\n    72.0 MiB + 30.0 MiB = 102.0 MiB    nginx (8)\r\n    807.7 MiB + 398.5 KiB = 808.1 MiB    mariadbd\r\n    ---------------------------------\r\n    1.4 GiB\r\n    =================================\r\n```\r\n\r\nActivating the script in crontab and creating a log for the run:\r\n\r\n```\r\n    #open crontab in edit mode\r\n    contab -e\r\n    #add the bellow line\r\n    */5 * * * * /bin/bash /opt/scripts/cpu-alert.sh >> /opt/scripts/cpu_check_cron.log 2>&1\r\n```\r\n\r\nThe above will create a log under /opt/scripts/cpu_check_cron.log with the output when this is not sending an email.\r\n\r\n<Button link=\"https://go.bitdoze.com/do\" text=\"DigitalOcean $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/vultr\" text=\"Vultr $100 Free\" />\r\n<Button link=\"https://go.bitdoze.com/hetzner\" text=\"Hetzner €⁠20 Free\" />","src/content/posts/monitor-cpu-usage-and-send-email-alerts-in-linux.mdx",[1461],"../../assets/images/cpu_monitoring.jpeg","5e765f2346ec6c9e","monitor-cpu-usage-and-send-email-alerts-in-linux.mdx","mac-find-big-files",{id:1464,data:1466,body:1475,filePath:1476,assetImports:1477,digest:1479,legacyId:1480,deferredRender:32},{title:1467,description:1468,date:1469,image:1470,authors:1471,categories:1472,tags:1473,canonical:1474},"How to Find the Largest Files on Your Mac with This Simple Script!","Learn how to efficiently find and manage large files on your Mac using a simple Bash script. Free up valuable storage space today!",["Date","2025-01-10T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/01/find-big-file.jpg",[19],[98],[153],"https://www.bitdoze.com/mac-find-big-files/","Finding and managing large files on your Mac is crucial for maintaining optimal performance and storage space. Over time, various files can consume significant disk space:\r\n- Video files and media projects\r\n- Virtual machine images\r\n- Large application installers\r\n- Old backup files\r\n- Downloaded content\r\n\r\nThis guide will show you how to efficiently identify and manage large files using a simple Bash script, helping you reclaim valuable storage space on your Mac.\r\n\r\n## Common Methods for Finding Large Files\r\n\r\n### Built-in Tools\r\nmacOS provides several built-in options for managing storage:\r\n1. **About This Mac > Storage**: Offers a general overview but lacks detailed file information\r\n2. **Finder's storage management**: Good for basic cleanup but not comprehensive\r\n3. **Disk Utility**: Primarily for disk management rather than file size analysis\r\n\r\n### Third-Party Applications\r\nWhile there are many paid applications available for disk space management, they often:\r\n- Require installation and regular updates\r\n- May include unnecessary features\r\n- Could impact system performance\r\n- Usually come with a subscription cost\r\n\r\nThis is why a simple script can be more efficient and cost-effective.\r\n\r\n> If you are interested to see some free cool Mac Apps you can check [toolhunt.net mac apps section](https://toolhunt.net/mac/).\r\n\r\n\r\n## How to Find the Biggest Files on Your Mac\r\n\r\n### Script\r\n\r\nThe following Bash script is designed to identify and display the largest files on your Mac. It searches through your file system while excluding certain directories that typically contain system files or temporary data.\r\n\r\n```sh\r\n#!/bin/bash\r\n\r\n# Default number of files to show if not specified\r\nif [ -z \"$1\" ]; then\r\n    num_files=20\r\nelse\r\n    num_files=$1\r\nfi\r\n\r\n# Print disk space information\r\necho \"===========================================\"\r\necho \"DISK SPACE INFORMATION\"\r\necho \"===========================================\"\r\ndf -h / | awk 'NR==2 {\r\n    printf \"Total Space: %s\\n\", $2\r\n    printf \"Used Space:  %s\\n\", $3\r\n    printf \"Free Space:  %s\\n\", $4\r\n    printf \"Usage:       %s\\n\", $5\r\n}'\r\necho \"===========================================\"\r\necho\r\n\r\n# Print script start message\r\necho \"Searching for the $num_files largest files on your Mac...\"\r\necho \"This may take a while depending on your system size...\"\r\necho \"You may be prompted for sudo password to search all directories\"\r\n\r\n# Create temporary file to store results\r\ntmp_file=$(mktemp)\r\n\r\n# Use sudo to ensure we can access all directories\r\n# Exclude system directories that might cause issues\r\nsudo find / \\                                                                                                                                                             -not \\( -path \"/System/*\" -prune \\) \\\r\n    -not \\( -path \"/private/var/vm/*\" -prune \\) \\\r\n    -not \\( -path \"/Library/Caches/*\" -prune \\) \\\r\n    -type f -print0 2>/dev/null | \\\r\n    xargs -0 du -h 2>/dev/null | \\\r\n    sort -rh | \\\r\n    head -n \"$num_files\" > \"$tmp_file\"\r\n\r\n# Clear screen and print results in a formatted way\r\necho \"===========================================\"\r\necho \"TOP $num_files LARGEST FILES ON YOUR SYSTEM\"\r\necho \"===========================================\"\r\necho\r\necho \"Size  |  File Path\"\r\necho \"-------------------------------------------\"\r\nwhile read -r line; do\r\n    size=$(echo \"$line\" | awk '{print $1}')\r\n    file=$(echo \"$line\" | cut -f2-)\r\n    printf \"%-6s | %s\\n\" \"$size\" \"$file\"\r\ndone < \"$tmp_file\"\r\necho \"===========================================\"\r\n\r\n# Clean up temporary file\r\nrm \"$tmp_file\"\r\n\r\necho \"Scan complete!\"\r\n```\r\n\r\n### Run the Script\r\n\r\nTo run this script:\r\n\r\n1. **Open Terminal**: You can find Terminal in Applications > Utilities or by searching for it using Spotlight (Cmd + Space).\r\n\r\n2. **Create a New Script File**:\r\n   ```sh\r\n   nano find_large_files.sh\r\n   ```\r\n   Paste the script into the editor and save it by pressing `CTRL + X`, then `Y`, and `Enter`.\r\n\r\n3. **Make the Script Executable**:\r\n   ```sh\r\n   chmod +x find_large_files.sh\r\n   ```\r\n\r\n4. **Run the Script**:\r\n   Execute the script by typing:\r\n   ```sh\r\n   ./find_large_files.sh [number_of_files]\r\n   ```\r\n   Replace `[number_of_files]` with how many of the largest files you want to display (default is 20).\r\n   In this process you will be promted for a password and will be asked to give various wrights to the terminal to make queries.\r\n\r\n5. **View Results**: The script will display disk space information followed by a list of the largest files found on your Mac.\r\n  ```sh\r\n  ===========================================\r\n  DISK SPACE INFORMATION\r\n  ===========================================\r\n  Total Space: 460Gi\r\n  Used Space:  10Gi\r\n  Free Space:  369Gi\r\n  Usage:       3%\r\n  ===========================================\r\n\r\n  Searching for the 20 largest files on your Mac...\r\n  This may take a while depending on your system size...\r\n  You may be prompted for sudo password to search all directories\r\n  Password:\r\n  Sorry, try again.\r\n  Password:\r\n  ===========================================\r\n  TOP 20 LARGEST FILES ON YOUR SYSTEM\r\n  ===========================================\r\n\r\n  Size  |  File Path\r\n  -------------------------------------------\r\n  13G    | /Users/dragos/.diffusionbee/downloaded_assets/FLUX.1-schnell_flux_schnell_q5p_NNC_all.sqlite\r\n  13G    | /Users/dragos/.diffusionbee/downloaded_assets/FLUX.1-dev_flux_dev_q5p_NNC_all.sqlite\r\n  8.4G   | /Users/dragos/.ollama/models/blobs/sha256-6e41c39f4490a9e8b7a65916425c6ed97f04ed95bab991c4ab6a462ff84d1608\r\n  1.9G   | /Users/dragos/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\r\n  949M   | /Applications/DaVinci Resolve/DaVinci Resolve.app/Contents/MacOS/Resolve\r\n  929M   | /Volumes/Recovery/434769EB-F939-481F-88EE-514C0C40DFD7/usr/standalone/firmware/arm64eBaseSystem.dmg\r\n  869M   | /Users/dragos/Desktop/ytvideos/ghostty capcut_h264.mov\r\n  759M   | /Users/dragos/Documents/ai images tools.mp4\r\n  687M   | /Users/dragos/Desktop/ytvideos/ai image tools.mp4\r\n  611M   | /Users/dragos/Desktop/ytvideos/ghostty yt.mp4\r\n  585M   | /Applications/CapCut.app/Contents/Frameworks/libVECreator.dylib\r\n  551M   | /Users/dragos/Desktop/ytvideos/1219.mov\r\n  528M   | /Users/dragos/Documents/ghostty.mp4\r\n  516M   | /Users/dragos/Desktop/ytvideos/mac mini m4 pro ollama and phi-4.mov\r\n  514M   | /Applications/DaVinci Resolve/DaVinci Control Panels Setup.app/Contents/Frameworks/libQt6WebEngineCore.6.dylib\r\n  504M   | /Applications/Docker.app/Contents/Resources/linuxkit/boot.img\r\n  313M   | /Applications/Logi Tune.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework\r\n  307M   | /Applications/DaVinci Resolve/DaVinci Resolve.app/Contents/Applications/Electron.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework\r\n  277M   | /Applications/Ollama.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework\r\n  235M   | /Applications/DaVinci Resolve/DaVinci Resolve.app/Contents/Libraries/libtorch_cpu.dylib\r\n  ===========================================\r\n  Scan complete!\r\n  ```\r\n  6. **Take Action**: Now you can choose to delete or keep the files that are very big, is up to you.\r\n\r\n\r\n## Related Terminal Tools\r\n\r\nIf you're getting comfortable with terminal-based tools, these might enhance your experience:\r\n\r\n- [Ghostty Terminal](https://www.bitdoze.com/ghostty-terminal/) - A modern terminal emulator offering enhanced features and better performance\r\n- [WezTerm](https://www.bitdoze.com/install-wezterm-mac/) - A powerful terminal emulator with extensive customization options\r\n- [Tmux Basics](https://www.bitdoze.com/tmux-basics/) - Learn to use Tmux for better terminal session management and productivity\r\n\r\n## Conclusions\r\n\r\nManaging disk space is crucial for maintaining optimal performance on your Mac, especialy with the prices that Apple are charging on storage :).\r\n\r\nBy utilizing this Bash script, you can efficiently identify large files that may no longer be needed, allowing you to free up valuable storage space.","src/content/posts/mac-find-big-files.mdx",[1478],"../../assets/images/25/01/find-big-file.jpg","c79a2a0cf7672df3","mac-find-big-files.mdx","memos-install",{id:1481,data:1483,body:1492,filePath:1493,assetImports:1494,digest:1496,legacyId:1497,deferredRender:32},{title:1484,description:1485,date:1486,image:1487,authors:1488,categories:1489,tags:1490,canonical:1491},"How to Install Memos with Docker Compose: EASY STEPS!","Learn how you can Memos note taking app with docker compose  and Postgres DB and take advantage of a self hosted note taking app",["Date","2024-08-14T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/08/memos-install.jpeg",[19],[98],[242],"https://www.bitdoze.com/memos-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/08/memos1.png\";\r\nimport imag2 from \"../../assets/images/24/08/memos2.jpeg\";\r\n\r\n[Memos](https://www.usememos.com/) is a self-hosted note-taking application designed for personal organization and information management. It allows users to create, store, and manage notes efficiently, providing features like markdown support, a user-friendly interface, and privacy-focused design. Memos can be run on various platforms, but using Docker Compose simplifies the deployment process, making it easy to set up and maintain.\r\n\r\n- **Privacy-First Approach:** Memos ensures that all user data is kept private and secure, allowing users to retain control over their information.\r\n\r\n- **Markdown Support:** Users can create notes using plain text with extensive Markdown syntax, facilitating easy formatting and organization.\r\n\r\n- **Lightweight Architecture:** Built with Go and React.js, Memos is designed to be lightweight, ensuring fast performance and minimal resource usage.\r\n\r\n- **Customizable Features:** Users can personalize their experience by customizing the server name, icon, description, and system styles.\r\n\r\n- **Open Source:** Memos is completely open source, allowing users to contribute to its development and customize the application as needed.\r\n\r\n- **Free to Use:** All features of Memos are available at no cost, with no hidden charges or subscriptions.\r\n\r\n- **Data Persistence:** Notes are saved in a SQLite database file, ensuring data is retained even after the application is closed.\r\n\r\n- **User-Friendly Interface:** Memos offers an intuitive interface that makes it easy to capture and manage notes.\r\n\r\n- **Multi-Device Accessibility:** Users can access their notes from various devices, enhancing convenience and flexibility.\r\n\r\n- **Collaboration Features:** Memos allows for easy sharing of notes, enabling collaboration among users.\r\n\r\nIf you are looking for a more complex app for your note taking needs you can check:\r\n\r\n- [How to Install Outline Wiki on Docker](https://www.bitdoze.com/outline-install/)\r\n- [Docmost Docker Compose Install](https://www.bitdoze.com/docmost-docker-install/)\r\n\r\n## Install Memos with Docker Compose\r\n\r\nTo install Memos using Docker Compose, you need to create a `docker-compose.yml` file that defines the services, networks, and volumes necessary for running the application. Below are two configurations for deploying Memos: one using SQLite and another using PostgreSQL.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/vBa4ogLNF14\"\r\n  label=\"Memos Installation\"\r\n/>\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### 1 Prerequizites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host Memos, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) You can use a VPS to have Memos installed but performances will not be that good. In our test we are using a 8 CPUs 16 GB RAM and is bearly moving or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Traefic with Docker set up, you can check: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/) or [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n\r\n### 2 Memos with SQLite\r\n\r\nThe following `docker-compose.yml` file sets up Memos with SQLite as the database:\r\n\r\n```yaml\r\nservices:\r\n  memos:\r\n    image: neosmemo/memos:stable\r\n    container_name: memos\r\n    user: root\r\n    restart: unless-stopped\r\n    networks:\r\n      - traefik-net\r\n    volumes:\r\n      - ./memos/:/var/opt/memos\r\n    labels:\r\n      - traefik.enable=true\r\n      - traefik.http.routers.memos.rule=Host(`memos.domain.com`)\r\n      - traefik.http.routers.memos.entrypoints=https\r\n      - traefik.http.services.memos.loadbalancer.server.port=5230\r\n\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\n**Explanation of the Configuration:**\r\n\r\n- **services:** This section defines the services that Docker will run. Here, we have a single service named `memos`.\r\n\r\n- **image:** Specifies the Docker image to use, in this case, `neosmemo/memos:stable`.\r\n\r\n- **container_name:** Assigns a name to the container for easier management.\r\n\r\n- **user:** Runs the container as the root user.\r\n\r\n- **restart:** Configures the restart policy. `unless-stopped` means the container will restart unless explicitly stopped.\r\n\r\n- **networks:** Connects the service to an external network named `traefik-net`, which is useful for routing.\r\n\r\n- **volumes:** Maps a local directory (`./memos/`) to the container's data storage directory (`/var/opt/memos`), ensuring data persistence.\r\n\r\n- **labels:** These are used for configuring Traefik, a reverse proxy, to route traffic to the Memos service based on the specified hostname and entry points.\r\n\r\nThis is the easiest way to have memos installed. You can expose also the port in case you want to use ClaudFlare tunnels or other reverse proxy or direct access with:\r\n\r\n```yaml\r\nports:\r\n  - 5230:5230\r\n```\r\n\r\n### 3 Memos with PostgreSQL DB\r\n\r\nIf you prefer to use PostgreSQL as the database, you can use the following configuration. This is for cases when you have a lot off notes to have a powerfull database. This are the [memos DB options with the doc](https://www.usememos.com/docs/install/database).\r\n\r\n```yaml\r\nservices:\r\n  memos:\r\n    image: neosmemo/memos:stable\r\n    container_name: memos\r\n    restart: unless-stopped\r\n    networks:\r\n      - traefik-net\r\n    depends_on:\r\n      memos-db:\r\n        condition: service_healthy\r\n    volumes:\r\n      - ./memos/:/var/opt/memos\r\n    environment:\r\n      MEMOS_DRIVER: postgres\r\n      MEMOS_DSN: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@memos-db:5432/${POSTGRES_DB}?sslmode=disable\r\n    labels:\r\n      - traefik.enable=true\r\n      - traefik.http.routers.memos.rule=Host(`memos.domain.com`)\r\n      - traefik.http.routers.memos.entrypoints=https\r\n      - traefik.http.services.memos.loadbalancer.server.port=5230\r\n  memos-db:\r\n    image: postgres:16.1-alpine\r\n    networks:\r\n      - traefik-net\r\n    healthcheck:\r\n      test:\r\n        - CMD-SHELL\r\n        - pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}\r\n      interval: 5s\r\n      timeout: 5s\r\n      retries: 5\r\n    volumes:\r\n      - ./memos-db:/var/lib/postgresql/data:rw\r\n    environment:\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n    restart: on-failure:5\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\n**Explanation of the Configuration:**\r\n\r\n- The `memos` service is similar to the SQLite configuration but includes environment variables for PostgreSQL.\r\n\r\n- **depends_on:** Ensures that the `memos-db` service starts before the `memos` service.\r\n\r\n- **environment:** Sets environment variables for the PostgreSQL connection, including the database name, user, and password.\r\n\r\n- The `memos-db` service runs a PostgreSQL database, with a health check to ensure it is ready before the Memos service starts.\r\n\r\n### 4 Create an `.env` File\r\n\r\nFor the PostgreSQL configuration, create a `.env` file in the same directory as your `docker-compose.yml` file to define the environment variables:\r\n\r\n```sh\r\nPOSTGRES_DB=memos\r\nPOSTGRES_USER=memos\r\nPOSTGRES_PASSWORD=memos\r\n```\r\n\r\nYou can change the details as you like for the database details.\r\n\r\n### 5 Start the Docker Compose File\r\n\r\nTo start the Memos application, run the following command in your terminal from the directory containing the `docker-compose.yml` file:\r\n\r\n```sh\r\ndocker compose up -d\r\n```\r\n\r\nThis command will download the necessary Docker images, create the containers, and start the services in detached mode.\r\n\r\n### 6 Access Memos UI and Create Your First User\r\n\r\nOnce the containers are running, you can access the Memos web interface by navigating to `http://memos.domain.com` in your web browser. You will be greeted by a sign-up screen where you can create your first user account.\r\n\r\nThen the memos UI will be like this:\r\n\r\n<Picture src={imag1} alt=\"Memos UI\" />\r\n\r\nThen you have the settings area where you can change the look frok light to dark, add users or enable SSO.\r\n\r\n<Picture src={imag2} alt=\"Memos Settings\" />\r\n\r\n## Conclusions\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\nSetting up Memos using Docker Compose provides a straightforward way to deploy a self-hosted note-taking application. By using either SQLite or PostgreSQL, users can choose the database that best fits their needs. The configurations provided allow for easy customization and scalability, making Memos a flexible solution for personal organization and note management. Enjoy using Memos to capture and organize your thoughts efficiently!\r\n\r\nIf you're interested in exploring more Docker containers for your home server or self-hosted setup, including other productivity tools and applications, check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you build a powerful and versatile self-hosted environment that can complement your Memos installation and enhance your overall productivity ecosystem.","src/content/posts/memos-install.mdx",[1495],"../../assets/images/24/08/memos-install.jpeg","f219f0c4bef4e36d","memos-install.mdx","migrate-astro-bun",{id:1498,data:1500,body:1509,filePath:1510,assetImports:1511,digest:1513,legacyId:1514,deferredRender:32},{title:1501,description:1502,date:1503,image:1504,authors:1505,categories:1506,tags:1507,canonical:1508},"How to Migrate Astro to Bun on CloudFlare","Learn how you can migrate your Astro project to Bun in CloudFlare Pages",["Date","2024-03-19T01:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/migrate-astro-bun.jpeg",[19],[98],[312,23],"https://www.bitdoze.com/migrate-astro-bun/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nBun is a fast all-in-one JavaScript runtime that can significantly speed up your Astro projects. With Cloudflare now providing native support for Bun, it's easier than ever to deploy Astro apps powered by Bun. This guide walks through the steps to migrate an existing Astro project from Node.js to Bun and deploy it on Cloudflare Pages.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/RUBWS6zp2us\"\r\n  label=\"How to Migrate Astro to Bun on CloudFlare\"\r\n/>\r\n\r\n## 1. Install Bun\r\n\r\nFirst, install Bun on your development machine. You can install it with a single command:\r\n\r\n```sh\r\ncurl -fsSL https://bun.sh/install | bash\r\n```\r\n\r\nThis will download and install the latest version of Bun.\r\n\r\nYou can check [Bun vs NPM, Yarn, PNPM, and Others](https://www.bitdoze.com/bun-package-manager/) to see some details and benchmark for Bun.\r\n\r\n## 2. Remove Existing Lock Files\r\n\r\nIf your Astro project used a different package manager like npm or Yarn, remove the existing lock files:\r\n\r\n```sh\r\n# If you were using NPM:\r\nrm package-lock.json\r\n\r\n# If you were using pnpm:\r\nrm pnpm-lock.yaml\r\n\r\n# If you were using Yarn:\r\nrm yarn.lock\r\n```\r\n\r\nThis ensures Bun can cleanly install dependencies without conflicts from other package manager lock files.\r\n\r\n## 3. Install Dependencies with Bun\r\n\r\nNow use Bun to install your project's dependencies:\r\n\r\n```sh\r\nbun install\r\n```\r\n\r\nBun will read your package.json and download the required packages much faster than Node.js.\r\n\r\n## 4. Test Your Astro Project Locally\r\n\r\nStart the Astro development server using Bun:\r\n\r\n```sh\r\nbun run dev\r\n```\r\n\r\nOpen http://localhost:4321 and verify your project works as expected.\r\n\r\nYou can also test building your project for production:\r\n\r\n```sh\r\nbun run build\r\n```\r\n\r\nFix any issues before proceeding to deployment.\r\n\r\n## 5. Update the Cloudflare Build Command\r\n\r\nLog into the Cloudflare dashboard and go to your Astro project's settings.\r\n\r\nChange the build command to use Bun:\r\n\r\n```\r\nbun astro build\r\n```\r\n\r\nThis tells Cloudflare to use Bun to build your Astro project.\r\n\r\n## 6. Deploy to Cloudflare\r\n\r\nCommit your changes and push to your Git repository.\r\n\r\n```sh\r\ngit add .\r\ngit commit -m \"Migrate to Bun\"\r\ngit push\r\n```\r\n\r\nCloudflare should automatically detect the update and begin building and deploying your Astro project with Bun.\r\n\r\n## 7. Check the Cloudflare Logs\r\n\r\nAfter pushing your changes, Cloudflare will attempt to build your project using the new settings. Monitor the build logs in the Cloudflare dashboard to ensure that the build completes successfully and that there are no errors during the process.\r\n\r\n## Conclusion\r\n\r\nMigrating your Astro project to use Bun on Cloudflare can lead to faster build times and a more efficient development process. By following the steps outlined in this article, you can smoothly transition to Bun and take advantage of its performance benefits. Remember to test your project thoroughly after migration and monitor the Cloudflare build logs for any potential issues. With Bun, you're now set to enjoy a modern and speedy JavaScript runtime for your Astro projects on Cloudflare.","src/content/posts/migrate-astro-bun.mdx",[1512],"../../assets/images/24/03/migrate-astro-bun.jpeg","cac6ba4d9aa05e64","migrate-astro-bun.mdx","multiple-postgres-databases-docker",{id:1515,data:1517,body:1527,filePath:1528,assetImports:1529,digest:1531,legacyId:1532,deferredRender:32},{title:1518,description:1519,date:1520,image:1521,authors:1522,categories:1523,tags:1524,canonical:1526},"Multiple PostgreSQL Databases in ONE Service: THE Docker Compose WAY!","Master multiple PostgreSQL databases effortlessly! Discover how Docker Compose simplifies your setup. Don't miss out – transform your workflow NOW!",["Date","2024-07-26T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/multiple-postgres-databases-docker.jpeg",[19],[98],[100,1525],"postgres","https://www.bitdoze.com/multiple-postgres-databases-docker/","Docker Compose allows you to set up complex application stacks, including database services. While PostgreSQL's official Docker image supports creating a single database by default, you can configure it to create multiple databases within one container.\r\n\r\nThere are a few common reasons why you might want to set up multiple databases in the same Docker Compose service:\r\n\r\n1. **Development and testing environments:** When developing or testing an application that uses multiple databases, it's convenient to have all the databases running in a single container for simplicity.\r\n\r\n2. **Resource efficiency:** Using a single container for multiple databases can be more resource-efficient than running separate containers for each database, especially in development environments.\r\n\r\n3. **Legacy application support:** Some legacy applications may expect multiple databases to be available on the same server. Replicating this setup in Docker can make migration easier.\r\n\r\n4. **Microservices architecture:** In a microservices architecture, you might have multiple small databases that are closely related and benefit from being grouped together.\r\n\r\n5. **Data isolation:** You may want to isolate different types of data (e.g. user data, product data, analytics) into separate databases for security or organizational reasons, while still keeping them in the same service.\r\n\r\n6. **Multi-tenant applications:** For applications serving multiple tenants, you might want a separate database for each tenant, but still keep them managed within a single service.\r\n\r\n7. **Testing database migrations:** When testing database migrations or upgrades, it can be useful to have multiple versions or states of a database available simultaneously.\r\n\r\n8. **Reducing complexity:** For smaller projects or prototypes, having all databases in one container can reduce the overall complexity of the Docker setup.\r\n\r\nHowever, it's important to note that while this approach can be useful for development and testing, for production environments it's generally recommended to use separate containers for different databases to ensure better isolation, scalability, and easier management. The specific needs of your project and environment should guide the decision on whether to use multiple databases in a single service or separate them into different containers.\r\n\r\n## Step 1: Create the Initialization Script\r\n\r\nFirst, create a bash script that will initialize multiple databases. Save this script as `init-multiple-databases.sh`:\r\n\r\n```sh\r\n#!/bin/bash\r\n\r\nset -e\r\nset -u\r\n\r\nfunction create_user_and_database() {\r\n  local database=$1\r\n  echo \"  Creating user and database '$database'\"\r\n  psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"postgres\" <<-EOSQL\r\n        CREATE USER $database;\r\n        CREATE DATABASE $database;\r\n        GRANT ALL PRIVILEGES ON DATABASE $database TO $database;\r\nEOSQL\r\n}\r\n\r\nif [ -n \"${POSTGRES_MULTIPLE_DATABASES:-}\" ]; then\r\n  echo \"Multiple database creation requested: $POSTGRES_MULTIPLE_DATABASES\"\r\n  for db in $(echo $POSTGRES_MULTIPLE_DATABASES | tr ',' ' '); do\r\n    create_user_and_database $db\r\n  done\r\n  echo \"Multiple databases created\"\r\nfi\r\n```\r\n\r\nThe script is taking a list of databases and connects to the Postgress and creates the databases and grants all privilages.\r\n\r\n## Step 2: Create the Docker Compose File\r\n\r\nCreate a `docker-compose.yml` file with the following content:\r\n\r\n```yml\r\nversion: \"3.8\"\r\nservices:\r\n  postgres:\r\n    image: postgres:16-alpine\r\n    container_name: postgres_multi_db\r\n    environment:\r\n      POSTGRES_USER: ${POSTGRES_USER:-postgres}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}\r\n      POSTGRES_MULTIPLE_DATABASES: db1,db2,db3\r\n    volumes:\r\n      - ./init-multiple-databases.sh:/docker-entrypoint-initdb.d/init-multiple-databases.sh\r\n      - ./postgres_data:/var/lib/postgresql/data\r\n    ports:\r\n      - 5432:5432\r\n    restart: unless-stopped\r\n```\r\n\r\nThe postgres 16 will be uses to create the container and the user and password will be fetched from `.env` file. The `POSTGRES_MULTIPLE_DATABASES` has the list with databases to be created. All the DBs will use the same user and pass. The script from previous points `init-multiple-databases.sh` is is loaded from the docker compose file.\r\n\r\n## Step 3: Set Up Environment Variables (Optional)\r\n\r\nCreate a .env file in the same directory as your docker-compose.yml to store sensitive information:\r\n\r\n```sh\r\nPOSTGRES_USER=myuser\r\nPOSTGRES_PASSWORD=mypassword\r\n```\r\n\r\n## Step 4: Run the Docker Compose Stack\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nLog:\r\n\r\n```sh\r\npostgres_multi_db  | The files belonging to this database system will be owned by user \"postgres\".\r\npostgres_multi_db  | This user must also own the server process.\r\npostgres_multi_db  |\r\npostgres_multi_db  | The database cluster will be initialized with locale \"en_US.utf8\".\r\npostgres_multi_db  | The default database encoding has accordingly been set to \"UTF8\".\r\npostgres_multi_db  | The default text search configuration will be set to \"english\".\r\npostgres_multi_db  |\r\npostgres_multi_db  | Data page checksums are disabled.\r\npostgres_multi_db  |\r\npostgres_multi_db  | fixing permissions on existing directory /var/lib/postgresql/data ... ok\r\npostgres_multi_db  | creating subdirectories ... ok\r\npostgres_multi_db  | selecting dynamic shared memory implementation ... posix\r\npostgres_multi_db  | selecting default max_connections ... 100\r\npostgres_multi_db  | selecting default shared_buffers ... 128MB\r\npostgres_multi_db  | selecting default time zone ... UTC\r\npostgres_multi_db  | creating configuration files ... ok\r\npostgres_multi_db  | running bootstrap script ... ok\r\npostgres_multi_db  | sh: locale: not found\r\npostgres_multi_db  | 2024-07-26 10:41:39.527 UTC [35] WARNING:  no usable system locales were found\r\npostgres_multi_db  | performing post-bootstrap initialization ... ok\r\npostgres_multi_db  | syncing data to disk ... ok\r\npostgres_multi_db  |\r\npostgres_multi_db  |\r\npostgres_multi_db  | Success. You can now start the database server using:\r\npostgres_multi_db  |\r\npostgres_multi_db  |     pg_ctl -D /var/lib/postgresql/data -l logfile start\r\npostgres_multi_db  |\r\npostgres_multi_db  | initdb: warning: enabling \"trust\" authentication for local connections\r\npostgres_multi_db  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.\r\npostgres_multi_db  | waiting for server to start....2024-07-26 10:41:40.720 UTC [42] LOG:  starting PostgreSQL 16.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, 64-bit\r\npostgres_multi_db  | 2024-07-26 10:41:40.722 UTC [42] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\npostgres_multi_db  | 2024-07-26 10:41:40.730 UTC [45] LOG:  database system was shut down at 2024-07-26 10:41:40 UTC\r\npostgres_multi_db  | 2024-07-26 10:41:40.739 UTC [42] LOG:  database system is ready to accept connections\r\npostgres_multi_db  |  done\r\npostgres_multi_db  | server started\r\npostgres_multi_db  | CREATE DATABASE\r\npostgres_multi_db  |\r\npostgres_multi_db  |\r\npostgres_multi_db  | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/init-multiple-databases.sh\r\npostgres_multi_db  | Multiple database creation requested: db1,db2,db3\r\npostgres_multi_db  |   Creating user and database 'db1'\r\npostgres_multi_db  | CREATE ROLE\r\npostgres_multi_db  | CREATE DATABASE\r\npostgres_multi_db  | GRANT\r\npostgres_multi_db  |   Creating user and database 'db2'\r\npostgres_multi_db  | CREATE ROLE\r\npostgres_multi_db  | CREATE DATABASE\r\npostgres_multi_db  | GRANT\r\npostgres_multi_db  |   Creating user and database 'db3'\r\npostgres_multi_db  | CREATE ROLE\r\npostgres_multi_db  | CREATE DATABASE\r\npostgres_multi_db  | GRANT\r\npostgres_multi_db  | Multiple databases created\r\npostgres_multi_db  |\r\npostgres_multi_db  | waiting for server to shut down....2024-07-26 10:41:41.202 UTC [42] LOG:  received fast shutdown request\r\npostgres_multi_db  | 2024-07-26 10:41:41.204 UTC [42] LOG:  aborting any active transactions\r\npostgres_multi_db  | 2024-07-26 10:41:41.214 UTC [42] LOG:  background worker \"logical replication launcher\" (PID 48) exited with exit code 1\r\npostgres_multi_db  | 2024-07-26 10:41:41.214 UTC [43] LOG:  shutting down\r\npostgres_multi_db  | 2024-07-26 10:41:41.215 UTC [43] LOG:  checkpoint starting: shutdown immediate\r\npostgres_multi_db  | 2024-07-26 10:41:41.465 UTC [43] LOG:  checkpoint complete: wrote 3687 buffers (22.5%); 0 WAL file(s) added, 0 removed, 1 recycled; write=0.088 s, sync=0.153 s, total=0.251 s; sync files=1196, longest=0.005 s, average=0.001 s; distance=17073 kB, estimate=17073 kB; lsn=0/259C0B8, redo lsn=0/259C0B8\r\npostgres_multi_db  | 2024-07-26 10:41:41.497 UTC [42] LOG:  database system is shut down\r\npostgres_multi_db  |  done\r\npostgres_multi_db  | server stopped\r\npostgres_multi_db  |\r\npostgres_multi_db  | PostgreSQL init process complete; ready for start up.\r\npostgres_multi_db  |\r\npostgres_multi_db  | 2024-07-26 10:41:41.555 UTC [1] LOG:  starting PostgreSQL 16.3 on x86_64-pc-linux-musl, compiled by gcc (Alpine 13.2.1_git20240309) 13.2.1 20240309, 64-bit\r\npostgres_multi_db  | 2024-07-26 10:41:41.555 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\r\npostgres_multi_db  | 2024-07-26 10:41:41.555 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\r\npostgres_multi_db  | 2024-07-26 10:41:41.559 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\npostgres_multi_db  | 2024-07-26 10:41:41.568 UTC [67] LOG:  database system was shut down at 2024-07-26 10:41:41 UTC\r\npostgres_multi_db  | 2024-07-26 10:41:41.582 UTC [1] LOG:  database system is ready to accept connections\r\n```\r\n\r\n## Step 5: Verify the Databases\r\n\r\nTo confirm that the databases were created successfully, you can connect to the PostgreSQL container and list the databases:\r\n\r\n```sh\r\ndocker exec -it postgres_multi_db psql -U myuser -c \"\\l\"\r\n```\r\n\r\n```sh\r\nroot@docker-cloud:/opt/stacks/multi-postgress# docker exec -it postgres_multi_db psql -U bitdoze -c \"\\l\"\r\n                                                     List of databases\r\n   Name    |  Owner  | Encoding | Locale Provider |  Collate   |   Ctype    | ICU Locale | ICU Rules |  Access privileges\r\n-----------+---------+----------+-----------------+------------+------------+------------+-----------+---------------------\r\n bitdoze   | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           |\r\n db1       | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =Tc/bitdoze        +\r\n           |         |          |                 |            |            |            |           | bitdoze=CTc/bitdoze+\r\n           |         |          |                 |            |            |            |           | db1=CTc/bitdoze\r\n db2       | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =Tc/bitdoze        +\r\n           |         |          |                 |            |            |            |           | bitdoze=CTc/bitdoze+\r\n           |         |          |                 |            |            |            |           | db2=CTc/bitdoze\r\n db3       | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =Tc/bitdoze        +\r\n           |         |          |                 |            |            |            |           | bitdoze=CTc/bitdoze+\r\n           |         |          |                 |            |            |            |           | db3=CTc/bitdoze\r\n postgres  | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           |\r\n template0 | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =c/bitdoze         +\r\n           |         |          |                 |            |            |            |           | bitdoze=CTc/bitdoze\r\n template1 | bitdoze | UTF8     | libc            | en_US.utf8 | en_US.utf8 |            |           | =c/bitdoze         +\r\n           |         |          |                 |            |            |            |           | bitdoze=CTc/bitdoze\r\n(7 rows)\r\n```\r\n\r\n## Different users and Passwords for Databases\r\n\r\nTo handle different user permissions for each database in Docker Compose when setting up multiple PostgreSQL databases, you can modify the initialization script and Docker Compose configuration. Here's how to achieve this:\r\n\r\n1. Update the initialization script (init-multiple-databases.sh) to accept custom users and permissions:\r\n\r\n```sh\r\n#!/bin/bash\r\n\r\nset -e\r\nset -u\r\n\r\nfunction create_user_and_database() {\r\n    local database=$1\r\n    local user=$2\r\n    local password=$3\r\n    echo \"Creating user '$user' and database '$database'\"\r\n    psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"postgres\" <<-EOSQL\r\n        CREATE USER $user WITH PASSWORD '$password';\r\n        CREATE DATABASE $database;\r\n        GRANT ALL PRIVILEGES ON DATABASE $database TO $user;\r\nEOSQL\r\n}\r\n\r\nif [ -n \"${POSTGRES_MULTIPLE_DATABASES:-}\" ]; then\r\n    echo \"Multiple database creation requested: $POSTGRES_MULTIPLE_DATABASES\"\r\n    for db_config in $(echo $POSTGRES_MULTIPLE_DATABASES | tr ',' ' '); do\r\n        IFS=':' read -r db user password <<< \"$db_config\"\r\n        create_user_and_database $db $user $password\r\n    done\r\n    echo \"Multiple databases created\"\r\nfi\r\n```\r\n\r\n2. Modify your `docker-compose.yml` file to pass the database configurations:\r\n\r\n```yml\r\nversion: \"3.8\"\r\nservices:\r\n  postgres:\r\n    image: postgres:16-alpine\r\n    container_name: postgres_multi_db\r\n    environment:\r\n      POSTGRES_USER: ${POSTGRES_USER:-postgres}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}\r\n      POSTGRES_MULTIPLE_DATABASES: db1:user1:pass1,db2:user2:pass2,db3:user3:pass3\r\n    volumes:\r\n      - ./init-multiple-databases.sh:/docker-entrypoint-initdb.d/init-multiple-databases.sh\r\n      - ./postgres_data:/var/lib/postgresql/data\r\n    ports:\r\n      - 5432:5432\r\n    restart: unless-stopped\r\n```\r\n\r\nHere `POSTGRES_MULTIPLE_DATABASES: db1:user1:pass1,db2:user2:pass2,db3:user3:pass3` will contain the DB,USER and PASS, you can add this into you `.env` if you want to make it secure.\r\n\r\nThis is how easy it is to have multiple databases created in docker compose for same postgres service.","src/content/posts/multiple-postgres-databases-docker.mdx",[1530],"../../assets/images/24/07/multiple-postgres-databases-docker.jpeg","e271bc3feb3c1e7b","multiple-postgres-databases-docker.mdx","nexterm-docker-install",{id:1533,data:1535,body:1544,filePath:1545,assetImports:1546,digest:1548,legacyId:1549,deferredRender:32},{title:1536,description:1537,date:1538,image:1539,authors:1540,categories:1541,tags:1542,canonical:1543},"Combine NexTerm & Docker for Unmatched SSH, VNC & RDP Management","Learn how you can install NexTerm on docker with docker-compose and manage easier your SSH, RDP or VNC connections.",["Date","2024-09-05T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/09/nexterm-docker-install.png",[19],[98],[242],"https://www.bitdoze.com/nexterm-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/09/nexterm-interface.png\";\r\nimport imag2 from \"../../assets/images/24/09/nexterm-add-server.png\";\r\n\r\n\r\n[Nexterm](https://nexterm.dev/) is an innovative open-source server management tool designed to simplify the management of SSH, VNC, and RDP connections. It is currently in development and available for preview, offering a modern and user-friendly interface that enhances the server management experience. Nexterm aims to provide a seamless solution for administrators looking to manage multiple server connections efficiently, all while being built on Docker for easy deployment and scalability. The project is actively developed, and users are encouraged to provide feedback to help shape its future features and functionalities.\r\n\r\n## NexTerm  Features\r\n\r\nNexterm boasts a variety of features that cater to the needs of system administrators:\r\n\r\n- **Multi-Protocol Support**: Nexterm supports SSH, VNC, and RDP protocols, allowing users to manage different types of connections from a single interface.\r\n\r\n- **User-Friendly Interface**: The application is designed with an intuitive layout, organizing connections into folders and tabs for easy navigation.\r\n\r\n- **Two-Factor Authentication**: To enhance security, Nexterm includes two-factor authentication, ensuring that only authorized users can access server connections.\r\n\r\n- **Session Management**: Users can manage their sessions effectively, keeping track of active connections and their statuses.\r\n\r\n- **Customizable Snippets**: Nexterm allows users to create and manage command snippets, making it easier to execute frequently used commands quickly. (future planned)\r\n\r\n- **Active Development**: The project is in its early stages, with plans for future enhancements, including integration with AI tools and improved synchronization features.\r\n\r\n## Install NexTerm on Docker\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/O4NmTxLXfrE\"\r\n  label=\"Nexterm install docker\"\r\n/>\r\n\r\nSetting up Nexterm on Docker is straightforward, allowing users to get started quickly. Below are the steps to install Nexterm using Docker.\r\n\r\n### 1. Prerequizites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host NexTerm, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) You can use a VPS to have Memos installed but performances will not be that good. In our test we are using a 8 CPUs 16 GB RAM and is bearly moving or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Traefic with Docker set up, you can check: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/) or [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n### 2. Nexterm Docker Compose File\r\n\r\nTo deploy Nexterm, you will need a `docker-compose.yml` file. Here’s a sample configuration:\r\n\r\n```yaml\r\nservices:\r\n  nexterm:\r\n    restart: always\r\n    volumes:\r\n      - ./nexterm:/app/data\r\n    image: germannewsmaker/nexterm:1.0.1-OPEN-PREVIEW\r\n    networks:\r\n      - traefik-net\r\n    labels:\r\n      - traefik.enable=true\r\n      - traefik.http.routers.nexterm.rule=Host(`nexterm.domain.com`)\r\n      - traefik.http.routers.nexterm.entrypoints=https\r\n      - traefik.http.services.nexterm.loadbalancer.server.port=6989\r\n\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\nYou can check [NexTerm doc](https://docs.nexterm.dev/preview) to see if something changed in the docker-compose file.\r\n\r\nThe provided Docker Compose configuration for Nexterm includes the following key elements:\r\n\r\n- **Image**: The Nexterm container uses the `germannewsmaker/nexterm:1.0.1-OPEN-PREVIEW` image, which is the current preview version of the Nexterm application.\r\n\r\n- **Networks**: The Nexterm container is connected to the `traefik-net` network, which is likely an external network set up for routing purposes.\r\n\r\n- **Labels**: The configuration includes several labels for Traefik, a popular reverse proxy and load balancer:\r\n  - `traefik.enable=true`: Enables Traefik for this service.\r\n  - `traefik.http.routers.memos.rule=Host(`nexterm.domain.com`)`: Specifies the domain name (`nexterm.domain.com`) for which Traefik should route traffic to this service.\r\n  - `traefik.http.routers.memos.entrypoints=https`: Configures Traefik to use the HTTPS entry point for this service.\r\n  - `traefik.http.services.memos.loadbalancer.server.port=6989`: Specifies the port (6989) on which the Nexterm service is listening for incoming traffic.\r\n\r\nIn summary, this configuration sets up the Nexterm container to use the latest preview image, connects it to a Traefik-managed network, and configures Traefik to route traffic to the Nexterm service using the specified domain name and HTTPS protocol. The Nexterm service is expected to be listening on port 6989 inside the container.\r\n\r\n### 3. Start the Container\r\n\r\nOnce you have your `docker-compose.yml` file ready, you can start the Nexterm container using the following command:\r\n\r\n```sh\r\ndocker compose up -d\r\n```\r\n\r\nThis command runs Nexterm in detached mode, allowing it to operate in the background.\r\n\r\n### 4. Add First Connections to NexTerm\r\n\r\nAfter starting the container, you can access the Nexterm web interface at `https://nexterm.domain.com`. From there, you can begin adding your first connections by entering the necessary details for each server you wish to manage.\r\nIn the beginning you will be prompted to create a user and a password that you will use for login. After you can go and add your SSH, RDP or VNC connections.\r\n\r\n<Picture src={imag1} alt=\"NexTerm UI\" />\r\n<Picture src={imag2} alt=\"NexTerm Add server\" />\r\n\r\n## Conclusions\r\n\r\nCombining Nexterm with Docker provides a powerful solution for managing server connections through SSH, VNC, and RDP. The ease of installation and the array of features make it an attractive option for system administrators looking for a modern management tool. As Nexterm continues to develop, it promises to evolve into a comprehensive platform that addresses the needs of users in diverse environments. By leveraging Docker, users can enjoy a scalable and efficient infrastructure that enhances their server management capabilities.","src/content/posts/nexterm-docker-install.mdx",[1547],"../../assets/images/24/09/nexterm-docker-install.png","b7208442066846e4","nexterm-docker-install.mdx","nicegui-get-started",{id:1550,data:1552,body:1562,filePath:1563,assetImports:1564,digest:1566,legacyId:1567,deferredRender:32},{title:1553,description:1554,date:1555,image:1556,authors:1557,categories:1558,tags:1559,canonical:1561},"NiceGUI For Beginners: Build An UI to Python App in 5 Minutes","Master NiceGUI quickly! Learn to add a user interface to your Python app in just 5 minutes with our beginner-friendly guide.",["Date","2024-03-07T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/nicegui-beginners.jpeg",[19],[98],[1560,294],"nicegui","https://www.bitdoze.com/nicegui-get-started/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n[NiceGUI](https://nicegui.io/) is a newer framework that aims to provide a simple and elegant way to create GUIs with Python. It uses Vue, Quasar, and Tailwind for its frontend, which allows you to create web-based GUIs with HTML, CSS, and JavaScript.\r\n\r\nOver time I have checked a few Python frameworks like Streamlit or Taipy that can help me build a Web UI for my Python application and in the end, I have decided to stick with NiceGUI as it provides a better speed and more customizations for my Python app. I have created some comparison articles that will help you see the exact differences: [Streamlit vs. NiceGUI](https://www.bitdoze.com/streamlit-vs-nicegui/) or [Streamlit vs Taipy](https://www.bitdoze.com/streamlit-vs-taipy/). There is also a master article with all the [Python Web UI frameworks](https://www.bitdoze.com/best-python-web-frameworks/) that you can use and there are a lot, you should choose in function of your needs.\r\n\r\n## NiceGUI Features\r\n\r\nIn the below section I would like to highlight the most important features of NiceGUI and why I think is one of the best if you want to build a UI for your Python apps:\r\n\r\n- **Performance**: NiceGUI has a very good performance when it comes to interacting with the components, websites need to be fast otherwise visitors will not like it. NiceGUI uses Vue, Quasar, and Tailwind and makes things very fast, you will not even know that the website is using Python behind the hood.\r\n- **Customizations**: I like to have the option to customize the app the way I like even if it takes longer, the Tailwind classes, Quasar props and direct CSS styles will help you customize the app in the way you like. Also is very easy to change the default things and add JavaScript if you need.\r\n- **Easy to Use**: I am not a Python expert nor a CSS or HTML one I know some things so I need the framework to be easy to use. After a couple of days, I understood most of the things and NiceGUI documentation and examples will help you understand most of the things.\r\n- **Components**: NiceGUI provides the components you need to build the app, easily. You have `row`, `columns`, `markdown`, `images`, `sliders`, `cards` and a lot of other things that you can easily integrate with your Python code.\r\n\r\nThese are some of the most important things for me, in function what you need to do you can check NiceGUI documentation and see if it has the things you need.\r\n\r\nIf you want to deploy NiceGUI or any Python App to Docker you can check: [How To Run Any Python App in Docker with Docker Compose](https://www.bitdoze.com/docker-run-python/)\r\n\r\n## Getting Started With NiceGUI\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/tfBKRxbCsao\"\r\n  label=\"NiceGUI For Beginners\"\r\n/>\r\n\r\nIn the below section we are going to see how you can start with NiceGUI and what are some of the components and customizations you can use [Bitdoze Tools](https://tools.bitdoze.com/) is build with NiceGUI so you can take a look.\r\n\r\n### Install and Run NiceGUI\r\n\r\nNiceGui is easy to install and you just need to run a pip command:\r\n\r\n```python\r\npip install nicegui\r\n```\r\n\r\nAfter you just need to create an `.py` file and start it:\r\n\r\n```python\r\nfrom nicegui import ui\r\n\r\nui.label('Hello NiceGUI!')\r\n\r\nui.run()\r\n```\r\n\r\n- first, you import the UI like most of the other frameworks `from nicegui import ui`\r\n- after you add your code with elements, `ui.label('Hello NiceGUI!')` will just add a text on the page, we will see next some of the other things.\r\n- after you add the `ui.run()` that will tell python to run it\r\n\r\nAt the end you just need to run your `.py` file and NiceGUI will start the app on `8080` port:\r\n\r\n```python\r\npython3 main.py\r\n```\r\n\r\n### Add Markdown\r\n\r\nIf you need to add text or other things that are formatted you can use the `ui.markdown` element:\r\n\r\n```python\r\nui.markdown('# This is H1 Header')\r\nui.markdown('## This is H2 Header')\r\nui.markdown('#### This is H3 Header')\r\n```\r\n\r\nThese lines demonstrate how to use markdown syntax to create headers of different levels (H1, H2, and H3) in the UI.\r\n\r\n### Add Rows\r\n\r\n```python\r\n# row element\r\nwith ui.row():\r\n  ui.label(' First row item')\r\n  ui.label(' Second row item')\r\n  ui.label(' Third row item')\r\n```\r\n\r\nThis block creates a row layout where labels (text elements) are arranged horizontally. You can add under it the elements you need.\r\n\r\n### Add Columns\r\n\r\n```python\r\n#Column element\r\nwith ui.column():\r\n  ui.label(' First column item')\r\n  ui.label(' Second column item')\r\n  ui.label(' Third colum item')\r\n```\r\n\r\nThis block creates a column layout where labels are stacked vertically.\r\n\r\n### Styling Components with Clases\r\n\r\n```python\r\nwith ui.column():\r\n  ui.label(' First column item').classes('font-bold')\r\n  ui.label(' Second column item').classes('text-2xl')\r\n  ui.label(' Third column item').classes('text-red-600 text-2xl')\r\n```\r\n\r\nThis block demonstrates how to add CSS classes to style text elements, such as making text bold, changing its size, or altering its color. You can use Tailwind classes to style the components in the way you like.\r\n\r\n### Inputs and Buttons\r\n\r\n```python\r\nwith ui.row():\r\n    ui.input(label='Type Something').props('square outlined dense').classes('shadow-lg')\r\n    ui.button('Click Me')\r\n```\r\n\r\nThis block adds an input field with a label and a button in a row layout. The input field has additional properties and classes for styling. You can use Tailwind classes in combination with Quasar props to style your components.\r\n\r\n### Add Images\r\n\r\n```python\r\nui.image('https://www.bitdoze.com/_astro/streamlit-vs-nicegui.CbrH4KaA_2qjgFm.webp').classes('h-auto max-w-lg rounded-lg flex justify-center')\r\n```\r\n\r\nThis line adds an image to the UI with a source URL and applies CSS classes for styling, such as setting the height automatically, limiting the maximum width, rounding the corners, and centering the image.\r\n\r\n### Add Header with Drawer Toggle\r\n\r\n```python\r\nwith ui.header(elevated=True).style('background-color: #3874c8').classes('items-center justify-between'):\r\n        ui.label('HEADER')\r\n        ui.button(on_click=lambda: right_drawer.toggle(), icon='menu').props('flat color=white')\r\n```\r\n\r\nThis block creates a header with a specific background color and styles. It includes a label for the header title and a button that toggles the visibility of a right-side drawer.\r\n\r\n### Add Right Sidebar/Drawer\r\n\r\n```python\r\nwith ui.right_drawer(fixed=False).style('background-color: #ebf1fa').props('bordered') as right_drawer:\r\n        ui.label('RIGHT DRAWER')\r\n```\r\n\r\nThis block defines a right-side drawer with a background color and border properties. It contains a label indicating it's the right drawer.\r\n\r\n### Add Footer\r\n\r\n```python\r\nwith ui.footer().style('background-color: #3874c8'):\r\n        ui.label('FOOTER')\r\n```\r\n\r\nThis block creates a footer with a specific background color and contains a label for the footer text. You can add the footer elements in here if you choose to, split them into columns,etc.\r\n\r\n### Add Code Into Header:\r\n\r\n```python\r\n ui.add_head_html('''\r\n     <script></script>\r\n     ''')\r\n```\r\n\r\nThis will make possible adding the code you like into header, you can add javascript or CSS code, you can add your analytics code in here.\r\n\r\nAt the end you will have a file with all the elements that will look like this:\r\n\r\n```python\r\nfrom nicegui import ui\r\n\r\n#demonstrate h1 and others\r\nui.markdown('# This is H1 Header')\r\nui.markdown('## This is H2 Header')\r\nui.markdown('#### This is H3 Header')\r\n\r\nui.separator()\r\n\r\n#row element\r\nwith ui.row():\r\n  ui.label(' First row item')\r\n  ui.label(' Second row item')\r\n  ui.label(' Third row item')\r\n\r\n\r\nui.separator()\r\n#Column element\r\nwith ui.column():\r\n  ui.label(' First column item')\r\n  ui.label(' Second column item')\r\n  ui.label(' Third colum item')\r\n\r\n#stile the text\r\nui.separator()\r\n\r\nwith ui.column():\r\n  ui.label(' First column item').classes('font-bold')\r\n  ui.label(' Second column item').classes('text-2xl')\r\n  ui.label(' Third column item').classes('text-red-600 text-2xl')\r\n\r\n#other elements\r\nui.separator()\r\nwith ui.row():\r\n    ui.input(label='Tipe Something').props('squere outlined dense').classes('shadow-lg')\r\n    ui.button('Click Me')\r\n\r\n\r\n#image add\r\nui.separator()\r\nui.image('https://www.bitdoze.com/_astro/streamlit-vs-nicegui.CbrH4KaA_2qjgFm.webp').classes('h-auto max-w-lg rounded-lg flex justify-center')\r\n\r\n\r\n## Header with right drawer\r\n\r\nwith ui.header(elevated=True).style('background-color: #3874c8').classes('items-center justify-between'):\r\n        ui.label('HEADER')\r\n        ui.button(on_click=lambda: right_drawer.toggle(), icon='menu').props('flat color=white')\r\nwith ui.right_drawer(fixed=False).style('background-color: #ebf1fa').props('bordered') as right_drawer:\r\n        ui.label('RIGHT DRAWER')\r\n\r\n# footer\r\nwith ui.footer().style('background-color: #3874c8'):\r\n        ui.label('FOOTER')\r\n\r\n\r\nui.run()\r\n```\r\n\r\nThis is just scratching the surface of what NiceGUI can do but should be enough to understand the capabilities of NIceGUI. Next, you can add your Python functions and link them to buttons and input fields.\r\n\r\n## Why Some Will Probably Not Like NiceGUI\r\n\r\nNiceGUI offers some advanced options to create a Python UI that is fast and can be customized the way you like, for some this will not be OK as they need to use CSS code and customize the UI and if you are not used to this it can be scary in beginning. That's why they would prefer Streamlit as you just throw the code in there and STreamlit will take care of the rest. But if you want a performant Python app and options to customize it then NiceGUI is better.\r\n\r\n## Conclusions\r\n\r\nThat's the introduction to NiceGUI and some of the things it has to offer. You can check [NiceGUI documentation](https://nicegui.io/documentation) if you want to understand all the options that NiceGUI has.","src/content/posts/nicegui-get-started.mdx",[1565],"../../assets/images/24/03/nicegui-beginners.jpeg","1eccf4e3acbdbed6","nicegui-get-started.mdx","nodejs-update-dependencies",{id:1568,data:1570,body:1579,filePath:1580,assetImports:1581,digest:1583,legacyId:1584,deferredRender:32},{title:1571,description:1572,date:1573,image:1574,authors:1575,categories:1576,tags:1577,canonical:1578},"How to Update All Node.js Dependencies to Their Latest Version","Learn how to update all your Node.js dependencies to their latest version using the npm-check-updates tool. Avoid breaking changes, dependency conflicts, and security issues by following this simple guide.",["Date","2023-12-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/12/update-nodejs.jpeg",[19],[98],[1198],"https://www.bitdoze.com/nodejs-update-dependencies/","Node.js is a popular JavaScript runtime environment that allows you to create and run various applications. One of the advantages of Node.js is that it has a rich ecosystem of packages that you can use to add functionality and features to your projects. These packages are called **dependencies** and they are managed by a tool called **`npm`** (Node Package Manager).\r\n\r\n\r\nDependencies are specified in a file called **`package.json`** that is located in the root directory of your project. This file contains information about your project, such as its name, version, description, scripts, and dependencies. For each dependency, you need to provide its name and version. For example, if you want to use the express package, which is a web framework for Node.js, you would add this line to your package.json file:\r\n\r\n```json\r\n\"dependencies\": {\r\n  \"express\": \"^4.17.1\"\r\n}\r\n```\r\n\r\nThe version number of a dependency can be specified in different ways, such as using exact numbers, ranges, or prefixes. For example, the `^` prefix means that you want to use the latest minor or patch version of the package that is compatible with the major version you specified. In this case, `^4.17.1` means that you want to use any version of express that is greater than or equal to 4.17.1 and less than 5.0.0.\r\n\r\nTo install the dependencies of your project, you need to run the command `npm install` in your terminal. This will download the packages from the npm registry and store them in a folder called **node_modules** in your project directory.\r\n\r\n## What you should be aware when updating all package dependencies\r\n\r\nUpdating your package dependencies is a good practice that can help you benefit from the latest features, bug fixes, and security patches of the packages you use. However, updating your dependencies can also introduce some risks and challenges, such as:\r\n\r\n- **Breaking changes**: Sometimes, a new version of a package may introduce changes that are not compatible with the previous versions. This can cause your code to stop working or behave differently. For example, a new version of a package may change the name, parameters, or return value of a function that you use in your code. To avoid breaking changes, you should always read the release notes and documentation of the packages you update and make sure you understand the changes and how they affect your code. You should also follow the semantic versioning convention, which is a standard way of labeling the versions of a package based on the type and impact of the changes. According to this convention, a version number consists of three parts: major, minor, and patch. A major version change indicates that there are breaking changes, a minor version change indicates that there are new features or improvements that are backward compatible, and a patch version change indicates that there are bug fixes or security patches that are backward compatible. For example, if you update express from 4.17.1 to 5.0.0, you should expect breaking changes, but if you update it from 4.17.1 to 4.18.0, you should expect new features or improvements that are backward compatible.\r\n- **Dependency conflicts**: Sometimes, a new version of a package may depend on a different version of another package that you also use in your project. This can cause conflicts and errors when you try to install or run your project. For example, if you update express from 4.17.1 to 5.0.0, and express 5.0.0 depends on body-parser 2.0.0, but you also use body-parser 1.0.0 in your project, you may encounter a conflict when you try to install or run your project. To avoid dependency conflicts, you should always check the dependencies of the packages you update and make sure they are compatible with the other packages you use in your project. You should also use tools like `npm audit` or `npm outdated` to identify and fix any vulnerabilities or outdated packages in your project.\r\n- **Testing and debugging**: Updating your package dependencies may require you to test and debug your code to ensure that everything works as expected. This can be time-consuming and tedious, especially if you have a large or complex project. To make testing and debugging easier, you should always use tools like unit tests, integration tests, and code coverage to verify the functionality and quality of your code. You should also use tools like git or GitHub to track and manage the changes in your code and revert to a previous version if something goes wrong.\r\n\r\n## How to Update All Node.js Dependencies to Their Latest Version\r\nIf you want to update all your package dependencies to their latest version, you can follow these steps:\r\n\r\n### 1. Install npm-check-updates\r\n\r\n**npm-check-updates** is a tool that allows you to check and update your package dependencies to their latest version. \r\nTo install it, you need to run the command:\r\n\r\n ```sh\r\n npm install -g npm-check-updates\r\n ```\r\n \r\nin your terminal. This will install the tool globally, so you can use it in any project.\r\n\r\n\r\n\r\n### 2. See the Packages that are outdated with ncu\r\n\r\nTo see the packages that are outdated in your project, you need to run the command:\r\n\r\n ```sh\r\n ncu\r\n ``` \r\n \r\n in your terminal. This will display a list of the packages that have a newer version available, along with their current and latest version numbers. For example, you may see something like this:\r\n\r\nYou can use also use alternatively:\r\n\r\n```sh\r\nnpm outdated \r\n```\r\n\r\nExample:\r\n\r\n```bash\r\n express           ^4.17.1  →   ^5.0.0\r\n body-parser       ^1.0.0   →   ^2.0.0\r\n```\r\n\r\nThis means that express and body-parser have newer versions available, and you can update them from 4.17.1 and 1.0.0 to 5.0.0 and 2.0.0, respectively.\r\n\r\n### 3. Update the dependencies in package.json with ncu\r\n\r\nTo update the dependencies in your package.json file with the latest version numbers, you need to run the command\r\n\r\n```sh\r\nncu -u\r\n``` \r\nAn more complex example with Astro Js is:\r\n\r\n```sh\r\nUpgrading /workspaces/workspace/package.json\r\n[====================] 27/27 100%\r\n\r\n @astrojs/mdx                   ^1.1.4  →    ^2.0.0\r\n @astrojs/react                 ^3.0.4  →    ^3.0.7\r\n @astrojs/rss                   ^3.0.0  →    ^4.0.0\r\n @astrojs/tailwind              ^5.0.2  →    ^5.0.3\r\n @types/react                 ^18.2.37  →  ^18.2.42\r\n astro                          ^3.5.5  →    ^4.0.2\r\n marked                        ^10.0.0  →   ^11.0.0\r\n postcss                       ^8.4.31  →   ^8.4.32\r\n prettier-plugin-tailwindcss    ^0.5.7  →    ^0.5.9\r\n react-icons                   ^4.11.0  →   ^4.12.0\r\n sharp                         ^0.32.6  →   ^0.33.0\r\n tailwind-bootstrap-grid        ^5.0.1  →    ^5.1.0\r\n tailwindcss                    ^3.3.5  →    ^3.3.6\r\n\r\nRun npm install to install new versions.\r\n```\r\n\r\n\r\nin your terminal. This will overwrite the version numbers in your `package.json` file with the latest ones. For example, your package.json file may look something like this after running the command:\r\n\r\n```json\r\n\"dependencies\": {\r\n  \"express\": \"^5.0.0\",\r\n  \"body-parser\": \"^2.0.0\"\r\n}\r\n```\r\n\r\nThis means that your package.json file now specifies the latest version of express and body-parser as your dependencies.\r\n\r\n### 4. Install the updated packages\r\nTo install the updated packages in your node_modules folder, you need to run the command:\r\n\r\n ```sh\r\n npm install\r\n ```\r\nin your terminal. This will download and install the latest version of the packages from the npm registry. You can then use the updated packages in your code as usual.\r\n\r\n## Conclusions\r\n\r\nUpdating your package dependencies to their latest version can help you improve the performance, security, and functionality of your Node.js project. However, you should also be aware of the potential risks and challenges that updating your dependencies may entail, such as breaking changes, dependency conflicts, and testing and debugging. \r\n\r\nTo update your package dependencies to their latest version, you can use the npm-check-updates tool, which allows you to check and update your package.json file with the latest version numbers of your dependencies. You can then install the updated packages with the npm install command. \r\n\r\nYou should always test and debug your code after updating your dependencies to ensure that everything works as expected. \r\n\r\nYou should also use tools like git or [GitHub](https://www.bitdoze.com/link-github-with-ssh-maco-linux/) to track and manage the changes in your code and revert to a previous version if something goes wrong.","src/content/posts/nodejs-update-dependencies.mdx",[1582],"../../assets/images/23/12/update-nodejs.jpeg","77368ec50e42261e","nodejs-update-dependencies.mdx","nicegui-pages",{id:1585,data:1587,body:1596,filePath:1597,assetImports:1598,digest:1600,legacyId:1601,deferredRender:32},{title:1588,description:1589,date:1590,image:1591,authors:1592,categories:1593,tags:1594,canonical:1595},"How To Add Multiple Pages to NiceGUI","Learn how you can add multiple pages to NiceGUI",["Date","2024-03-18T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/nicegui_multiple-pages.jpeg",[19],[98],[1560,294],"https://www.bitdoze.com/nicegui-pages/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nNiceGUI is a powerful framework that can help you build UIs to your Python App. In this article we are going to see how you can add multiple pages to your NiceGUI website and display them in a sidebar and header. You can check: [NiceGUI For Beginners](https://www.bitdoze.com/nicegui-get-started/) to install NiceGUI and see exactly how easy it is to use it.\r\n\r\nIf you want to deploy NiceGUI or any Python App to Docker you can check: [How To Run Any Python App in Docker with Docker Compose](https://www.bitdoze.com/docker-run-python/)\r\n\r\nWe will create a modular multi page layout that will inherit the elements you add in there.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/bW3ifL2hdfc\"\r\n  label=\"How To Add Multiple Pages to NiceGUI\"\r\n/>\r\n\r\n## 1. Create the theme.py with the layout\r\n\r\n```python\r\nfrom contextlib import contextmanager\r\n\r\nfrom menu import menu\r\n\r\nfrom nicegui import ui\r\n\r\n\r\n@contextmanager\r\ndef frame(navtitle: str):\r\n    \"\"\"Custom page frame to share the same styling and behavior across all pages\"\"\"\r\n    ui.colors(primary='#6E93D6', secondary='#53B689', accent='#111B1E', positive='#53B689')\r\n    with ui.column().classes('absolute-center items-center h-screen no-wrap p-9 w-full'):\r\n        yield\r\n    with ui.header().classes(replace='row items-center') as header:\r\n        ui.button(on_click=lambda: left_drawer.toggle(), icon='menu').props('flat color=white')\r\n        ui.label('Getting Started').classes('font-bold')\r\n\r\n    with ui.footer(value=False) as footer:\r\n        ui.label('Footer')\r\n    with ui.left_drawer().classes('bg-blue-100') as left_drawer:\r\n        ui.label('Menu')\r\n        with ui.column():\r\n            menu()\r\n    with ui.page_sticky(position='bottom-right', x_offset=20, y_offset=20):\r\n        ui.button(on_click=footer.toggle, icon='contact_support').props('fab')\r\n\r\n```\r\n\r\nThe `@contextmanager` decorator in the provided code is used to define a context manager function called `frame`. This function is designed to set up a consistent layout and styling for pages in a web application using the NiceGUI framework. The context manager is responsible for setting up the environment before the body of the `with` statement is executed and for cleaning up afterwards.\r\n\r\nHere's a breakdown of what happens in the `frame` function:\r\n\r\n1. The `ui.colors` function is called to set the color scheme for the UI elements.\r\n2. A header is created using `ui.header()` which is styled to justify content between elements and set the text color to white.\r\n3. Inside the header, two labels are created: one for the title of the application and another for the navigation title (`navtitle`) passed as an argument to the `frame` function.\r\n4. A row is created to include the navigation menu by calling the `menu()` function.\r\n5. A column is created to center the content that will be placed inside the context manager's block.\r\n\r\nThe `yield` statement is the point at which the function will pause and return control back to the block of code that is using the `frame` context manager. This allows the caller to insert custom content into the column that was set up. After the block of code using the `frame` context manager is executed, the function will resume if there were any cleanup actions to perform, which in this case, there are none.\r\n\r\nThis will result in a page with a header containing the application title, the specified page title, and the navigation menu, followed by the custom content centered on the page. The `@contextmanager` decorator makes it easy to reuse this layout across different pages of the application, ensuring a consistent look and feel.\r\n\r\n## 2. Create the menu.py with the pages\r\n\r\nThis file will contain the pages that will be displayed in NiceGUI\r\n\r\n```python\r\nfrom nicegui import ui\r\n\r\n\r\ndef menu() -> None:\r\n    ui.link('Home', '/').classes(replace='text-black')\r\n    ui.link('YouTube Titles', '/youtube-title-generator/').classes(replace='text-black')\r\n    ui.link('YouTube Script Generator', '/youtube-script/').classes(replace='text-black')\r\n```\r\n\r\nThe above code will create a menu function with the pages that we want, you can add your desired ones in here, the `classes` it hold the text color\r\n\r\n## 3. Create the `pages` in NiceGUI\r\n\r\nIn this section we are going to create our actual pages under a `page` folder, they will be used to store the pages code. Here we are going to create 2 files `title_generator.py` and `script_generator.py` that will have the code that will run on the pages.\r\n\r\n**title_generator.py**\r\n\r\n```python\r\nimport theme\r\nfrom nicegui import ui\r\n\r\n\r\ndef title_generator():\r\n    with theme.frame('YouTube Title Generator'):\r\n        ui.page_title('YouTube Title Generator')\r\n        ui.markdown('# This is My Title Generator Page')\r\n```\r\n\r\n**script_generator.py**\r\n\r\n```python\r\nimport theme\r\nfrom nicegui import ui\r\n\r\n\r\ndef script_generator():\r\n    with theme.frame('YouTube Script Generator'):\r\n        ui.page_title('YouTube Script Generator')\r\n        ui.markdown('# This is My YouTube Script Generator Page')\r\n```\r\n\r\n## 4. Create the `all_pages` in NiceGUI\r\n\r\nWe will create an `all_pages.py` file that will fetch the code for our all pages.\r\n\r\n```python\r\nfrom nicegui import ui\r\nfrom pages.title_generator import title_generator\r\nfrom pages.script_generator import script_generator\r\n\r\ndef create() -> None:\r\n    ui.page('/youtube-title-generator/')(title_generator)\r\n    ui.page('/youtube-script/')(script_generator)\r\n\r\nif __name__ == '__main__':\r\n    create()\r\n```\r\n\r\nThis Python code defines a function `create()` that uses the NiceGUI library to set up routing for a web application. It maps specific URL paths to functions that generate content for those pages. For example, visiting `/youtube-title-generator/` in the application will execute the `title_generator` function from the `title_generator` module within the `pages` package. Similarly, other paths are mapped to their respective content-generating functions (`script_generator`, `description_generator`, and `script_generator2`). The `if __name__ == '__main__':` block ensures that `create()` is called to set up the routes when the script is run directly.\r\n\r\n## 5. Create the `home_page` in NiceGUI\r\n\r\nThis file will be the one that will hold the home page details we are just going to create the file and add a markdown.\r\n\r\n```python\r\nfrom nicegui import ui\r\n\r\ndef content() -> None:\r\n    with ui.column():\r\n        ui.markdown('''\r\n\r\n            Elevate your YouTube content creation with Bitdoze's suite of intelligent AI tools.\r\n\r\n            ## Catchy Titles and Descriptions\r\n\r\n            Struggling to find the perfect title or description? Bitdoze's AI analyzes trending topics and keywords to generate ideas that are both engaging and optimized for search.\r\n\r\n            ## Compelling Video Scripts\r\n\r\n            Need help crafting a video script? Our AI script generator can:\r\n\r\n             - Outline key points for your video\r\n             - Suggest hooks and captivating introductions\r\n             - Provide creative transitions and calls-to-action\r\n\r\n            ## And More!\r\n\r\n            Bitdoze is continuously expanding its AI capabilities for YouTubers. Stay tuned for exciting new features!\r\n\r\n            **Transform your YouTube workflow with BitDoze. Try it today!**\r\n\r\n            ''')\r\n\r\n\r\n```\r\n\r\n## 6. Create the `main.py` in NiceGUI\r\n\r\nWhat remains is to create the `main.py` that will stick everything together.\r\n\r\n```python\r\nimport all_pages\r\nimport home_page\r\nimport theme\r\n\r\nfrom nicegui import app, ui\r\n\r\n\r\n# here we use our custom page decorator directly and just put the content creation into a separate function\r\n@ui.page('/')\r\ndef index_page() -> None:\r\n    with theme.frame('Homepage'):\r\n        home_page.content()\r\n\r\n\r\n# this call shows that you can also move the whole page creation into a separate file\r\nall_pages.create()\r\n\r\n\r\nui.run(title='Getting Started With NiceGUI')\r\n```\r\n\r\nThis code snippet sets up a web application using the NiceGUI library. It defines a homepage (`index_page`) using a custom decorator `@ui.page('/')`, which maps the root URL (`'/'`) to this function. Inside this function, it uses a custom frame from the `theme` module to style the page and calls `home_page.content()` to populate it with content. Additionally, it imports and executes `all_pages.create()` to define more pages, likely mapping other URLs to their respective content functions. Finally, `ui.run(title='Getting Started With NiceGUI')` starts the web server, setting the title of the application to 'Getting Started With NiceGUI'.\r\n\r\n## Conclusions\r\n\r\nThis is how you can add multiple pages in NiceGUI, the code was created based on [NiceGUI examples](https://github.com/zauberzeug/nicegui/tree/main/examples/) so you can check their examples for more things that you may want.\r\n\r\nYou can check the [NiceGUI Starter Theme](https://github.com/bitdoze/nicegui-starter) to have the updated code. I will add the future things here, who want to help are welcome.","src/content/posts/nicegui-pages.mdx",[1599],"../../assets/images/24/03/nicegui_multiple-pages.jpeg","cfd9380d83f088b6","nicegui-pages.mdx","oha-website-load-testing",{id:1602,data:1604,body:1614,filePath:1615,assetImports:1616,digest:1618,legacyId:1619,deferredRender:32},{title:1605,description:1606,date:1607,image:1608,authors:1609,categories:1610,tags:1611,canonical:1613},"Unleash the Power of oha: Website Performance Testing Made Simple","Learn how to use oha for website load testing. Simple guide with practical examples to measure your site's performance, throughput, and response times. Includes installation and basic usage.",["Date","2025-02-10T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/25/02/oha-load-testing.jpeg",[19],[77],[572,1612],"load-tests","https://www.bitdoze.com/oha-website-load-testing/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\nWebsite performance is crucial for user experience and SEO. In this guide, you'll learn how to use oha, a lightweight HTTP load testing tool, to measure your website's performance under load. Whether you're a developer or site owner, these tests will help you understand your site's capabilities.\r\n\r\n## What is oha?\r\n\r\n![oha](../../assets/images/25/02/oha.gif)\r\n\r\n\r\n[oha](https://github.com/hatoo/oha) (おはよう) is:\r\n- A modern, lightweight HTTP load testing tool\r\n- Written in Rust for optimal performance\r\n- Perfect for quick website performance testing\r\n- Features real-time visualization of results\r\n\r\nKey benefits:\r\n| Feature | Benefit |\r\n|---------|----------|\r\n| Speed | Fast execution with minimal resource usage |\r\n| Simplicity | Single command operation |\r\n| Visual Output | Real-time metrics display |\r\n| Detailed Reports | Comprehensive performance statistics |\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/iVcvbmY0pk8\"\r\n  label=\"Unleash the Power of oha: Website Performance Testing Made Simple\"\r\n/>\r\n\r\n## Installation\r\nChoose your operating system and follow these simple steps:\r\n\r\n### Linux:\r\n```bash\r\ncargo install oha\r\n```\r\n\r\n### MacOS:\r\n```bash\r\nbrew install oha\r\n```\r\n\r\n### Windows:\r\n```bash\r\nwinget install hatoo.oha\r\n```\r\n\r\n## Basic Usage\r\n\r\n### Simple Test Command:\r\n```bash\r\noha https://yourwebsite.com\r\n```\r\n\r\n### Common Options Table:\r\n| Option | Description | Example |\r\n|--------|-------------|---------|\r\n| -n | Total requests | oha -n 200 https://site.com |\r\n| -c | Concurrent users | oha -c 50 https://site.com |\r\n| -q | Requests per second | oha -q 100 https://site.com |\r\n| --no-tui | Disable visual interface | oha --no-tui https://site.com |\r\n\r\n### Understanding Test Results\r\n\r\nLet's analyze a typical output:\r\n```sh\r\nSummary:\r\n  Success rate: 100.00%\r\n  Total:        0.6689 secs\r\n  Slowest:      0.4123 secs\r\n  Fastest:      0.0733 secs\r\n  Average:      0.1557 secs\r\n  Requests/sec: 299.0098\r\n```\r\n\r\nKey Metrics Explained:\r\n| Metric | What It Means | Good Values |\r\n|--------|---------------|-------------|\r\n| Success rate | Percentage of successful requests | Should be close to 100% |\r\n| Average time | Mean response time | Under 1 second |\r\n| Requests/sec | Throughput capacity | Depends on your needs |\r\n| Slowest | Worst response time | Should not be more than 3x average |\r\n\r\n### Response Time Distribution\r\nThe percentile breakdown shows how your site performs across all requests:\r\n- 50th percentile (median): Normal user experience\r\n- 90th percentile: Slower but acceptable responses\r\n- 99th percentile: Worst-case scenarios\r\n\r\n\r\n**Real Output:**\r\n\r\n```sh\r\n╰─❯ oha https://www.bitdoze.com\r\nSummary:\r\n  Success rate: 100.00%\r\n  Total:        0.6689 secs\r\n  Slowest:      0.4123 secs\r\n  Fastest:      0.0733 secs\r\n  Average:      0.1557 secs\r\n  Requests/sec: 299.0098\r\n\r\n  Total data:   2.08 MiB\r\n  Size/request: 10.64 KiB\r\n  Size/sec:     3.11 MiB\r\n\r\nResponse time histogram:\r\n  0.073 [1]   |\r\n  0.107 [144] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\r\n  0.141 [5]   |■\r\n  0.175 [0]   |\r\n  0.209 [0]   |\r\n  0.243 [0]   |\r\n  0.277 [0]   |\r\n  0.311 [5]   |■\r\n  0.345 [15]  |■■■\r\n  0.378 [18]  |■■■■\r\n  0.412 [12]  |■■\r\n\r\nResponse time distribution:\r\n  10.00% in 0.0803 secs\r\n  25.00% in 0.0847 secs\r\n  50.00% in 0.0935 secs\r\n  75.00% in 0.3010 secs\r\n  90.00% in 0.3617 secs\r\n  95.00% in 0.3813 secs\r\n  99.00% in 0.4108 secs\r\n  99.90% in 0.4123 secs\r\n  99.99% in 0.4123 secs\r\n\r\n\r\nDetails (average, fastest, slowest):\r\n  DNS+dialup:   0.0811 secs, 0.0488 secs, 0.1036 secs\r\n  DNS-lookup:   0.0001 secs, 0.0000 secs, 0.0005 secs\r\n\r\nStatus code distribution:\r\n  [200] 200 responses\r\n```\r\n\r\n## Basic Testing Scenarios\r\n\r\n1. **Quick Health Check**\r\n```bash\r\noha -n 100 https://yoursite.com\r\n```\r\nPurpose: Quick overview of site performance\r\n\r\n2. **Load Testing**\r\n```bash\r\noha -n 1000 -c 50 https://yoursite.com\r\n```\r\nPurpose: Simulate multiple concurrent users\r\n\r\n3. **Stress Testing**\r\n```bash\r\noha -n 2000 -c 100 -q 200 https://yoursite.com\r\n```\r\nPurpose: Find performance limits\r\n\r\n\r\n## Real-World Testing Examples\r\n\r\n### 1. Testing API Endpoints\r\n```bash\r\noha -n 500 -c 50 -m POST -T \"application/json\" -d '{\"key\":\"value\"}' https://api.yoursite.com/endpoint\r\n```\r\n\r\nAPI Testing Parameters:\r\n| Parameter | Description | Usage |\r\n|-----------|-------------|--------|\r\n| -m POST | HTTP method | For API calls |\r\n| -T | Content type | Specify data format |\r\n| -d | Request body | Send data |\r\n\r\n### 2. Simulating Peak Traffic\r\n```bash\r\noha -n 2000 -c 100 --disable-keepalive https://yoursite.com\r\n```\r\n\r\nPeak Traffic Settings:\r\n- Higher concurrent connections (-c)\r\n- Disabled keepalive for realism\r\n- Larger number of requests (-n)\r\n\r\n## Interpreting Results\r\n\r\n### Performance Metrics Table\r\n| Metric | Good | Warning | Critical |\r\n|--------|------|---------|-----------|\r\n| Response Time | < 1s | 1-3s | > 3s |\r\n| Success Rate | > 99% | 95-99% | < 95% |\r\n| Requests/sec | Site-specific | 20% drop | > 30% drop |\r\n\r\n### Common Issues and Solutions\r\n\r\n1. **High Response Times**\r\n- Possible Causes:\r\n  - Server resources maxed out\r\n  - Database bottlenecks\r\n  - Unoptimized code\r\n- Solutions:\r\n  - Implement caching\r\n  - Optimize database queries\r\n  - Scale server resources\r\n\r\n2. **Failed Requests**\r\n- Possible Causes:\r\n  - Server timeout\r\n  - Rate limiting\r\n  - Network issues\r\n- Solutions:\r\n  - Increase timeout values\r\n  - Adjust rate limits\r\n  - Check network configuration\r\n\r\n## Best Practices for Load Testing\r\n\r\n### Do's and Don'ts\r\n\r\n✅ Do:\r\n- Start with small tests\r\n- Test during low-traffic periods\r\n- Monitor server resources\r\n- Test regularly\r\n- Document results\r\n\r\n❌ Don't:\r\n- Test production without warning\r\n- Run tests from production servers\r\n- Ignore error rates\r\n- Test single endpoints only\r\n\r\n\r\n\r\n## Comparing oha with k6\r\n\r\n[k6](https://k6.io) is a modern load testing tool by Grafana Labs that uses JavaScript for creating test scenarios. Unlike oha's simple command-line approach, k6 allows you to write complex testing scripts that can simulate real user behaviors.\r\n\r\n\r\n### Feature Comparison\r\n\r\n| Feature | oha | k6 |\r\n|---------|-----|-----|\r\n| Ease of Use | ★★★★★ | ★★★☆☆ |\r\n| Scripting Required | No | Yes |\r\n| Real-time Metrics | Basic | Advanced |\r\n| Learning Curve | Minimal | Moderate |\r\n| CI/CD Integration | Limited | Extensive |\r\n\r\n### When to Use Each Tool\r\n\r\nUse oha for:\r\n- Quick performance checks\r\n- Simple HTTP testing\r\n- Immediate results\r\n- Command-line operations\r\n\r\nUse k6 for:\r\n- Complex user scenarios\r\n- Detailed performance analysis\r\n- CI/CD pipeline integration\r\n- Custom test scripts\r\n\r\n\r\n\r\n## Conclusion\r\n\r\nWebsite load testing with oha provides a straightforward and efficient way to measure and understand your site's performance under various conditions. Through this guide, we've explored how to install and use oha, interpret its results, and apply best practices for effective load testing.\r\n\r\nRemember that regular testing, careful documentation of results, and gradual scaling of test parameters are key to maintaining optimal website performance. Whether you're managing a small blog or a complex web application, oha's simplicity and powerful features make it an excellent choice for routine performance monitoring and load testing. As you implement these testing practices, focus on establishing baseline metrics, monitoring changes over time, and using the insights gained to continuously improve your website's performance and user experience.","src/content/posts/oha-website-load-testing.mdx",[1617],"../../assets/images/25/02/oha-load-testing.jpeg","4ece07e7e3eb9a8d","oha-website-load-testing.mdx","opnform-open-source",{id:1620,data:1622,body:1631,filePath:1632,assetImports:1633,digest:1635,legacyId:1636,deferredRender:32},{title:1623,description:1624,date:1625,image:1626,authors:1627,categories:1628,tags:1629,canonical:1630},"OpnForm Free Open Source Form Builder Tool","OpnForm is an open-source free form builder tool can help host forms online.",["Date","2023-08-18T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/08/opnforms-tool.png",[19],[77],[572,242],"https://www.bitdoze.com/opnform-open-source/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport Button from \"../../layouts/components/widgets/Button.astro\";\r\n\r\n## 🔭 OpnForm Overview\r\n\r\n[OpnForm](https://opnform.com/) is a user-friendly, open-source form builder designed to enable individuals and businesses to create beautiful and functional forms with ease. The platform is currently in its beta phase and offers a range of features that make form creation quick and simple. Users can create a form in less than two minutes, with more than 10 input types available, including images and logic, without the need for coding knowledge. Once a form is created, OpnForm generates a unique link that users can share widely or embed directly into their websites.\r\n\r\nThe platform also provides a robust set of response management tools, allowing users to receive notifications of new submissions, send confirmations, export submissions as CSV files, and view detailed analytics of form views and submissions. Additional features include file uploads (up to 5MB), extensive customization options (themes, texts, colors, images, custom thank you pages), and advanced functionalities such as form logic, URL pre-fill, unique submission IDs, hidden fields, form passwords, webhooks, custom code, and closing dates. OpnForm is committed to accessibility, offering these features under a generous, unlimited free plan.\r\n\r\n## 🎥 OpnForm Video Overview\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/EhE6H9nCm8U\"\r\n  label=\"OpnForm - Add A Contact Form To Astro Free\"\r\n/>\r\n\r\n## ❔ Why Use OpnForm\r\n\r\nOpnForm stands out as an exceptional choice for creating online forms due to its user-friendly, open-source nature. It empowers users to craft beautiful, functional forms in less than two minutes, without requiring any coding skills. This makes it accessible to individuals and businesses of all sizes, whether for contact forms, surveys, or complex data collection tasks.\r\n\r\nOpnForm offers a generous, unlimited free plan, which includes unlimited forms, fields, and responses, making it a cost-effective solution for any budget. The platform provides a unique link for each form that users can share widely or embed directly into their websites, facilitating easy distribution.\r\n\r\n## 📋 OpnForm Features\r\n\r\n- **Quick Form Creation:** - Craft beautiful forms in under 2 minutes without coding skills.\r\n- **Unlimited Free Plan:** - Enjoy unlimited forms, fields, and responses at no cost.\r\n- **Customizable Design:** - Tailor form themes, text, colors, and images to match your brand.\r\n- **Easy Sharing & Embedding:** - Distribute forms with a unique link or embed them directly into your website.\r\n- **Instant Notifications:** - Receive alerts in Slack or your mailbox when new submissions arrive.\r\n- **File Upload Capability:** - Securely add file upload inputs to your forms, with up to 5MB storage per file.\r\n- **AI-Powered Form Generation:** - Generate fully working forms in seconds using OpnForm's intelligent AI feature.\r\n- **Detailed Analytics and Export:** - Track form views and submissions, and easily export data as CSV files.\r\n- **Advanced Form Logic:** - Implement conditional logic to create dynamic, interactive forms that adapt to user inputs.\r\n- **Secure and Private Data Handling:** - Ensure the confidentiality and integrity of collected data with robust security features.\r\n\r\n## 🏷️ Pricing\r\n\r\n- **Free**\r\n\r\n<Button link=\"https://opnform.com/\" text=\"Check OpnForm\" />","src/content/posts/opnform-open-source.mdx",[1634],"../../assets/images/23/08/opnforms-tool.png","a9c36e621359ffba","opnform-open-source.mdx","ollama-docker-install",{id:1637,data:1639,body:1648,filePath:1649,assetImports:1650,digest:1652,legacyId:1653,deferredRender:32},{title:1640,description:1641,date:1642,image:1643,authors:1644,categories:1645,tags:1646,canonical:1647},"How to Setup Ollama with Open-Webui using Docker Compose","Learn how to Setup Ollama with Open-WebUI using Docker Compose and have your own local AI",["Date","2024-08-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/08/ollama-docker-install.jpeg",[19],[98],[242],"https://www.bitdoze.com/ollama-docker-install/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/08/openwebui-pull-model.png\";\r\nimport imag2 from \"../../assets/images/24/08/openweb-ui-use.png\";\r\n\r\nIn the rapidly evolving landscape of artificial intelligence and machine learning, tools that simplify the deployment and interaction with large language models (LLMs) are becoming increasingly valuable.\r\nTwo such tools that have gained significant attention are Ollama and Ollama-WebUI. Let's dive into what these technologies are and how they can benefit developers, researchers, and AI enthusiasts.\r\n\r\n## What is Ollama?\r\n\r\n[Ollama](https://ollama.ai/) is an open-source project that aims to make running and deploying large language models as simple and efficient as possible. It's designed to run LLMs locally on your machine, providing a lightweight and user-friendly interface for interacting with various AI models.\r\n\r\n- **Local Execution**: Run AI models on your own hardware, ensuring privacy and control over your data.\r\n- **Easy Installation**: With a simple one-line installation process, getting started with Ollama is remarkably straightforward.\r\n- **Model Management**: Easily download, run, and manage different LLMs without complex setup procedures.\r\n- **API Integration**: Ollama provides a RESTful API, allowing seamless integration with other applications and services.\r\n- **Cross-Platform Support**: Available for macOS, Linux, and Windows, ensuring broad accessibility.\r\n- **Resource Efficiency**: Optimized to run efficiently on consumer-grade hardware, making AI accessible to a wider audience.\r\n\r\nOne of the most significant advantages of Ollama is its ability to run models locally. This local execution not only ensures data privacy but also reduces latency, making it ideal for applications that require quick responses or handle sensitive information.\r\n\r\n## What is Open-Webui\r\n\r\nWhile Ollama provides a powerful command-line interface, many users prefer a more visual and interactive experience. This is where [Open-Webui](https://openwebui.com/) comes into play.\r\n\r\nOpen-Webui is a web-based graphical user interface designed specifically to work with Ollama. It provides a user-friendly front-end that allows users to interact with Ollama-managed models through a web browser. This interface bridges the gap between Ollama's powerful backend capabilities and users who may not be comfortable with command-line interfaces.\r\n\r\nKey functionalities ofOpen-Webui include:\r\n\r\n- **Model Selection**: Easily switch between different LLMs available through Ollama.\r\n- **Chat Interface**: Engage in conversations with AI models in a familiar chat-like environment.\r\n- **Prompt Templates**: Save and reuse common prompts to streamline interactions.\r\n- **History Management**: Keep track of past conversations and easily reference or continue them.\r\n- **Export Options**: Save conversations or generated content in various formats for further use or analysis.\r\n\r\n## How to Set up Ollama and openWebUI with Docker Compose\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/FHTYrMtLkmQ\"\r\n  label=\"How to Setup Ollama with Open-Webui using Docker Compose\"\r\n/>\r\n\r\nIn this section we are going to see how we are going to set up Ollama and Open-Webui.\r\n\r\n### 1. Prerequizites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host Ollama, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) You can use a VPS to have ollama installed but performances will not be that good. In our test we are using a 8 CPUs 16 GB RAM and is bearly moving. Best will be to have a GPU powered system or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Traefic with Docker set up, you can check: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/) or [Traefik FREE Let's Encrypt Wildcard Certificate With CloudFlare Provider](https://www.bitdoze.com/traefik-wildcard-certificate/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n\r\n### 2. Docker Compose\r\n\r\n#### CPU Only\r\n\r\n```yml\r\nservices:\r\n  openWebUI:\r\n    image: ghcr.io/open-webui/open-webui:main\r\n    container_name: openwebui\r\n    hostname: openwebui\r\n    networks:\r\n      - traefik-net\r\n    restart: unless-stopped\r\n    volumes:\r\n      - ./open-webui-local:/app/backend/data\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.http.routers.openwebui.rule=Host(`openwebui.domain.com`)\"\r\n      - \"traefik.http.routers.openwebui.entrypoints=https\"\r\n      - \"traefik.http.services.openwebui.loadbalancer.server.port=8080\"\r\n    environment:\r\n      OLLAMA_BASE_URLS: http://ollama:11434\r\n\r\n  ollama:\r\n    image: ollama/ollama:latest\r\n    container_name: ollama\r\n    hostname: ollama\r\n    networks:\r\n      - traefik-net\r\n    volumes:\r\n      - ./ollama-local:/root/.ollama\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\nThis is adding the open-webui and adds it to traefik network, is not exposing any port to outside.\r\n\r\n- traefik.enable=true: Enables Traefik for this service.\r\n- traefik.http.routers.openwebui.rule=Host(openwebui.domain.com): Routes traffic to this service when the host matches openwebui.domain.com.\r\n- traefik.http.routers.openwebui.entrypoints=https: Specifies that this service should be accessible over HTTPS.\r\n- traefik.http.services.openwebui.loadbalancer.server.port=8080: Indicates that the service listens on port 8080 inside the container.\r\n\r\nOllama is also downloaded but is not exposing again no port.\r\n\r\n#### Docker Compose NVIDIA GPU\r\n\r\nBefore we dive into the Docker Compose setup, it's crucial to understand the importance of the NVIDIA Container Toolkit. This toolkit is essential for enabling GPU acceleration within Docker containers, allowing your AI models to run at peak performance.\r\n\r\nYou install it like this:\r\n\r\n```sh\r\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\r\n && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\r\nsed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\r\nsudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\r\nsudo apt-get update\r\nsudo apt-get install -y nvidia-container-toolkit\r\n\r\n# Configure NVIDIA Container Toolkit\r\nsudo nvidia-ctk runtime configure --runtime=docker\r\nsudo systemctl restart docker\r\n\r\n# Test GPU integration\r\ndocker run --gpus all nvidia/cuda:11.5.2-base-ubuntu20.04 nvidia-smi\r\n```\r\n\r\nCompose File for Nvidia :\r\n\r\n```yml\r\nservices:\r\n  openWebUI:\r\n    image: ghcr.io/open-webui/open-webui:main\r\n    container_name: openwebui\r\n    hostname: openwebui\r\n    networks:\r\n      - traefik-net\r\n    restart: unless-stopped\r\n    volumes:\r\n      - ./open-webui-local:/app/backend/data\r\n    labels:\r\n      - \"traefik.enable=true\"\r\n      - \"traefik.http.routers.openwebui.rule=Host(`openwebui.my.bitdoze.com`)\"\r\n      - \"traefik.http.routers.openwebui.entrypoints=https\"\r\n      - \"traefik.http.services.openwebui.loadbalancer.server.port=8080\"\r\n    environment:\r\n      OLLAMA_BASE_URLS: http://ollama:11434\r\n\r\n  ollama:\r\n    image: ollama/ollama:latest\r\n    container_name: ollama\r\n    hostname: ollama\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              capabilities: [\"gpu\"]\r\n              count: all\r\n    networks:\r\n      - traefik-net\r\n    volumes:\r\n      - ./ollama-local:/root/.ollama\r\nnetworks:\r\n  traefik-net:\r\n    external: true\r\n```\r\n\r\nThe most critical part of this setup for AI performance is the GPU configuration in the Ollama service:\r\n\r\n```yml\r\ndeploy:\r\n  resources:\r\n    reservations:\r\n      devices:\r\n        - driver: nvidia\r\n          capabilities: [\"gpu\"]\r\n          count: all\r\n```\r\n\r\nThis configuration ensures that Ollama has access to all available NVIDIA GPUs on your system. According to [NVIDIA's benchmarks](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/), GPU acceleration can provide up to 100x faster inference times compared to CPU-only setups for certain AI models.\r\n\r\n#### Docker Compose AMD GPU\r\n\r\nFor users with AMD GPUs that support [ROCm](https://www.amd.com/en/products/software/rocm/ai.html), setting up Ollama and OpenWebUI using Docker Compose is a straightforward process. This configuration allows you to leverage the power of your AMD GPU for running large language models efficiently. Let's explore how to set this up and the benefits it offers.\r\n\r\nThe only diffference here is to use the correct image:\r\n\r\n```yml\r\nimage: ollama/ollama:rocm\r\n```\r\n\r\n### 3. Start the Docker Compose file\r\n\r\n```sh\r\ndocker compose up -d\r\n```\r\n\r\n### 4. Access the Open WebUI\r\n\r\nNow you can access the Open WebUI app, to do that you just need to use the domain you have set in the compose file. You will be promted to create a user and a password and you will do that.\r\nAfter you create the user and pasword you can alter the docker-compose file and update everything by adding :\r\n\r\n```yml\r\nENABLE_SIGNUP: false\r\n\r\n## run\r\ndocker compose up -d --force-recreate\r\n```\r\n\r\n### 5. Pulling a Model\r\n\r\nAfter we access the Open WebUI we will need to pul out a model and use it. In function of the server power you can choose the one best for you.\r\n\r\nOllama offers several language models (LLMs) with fewer than 3 billion parameters. Here is a list of notable models available through Ollama that meet this criterion:\r\n\r\n- **Phi-3**: A 3B model by Microsoft, known for its strong reasoning and language understanding capabilities.\r\n\r\n- **Gemma2**: Google Gemma 2 is a high-performing and efficient model thathas 2B size\r\n\r\n- **CodeGemma**: This model is available in sizes of 2B and 7B, designed for various coding tasks including code completion and generation.\r\n\r\n- **DeepSeek Coder**: A coding model with a size of 1B, trained on a vast dataset of code and natural language tokens.\r\n\r\n- **TinyLlama**: Specifically, TinyLlama-1.1B is a lightweight model that offers decent performance for small-scale applications.\r\n\r\n- **StableLM**: The StableLM series includes models like StableLM-Zephyr-3B, which is optimized for multilingual tasks and is lightweight enough to run on less powerful hardware.\r\n\r\n- **Dolphin**: The Dolphin model has a variant that operates at 2.8B, focusing on coding tasks.\r\n\r\n- **Qwen2**: This series includes models with sizes of 0.5B and 1.5B, suitable for various applications.\r\n\r\nThese models are particularly advantageous for users with limited computational resources, allowing for effective use in coding, reasoning, and general language tasks.\r\n\r\nTo do that you go to **Admin Panel - Settings - Models - Pull a model from Ollama.com**\r\n\r\nFor this small server `qwen2:0.5b` or `gemma2:2b` is the way to go.\r\n\r\n<Picture src={imag1} alt=\"openwebui pull model\" />\r\n\r\n### 6. Using Open-WebUI\r\n\r\nAfter you can go ahead and start using the Open-WebUI, you choose the model and start communicating.\r\n\r\n<Picture src={imag2} alt=\"openwebui start\" />\r\n\r\n## Conclusions\r\n\r\nSetting up Ollama and OpenWebUI with Docker Compose provides a robust, flexible, and high-performance environment for working with large language models. By following the steps outlined in this guide, you'll be well-equipped to harness the power of AI in your projects, whether you're a researcher, developer, or AI enthusiast.\r\n\r\nThe combination of Ollama's efficient model management and OpenWebUI's user-friendly interface creates a powerful toolset for AI experimentation and development. This setup allows you to run sophisticated AI models locally, ensuring data privacy and reducing latency, while also providing an intuitive interface for interaction.\r\n\r\nIf you're interested in exploring more Docker containers for your home server or self-hosted setup, including other AI tools and applications, check out our comprehensive guide on [Best 100+ Docker Containers for Home Server](https://www.bitdoze.com/docker-containers-home-server/). This resource provides a wealth of options for various applications and services you can run using Docker, helping you build a powerful and versatile self-hosted environment that can complement your Ollama and OpenWebUI installation and enhance your overall AI development ecosystem.","src/content/posts/ollama-docker-install.mdx",[1651],"../../assets/images/24/08/ollama-docker-install.jpeg","06a15083bc539d56","ollama-docker-install.mdx","outline-install",{id:1654,data:1656,body:1665,filePath:1666,assetImports:1667,digest:1669,legacyId:1670,deferredRender:32},{title:1657,description:1658,date:1659,image:1660,authors:1661,categories:1662,tags:1663,canonical:1664},"How to Install Outline Wiki on Docker","Learn how you can install Outline Wiki with docker compose to have a self hosted notion alternative.",["Date","2024-02-06T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/outline-install.jpeg",[19],[98],[242],"https://www.bitdoze.com/outline-install/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/02/slack-oauth.jpeg\";\r\n\r\n[Outline](https://www.getoutline.com/) is a modern team knowledge base and wiki designed to help teams organize their internal documentation, product specs, support answers, meeting notes, onboarding materials, and more. It is positioned as a more focused alternative to Notion, which is an all-in-one workspace that includes a wider range of features such as project management, databases, and content management systems.\r\n\r\n## Outline Overview\r\n\r\n### Key Features\r\n\r\n- Intuitive Editing Experience: Outline offers a fast editor with markdown support, slash commands, and interactive embeds.\r\n- Simple Integrations: It integrates with tools like Slack, Figma, and Loom, and also provides an open API for additional integrations.\r\n- Language Support: Outline supports right-to-left (RTL) text and is available in 17 languages.\r\n- Open Source: The source code for Outline is publicly available, and it can be self-hosted on your own infrastructure.\r\n- Real-time Collaboration: The platform is designed for real-time collaboration, making document editing and sharing seamless.\r\n- Dark Mode: A dark mode is available for users who prefer it.\r\n- Security & Permissions: Outline allows for detailed management of user permissions.\r\n\r\n### Outline as a Notion Alternative\r\n\r\n- Focus on Knowledge Base: Outline is primarily a knowledge base and documentation platform, whereas Notion is an all-in-one workspace.\r\n- Open Source: For those who prefer open-source solutions, Outline is a strong contender.\r\n- Simplicity: Outline is considered to be simpler and more straightforward compared to Notion, which has a steeper learning curve due to its extensive features.\r\n- Real-time Collaboration: Outline emphasizes real-time collaboration and has been described as closer to a wiki in terms of its functionality.\r\n\r\n### Considerations\r\n\r\n- Limited Use Cases: Outline is more limited in scope compared to Notion, focusing mainly on documentation and knowledge sharing.\r\n- Self-Hosting: Outline can be self-hosted, which is an important consideration for teams with specific privacy or infrastructure requirements.\r\n- No Mobile Apps: Outline does not have mobile apps, which may be a limitation for some users.\r\n\r\nIn summary, Outline is a streamlined, open-source knowledge base and wiki that can serve as a simpler, more focused alternative to Notion for teams looking to organize their internal documentation and collaborate in real-time. It is particularly appealing to those who prefer open-source software and want to avoid the complexity of an all-in-one tool like Notion.\r\n\r\nIf you are looking for other not taking apps that are simpler you can check:\r\n\r\n- [Docmost Docker Compose Install](https://www.bitdoze.com/docmost-docker-install/)\r\n- [How to Install Memos with Docker Compose](https://www.bitdoze.com/memos-install/)\r\n\r\nOutline can be deployed on a server that has a minimum of 1 CPU and 512MB of RAM. It's lightweight and can be installed anywhere.\r\n\r\n## What you Should know Before Installing Outline\r\n\r\nBelow are some of the things that you should keep in mind if you are trying to install Outline\r\n\r\n- **Authentication** - to authenticate in the app you will need a 3PP, you can use Google Authentication (if you are a Workspace user), Slack or Microsoft, you can also use your own image with oidc-server\r\n- **Storage** - files can be stored on S3 buckets or locally, you can use Amazon S3 or Minio image to store files.\r\n- **Databases** - it uses Redis and Postgress SQL for databases.\r\n\r\n## How To Install Outline with Docker Compose\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/IY1jONuTEic\"\r\n  label=\"Outline Install\"\r\n/>\r\n\r\nIn this section, we are going to see what exactly we need to do to have Outline working on our docker compose as easily as possible. We are going to store the files locally as Outline is permitting this now.\r\n\r\n### 1. Prerequisites\r\n\r\nBefore you begin, make sure you have the following prerequisites in place:\r\n\r\n- VPS where you can host Slash, you can use one from [Hetzner](https://go.bitdoze.com/hetzner) or use a [Mini PC as Home Server](https://www.bitdoze.com/best-mini-pc-home-server/)\r\n- Docker and Dockge installed on your server, you can check the [Dockge - Portainer Alternative for Docker Management](https://www.bitdoze.com/dockge-install/) for the full tutorial.\r\n  CloudFlare Tunnels are configured for your VPS server, the details are in the article here I deployed [Dockge](https://www.bitdoze.com/dockge-install/)\r\n\r\n> You can use also Traefik as a reverse proxy for your apps. I have created a full tutorial with Dockge install also to manage your containers on: [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/)\r\n\r\nHaving all of this you will be ready to move to next step and add the container in dockge.\r\n\r\n### 2. Get Slack credentials\r\n\r\nSlack is free to use and we are going to use Slack as an authentication method for our Outline. To do this you need to go in Slack in [Slack API Apps](https://api.slack.com/apps/) and you are going to create an app. In there under **OAuth & Permissions** you add your redirect URL it needs to be with `/auth/slack.callback` at the end, in my case:\r\n\r\n```\r\nhttps://docs.bitdoze.com/auth/slack.callback\r\n```\r\n\r\n<Picture src={img1} alt=\"Slack Oauth\" />\r\n\r\nAfter you can go under **Basic Information** and get your `Client ID` and `Client Secret`. You will need them in the next section\r\n\r\n### 3. Docker Compose File\r\n\r\nDocker-compose file will make all of this possible and now we have the below file created for outline that can be used:\r\n\r\n```yaml\r\nversion: \"3.2\"\r\nservices:\r\n  outline:\r\n    image: docker.getoutline.com/outlinewiki/outline:latest\r\n    container_name: outline-app\r\n    hostname: outline-app\r\n    ports:\r\n      - 3000:3000\r\n    volumes:\r\n      - ./storage-data:/var/lib/outline/data\r\n    depends_on:\r\n      - postgres\r\n      - redis\r\n    environment:\r\n      PGSSLMODE: disable\r\n      SECRET_KEY: ${SECRET_KEY}\r\n      UTILS_SECRET: ${UTILS_SECRET}\r\n      DATABASE_URL: postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@outline-postgres:5432/${POSTGRES_DB}\r\n      REDIS_URL: redis://outline-redis:6379\r\n      URL: ${URL}\r\n      PORT: ${PORT}\r\n      FILE_STORAGE: local\r\n      FILE_STORAGE_LOCAL_ROOT_DIR: /var/lib/outline/data\r\n      FILE_STORAGE_UPLOAD_MAX_SIZE: 26214400\r\n      SLACK_CLIENT_ID: ${SLACK_CLIENT_ID}\r\n      SLACK_CLIENT_SECRET: ${SLACK_CLIENT_SECRET}\r\n    restart: unless-stopped\r\n  redis:\r\n    container_name: outline-redis\r\n    hostname: outline-redis\r\n    image: redis\r\n    volumes:\r\n      - ./redis.conf:/redis.conf\r\n    command:\r\n      - redis-server\r\n      - /redis.conf\r\n    healthcheck:\r\n      test:\r\n        - CMD\r\n        - redis-cli\r\n        - ping\r\n      interval: 10s\r\n      timeout: 30s\r\n      retries: 3\r\n    restart: unless-stopped\r\n  postgres:\r\n    image: postgres\r\n    container_name: outline-postgres\r\n    hostname: outline-postgres\r\n    volumes:\r\n      - ./database-data:/var/lib/postgresql/data\r\n    healthcheck:\r\n      test:\r\n        - CMD\r\n        - pg_isready\r\n      interval: 30s\r\n    environment:\r\n      POSTGRES_USER: ${POSTGRES_USER}\r\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\r\n      POSTGRES_DB: ${POSTGRES_DB}\r\n    restart: unless-stopped\r\n```\r\n\r\nThis docker compose file for Outline has all the required options to run Outline with local storage for images, all the secrets and details will be kept in the env file.\r\n\r\nThe Outline environments are set to use local storage and connections to DB's.\r\n\r\nThe file has local disk for DB, Outline and Redis to store all the details. The recommendation is to use a exact version for images in case you don't want problems when you redeploy this and things update. You want to control your version so in case it gets updated to not mess everything for you.\r\n\r\n### 4. Create The outline env file.\r\n\r\nThe `.env` file has all the secrets and variables to have this working in my case it's looking like below:\r\n\r\n```\r\nPOSTGRES_USER='user'\r\nPOSTGRES_PASSWORD='pass'\r\nPOSTGRES_DB='outline'\r\nSECRET_KEY=d1155bc02c5bfd6b2a4c3313113b0b5f0360366aa3b68c56c5299bc4da4efdf8\r\nUTILS_SECRET=14fec600b3d8b3524421785aef5711805a0a93d21f9ca7f251de88b675862eb\r\nURL=https://docs.bitdoze.com\r\nPORT=3000\r\nSLACK_CLIENT_ID=444642289953.6587112119140\r\nSLACK_CLIENT_SECRET=dbdcb40d85dc3097511179945e2a4bc7\r\n```\r\n\r\nIn here you see you have:\r\n\r\n- SERVER_IP which is the public IP of the server, I couldn't make this work with the localhost you can try.\r\n- Postgres users and passwords\r\n- secrets for outline which are generated with: `openssl rand -hex 32`\r\n- URL and Port - here you will use what you like for your application\r\n- Slack Details - the `Client ID` and `Client Secret` generated in setp 2 can be found here.\r\n\r\nDockge has a UI where you can add all of this and change them in case you need.\r\n\r\n### 5. Deploy Outline\r\n\r\nNow you have the docker-compose file and .env details, what remains to be done is to go in dockde and add a name for your stack and hit deploy. Things should start.\r\nIf you just want to use docker compose to run this you just do:\r\n\r\n```sh\r\ndocker-compose up -d\r\n```\r\n\r\nThis will start all the stacks and you can verify them with:\r\n\r\n```sh\r\ndocker ps\r\n```\r\n\r\n### 6. Grant Outline Permissions to Add images\r\n\r\nOnly with these configs if you try to upload an image will fail as you will not be able to write into the path where images are stored. nodejs user is used by app and needs access there. [Outline File Storage](https://docs.getoutline.com/s/hosting/doc/file-storage-N4M0T6Ypu7) documentation has some details about this. To make this working you will need to run:\r\n\r\n```sh\r\ncd /opt/stacks/outline\r\nchown 1001 ./storage-data\r\n```\r\n\r\nchown with 1001 on the disk path will make sure that the user has permissions there.\r\n\r\n### 7. Configure the CloudFlare Tunnels\r\n\r\nYou need to let CloudFlare Tunel to know which port is using, you just need to go in the Access - Tunnels and choose the tunnel you created and add a hostname that will link a domain or subdomain and the service and port . This will need to be as for the URL you have set in the .env file.\r\n\r\n> You can also check [Setup CloudPanel as Reverse Proxy with Docker and Dokge](https://www.bitdoze.com/cloudpanel-setup-dockge/) to use CloudPanel as a reverse proxy to your Docker containers or [How to Use Traefik as A Reverse Proxy in Docker](https://www.bitdoze.com/traefik-proxy-docker/).\r\n\r\nAnd that's about it, now you can use outline, test it and see how it works.\r\n\r\n## Conclusions\r\n\r\nOutline documentation can be scary if you are a newbie in docker and Linux, this tutorial was simplified so you can start and install Outline as easy as possible to test it and use it. For other configurations and official doc you can check: [Outline Docker Installation](https://docs.getoutline.com/s/hosting/doc/docker-7pfeLP5a8t)","src/content/posts/outline-install.mdx",[1668],"../../assets/images/24/02/outline-install.jpeg","649d0f198d470de1","outline-install.mdx","pdf-extract-text-linux-cmd",{id:1671,data:1673,body:1682,filePath:1683,assetImports:1684,digest:1686,legacyId:1687,deferredRender:32},{title:1674,description:1675,date:1676,image:1677,authors:1678,categories:1679,tags:1680,canonical:1681},"How To Extract Text From PDF In Command Line On Linux","Learn how to extract text from PDF files in command line on Linux using a simple and powerful tool called poppler-utils.",["Date","2024-01-23T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/extract-text-pdf-cmd.jpeg",[19],[98],[24],"https://www.bitdoze.com/pdf-extract-text-linux-cmd/","Merging PDF files on Linux can be efficiently done through the command line using the pdfunite utility from the poppler-utils package. This method is particularly useful for users who prefer working within a terminal or for those who need to handle PDF operations in batch scripts or automation tasks.\r\n\r\n## What is poppler-utils?\r\n\r\nPoppler-utils is a collection of command-line utilities for manipulating PDF files. It is based on the poppler library, which is a fork of the xpdf library. Poppler-utils includes several tools, such as:\r\n\r\n- **pdfinfo:** prints information about a PDF file, such as title, author, pages, etc.\r\n- **pdftotext:** converts a PDF file to plain text.\r\n- **pdftohtml:** converts a PDF file to HTML.\r\n- **pdfimages:** extracts images from a PDF file.\r\n- **pdfseparate:** splits a PDF file into single-page PDF files.\r\n- **pdfunite:** [merges several PDF](https://www.bitdoze.com/pdf-merge-linux-cmd/) files into one.\r\n\r\nIn this article, we will focus on the pdfunite tool, which allows us to merge PDF files in command line on Linux.\r\n\r\n## How to install poppler-utils?\r\n\r\nPoppler-utils is available in the official repositories of most Linux distributions. You can install it using your package manager. For example, on Debian-based systems, such as Ubuntu, you can use the following command:\r\n\r\nFor Debian-based systems like Ubuntu, you can install poppler-utils using the following command:\r\n\r\n```sh\r\nsudo apt-get install poppler-utils\r\n```\r\n\r\nFor Red Hat-based systems such as Fedora or CentOS, use:\r\n\r\n```sh\r\nsudo yum install poppler-utils\r\n```\r\n\r\nTo verify that poppler-utils is installed, you can run the following command:\r\n\r\n```sh\r\npdftotext --version\r\n```\r\n\r\nYou should see something like this:\r\n\r\n```\r\npdftotext version 0.86.1\r\nCopyright 2005-2019 The Poppler Developers - http://poppler.freedesktop.org\r\nCopyright 1996-2011 Glyph & Cog, LLC\r\n```\r\n\r\n## How to use pdftotext?\r\n\r\nThe syntax of pdftotext is very simple. You just need to specify the PDF file that you want to convert to text, and optionally the name of the output text file. For example, if you want to convert a file called input.pdf to a file called output.txt, you can use the following command:\r\n\r\n```sh\r\npdftotext input.pdf output.txt\r\n```\r\n\r\nIf you do not specify the output file name, pdftotext will use the same name as the input file, but with the .txt extension. For example, if you want to convert a file called input.pdf to a file called input.txt, you can use the following command:\r\n\r\n```sh\r\npdftotext input.pdf\r\n```\r\n\r\nYou can also use the - option to read the PDF file from the standard input or write the text to the standard output. For example, if you want to convert a PDF file that is piped from another command, you can use the following command:\r\n\r\n```sh\r\ncat input.pdf | pdftotext - -\r\n```\r\n\r\n## How to customize the output format?\r\n\r\nPdftotext has several options that allow you to customize the output format of the text. Here are some of the most useful ones:\r\n\r\n- -layout: preserves the original layout of the PDF file, including columns, tables, etc.\r\n- -raw: keeps the original order of the text, but ignores the layout.\r\n- -htmlmeta: generates an HTML file with meta information, such as title, author, etc.\r\n- -bbox: generates an HTML file with bounding box information for each word.\r\n- -f number: specifies the first page to convert.\r\n- -l number: specifies the last page to convert.\r\n- -enc encoding: specifies the encoding of the output text, such as UTF-8, ISO-8859-1, etc.\r\n\r\nFor example, if you want to convert the first 10 pages of a PDF file to an HTML file with meta information and bounding boxes, you can use the following command:\r\n\r\n```sh\r\npdftotext -htmlmeta -bbox -f 1 -l 10 input.pdf output.html\r\n```\r\n\r\nYou can see the full list of options by running the following command:\r\n\r\n```sh\r\npdftotext -h\r\n```\r\n\r\n## Conclusion\r\n\r\nThe `pdftotext` utility from the `poppler-utils` package provides a simple and efficient way to extract text from PDF files on Linux. Whether you need to extract text for data analysis, content repurposing, or any other reason, `pdftotext` can be a valuable tool in your Linux command line toolkit.","src/content/posts/pdf-extract-text-linux-cmd.mdx",[1685],"../../assets/images/24/01/extract-text-pdf-cmd.jpeg","cffa590fde5ddea4","pdf-extract-text-linux-cmd.mdx","pdf-merge-linux-cmd",{id:1688,data:1690,body:1699,filePath:1700,assetImports:1701,digest:1703,legacyId:1704,deferredRender:32},{title:1691,description:1692,date:1693,image:1694,authors:1695,categories:1696,tags:1697,canonical:1698},"How To Merge PDF Files In Command Line On Linux","Learn how to merge multiple PDF files into one using a simple and powerful tool called poppler-utils. This article will show you how to install poppler-utils, how to use the pdfunite command, and how to troubleshoot common errors",["Date","2024-01-23T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/merge-pdf-cmd.jpeg",[19],[98],[24],"https://www.bitdoze.com/pdf-merge-linux-cmd/","Merging PDF files on Linux can be efficiently done through the command line using the pdfunite utility from the poppler-utils package. This method is particularly useful for users who prefer working within a terminal or for those who need to handle PDF operations in batch scripts or automation tasks.\r\n\r\n## What is poppler-utils?\r\n\r\nPoppler-utils is a collection of command-line utilities for manipulating PDF files. It is based on the poppler library, which is a fork of the xpdf library. Poppler-utils includes several tools, such as:\r\n\r\n- **pdfinfo:** prints information about a PDF file, such as title, author, pages, etc.\r\n- **pdftotext:** [converts a PDF file to plain text](https://www.bitdoze.com/pdf-extract-text-linux-cmd/).\r\n- **pdftohtml:** converts a PDF file to HTML.\r\n- **pdfimages:** extracts images from a PDF file.\r\n- **pdfseparate:** splits a PDF file into single-page PDF files.\r\n- **pdfunite:** merges several PDF files into one.\r\n\r\nIn this article, we will focus on the pdfunite tool, which allows us to merge PDF files in command line on Linux.\r\n\r\n## How to install poppler-utils?\r\n\r\nPoppler-utils is available in the official repositories of most Linux distributions. You can install it using your package manager. For example, on Debian-based systems, such as Ubuntu, you can use the following command:\r\n\r\nFor Debian-based systems like Ubuntu, you can install poppler-utils using the following command:\r\n\r\n```sh\r\nsudo apt-get install poppler-utils\r\n```\r\n\r\nFor Red Hat-based systems such as Fedora or CentOS, use:\r\n\r\n```sh\r\nsudo yum install poppler-utils\r\n```\r\n\r\nTo verify that poppler-utils is installed, you can run the following command:\r\n\r\n```sh\r\npdfunite --version\r\n```\r\n\r\nYou should see something like this:\r\n\r\n```\r\npdfunite version 0.86.1\r\nCopyright 2005-2019 The Poppler Developers - http://poppler.freedesktop.org\r\nCopyright 1996-2011 Glyph & Cog, LLC\r\n```\r\n\r\n## How to use pdfunite?\r\n\r\nThe syntax of pdfunite is very simple. You just need to specify the PDF files that you want to merge, followed by the name of the output file. For example, if you want to merge file1.pdf, file2.pdf, and file3.pdf into a single file called output.pdf, you can use the following command:\r\n\r\n```sh\r\npdfunite file1.pdf file2.pdf file3.pdf output.pdf\r\n```\r\n\r\nYou can also use wildcards to merge all the PDF files in a directory. For example, if you want to merge all the PDF files in the current directory into a file called output.pdf, you can use the following command:\r\n\r\n```sh\r\npdfunite *.pdf output.pdf\r\n```\r\n\r\nYou can also use the - option to read the PDF files from the standard input. For example, if you want to merge the PDF files that are listed in a text file called files.txt, you can use the following command:\r\n\r\n```sh\r\npdfunite - < files.txt output.pdf\r\n```\r\n\r\nThe files.txt file should contain one PDF file name per line, such as:\r\n\r\n```\r\nfile1.pdf\r\nfile2.pdf\r\nfile3.pdf\r\n```\r\n\r\n## How to troubleshoot common errors?\r\n\r\nSometimes, you may encounter some errors when using pdfunite. Here are some common ones and how to fix them:\r\n\r\n- _Permission denied:_ This means that you do not have the permission to read or write the PDF files. You can check the file permissions using the ls -l command, and change them using the chmod command. For example, to give read and write permission to the owner of the file, you can use the following command:\r\n\r\n```sh\r\nchmod u+rw file.pdf\r\n```\r\n\r\n- **No such file or directory:** This means that the PDF file does not exist or the file name is incorrect. You can check the file name using the ls command, and make sure that it matches the one you typed. You can also use the tab key to autocomplete the file name.\r\n\r\n- **Invalid or damaged PDF file:** This means that the PDF file is corrupted or not a valid PDF file. You can try to repair the PDF file using the pdfinfo command with the -repair option. For example, to repair a file called file.pdf, you can use the following command:\r\n\r\n```sh\r\npdfinfo -repair file.pdf\r\n```\r\n\r\nIf the repair is successful, you will see a message like this:\r\n\r\n```\r\nRepairing PDF file\r\nDone. Output file written to file.repaired.pdf\r\n```\r\n\r\nYou can then use the repaired file instead of the original one.\r\n\r\n## Conclusion\r\n\r\nMerging PDF files on Linux using the command line is a quick and straightforward process with the help of `poppler-utils`. Whether you're dealing with a couple of documents or a batch of files, pdfunite provides a reliable way to combine PDFs without the need for a graphical interface.\r\nRemember to check the merged document to ensure all pages are present and in the correct order. With these simple steps, you can efficiently manage PDF files on your Linux system.","src/content/posts/pdf-merge-linux-cmd.mdx",[1702],"../../assets/images/24/01/merge-pdf-cmd.jpeg","a4b6622f4a718643","pdf-merge-linux-cmd.mdx","php-cloud-hosting",{id:1705,data:1707,body:1717,filePath:1718,assetImports:1719,digest:1721,legacyId:1722,deferredRender:32},{title:1708,description:1709,date:1710,image:1711,authors:1712,categories:1713,tags:1714,canonical:1716},"Best PHP Cloud Hosting Providers for 2024","Looking for the best PHP cloud hosting providers for 2024? This article will help you compare the features, performance, pricing, and support of the top PHP cloud hosting providers and help you make an informed decision.",["Date","2024-02-01T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/php-cloud-hosting.jpeg",[19],[98],[1715],"php","https://www.bitdoze.com/php-cloud-hosting/","PHP hosting is a critical aspect for businesses, startups, freelancers, and developers. It plays a pivotal role in ensuring the scalability and efficiency of PHP applications. This article provides a comprehensive guide on the best PHP cloud hosting providers for 2024, considering their performance, pricing, and features.\r\n\r\nPHP stands for PHP: Hypertext Preprocessor, a scripting language well-suited for creating dynamic websites. PHP hosting allows web developers to rent server space to store and execute their PHP files, making their websites or applications accessible to users on various devices. In this article, we are going to see the different options available that can help with hosting your PHP applications.\r\n\r\n> I am hosting all my PHP applications on _[Hetzner](https://go.bitdoze.com/hetzner)_ with _[CloudPanel](https://www.bitdoze.com/install-cloudpanel-host-nodejs/)_ as a server manager panel and I will not change anything so in case you want the quick answer this is it. If you want something that doesn't need a lot of work and you are willing to pay more I will recommand [Cloudways](https://go.bitdoze.com/cloudways)\r\n\r\n## The Best PHP Cloud Hosting Providers for 2024\r\n\r\n### PHP Hosting Providers By Price\r\n\r\nBelow is an overview of the prices that you can have for PHP hosting, be careful to check the exact features before making a decision. Even if some are more expensive like Cloudways they come packed with a lot of features. Don't just go and choose the cheapest choose the one that suits your needs best. This selection is to cover all the PHP application types.\r\n\r\n| Cloud Hosting Provider                           | Lowest Monthly Price | Is Managed |\r\n| ------------------------------------------------ | -------------------- | ---------- |\r\n| [Hostinger](https://www.wpdoze.com/go/hostinger) | $1.99                | Yes        |\r\n| [Vultr](https://go.bitdoze.com/vultr)            | $2.5                 | No         |\r\n| [Hetzner Cloud](https://go.bitdoze.com/hetzner)  | $5                   | No         |\r\n| [DigitalOcean](https://go.bitdoze.com/do)        | $5                   | No         |\r\n| [Cloudways](https://go.bitdoze.com/cloudways)    | $14                  | Yes        |\r\n\r\n> For servers that provides only hosting like Hetzner, Vultr, DigitalOcean or Hostinger VPS you will need an hosting panal to help you manage your PHP projects. The recommandations are [CloudPanel](https://www.cloudpanel.io/) or [Ploi.io](https://ploi.io/).\r\n\r\n### #1.Hetzner\r\n\r\nHetzner is a German cloud computing provider that offers high-performance and low-cost cloud servers, storage, and networking solutions. Hetzner does not offer PHP-specific hosting plans, but users can install and run any PHP application on their cloud servers, using their own or third-party server management tools.\r\n\r\nHetzner has ARM, Intel or AMD CPU, you can choose between dedicated or shared and different RAM sizes at very good prices. The performance is very good I am using them for more than 3 years and everything is working fine.\r\n\r\nHetzner in comparison with other providers like Vultr, Linode or DigitalOcean keeps low prices and good performance, a server with similar specs can be half of the price of others or even lower.\r\n\r\nHetzner has also backups, block storage, load balancer, floating IP addresses, DDoS protection, and more.\r\n\r\nGivin the fact that the performance is very good and prices are good it is at the `top` of my list.\r\n\r\n**[Try Hetzner, $20 FREE Credits](https://go.bitdoze.com/hetzner)**\r\n\r\n### #2.Cloudways\r\n\r\nCloudways is a managed cloud hosting platform that allows users to host their PHP applications on various cloud providers, such as DigitalOcean, AWS, and Google Cloud. Cloudways takes care of the server setup, configuration, optimization, and security, and provides users with a simple and intuitive dashboard to manage their PHP applications.\r\n\r\nCloudways is offering integrations with CloudFlare Enterprise for an extra $5 for domain which is pretty good. Cloudways servers and configs can be managed from UI and can make things very easy. The one downside is that you don't have root access to the VPS.\r\n\r\nThe price is definitely higher and it starts from $14 but it comes packed with features. Their application makes administrating a server very easy and it comes packed with a lot of functions that's why it ranks `number 2` on my list. I am suing them for clients in special that want to do things on their own and doesn't have a lot of technical knowledge.\r\n\r\n**[Try Cloudways](https://go.bitdoze.com/cloudways)**\r\n\r\n### #3.Vultr\r\n\r\nVultr is a cloud hosting provider known for its high-performance SSD VPS servers, powered by the latest AMD and Intel CPUs. These servers can be deployed in under 60 seconds, making them ideal for hosting PHP applications. Vultr has 32 global data centers, which can help reduce latency and improve app performance.\r\n\r\nIt also offers robust security features, including dedicated IP addresses, DDoS protection, and a built-in firewall. Vultr's managed databases for MySQL, PostgreSQL, and Redis simplify data management, while its scalable storage solutions can handle large amounts of data. The platform supports a wide range of operating systems and offers a user-friendly control panel for easy server management.\r\n\r\nVultr also provides a range of developer tools and services, including Kubernetes support for containerized PHP applications. However, managing a VPS server on Vultr requires technical expertise, so those unfamiliar with server setup and maintenance may prefer a managed hosting provider.\r\n\r\nVultr like Hetzner can be used with a hosting panel like **CloudPanel** or **Ploi**. Vultr starts with plans from $2.5 and it has the fastest shared cloud servers from DigitalOcean and Hetzner, made an article in here: [DigitalOcean vs Vultr vs Hetzner](https://www.wpdoze.com/digitalocean-vs-vultr-vs-hetzner/)\r\n\r\nI have used Vultr in the past and the performance was way better than DigitalOcean one as can be seen in the article above. What made me move was Hetzner good prices and performance. Why would you not pay less for the same server with CPU, RAM and Storage and have similar performance :)\r\n\r\nIf you are in need of a datacenter closer to your visitors Vultr can help you with very good services.\r\n\r\n**[Try Vultr and Get $100 Free to Test Them](https://go.bitdoze.com/vultr)**\r\n\r\n### #4.Digitalocean\r\n\r\nDigitalocean is offering services similar to Vultr and they have even more. They are some of the first that appeared and I remember that more than 5 years ago they were the ones that helped me host all the sites.\r\n\r\nLately I didn't have a good experience with them as I have seen some slowness in my server and my clients and decided to use Vultr then. This happened years ago and I guess it depends on the datacenter and how loaded it was.\r\n\r\nDigitalocean comes with a lot of services and their servers are called droplets, which have a wide range of servers with high performance and so on.\r\n\r\nSame as for the others you will need to be a little technical and use a server panel to administrate your applications. DigitalOcean is offering free credits for new comes so you can test and check for yourself the performance.\r\n\r\n**[Try Digitalocean and Get $100 Free to Test Them](https://go.bitdoze.com/do)**\r\n\r\n### #5.Hostinger\r\n\r\nHostinger's shared hosting service is an affordable and user-friendly solution for hosting PHP applications. It allows multiple websites to share the same server, making it a cost-effective choice for small to medium-sized projects.\r\n\r\nHostinger's shared hosting comes with a custom-made control panel, hPanel, which simplifies the setup and management of websites. It also provides an Auto Installer tool, making it easy to install PHP and other applications.\r\n\r\nHostinger uses CloudLinux with LVE containers, ensuring each user gets a dedicated container with resource boundaries for smooth operation. It also supports different PHP versions, allowing you to switch between versions as needed.\r\n\r\nHostinger's shared hosting plans are optimized for performance, using LiteSpeed web server and Non-Volatile Memory Express (NVMe) solid-state drives for fast data storage and retrieval.\r\nIn addition, Hostinger offers a range of security features, including malware scanning and Cloudflare-protected name servers.\r\n\r\nIf you are not technical at all and you don't have a big application you can check Hostinger they have plans that starts from about $2 a month. They are offering also VPS solution in case you are wandering and you don't like the rest.\r\n\r\n**[Try Hostinger](https://www.wpdoze.com/go/hostinger)**\r\n\r\n## Free PHP Hosting: Is It Possible?\r\n\r\nIf you are looking for a free way to host your PHP applications, you may be wondering if there are any free PHP hosting providers available. The answer is yes, there are some free PHP hosting providers that offer limited resources and features for PHP applications, but they come with many risks and limitations.\r\n\r\n### Free PHP Hosting and Risks\r\n\r\nFree PHP hosting may sound appealing, but it is not recommended for serious or professional PHP applications, as it has many drawbacks, such as:\r\n\r\n- Low performance and reliability: Free PHP hosting providers usually have low-quality servers, overcrowded resources, and frequent downtime, which can affect the speed, uptime, and functionality of your PHP applications.\r\n- Limited features and flexibility: Free PHP hosting providers usually have outdated PHP versions, extensions, and frameworks, as well as restricted databases, storage, bandwidth, and domains. You may not be able to customize or optimize your server environment according to your needs.\r\n- Poor security and support: Free PHP hosting providers usually have weak security measures, such as no SSL certificates, firewalls, backups, or malware protection, which can expose your PHP applications to hackers, data loss, or corruption. You may also have no or minimal customer support, documentation, or community resources to help you with your PHP applications.\r\n- Hidden fees and ads: Free PHP hosting providers may not be truly free, as they may charge you for extra resources, features, or services, or display unwanted ads on your PHP applications, which can affect your user experience and reputation\r\n\r\n### Free PHP Hosting Providers\r\n\r\nIf you still want to try free PHP hosting, here are some of the free PHP hosting providers that you can check out:\r\n\r\n**000webhost Free PHP Hosting**\r\n\r\n000webhost Free offers a comprehensive package for hosting PHP apps without any financial commitment. It provides almost unrestricted PHP support, ensuring compatibility with the latest PHP versions for free, which is crucial for running dynamic PHP-based applications. The service includes 300 MB of disk space and 3 GB of bandwidth, accommodating small to medium-sized projects. Users benefit from a free version of cPanel, facilitating easy management of their hosting environment, and MySQL databases for data storage, essential for PHP applications.\r\n\r\n**InfinityFree**\r\n\r\nInfinityFree is a free web hosting platform that offers a range of features suitable for hosting PHP applications. It provides 5 GB of disk space and unlimited bandwidth, supporting PHP 8.2 and MySQL 5.7 / MariaDB 10.4, which are essential for running PHP-based websites and applications.\r\n\r\nIf you are testing something or you don't have the budget you can give them a look.\r\n\r\n## Free Hosting Alternatives to PHP\r\n\r\nIf you are looking for a free way to host your websites, but you do not want to use PHP or deal with the risks and limitations of free PHP hosting, you may want to consider some free hosting alternatives to PHP. One of the most popular and easy-to-use alternatives is static websites.\r\n\r\n### Static Websites (HTML, CSS, and JavaScript)\r\n\r\nStatic websites are websites that consist of only HTML, CSS, and JavaScript files, without any server-side scripting or database interaction. Static websites are simple, fast, secure, and easy to maintain, as they do not require any server configuration, installation, or processing. Static websites are also suitable for many types of websites, such as blogs, portfolios, landing pages, documentation, etc.\r\n\r\n### Best Free Hostings for Static Websites\r\n\r\nIf you want to create and host static websites for free, you can use some of the best free hostings for static websites, such as:\r\n\r\n**Cloudflare Pages**\r\n\r\nCloudflare Pages' free tier offers a comprehensive set of features for hosting websites, making it an attractive option for developers and teams. With the ability to deploy up to 500 times per month, it caters to active development environments. Each build has a maximum duration of 20 minutes, ensuring efficient use of resources. The platform supports up to 20,000 files per site, with a maximum file size limit of 25 MiB, accommodating a wide range of web projects. Users can assign up to 100 custom domains per project, providing flexibility in branding and accessibility.\r\n\r\nA standout feature is the unlimited collaborators option, which encourages teamwork and project sharing without additional costs. Cloudflare Pages integrates seamlessly with Git, automating deployments and providing preview links for every commit and pull request, which simplifies the review and collaboration process. Additionally, the free tier includes essential security features like SSL, CDN, and DDoS protection, highlighting Cloudflare's commitment to secure and performant web hosting. This plan is designed to meet the needs of individual developers to small teams by offering a robust platform for efficiently deploying and managing web projects.\r\n\r\nYou can check: [How To Deploy An Astro.JS Blog On Cloudflare](https://www.bitdoze.com/deploy-astrojs-cloudflare/)\r\n\r\n**Netlify**\r\n\r\nNetlify's free tier offers a generous package for hosting websites, making it an attractive option for personal projects, hobby sites, or experiments. Users can deploy unlimited websites with a bandwidth limit of 100GB per month and are allocated 300 build minutes. This should suffice for websites with moderate traffic. Additionally, the free tier includes serverless functions with a limit of 125k requests per site per month, with additional usage incurring a fee. Forms can be integrated into websites by adding a simple attribute to any HTML form, although the free tier has limitations on the number of form submissions and large media storage.\r\n\r\n**Vercel**\r\n\r\nVercel's free tier, known as the Hobby plan, caters to personal projects, developers, and small-scale applications with a suite of features for web hosting.The plan ensures secure connections with automatic HTTPS/SSL and offers previews for every git push, enabling users to visualize changes before they go live. Notably, the Hobby plan is free and designed for non-commercial, personal use only, with no billing cycles. However, exceeding usage limits requires waiting until 30 days have passed for feature reuse.\r\n\r\n**GitHub Pages**\r\n\r\nGitHub Pages Free Tier offers a static site hosting service that allows you to host websites directly from a GitHub repository. It supports HTML, CSS, and JavaScript files, and can optionally run these files through a build process before publishing. You can host your site on GitHub's github.io domain or your own custom domain.\r\n\r\nThere are three types of GitHub Pages sites: project, user, and organization. You can create publicly available sites, and organizations using GitHub Enterprise Cloud can also publish sites privately. The service is available in public repositories only with the Free tier, although Free tier users can set up an unlimited number of public and private repositories. However, private repositories can't be used with GitHub Pages in the Free tier.\r\n\r\n## Conclusions\r\n\r\nIn this article, we have discussed the best PHP cloud hosting providers for 2024, the free PHP hosting providers and their risks, and the free hosting alternatives to PHP. We hope this article has helped you find the best hosting solution for your PHP applications or static websites. Thank you for reading.","src/content/posts/php-cloud-hosting.mdx",[1720],"../../assets/images/24/02/php-cloud-hosting.jpeg","57f0fe0915784943","php-cloud-hosting.mdx","plausible-tool",{id:1723,data:1725,body:1734,filePath:1735,assetImports:1736,digest:1738,legacyId:1739,deferredRender:32},{title:1726,description:1727,date:1728,image:1729,authors:1730,categories:1731,tags:1732,canonical:1733},"Plausible.io - Google Analytics Lightweight Alternative","Plausible.io self-hosted Google analytics lightweight alternative",["Date","2023-03-17T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/03/plausible_blog.jpeg",[19],[77],[572,242],"https://www.bitdoze.com/plausible-tool/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport Button from \"../../layouts/components/widgets/Button.astro\";\r\n\r\n## 🔭 Plausible Overview\r\n\r\n[Plausible.io](https://plausible.io/) is an open-source analytics tool that provides website owners with detailed metrics about their visitors. It's designed to be simple, lightweight, and privacy-first; plus it offers a suite of features that make it an attractive alternative to the big players in the industry, such as Google Analytics. Plausible can be self-hosted on your own server so no one but you to have access to your traffic data.\r\n\r\n## 🎥 Plausible Video Overview\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/NoBp7bqAHUI\"\r\n  label=\"Plausible Analytics\"\r\n/>\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## ❔ Why Use Plausible\r\n\r\nIf you want to switch from Google Analytics to a self-hosted tool, Plausible is the analytics tool for you. Plausible can be quickly installed on your servers and start tracking your website traffic. Plausible Analytics is lightweight (< 1 KB) and will not slow down your website. Plausible puts you in control of your statistics.\r\n\r\n## 📋 Plausible Features\r\n\r\n- **Simple analysis:** Reports generated by Plausible are simpler and easier to understand.\r\n- **Lightweight script:** The script used by Plausible is light and has less than 1KB in size.\r\n- **No need for cookie banners or GDPR consent:** No cookies are used and no personal data is collected.\r\n- **Track events, goal conversions, campaigns:** You can easily track different sources and events.\r\n- **Self-hosted:** Plausible can be installed on your own server.\r\n- **Powerful API:** You can integrate Plausible with your application using the provided API.\r\n\r\n## 🏷️ Pricing\r\n\r\n- **Free** - if you self host it yourself\r\n- **Paid** - from $9 for 10k in Plausible cloud\r\n\r\n<Button link=\"https://plausible.io/\" text=\"Check Plausible\" />","src/content/posts/plausible-tool.mdx",[1737],"../../assets/images/23/03/plausible_blog.jpeg","9928510bf90409fb","plausible-tool.mdx","perplexity",{id:1740,data:1742,body:1751,filePath:1752,assetImports:1753,digest:1755,legacyId:1756,deferredRender:32},{title:1743,description:1744,date:1745,image:1746,authors:1747,categories:1748,tags:1749,canonical:1750},"The Future of AI and Search: How Perplexity is Changing the Game","Perplexity.ai presentation with features it has and $10 discount code for your first month.",["Date","2024-03-05T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/03/perplexity.jpeg",[19],[77],[79],"https://www.bitdoze.com/perplexity/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/24/02/groq-mistral-streamlit.png\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\nAlong the time I have used ChatGPT, Google Gemini or Claude AI chat tools to chat with AI and received help in day-to-day tasks that I am doing but neither is as useful as the [Perplexity.ai](https://go.bitdoze.com/perplexity)\r\n\r\nPerplexity made me continue the PRO subscription to them while the others were canceled. In this article we are going to go thru Perplexity and what has to offer.\r\n\r\nPerplexity AI is revolutionizing the way we search for and discover information online. By leveraging cutting-edge artificial intelligence and large language models, Perplexity provides users with highly accurate, relevant, and up-to-date answers to their questions.\r\n\r\nPerplexity goes beyond traditional search engines by understanding the intent behind queries and engaging in conversational interactions to clarify ambiguities. This leads to a more intuitive and efficient search experience that saves users time and effort in finding the information they need.\r\n\r\nAt its core, Perplexity uses a metric called perplexity to evaluate the performance of its language models. Perplexity measures how well a model predicts a sample of text - the lower the perplexity, the better the model.\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/oE0JjmuaPPY\"\r\n  label=\"Perplexity Video\"\r\n/>\r\n\r\n## 1. Perplexity Free Option\r\n\r\nOne of the great things about Perplexity is that it offers a robust free tier for users to experience its powerful search capabilities. With the free version, you can ask questions on any topic and get relevant, well-researched answers drawing from Perplexity's vast knowledge base.\r\n\r\nThe free tier is an excellent way to explore what Perplexity has to offer before upgrading to the Pro plan. You'll be able to test out features like:\r\n\r\n- Asking open-ended questions\r\n- Engaging in multi-turn conversations to clarify your search intent\r\n- Getting answers with key points and details highlighted\r\n\r\nWhile the free tier does have some usage limits and restrictions compared to the Pro version, it still provides an extremely capable and valuable search experience for casual users and those getting started with the platform.\r\n\r\n## 2. How Perplexity Pro Can Help\r\n\r\nFor power users and those who want to unlock the full potential of Perplexity, the Pro plan offers a suite of advanced features and benefits, the PRO plan is $20 but you can access it with only $10 in your fist month to see what it can do.\r\n\r\n<Button\r\n  link=\"https://go.bitdoze.com/perplexity\"\r\n  text=\"Get $10 OFF Perplexity.ai\"\r\n/>\r\n\r\n### 2.1 Answer Questions with Search Help\r\n\r\nPerplexity Pro's \"Copilot\" mode provides an interactive search experience powered by the platform's language models. Copilot excels at handling complex, ambiguous queries by asking clarifying questions and delivering in-depth, extensively researched answers.\r\n\r\nWith Pro, you get over 600 Copilot queries per day, ensuring you always have access to Perplexity's most powerful search companion when you need it. Copilot is perfect for diving deep into topics, getting multiple perspectives on an issue, and discovering information you didn't even know you were looking for.\r\n\r\n### 2.2 Different LLMs\r\n\r\nPerplexity Pro gives you access to some of the most advanced language models available:\r\n\r\n| Model         | Description                                                                        |\r\n| ------------- | ---------------------------------------------------------------------------------- |\r\n| GPT-4         | OpenAI's state-of-the-art model, known for its accuracy and reasoning capabilities |\r\n| Claude 3      | Anthropic's model focused on safe and natural conversations                        |\r\n| Mistral Large | Perplexity's own high-performance model optimized for concise responses            |\r\n\r\nWith Pro, you can easily switch between these models based on your needs and preferences. GPT-4 provides unparalleled accuracy, Claude excels at natural conversations, and Mistral Large delivers concise, to-the-point answers.\r\n\r\n### 2.3 Image Generation\r\n\r\nIn addition to its search capabilities, Perplexity Pro also offers AI image generation powered by models:\r\n\r\n- DALL-E 2 - OpenAI's model that can create and edit realistic images from text descriptions\r\n- Stable Diffusion XL - A powerful open-source model for high-resolution image synthesis\r\n- Playground v2.5 - Perplexity's own model that allows for interactive image generation and manipulation\r\n\r\nWith these tools, you can create images.\r\n\r\n### 2.4 Use Different Search Options\r\n\r\nPerplexity Pro allows you to focus your searches on specific websites and content types to get more targeted, relevant results:\r\n\r\n- **Reddit** - Search Reddit forums and discussions for authentic user opinions and experiences\r\n- **YouTube** - Find video content related to your query, Perplexity will use YouTube to answer your question.\r\n- **Writing Mode** - With this you can chat directly with the LLM you have chosen.\r\n\r\nThese focused search options help you quickly find the most useful information for your specific needs, whether it's social media discussions, multimedia content, or creative writing help.\r\n\r\n### 2.5 $5 Perplexity API Credits\r\n\r\nAs a Pro subscriber, you get $5 worth of API credits to use each month. Perplexity's API gives developers access to the same powerful language models that power the platform's search.\r\n\r\nWith the API, you can integrate Perplexity's AI capabilities into your own applications, websites, and workflows. Use cases include:\r\n\r\n- Semantic search and recommendations\r\n- Chatbots and conversational AI\r\n- Text analysis and summarization\r\n- Creative writing assistance\r\n\r\n### 2.6 Perplexity Chrome Extension\r\n\r\nPerplexity Pro users get exclusive access to the platform's Chrome extension. The extension allows you to conveniently access Perplexity's search capabilities from anywhere on the web.\r\n\r\nWith the extension, you can:\r\n\r\n- Highlight text on any webpage and ask Perplexity questions about it\r\n- Get definitions, explanations, and related information about selected terms and concepts\r\n- Quickly search for additional context and sources while browsing the web\r\n\r\nYou can get the extension from [here](https://chromewebstore.google.com/detail/perplexity-ai-companion/hlgbcneanomplepojfcnclggenpcoldo?pli=1)\r\n\r\n## 3. Perplexity $10 Promo Code\r\n\r\nFor a limited time, Perplexity is offering a special $10 promo code for new Pro subscribers. With this code, you can get your first month of Pro for just $10 (normally $20/month).\r\n\r\nTo redeem the promo code you can use the below link, you get $10 off and I get $10 off.\r\n\r\n<Button\r\n  link=\"https://go.bitdoze.com/perplexity\"\r\n  text=\"Get $10 OFF Perplexity.ai\"\r\n/>\r\n\r\nThis is a great opportunity to experience all the powerful features and benefits of Perplexity Pro at a significantly reduced price.\r\n\r\n## 4. Conclusion\r\n\r\nPerplexity does a nice job helping you in day to day activities and can help you use different LLMs models from their settings page. I am quite happy with Perplexity.ai and I am using it for my daily tasks, I have started to ditch Goggle more and more.","src/content/posts/perplexity.mdx",[1754],"../../assets/images/24/03/perplexity.jpeg","3be9e97b799ffb2c","perplexity.mdx","pm2-manage-apps",{id:1757,data:1759,body:1769,filePath:1770,assetImports:1771,digest:1773,legacyId:1774,deferredRender:32},{title:1760,description:1761,date:1762,image:1763,authors:1764,categories:1765,tags:1766,canonical:1768},"How to Manage Applications with PM2: A Complete Guide","Learn how to use PM2, a powerful and easy-to-use process manager for Node.js, to run and monitor your applications in production. Whether you are using Node.js, Python, or any other language, PM2 can help you improve the scalability, performance, and reliability of your applications.",["Date","2024-01-19T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/pm2-manage.jpeg",[19],[98],[1767],"pm2","https://www.bitdoze.com/pm2-manage-apps/","import { Picture } from \"astro:assets\";\r\nimport imag1 from \"../../assets/images/24/01/pm2-monit.png\";\r\n\r\nIf you are developing applications for the web, you know how important it is to have a reliable and efficient way to run and manage them in production. You need a process manager that can help you run your applications in the background, scale them up or down, monitor their status and performance, handle errors and crashes, and more.\r\n\r\nPM2 is a process manager for Node.js applications that can also be used to manage applications written in any other language, such as Python, Ruby, PHP, etc. PM2 is powerful, easy to use, and has a lot of features that can make your life easier as a developer. In this article, we will show you how to use PM2 to manage your applications, and explain the benefits of using PM2 for your projects.\r\n\r\n## What is PM2\r\n\r\n[PM2](https://pm2.keymetrics.io/) is a process manager for Node.js applications that can also be used to manage applications written in any other language. PM2 is an open-source project that has been developed since 2014, and has over 30 million downloads on npm. PM2 is used by many companies and organizations, such as Microsoft, Netflix, NASA, IBM, and more.\r\n\r\nPM2 allows you to run your applications in the background, as daemon processes, without blocking your terminal or console. PM2 can also help you scale your applications horizontally or vertically, by running multiple instances of your application on the same or different machines. PM2 can also monitor your applications, and provide you with useful information, such as the status, logs, memory, cpu, event loop, and network usage of your applications. PM2 can also handle errors and crashes gracefully, by restarting your applications automatically, and keeping them alive. PM2 can also provide you with a web interface, where you can access the same dashboard as the command-line interface, but from your browser.\r\n\r\nPM2 supports applications written in any language, such as Node.js, Python, Ruby, PHP, etc. PM2 can also run any type of application, such as web servers, microservices, cron jobs, chatbots, etc. PM2 can also run any type of script, such as shell scripts, bash scripts, python scripts, etc.\r\n\r\nPM2 is designed to be simple and intuitive, and has a lot of features and options that you can customize to suit your needs. PM2 is also well-documented, and has a large and active community that can help you with any questions or issues that you may have.\r\n\r\n## How to Install PM2\r\n\r\nBefore you can use PM2 to manage your applications, you need to install PM2 on your system. PM2 is a Node.js module, so you need to have Node.js and npm installed first. You can check the official documentation for Node.js installation instructions for your operating system.\r\n\r\nOnce you have Node.js and npm installed, you can install PM2 globally with the following command:\r\n\r\n```sh\r\nnpm install -g pm2\r\n```\r\n\r\nThis will install PM2 as a global module that you can access from anywhere on your system. You can verify that PM2 is installed by running:\r\n\r\n```sh\r\npm2 --version\r\n```\r\n\r\nThis should display the current version of PM2 that you have installed.\r\n\r\n```\r\n[PM2] PM2 Successfully daemonized\r\n5.3.0\r\n```\r\n\r\n## How to Manage Applications with PM2\r\n\r\nIn this section, we are going to see how you can use PM2 to run your node.js applications or python ones. First, we will start with some of the things you need to look at before being informed.\r\n\r\n### Best Practices for Managing Applications with PM2\r\n\r\nDepending on the type and language of your application, there are some best practices that you should follow when managing your applications with PM2. Here are some of the best practices for managing Node.js and Python applications with PM2.\r\n\r\n#### Node.js Applications\r\n\r\nNode.js is a JavaScript runtime environment that allows you to run JavaScript code outside of a browser. Node.js is also the platform that PM2 is built on, and that PM2 supports natively. PM2 can run and manage any Node.js application, such as web servers, microservices, cron jobs, chatbots, etc.\r\n\r\nHere are some of the best practices for managing Node.js applications with PM2:\r\n\r\n- Use the `node` interpreter to run your Node.js applications. PM2 will automatically detect and use the node interpreter if you don’t specify one.\r\n- Use the `cluster mode` to run your Node.js applications. The `cluster` mode allows you to run multiple instances of your application, and enable load balancing and inter-process communication between them. The [cluster mode](https://www.bitdoze.com/pm2-fork-cluster/) can improve the performance and scalability of your Node.js applications, especially on multi-core machines. PM2 will automatically use the cluster mode if you don’t specify one.\r\n- Use the `max` value for the `instances` field to run as many instances of your application as the number of CPU cores on your system. This can optimize the resource utilization and throughput of your Node.js applications. PM2 will automatically use the max value if you don’t specify one.\r\n- Use the `--watch` option to enable the watch mode for your Node.js applications. The watch mode allows PM2 to monitor the changes in your code, and reload your applications automatically. The watch mode can help you with the development and testing of your Node.js applications, as you don’t have to restart them manually every time you make a change. You can enable the watch mode by adding the `--watch` option to your PM2 start command, or by setting the watch field to true in your JSON configuration file.\r\n\r\n#### Python Applications\r\n\r\nPython is a popular and versatile programming language that can be used for web development, data science, machine learning, and more. PM2 can also run and manage any Python application, such as web servers, microservices, cron jobs, chatbots, etc.\r\n\r\nHere are some of the best practices for managing Python applications with PM2:\r\n\r\n- Use the `python` interpreter to run your Python applications. PM2 can use any interpreter that you have installed on your system, such as python, python3, python2, etc. You need to specify the interpreter that you want to use.\r\n- Use the `fork` mode to run your Python applications. The [fork mode](https://www.bitdoze.com/pm2-fork-cluster/) allows you to run each instance of your application in a separate process, without any load balancing or inter-process communication. The `fork` mode is more suitable for Python applications, as Python has a global interpreter lock (GIL) that prevents multiple threads from executing Python code at the same time. You need to specify the fork mode in your JSON configuration file or command.\r\n- Use a fixed number for the `instances` field to run a specific number of instances of your application. You can choose the number of instances that best suits your needs, depending on the resources and workload of your Python applications.\r\n- Use the `--no-autorestart` option to disable the automatic restarts for your Python applications. PM2 will restart your applications automatically if they exit with a non-zero exit code, which can happen for various reasons, such as syntax errors, exceptions, signals, etc. However, this can also cause an infinite loop of restarts, if your Python applications have some persistent issues that prevent them from running properly. You can disable the automatic restarts by adding the `--no-autorestart` option to your PM2 start command, or by setting the autorestart field to false in your JSON configuration file.\r\n\r\n### How to Start Your Application with PM2\r\n\r\n#### You can use a JSON to start your application\r\n\r\nBelow is an example:\r\n\r\n```json\r\n{\r\n  \"apps\": [\r\n    {\r\n      \"name\": \"your-app-name\",\r\n      \"script\": \"your-app-script.py\",\r\n      \"interpreter\": \"python3\",\r\n      \"instances\": \"max\",\r\n      \"exec_mode\": \"cluster\",\r\n      \"env\": {\r\n        \"your-env-variable\": \"your-env-value\"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nHere is a brief explanation of the main fields in the JSON configuration file:\r\n\r\n- name: The name of your application. This will be used by PM2 to identify and display your application.\r\n- script: The path to the main script of your application. This is the entry point of your application that PM2 will execute.\r\n- interpreter: The interpreter to use to run your script. In this case, we are using python3, but you can also specify the full path to your Python executable if you have multiple versions installed.\r\n- instances: The number of instances of your application that you want to run. You can specify a fixed number, or use max to run as many instances as the number of CPU cores on your system.\r\n- exec_mode: The execution mode of your application. You can use fork to run each instance in a separate process, or cluster to enable load balancing and inter-process communication between your instances.\r\n- env: The [environment variables](https://www.bitdoze.com/pm2-env-vars/) that you want to pass to your application. You can specify any key-value pairs that you need for your application.\r\n\r\nYou can save this file as `your-app-config.json` in the same directory as your Python script, or in any other location that you prefer. You can also customize the file name as you wish, as long as it has a `.json` extension.\r\n\r\nMore complex one for streamlit would be:\r\n\r\n```json\r\n{\r\n  \"apps\": [\r\n    {\r\n      \"name\": \"streamlit-app\",\r\n      \"script\": \"streamlit\",\r\n      \"args\": [\"run\", \"app.py\"],\r\n      \"cwd\": \"/path/to/your/streamlit/app\",\r\n      \"interpreter\": \"python3\",\r\n      \"instances\": 1,\r\n      \"autorestart\": true,\r\n      \"watch\": false,\r\n      \"max_memory_restart\": \"1G\",\r\n      \"log_date_format\": \"YYYY-MM-DD HH:mm Z\",\r\n      \"error_file\": \"/path/to/your/log/pm2/streamlit-app-error.log\",\r\n      \"out_file\": \"/path/to/your/log/pm2/streamlit-app-out.log\",\r\n      \"merge_logs\": true\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThen to run it just do:\r\n\r\n```sh\r\npm2 start pm2-config.json\r\n```\r\n\r\n#### You can use command line:\r\n\r\nCommand line is easeier and you can start a command right away with:\r\n\r\n```sh\r\n#node\r\npm2 start your_app.js --name your-node-app\r\n#python\r\npm2 start your_app.py --name your-python-app --interpreter python3\r\n#python streamlit app\r\npm2 start 'streamlit run path/to/app/app.py' --name my-streamlit-app\r\n```\r\n\r\n```\r\nroot@streamlit:~/streamlit-app/streamlit-tools# pm2 start  'streamlit run  app.py' --name my-streamlit-app\r\n[PM2] Starting /usr/bin/bash in fork_mode (1 instance)\r\n[PM2] Done.\r\n┌────┬─────────────────────┬─────────────┬─────────┬─────────┬──────────┬────────┬──────┬───────────┬──────────┬──────────┬──────────┬──────────┐\r\n│ id │ name                │ namespace   │ version │ mode    │ pid      │ uptime │ ↺    │ status    │ cpu      │ mem      │ user     │ watching │\r\n├────┼─────────────────────┼─────────────┼─────────┼─────────┼──────────┼────────┼──────┼───────────┼──────────┼──────────┼──────────┼──────────┤\r\n│ 0  │ my-streamlit-app    │ default     │ N/A     │ fork    │ 9484     │ 0s     │ 0    │ online    │ 0%       │ 10.3mb   │ root     │ disabled │\r\n```\r\n\r\n### How to Check the Status and Logs of Your Application with PM2\r\n\r\nPM2 provides several commands and features that allow you to check the status and logs of your application easily and effectively. Here are some of the most useful ones:\r\n\r\n```sh\r\npm2 status your-app-name:\r\n```\r\n\r\nThis will display the status of your application, such as the name, the id, the mode, the pid, the uptime, the memory, and the cpu usage.\r\n\r\n```sh\r\npm2 logs your-app-name\r\n```\r\n\r\nThis will display the logs of your application, and show you any output or errors that your application generates. You can also use the --lines option to specify the number of lines to display, or the --err option to display only the error logs, or the --out option to display only the output logs.\r\n\r\n```sh\r\npm2 logrotate your-app-name\r\n```\r\n\r\nThis will rotate the logs of your application, and create a new log file every day. This can help you manage the size and number of your log files, and avoid filling up your disk space. You can also use the --rotate-interval option to specify the interval of log rotation, or the --max-size option to specify the maximum size of a log file, or the --retain option to specify the number of log files to keep.\r\n\r\n### How to Monitor the Performance of Your Application with PM2\r\n\r\nPM2 provides several commands and features that allow you to monitor the performance of your application easily and effectively. Here are some of the most useful ones:\r\n\r\n```sh\r\npm2 monit your-app-name\r\n```\r\n\r\nThis will open a dashboard that shows you the performance metrics of your application, such as the memory, the cpu, the event loop, and the network usage. You can also see the logs, the processes, and the custom metrics of your application on the same dashboard.\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"pm2 monit\"\r\n/>\r\n\r\n### How to Manage the State of Your Application with PM2\r\n\r\nPM2 provides several commands and features that allow you to manage the state of your application easily and effectively. Here are some of the most useful ones:\r\n\r\n```sh\r\npm2 stop your-app-name\r\n```\r\n\r\nThis will stop your application gracefully, and free up the resources that it was using. You can also use the --kill-timeout option to specify the timeout for the graceful stop, or the --force option to force the stop without waiting for the graceful stop.\r\n\r\n```sh\r\npm2 restart your-app-name\r\n```\r\n\r\nThis will restart your application, and reload any changes that you made to your code or configuration file. You can also use the --update-env option to update the environment variables of your application, or the --only option to restart only a specific instance of your application.\r\n\r\n```sh\r\npm2 reload your-app-name:\r\n```\r\n\r\nThis will reload your application gracefully, without losing any requests or connections. This is similar to the restart command, but more gentle and smooth. You can also use the --update-env option to update the environment variables of your application, or the --only option to reload only a specific instance of your application.\r\n\r\n```sh\r\npm2 delete your-app-name\r\n```\r\n\r\nThis will delete your application from PM2, and remove it from the list of applications that PM2 manages. You can also use the --force option to force the delete without stopping the application first.\r\n\r\n### How to Configure Automatic Restarts with PM2 for Your Application\r\n\r\nPM2 can help you keep your application alive, by restarting it automatically if it exits with a non-zero exit code, which can happen for various reasons, such as syntax errors, exceptions, signals, etc. PM2 can also restart your application automatically if it exceeds a certain memory limit, which can happen if your application has a memory leak or a high memory consumption.\r\n\r\nTo configure the automatic restarts with PM2 for your application, you can use the following fields in your JSON configuration file or in your command line:\r\n\r\n- `autorestart`: This field determines whether PM2 should restart your application automatically or not. The default value is true, which means that PM2 will restart your application automatically. You can set this field to false if you want to disable the automatic restarts for your application.\r\n- `max_memory_restart`: This field specifies the maximum memory limit for your application, in megabytes. If your application exceeds this limit, PM2 will restart it automatically. The default value is 0, which means that PM2 will not restart your application based on the memory limit. You can set this field to any positive number if you want to enable the memory-based restarts for your application.\r\n- `min_uptime`: This field specifies the minimum uptime for your application, in milliseconds. If your application exits before this time, PM2 will consider it as an abnormal exit, and restart it automatically. The default value is 1000, which means that PM2 will restart your application if it exits within one second. You can set this field to any positive number if you want to adjust the minimum uptime for your application.\r\n- `max_restarts`: This field specifies the maximum number of restarts for your application, within a certain time window. If your application reaches this limit, PM2 will stop trying to restart it, and mark it as errored. The default value is 15, which means that PM2 will try to restart your application up to 15 times, within 15 minutes. You can set this field to any positive number if you want to adjust the maximum restarts for your application.\r\n\r\nIf you want the app to be restarted automatically when the server is rebooted you should:\r\n\r\n```sh\r\npm2 startup\r\npm2 save\r\n```\r\n\r\nThe pm2 startup command is used to generate and configure a startup script for your operating system, ensuring that PM2 starts automatically when your system boots up. This command requires root (or administrator) privileges because it involves modifying system configurations.\r\n\r\nTo do that for a different user you can:\r\n\r\n```sh\r\nsudo env PATH=$PATH:/usr/bin pm2 startup systemd -u your_user --hp /home/your_user\r\n```\r\n\r\nReplace your_user with your actual username. Running this command will create a systemd service to start PM2 on boot.\r\n\r\nThe pm2 save command is used to save the current list of running processes managed by PM2. It generates a dump file containing information about the running applications and their configurations. This dump file is then used by PM2 to restore the processes on system reboot.\r\n\r\n## Conclusion\r\n\r\nIn conclusion, PM2 is a powerful tool for managing Python, nodejs or other applications . With its simple and user-friendly interface, it allows developers to easily deploy and monitor their applications in a production environment. By using PM2, you can ensure that your Python applications are running smoothly and efficiently.\r\n\r\nOne of the key benefits of using PM2 is its process management capabilities. It allows you to easily start, stop, restart, and reload your application processes with just a few simple commands. Additionally, PM2 provides built-in load balancing features that help distribute incoming traffic across multiple instances of your application, ensuring high availability and optimal performance.\r\n\r\nFurthermore, PM2 offers detailed monitoring functionality that enables you to keep track of important metrics such as CPU usage, memory consumption, and response times. This real-time monitoring allows you to quickly identify any issues or bottlenecks in your application's performance so that you can take appropriate measures to address them.\r\n\r\nOverall, by leveraging the power of PM2 for managing your Python applications, you can streamline your development workflow and enhance the stability and scalability of your projects. So why wait? Start using PM2 today and experience the benefits it brings to managing Python applications effortlessly!","src/content/posts/pm2-manage-apps.mdx",[1772],"../../assets/images/24/01/pm2-manage.jpeg","9dd7f26f8194ad8d","pm2-manage-apps.mdx","pm2-env-vars",{id:1775,data:1777,body:1786,filePath:1787,assetImports:1788,digest:1790,legacyId:1791,deferredRender:32},{title:1778,description:1779,date:1780,image:1781,authors:1782,categories:1783,tags:1784,canonical:1785},"Mastering Environment Variables in PM2: A Comprehensive Guide","Learn how to effectively pass and manage environment variables in PM2. This guide covers commands, configuration files, updating variables, best practices, and more.",["Date","2024-01-20T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/pm2-env-vars.jpeg",[19],[98],[1767],"https://www.bitdoze.com/pm2-env-vars/","PM2 is a powerful, feature-rich process manager for applications. It provides developers with a robust set of tools to keep applications online 24/7, manage log files, and automate common tasks. One of the key features of PM2 is its ability to manage environment variables, which are crucial for configuring applications and making them adaptable to different environments.\r\n\r\nEnvironment variables in PM2 can be set in various ways, including directly in the command line when starting a process, or within the PM2 configuration file, also known as the ecosystem file. These variables can be updated even while an application is running, providing flexibility and control over the application's environment.\r\nCheck [How to Manage Applications with PM2](https://www.bitdoze.com/pm2-manage-apps/) for a complete tutorial.\r\n\r\nIn the following sections, we will delve deeper into how to set, manage, and update environment variables in PM2, along with some best practices to follow.\r\n\r\n## What is the command to set an environment variable when starting a process in PM2?\r\n\r\nEnvironment variables are variables that are defined outside of your application code, and can be accessed by your application at runtime. Environment variables are useful for storing configuration settings, secrets, or other information that you don’t want to hardcode in your application code, or that you want to change depending on the environment (such as development, testing, or production).\r\n\r\nTo set an environment variable when starting a process in PM2, you can use the `--env` option, followed by the name of the environment variable and its value. For example, to set the environment variable `PORT` to `3000` when starting `app.js`, you can run the following command:\r\n\r\n```sh\r\n# Set the PORT environment variable to 3000 when starting app.js\r\npm2 start app.js --env PORT=3000\r\n```\r\n\r\nYou can also set multiple environment variables at once, by separating them with commas. For example, to set the environment variables `PORT`, `DB_URL`, and `SECRET` when starting `app.js`, you can run the following command:\r\n\r\n```sh\r\n# Set multiple environment variables when starting app.js\r\npm2 start app.js --env PORT=3000,DB_URL=mongodb://localhost:27017/mydb,SECRET=abc123\r\n```\r\n\r\nAlternatively, you can use the env option in your configuration file, and specify the environment variables as key-value pairs. For example, to set the same environment variables as above when starting `app.js`, you can use the following configuration file:\r\n\r\n```js\r\nmodule.exports = {\r\n  apps: [\r\n    {\r\n      script: \"app.js\",\r\n      env: {\r\n        PORT: 3000,\r\n        DB_URL: \"mongodb://localhost:27017/mydb\",\r\n        SECRET: \"abc123\",\r\n      },\r\n    },\r\n  ],\r\n};\r\n```\r\n\r\nThen, you can run the following command to start your application with the configuration file:\r\n\r\n```sh\r\n# Start app.js with the configuration file\r\npm2 start config.js\r\n```\r\n\r\nThe advantages of setting environment variables when starting a process in PM2 are:\r\n\r\n- It allows you to pass and access configuration settings, secrets, or other information to your application without hardcoding them in your code.\r\n- It allows you to change the environment variables depending on the environment (such as development, testing, or production), by using different configuration files or commands.\r\n- It allows you to keep your environment variables consistent across all your processes, by using the same configuration file or command.\r\n\r\n## How to update an environment variable for a running application in PM2?\r\n\r\nSometimes, you may need to update an environment variable for a running application in PM2, without stopping or restarting the application. For example, you may want to change the value of a configuration setting, a secret, or a feature flag.\r\n\r\nTo update an environment variable for a running application in PM2, you can use the `pm2 set` command, followed by the name of the application, the name of the environment variable, and the new value. For example, to update the environment variable `PORT` to `4000` for the application app.js, you can run the following command:\r\n\r\n```sh\r\n# Update the PORT environment variable to 4000 for app.js\r\npm2 set app.js:PORT 4000\r\n```\r\n\r\nThis command will update the environment variable in the PM2 process list, and send a SIGUSR2 signal to the application, which will trigger a reload of the environment variables. However, this command will not update the environment variable in the configuration file, so you need to manually edit the file if you want to persist the change.\r\n\r\nThe advantages of updating an environment variable for a running application in PM2 are:\r\n\r\n- It allows you to change the environment variables without stopping or restarting the application, which can improve the availability and responsiveness of your application.\r\n- It allows you to test different values of the environment variables, and see the effects on your application in real time.\r\n- It allows you to override the environment variables that are set in the configuration file, or the system, for a specific application.\r\n\r\nThe disadvantages of updating an environment variable for a running application in PM2 are:\r\n\r\n- It does not update the environment variable in the configuration file, so you need to manually edit the file if you want to persist the change.\r\n- It does not update the environment variable for all the processes of the application, if you are using [cluster mode](https://www.bitdoze.com/pm2-fork-cluster/). You need to specify the process ID or the process name with the index to update the environment variable for a specific process. For example, to update the environment variable PORT to 4000 for the first process of app.js, you can run the following command:\r\n\r\n```sh\r\n# Update the PORT environment variable to 4000 for the first process of app.js\r\npm2 set app.js-0:PORT 4000\r\n```\r\n\r\nIt does not work for some environment variables that are read only once at the start of the application, such as NODE_ENV or DEBUG. You need to restart the application to apply the change for these environment variables.\r\n\r\n## How to List All Environment Variables Set in PM2\r\n\r\nTo list all the environment variables for a specific process in PM2, you can use the pm2 env `process_name_or_id` command. Replace `process_name_or_id` with the name or ID of the process you're interested in. This command will display all the active environment variables for the specified process.\r\nPlease note that this command only shows the environment variables for the specified process. It does not show the environment variables defined in the ecosystem file that are not currently active for the process. To see these variables, you would need to look at the ecosystem file directly.\r\n\r\n## Best Practices for Managing Environment Variables in PM2?\r\n\r\nEnvironment variables are an essential part of any Node.js application, as they allow you to store and access configuration settings, secrets, or other information that you don’t want to hardcode in your application code, or that you want to change depending on the environment. However, managing environment variables in PM2 can be tricky, as you need to consider the following aspects:\r\n\r\n- Security: You need to ensure that your environment variables are not exposed to unauthorized parties, such as hackers, competitors, or malicious users. You should avoid storing your environment variables in plain text files, such as .env or .json, or in your version control system, such as Git or SVN. You should also use encryption or hashing techniques to protect your sensitive environment variables, such as passwords, tokens, or keys.\r\n- Consistency: You need to ensure that your environment variables are consistent across all your processes, environments, and servers. You should use the same names, values, and formats for your environment variables, and avoid using conflicting or ambiguous names. You should also use the same configuration files or commands to set your environment variables, and avoid using different methods for different applications or environments.\r\n- Maintenance: You need to ensure that your environment variables are up to date and easy to maintain. You should document your environment variables and their meanings, and keep them organized and structured. You should also review your environment variables regularly, and remove any unused or obsolete ones. You should also use the PM2 commands or features to update or reload your environment variables, and avoid manually editing or restarting your processes.\r\n\r\nHere are some best practices for managing environment variables in PM2, based on the aspects mentioned above:\r\n\r\n- Use the configuration file to set and manage your environment variables, as it allows you to specify the environment variables for each application, or for all applications at once, in one place. You can also use different configuration files for different environments, such as development, testing, or production, by using the `--env` option to specify the name of the file.\r\n- Use the `env` option in the top level of the configuration file to set the environment variables for all applications, and use the env option in each application to override or add the environment variables for a specific application. This will help you keep your environment variables consistent and organized.\r\n- Use the `process.env` object to access the environment variables in your application code, and avoid using hard-coded values or global variables. This will help you keep your application code secure and flexible.\r\n- Use the `pm2 set` command to update an environment variable for a running application, without stopping or restarting the application. This will help you change the environment variables dynamically and efficiently.\r\n- Use the `pm2 reload` command to reload your application with the updated environment variables, without losing any requests or causing any downtime. This will help you apply the changes to your environment variables gracefully and reliably.\r\n- Use the `pm2 env` command to list all the environment variables set in PM2, and their values. This will help you check and verify your environment variables easily and quickly.\r\n\r\n## Conclusions\r\n\r\nEnvironment variables are an essential part of any application, as they allow you to store and access configuration settings, secrets, or other information that you don’t want to hardcode in your application code, or that you want to change depending on the environment. However, managing environment variables in PM2 can be tricky, as you need to consider the security, consistency, and maintenance aspects of your environment variables.","src/content/posts/pm2-env-vars.mdx",[1789],"../../assets/images/24/01/pm2-env-vars.jpeg","7b25a2eef0b07964","pm2-env-vars.mdx","pm2-fork-cluster",{id:1792,data:1794,body:1803,filePath:1804,assetImports:1805,digest:1807,legacyId:1808,deferredRender:32},{title:1795,description:1796,date:1797,image:1798,authors:1799,categories:1800,tags:1801,canonical:1802},"How to Choose Between Fork and Cluster Mode in PM2","Learn what fork and cluster mode are, how they differ, and when to use them for your projects with PM2, a popular process manager for applications.\"",["Date","2024-01-21T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/01/pm2-fork-vs-cluster.jpeg",[19],[98],[1767],"https://www.bitdoze.com/pm2-fork-cluster/","[PM2](https://pm2.keymetrics.io/) is a popular process manager for Node.js applications that allows you to run, monitor, and scale your applications with ease. PM2 is designed to make your Node.js development and deployment more efficient and reliable, by providing features such as:\r\n\r\n- **Process management:** You can start, stop, restart, delete, or list your Node.js processes with simple commands or a web interface.\r\n- **Load balancing:** You can distribute the load of your applications across multiple CPU cores, by using cluster mode or process scaling.\r\n- **Monitoring:** You can monitor the performance and status of your applications, such as CPU usage, memory usage, requests per second, errors, logs, and more.\r\n- **Logging:** You can manage the logs of your applications, such as rotating, streaming, or flushing them.\r\n- **Watch and reload:** You can enable watch mode to automatically reload your applications when the source code changes.\r\n- **Deployment:** You can deploy your applications to remote servers with a single command, by using the PM2 deploy feature.\r\n\r\nCheck [How to Manage Applications with PM2](https://www.bitdoze.com/pm2-manage-apps/) for a complete tutorial.\r\n\r\n## What is Fork Mode?\r\n\r\nFork mode is the default mode of PM2, where it spawns a single process for each application. This mode is suitable for applications that use different versions of Node.js, or applications that are not networked, such as scripts, workers, or cron jobs.\r\n\r\nFork mode also supports some advanced features of PM2, such as:\r\n\r\n- Cron restarts: You can specify a cron pattern to restart your application at a specific time or interval.\r\n- Source map support: You can enable source map support to get the original source code location of errors in your application.\r\n- Custom log formats: You can customize the format of the logs generated by your application, such as adding timestamps, colors, or prefixes.\r\n\r\nTo use fork mode, you can simply run your application with PM2 without any additional options, or specify the exec_mode as fork in your configuration file. For example:\r\n\r\n```sh\r\n# Run app.js in fork mode\r\npm2 start app.js\r\n\r\n# Or via a configuration file\r\nmodule.exports = {\r\n  apps : [\r\n    {\r\n      script : \"app.js\",\r\n      exec_mode : \"fork\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe advantages of fork mode are:\r\n\r\n- It allows you to run applications with different versions of Node.js, by using the node_args option to specify the path to the Node.js binary.\r\n- It allows you to run applications that are not networked, such as scripts, workers, or cron jobs, by using the type option to specify the type of the application.\r\n- It offers more flexibility and control over your applications, by allowing you to use custom [environment variables](https://www.bitdoze.com/pm2-env-vars/), arguments, log formats, etc.\r\n\r\nThe disadvantages of fork mode are:\r\n\r\n- It does not take advantage of the multi-core capabilities of your CPU, by creating only one process per application.\r\n- It does not support some features of PM2 that are available in cluster mode, such as graceful reload, zero-downtime restart, and automatic port sharing.\r\n- It requires more configuration and management, by requiring you to specify the options for each application individually.\r\n\r\n## What is Cluster Mode?\r\n\r\nCluster mode is a more advanced mode of PM2, where it creates multiple processes for each application, and distributes the incoming requests among them. This mode takes advantage of the multi-core capabilities of your CPU, and improves the performance and reliability of your applications.\r\n\r\nCluster mode is ideal for networked applications, such as HTTP, TCP, or UDP servers, that can handle concurrent requests from multiple clients. Cluster mode also supports some features of PM2 that are not available in fork mode, such as:\r\n\r\n- Graceful reload: You can reload your application without losing any requests, by waiting for the existing connections to close before restarting the processes.\r\n- Zero-downtime restart: You can restart your application without any downtime, by spawning new processes before killing the old ones.\r\n- Automatic port sharing: You can run multiple processes on the same port, by using the cluster module of Node.js to enable load balancing and port sharing.\r\n\r\nTo use cluster mode, you need to pass the -i option to PM2, and specify the number of processes you want to create. You can also use max to create as many processes as the number of CPU cores, or -1 to create one less than the number of CPU cores. Alternatively, you can specify the exec_mode as cluster in your configuration file. For example:\r\n\r\n```sh\r\n# Run app.js in cluster mode with 4 processes\r\npm2 start app.js -i 4\r\n\r\n# Or use max to auto-detect the number of CPU cores\r\npm2 start app.js -i max\r\n\r\n# Or via a configuration file\r\nmodule.exports = {\r\n  apps : [\r\n    {\r\n      script : \"app.js\",\r\n      instances : \"max\",\r\n      exec_mode : \"cluster\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe advantages of cluster mode are:\r\n\r\n- It improves the performance and reliability of your networked applications, by creating multiple processes that can handle the load and recover from failures.\r\n- It supports some features of PM2 that are not available in fork mode, such as graceful reload, zero-downtime restart, and automatic port sharing.\r\n- It offers better scalability and fault tolerance, by allowing you to add or remove processes dynamically, or use the PM2 reload feature to update your code without stopping your application.\r\n\r\nThe disadvantages of cluster mode are:\r\n\r\n- It consumes more memory and CPU resources, by creating multiple processes for each application.\r\n- It requires the same version of Node.js and networked applications, by using the cluster module of Node.js that reuses the same Node.js instance for each process.\r\n- It does not support some advanced features of PM2 that are available in fork mode, such as cron restarts, source map support, and custom log formats.\r\n\r\n## How to Choose Between Fork and Cluster Mode?\r\n\r\nThe choice between fork and cluster mode depends on several factors, such as:\r\n\r\n- The type of your application: If your application is networked and can handle concurrent requests, cluster mode is a better option. If your application is not networked or uses different versions of Node.js, fork mode is more suitable.\r\n- The features you need: If you need features such as cron restarts, source map support, or custom log formats, fork mode is the only option. If you need features such as graceful reload, zero-downtime restart, or automatic port sharing, cluster mode is the preferred option.\r\n- The trade-offs you are willing to make: Cluster mode offers better scalability and fault tolerance, but it also consumes more memory and CPU resources. Fork mode offers more flexibility and control, but it also requires more configuration and management.\r\n\r\nTo help you decide which mode to use, you can ask yourself the following questions:\r\n\r\n- Do I need to run applications with different versions of Node.js, or applications that are not networked?\r\n- Do I need features such as cron restarts, source map support, or custom log formats?\r\n- Do I have enough memory and CPU resources to run multiple processes for each application?\r\n- Do I need features such as graceful reload, zero-downtime restart, or automatic port sharing?\r\n- Do I want to improve the performance and reliability of my networked applications?\r\n\r\nIf you answered yes to the first two questions, fork mode is the best option for you. If you answered yes to the last three questions, cluster mode is the best option for you. If you answered yes to some of the questions from both groups, you need to weigh the pros and cons of each mode, and choose the one that best suits your needs and preferences.\r\n\r\n## What are the best practices for using cluster mode in PM2?\r\n\r\nCluster mode is a powerful feature of PM2 that can boost the performance and reliability of your networked applications. However, to use cluster mode effectively, you need to follow some best practices, such as:\r\n\r\n- Use the same version of Node.js for all your applications in cluster mode, by using the `node_args option` to specify the path to the Node.js binary. This will ensure that the cluster module of Node.js works properly and consistently across all your processes.\r\n- Use the `max_memory_restart` option to specify the maximum memory limit for each process in cluster mode. This will prevent memory leaks and optimize the memory usage of your applications.\r\n- Use the `wait_ready` option to enable the graceful reload feature in cluster mode. This will allow your processes to send a ready signal to PM2 when they are ready to receive requests, and wait for the existing connections to close before restarting. This will ensure that no requests are lost or interrupted during the reload process.\r\n- Use the `listen_timeout` and kill_timeout options to adjust the timeout values for the graceful reload feature in cluster mode. The listen_timeout option specifies the maximum time to wait for the ready signal from the processes, and the kill_timeout option specifies the maximum time to wait for the connections to close before killing the processes. These options will help you avoid errors or delays during the reload process.\r\n- Use the `pm2 reload` command to update your code without stopping your application in cluster mode. This command will spawn new processes with the updated code, and replace the old processes gradually. This will ensure that your application is always running and serving requests, even during the update process.\r\n\r\nHere is an example of a configuration file that uses cluster mode with some of the best practices mentioned above:\r\n\r\n```sh\r\nmodule.exports = {\r\n  apps : [\r\n    {\r\n      script : \"app.js\",\r\n      instances : \"max\",\r\n      exec_mode : \"cluster\",\r\n      node_args : \"/usr/local/bin/node\",\r\n      max_memory_restart : \"300M\",\r\n      wait_ready : true,\r\n      listen_timeout : 5000,\r\n      kill_timeout : 5000\r\n    }\r\n  ]\r\n}\r\n\r\n```\r\n\r\n##Conclusion\r\n\r\nPM2 is a powerful tool for running, monitoring, and scaling your applications. It offers two modes of operation: fork and cluster, that have different advantages and disadvantages. You should choose the mode that best suits your application type, features, and trade-offs. You can also switch between the modes easily by changing the options or the configuration file of PM2. We hope this article helped you understand the difference between fork and cluster mode in PM2, and how to choose the best one for your projects.","src/content/posts/pm2-fork-cluster.mdx",[1806],"../../assets/images/24/01/pm2-fork-vs-cluster.jpeg","3dc5f613ea392aae","pm2-fork-cluster.mdx","postfix-external-smtp",{id:1809,data:1811,body:1820,filePath:1821,assetImports:1822,digest:1824,legacyId:1825,deferredRender:32},{title:1812,description:1813,date:1814,image:1815,authors:1816,categories:1817,tags:1818,canonical:1819},"Configure Postfix to Send Email Using External SMTP Servers","Learn how you can configure Postfix to send emails with an external SMTP provider.",["Date","2024-09-18T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/09/postfix-external-smtp.jpeg",[19],[98],[135],"https://www.bitdoze.com/postfix-external-smtp/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\n\r\n\r\nPostfix is a powerful and versatile Mail Transfer Agent (MTA) used in many Unix-like operating systems. It serves several key purposes in email infrastructure:\r\n\r\n1. **Email Routing:** Postfix efficiently routes emails between servers, ensuring messages reach their intended recipients.\r\n\r\n2. **Email Delivery:** It handles the delivery of outgoing emails to their destination mail servers.\r\n\r\n3. **SMTP Server:** Postfix acts as an SMTP server, allowing applications and users to send emails through it.\r\n\r\n4. **Mail Queuing:** It manages email queues, storing messages temporarily if immediate delivery isn't possible.\r\n\r\n5. **Security:** Postfix includes built-in security features to protect against spam, email spoofing, and unauthorized relaying.\r\n\r\n6. **Flexibility:** It can be configured to work with various email protocols and can integrate with other email-related services.\r\n\r\n7. **Local Mail Delivery:** In addition to routing, Postfix can deliver mail to local mailboxes on the server.\r\n\r\n8. **Policy Enforcement:** Administrators can set up rules and policies for email handling, such as size limits or content filtering.\r\n\r\n## Why use Postfix with external SMTP service\r\n\r\nWhile Postfix is capable of handling email delivery on its own, there are several compelling reasons to use it in conjunction with an external SMTP service:\r\n\r\n1. **Improved Deliverability:** External SMTP providers often have better IP reputations and relationships with major email providers, increasing the chances of your emails reaching the inbox.\r\n\r\n2. **Scalability:** SMTP services are designed to handle large volumes of email, allowing your system to scale without significant infrastructure changes.\r\n\r\n3. **Reduced Server Load:** Offloading email sending to an external service frees up your server resources for other tasks.\r\n\r\n4. **Simplified Maintenance:** You don't need to manage the complexities of maintaining your own mail server, such as handling IP blacklists or configuring SPF, DKIM, and DMARC.\r\n\r\n5. **Advanced Features:** Many SMTP services offer additional features like email tracking, analytics, and automated bounce handling.\r\n\r\n6. **Better Compliance:** External providers often help ensure compliance with email regulations like GDPR, CAN-SPAM, or CASL.\r\n\r\n7. **Cost-Effective:** For many organizations, using an external SMTP service can be more cost-effective than maintaining their own email infrastructure.\r\n\r\n8. **Reliability:** Professional SMTP services typically offer high uptime and redundancy, ensuring your emails get sent even if your primary server goes down.\r\n\r\n9. **Flexibility:** You can easily switch between different SMTP providers or use multiple providers for different purposes without major reconfiguration.\r\n\r\n10. **Focus on Core Business:** By outsourcing email delivery, your team can focus on core business activities rather than email server management.\r\n\r\nBy combining Postfix with an external SMTP service, you get the best of both worlds: the flexibility and control of Postfix for local email handling, and the reliability and advanced features of a professional email delivery service for outgoing mail.\r\n\r\n\r\nCertainly. Here's a section on SMTP services that can be used with Postfix:\r\n\r\n## SMTP services that can be used\r\n\r\nWhen configuring Postfix to use an external SMTP service, there are several reliable options available. Here are some popular SMTP services that work well with Postfix:\r\n\r\n### Brevo free SMTP\r\n\r\n[Brevo](https://www.brevo.com/) (formerly known as Sendinblue) offers a robust SMTP relay service with a generous free tier:\r\n\r\n- Free plan includes 300 emails per day (9,000 per month)\r\n- Easy integration with Postfix\r\n- Advanced features like email tracking and templating\r\n- Good deliverability rates\r\n- API access for additional functionality\r\n- Paid plans available for higher volume needs\r\n\r\n### Zeptomail from Zoho\r\n\r\n[Zeptomail](https://www.zoho.com/zeptomail/), part of the Zoho ecosystem, provides a reliable SMTP service with the following features:\r\n\r\n- Free tier offering 10,000 emails\r\n- Known for excellent deliverability rates\r\n- Email validation and domain authentication included\r\n- Detailed analytics and reporting\r\n- API access for developers\r\n- Seamless integration with other Zoho products\r\n- Scalable paid plans for growing businesses\r\n\r\n### mail.baby\r\n\r\n[mail.baby](https://www.mail.baby/) is a cost-effective SMTP relay service that focuses on simplicity:\r\n\r\n- Affordable plans starting from very low monthly fees\r\n- No free tier, but pricing is budget-friendly\r\n- Unlimited domains on all plans\r\n- Simple setup process\r\n- Good for businesses looking for a straightforward, no-frills solution\r\n- 24/7 support included\r\n\r\nYou can check [mail.baby review](https://www.bitdoze.com/mail-baby-review/) for more details.\r\n\r\n### Amazon SES (Simple Email Service)\r\n\r\nWhile not mentioned in your original list, Amazon SES is worth considering:\r\n\r\n- Very cost-effective, especially for high volume\r\n- 62,000 free emails per month when sent from Amazon EC2\r\n- High deliverability and scalability\r\n- Detailed sending statistics\r\n- Integration with other AWS services\r\n- Pay-as-you-go pricing model\r\n\r\n### Mailgun\r\n\r\n[Mailgu](https://www.mailgun.com/) is another popular option for developers and businesses:\r\n\r\n- Free tier includes 5,000 emails for 3 months\r\n- Powerful API for developers\r\n- Advanced email validation\r\n- Excellent documentation and support\r\n- Flexible pricing based on volume\r\n\r\n### SendGrid\r\n\r\n[SendGrid](https://sendgrid.com/en-us/free) iswned by Twilio, SendGrid is a widely used SMTP service:\r\n\r\n- Free plan includes 100 emails per day\r\n- Known for high deliverability rates\r\n- Comprehensive APIs and webhooks\r\n- Advanced analytics and reporting\r\n- Email validation and testing tools\r\n\r\nWhen choosing an SMTP service to use with Postfix, consider factors such as your expected email volume, budget, required features, and ease of integration. Many of these services offer free tiers or trial periods, allowing you to test them with your Postfix setup before committing to a paid plan.\r\n\r\n\r\n## Configure Postfix to Send Email Using External SMTP Servers\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/lEPuB0Eh8Sg\"\r\n  label=\"Postfix 1 Click Setup\"\r\n/>\r\n\r\nConfiguring Postfix to use an external SMTP server involves modifying several configuration files and settings. Here's a step-by-step guide to manually configure Postfix:\r\n\r\n### Manual Configs based on\r\n\r\nTo manually configure Postfix to use an external SMTP server, you'll need to modify the main Postfix configuration file and create a few additional files. Here's how to do it:\r\n\r\n1. Edit the main Postfix configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/postfix/main.cf\r\n   ```\r\n\r\n   Add or modify the following lines:\r\n\r\n   ```sh\r\n   relayhost = [smtp.provider.com]:587\r\n   smtp_sasl_auth_enable = yes\r\n   smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd\r\n   smtp_sasl_security_options = noanonymous\r\n   smtp_tls_security_level = may\r\n   header_size_limit = 4096000\r\n   ```\r\n\r\n   Replace `[smtp.provider.com]:587` with your SMTP provider's server and port.\r\n\r\n2. Create the SASL password file:\r\n\r\n   ```sh\r\n   sudo nano /etc/postfix/sasl_passwd\r\n   ```\r\n\r\n   Add the following line:\r\n\r\n   ```sh\r\n   [smtp.provider.com]:587 username:password\r\n   ```\r\n\r\n   Replace `smtp.provider.com:587`, `username`, and `password` with your SMTP provider's details.\r\n\r\n3. Create the hash database file from the sasl_passwd file:\r\n\r\n   ```sh\r\n   sudo postmap /etc/postfix/sasl_passwd\r\n   ```\r\n\r\n4. Secure the sasl_passwd files:\r\n\r\n   ```sh\r\n   sudo chown root:root /etc/postfix/sasl_passwd /etc/postfix/sasl_passwd.db\r\n   sudo chmod 0600 /etc/postfix/sasl_passwd /etc/postfix/sasl_passwd.db\r\n   ```\r\n\r\n5. If you want to set a specific sender address for all outgoing emails:\r\n\r\n   Create and edit the sender_canonical file:\r\n\r\n   ```sh\r\n   sudo nano /etc/postfix/sender_canonical\r\n   ```\r\n\r\n   Add the following line:\r\n\r\n   ```sh\r\n   /.+/    your_email@example.com\r\n   ```\r\n\r\n   Create the smtp_header_checks file:\r\n\r\n   ```sh\r\n   sudo nano /etc/postfix/smtp_header_checks\r\n   ```\r\n\r\n   Add the following line:\r\n\r\n   ```sh\r\n   /From:.*/ REPLACE From: your_email@example.com\r\n   ```\r\n\r\n   Update the main.cf file to use these new files:\r\n\r\n   ```sh\r\n   sudo postconf -e 'sender_canonical_classes = envelope_sender, header_sender'\r\n   sudo postconf -e 'sender_canonical_maps = regexp:/etc/postfix/sender_canonical'\r\n   sudo postconf -e 'smtp_header_checks = regexp:/etc/postfix/smtp_header_checks'\r\n   ```\r\n\r\n6. Set the correct domain in /etc/mailname:\r\n\r\n   ```sh\r\n   sudo echo \"yourdomain.com\" > /etc/mailname\r\n   ```\r\n\r\n   Replace \"yourdomain.com\" with your actual domain.\r\n\r\n7. Restart Postfix to apply all changes:\r\n\r\n   ```sh\r\n   sudo systemctl restart postfix\r\n   ```\r\n\r\nThese manual configurations allow you to set up Postfix with an external SMTP server. However, the process can be error-prone and time-consuming, especially for those not familiar with Postfix configuration. That's why using an automated script, like the one provided earlier, can be beneficial. It simplifies the process, reduces the chance of errors, and saves time.\r\n\r\n\r\n### Using The Postfix Config script:\r\n\r\nUsing the provided script is a much simpler and less error-prone method to configure Postfix with an external SMTP server. I have created a script that will work on Debian OS like Ubuntu and other scripts on [utils.bitdoze.com](https://utils.bitdoze.com/) you just need to download and run it:\r\n\r\nHere's how to use it:\r\n\r\n1. Download the script:\r\n\r\n   ```sh\r\n   curl -sSL https://utils.bitdoze.com/scripts/postfix-setup.sh -o postfix-setup.sh\r\n   ```\r\n\r\n   This command downloads the script from the provided URL and saves it as `postfix-setup.sh` in your current directory.\r\n\r\n2. Make the script executable (optional, but recommended):\r\n\r\n   ```sh\r\n   chmod +x postfix-setup.sh\r\n   ```\r\n\r\n3. Run the script:\r\n\r\n   ```sh\r\n   bash postfix-setup.sh\r\n   ```\r\n\r\n   Or, if you made it executable:\r\n\r\n   ```sh\r\n   ./postfix-setup.sh\r\n   ```\r\n\r\n\r\n\r\n4. Follow the prompts:\r\n   The script will ask you for various pieces of information. Be prepared to provide:\r\n   - Your SMTP username\r\n   - Your SMTP password\r\n   - The domain you're using with your SMTP provider\r\n   - A sender email address (optional)\r\n   - Your Postfix hostname\r\n   - Your SMTP server address and port\r\n\r\n   Output:\r\n\r\n```sh\r\nroot@cloud:/var/log# bash postfix-setup.sh\r\n[2024-09-18 06:04:42] Step 1: Make sure you have already set up your domain with your SMTP provider and added any necessary DNS records (like SPF, DKIM, and CNAME).\r\nEnter your SMTP Username: bitdoze1@gmail.com\r\nEnter your SMTP Password:\r\nEnter the domain you are using with your SMTP provider (e.g. example.com): bitdoze.ro\r\nEnter the sender email address (optional, press Enter to skip):\r\nEnter your Postfix hostname (e.g. yourdomain.com): bitdoze.ro\r\nEnter your SMTP server with port (e.g. [smtp.provider.com]:587): [smtp-relay.brevo.com]:587\r\n[2024-09-18 06:05:20] Step 2: Updating system and installing Postfix...\r\nHit:1 https://download.docker.com/linux/ubuntu jammy InRelease\r\nHit:2 https://mirror.hetzner.com/ubuntu/packages noble InRelease\r\nHit:3 https://mirror.hetzner.com/ubuntu/packages noble-updates InRelease\r\nHit:4 https://mirror.hetzner.com/ubuntu/packages noble-backports InRelease\r\nHit:5 https://mirror.hetzner.com/ubuntu/security noble-security InRelease\r\nReading package lists... Done\r\nReading package lists... Done\r\nBuilding dependency tree... Done\r\nReading state information... Done\r\nmailutils is already the newest version (1:3.17-1.1build3).\r\n0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\r\n[2024-09-18 06:05:23] Step 3: Configuring Postfix...\r\n[2024-09-18 06:05:23] Backed up /etc/postfix/main.cf to /etc/postfix/main.cf.bak\r\n[2024-09-18 06:05:23] Step 4: Creating /etc/postfix/sasl_passwd file with SMTP credentials...\r\n[2024-09-18 06:05:23] Backed up /etc/postfix/sasl_passwd to /etc/postfix/sasl_passwd.bak\r\n[2024-09-18 06:05:23] Securing /etc/postfix/sasl_passwd and creating hash...\r\n[2024-09-18 06:05:25] Step 5: Configuring sender address settings...\r\n[2024-09-18 06:05:25] Resetting sender address configuration...\r\n[2024-09-18 06:05:25] Step 6: Configuring /etc/mailname...\r\n[2024-09-18 06:05:25] Backed up /etc/mailname to /etc/mailname.bak\r\n[2024-09-18 06:05:25] Step 7: Restarting Postfix...\r\npostfix/postfix-script: refreshing the Postfix mail system\r\n[2024-09-18 06:05:27] All done! Postfix has been configured with your SMTP settings.\r\n```\r\n\r\n5. Review the output:\r\n   The script will show its progress and any actions it takes. It's a good idea to review this output to ensure everything was configured correctly.\r\n\r\n6. Test the configuration:\r\n   After the script completes, it's recommended to test your Postfix configuration by sending a test email.\r\n\r\nUsing this script offers several advantages:\r\n\r\n- Simplicity: You don't need to manually edit configuration files or run multiple commands.\r\n- Consistency: The script ensures that all necessary configurations are made in the correct way.\r\n- Time-saving: What could take 30 minutes or more to do manually can be done in just a few minutes with the script.\r\n- Error reduction: The script reduces the chance of typographical errors or missed steps that can occur with manual configuration.\r\n\r\nRemember to run this script with root privileges or using sudo, as it needs to modify system configuration files. Also, always review scripts from the internet before running them to ensure they're safe and do what you expect.\r\n\r\n\r\n### Explain what the script is doing\r\n\r\nThe provided script automates the process of configuring Postfix to use an external SMTP server. Let's break down its main functions:\r\n\r\n1. Logging and Error Handling:\r\n   - The script defines functions for logging messages and checking if commands succeed.\r\n   - This ensures that each step is logged and any errors are caught and reported.\r\n\r\n2. File Backup:\r\n   - Before modifying any configuration files, the script creates backups.\r\n   - This allows for easy rollback if something goes wrong.\r\n\r\n3. User Input:\r\n   - The script prompts the user for necessary information such as SMTP credentials, domain, and server details.\r\n   - It includes input validation to ensure that critical fields are not left empty.\r\n\r\n4. System Update and Package Installation:\r\n   - Updates the system's package list.\r\n   - Installs the `mailutils` package if it's not already present.\r\n\r\n5. Postfix Configuration:\r\n   - Modifies the main Postfix configuration file (`/etc/postfix/main.cf`) with the provided SMTP settings.\r\n   - Sets up SASL authentication for the SMTP server.\r\n   - Configures TLS settings for secure communication.\r\n\r\n6. SMTP Credential Management:\r\n   - Creates a `sasl_passwd` file with the SMTP server credentials.\r\n   - Secures this file by setting appropriate permissions.\r\n   - Generates a hash database from the `sasl_passwd` file for Postfix to use.\r\n\r\n7. Sender Address Configuration (Optional):\r\n   - If a sender email is provided, it sets up sender canonical mapping and header checks.\r\n   - This ensures all outgoing emails use the specified sender address.\r\n\r\n8. Hostname Configuration:\r\n   - Updates the `/etc/mailname` file with the provided Postfix hostname.\r\n\r\n9. Service Restart:\r\n   - Reloads the Postfix service to apply all the new configurations.\r\n\r\n10. Logging Completion:\r\n    - Logs the successful completion of the setup process.\r\n\r\nThroughout the script, there are checks to ensure each step completes successfully. If any step fails, the script will log an error message and exit, preventing partial or incorrect configurations.\r\n\r\nThis script essentially automates what would otherwise be a manual process involving multiple command-line operations and file edits. It simplifies the setup process, reduces the chance of human error, and ensures a consistent configuration across different systems. By prompting the user for necessary information and handling the technical details internally, it makes the process of configuring Postfix with an external SMTP server accessible even to users who might not be familiar with the intricacies of Postfix configuration.\r\n\r\n\r\n### Testing email\r\n\r\nAfter configuring Postfix to use an external SMTP server, it's crucial to test the setup to ensure emails are being sent correctly. Here are some methods to test your email configuration:\r\n\r\n#### Send email\r\n\r\nYou can use the command line to send test emails. Here are two examples:\r\n\r\n1. Basic email send:\r\n\r\n```sh\r\necho \"This is a test message\" | mail -s \"Test Subject\" recipient@example.com\r\n```\r\n\r\nThis command sends an email with the subject \"Test Subject\" and the body \"This is a test message\" to recipient@example.com.\r\n\r\n2. Email send with custom sender:\r\n\r\n```sh\r\necho \"This is another test message\" | mail -s \"Test Subject 2\" -r sender@yourdomain.com recipient@example.com\r\n```\r\n\r\nThis command does the same as the previous one, but also specifies a custom sender address (sender@yourdomain.com) using the `-r` flag.\r\n\r\nReplace recipient@example.com with an email address you can access to verify receipt of the test emails.\r\n\r\n#### Check the mail log\r\n\r\nAfter sending test emails, it's important to check the mail log for any errors or issues. You can do this using the following command:\r\n\r\n```sh\r\ntail -100 /var/log/mail.log\r\n```\r\n\r\nThis command displays the last 100 lines of the mail log file. Look for entries related to your test emails. Successful sending usually looks something like this:\r\n\r\n```sh\r\nMay 10 15:30:45 hostname postfix/smtp[12345]: 1AB2C3D4E5F: to=<recipient@example.com>, relay=smtp.yourprovider.com[xxx.xxx.xxx.xxx]:587, delay=0.8, delays=0.02/0.00/0.5/0.28, dsn=2.0.0, status=sent (250 2.0.0 OK 1234567890abcdef)\r\n```\r\n\r\nIf there are any issues, you'll see error messages in the log. Common issues include authentication failures, connection problems, or relay access denied.\r\n\r\n#### Additional Testing Tips:\r\n\r\n1. Send emails to different domains: Test sending to Gmail, Outlook, Yahoo, etc., to ensure your emails are not being marked as spam by major providers.\r\n\r\n2. Check spam folders: Sometimes, even when emails are sent successfully, they might end up in the recipient's spam folder. Always check there if you don't see the test emails in the inbox.\r\n\r\n3. Use online SMTP tests: Websites like https://www.mail-tester.com/ can provide detailed reports on your email deliverability.\r\n\r\n4. Monitor your IP reputation: Use services like MXToolbox to check if your IP is on any blacklists.\r\n\r\n5. Verify SPF, DKIM, and DMARC: Ensure these email authentication methods are correctly set up for your domain to improve deliverability.\r\n\r\nBy thoroughly testing your email configuration, you can catch and resolve any issues early, ensuring smooth email delivery for your applications or services using Postfix.\r\n\r\n\r\n## Conclusions\r\nIn conclusion, configuring Postfix to use an external SMTP server is a smart choice for businesses and individuals looking to optimize their email infrastructure. It offers a balance between control and convenience, allowing you to leverage the strengths of both Postfix and professional SMTP services. By following the steps outlined in this guide and regularly maintaining your setup, you can ensure reliable, efficient, and secure email delivery for your organization","src/content/posts/postfix-external-smtp.mdx",[1823],"../../assets/images/24/09/postfix-external-smtp.jpeg","b2683a45f510aaa1","postfix-external-smtp.mdx","podman-vs-docker",{id:1826,data:1828,body:1838,filePath:1839,assetImports:1840,digest:1842,legacyId:1843,deferredRender:32},{title:1829,description:1830,date:1831,image:1832,authors:1833,categories:1834,tags:1835,canonical:1837},"Podman Vs Docker - Which One to Choose?","Podman Vs Docker - Learn everything that you need to decide the best container engine for you.",["Date","2024-07-17T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/podman-vs-docker.jpeg",[19],[98],[100,1836],"podman","https://www.bitdoze.com/podman-vs-docker/","In the ever-evolving world of software development and deployment, containerization has emerged as a game-changing technology. It's revolutionized the way we build, ship, and run applications, making it easier than ever to create consistent environments across different platforms. Let's dive into the fascinating world of containers and explore two of the most prominent players in this space: Docker and Podman.\r\n\r\n## Background on containerization\r\n\r\nContainerization is like packing your entire application and its dependencies into a neat, portable box. Imagine you're moving to a new house, and instead of packing everything separately, you could just pick up your entire room and plop it down in your new place, exactly as it was. That's essentially what containerization does for software!\r\n\r\nThis technology has its roots in the concept of operating system-level virtualization, which dates back to the early 2000s. However, it wasn't until the [2010s](https://www.redhat.com/en/topics/containers/whats-a-linux-container) that containerization really took off, thanks to advancements in Linux kernel features like cgroups and namespaces.\r\n\r\nThe benefits of containerization are numerous:\r\n\r\n- **Consistency**: Containers ensure that applications run the same way, regardless of where they're deployed.\r\n- **Efficiency**: They're lightweight and start up quickly, using fewer resources than traditional virtual machines.\r\n- **Scalability**: Containers make it easy to scale applications up or down as needed.\r\n- **Isolation**: Each container runs in its own environment, reducing conflicts between applications.\r\n\r\nThese advantages have led to widespread adoption of containerization across various industries. According to a 2021 survey by the Cloud Native Computing Foundation, 92% of organizations were using containers in production, up from 84% the previous year. This rapid growth underscores the transformative impact of containerization on modern software development and deployment practices.\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\n## Rise of Docker and emergence of Podman\r\n\r\nWhen we talk about containerization, it's almost impossible not to mention Docker. Launched in 2013, [Docker](https://www.docker.com/) quickly became synonymous with containers, much like how \"Googling\" became the go-to term for internet searches. Docker's user-friendly interface and robust ecosystem made containerization accessible to a broader range of developers and organizations.\r\n\r\nSome key milestones in Docker's rise include:\r\n\r\n- 2013: Docker is released as an open-source project\r\n- 2014: Google, IBM, and other tech giants announce support for Docker\r\n- 2015: Docker reaches 100 million container downloads\r\n- 2017: Docker Enterprise Edition is launched\r\n\r\nDocker's popularity soared, and it became the de facto standard for containerization. However, as with any technology that gains widespread adoption, alternatives began to emerge. One such alternative is Podman.\r\n\r\n[Podman](https://podman.io/), short for Pod Manager, was introduced by Red Hat in 2018 as a daemonless container engine. It was designed to be a more secure and lightweight alternative to Docker, addressing some of the concerns that had arisen around Docker's architecture.\r\n\r\nKey features of Podman include:\r\n\r\n- Daemonless architecture\r\n- Rootless containers\r\n- OCI (Open Container Initiative) compliance\r\n- Compatibility with Docker commands\r\n\r\nPodman's emergence has sparked interest in the developer community, particularly among those concerned with security and those working in enterprise environments. While Docker still holds a significant market share, Podman has been gaining traction, especially in Linux-based environments.\r\n\r\n## Importance of comparing container technologies\r\n\r\nAs the containerization landscape evolves, it's crucial for developers, DevOps engineers, and organizations to understand the differences between various container technologies. Here's why:\r\n\r\n1. **Informed decision-making**: Choosing the right container technology can significantly impact your development workflow, deployment processes, and overall system performance.\r\n\r\n2. **Security considerations**: Different container technologies may have varying security models and features, which are critical in today's cybersecurity landscape.\r\n\r\n3. **Compatibility and integration**: Understanding how different container technologies work helps ensure smooth integration with existing tools and workflows.\r\n\r\n4. **Performance optimization**: Each container technology may have unique performance characteristics that could affect your application's efficiency.\r\n\r\n5. **Future-proofing**: As the container ecosystem evolves, staying informed about different technologies helps you adapt to future changes and trends.\r\n\r\nTo illustrate the importance of comparing container technologies, let's look at a brief comparison of Docker and Podman:\r\n\r\n| Feature           | Docker                    | Podman                       |\r\n| ----------------- | ------------------------- | ---------------------------- |\r\n| Architecture      | Client-server with daemon | Daemonless                   |\r\n| Root privileges   | Required for daemon       | Supports rootless containers |\r\n| OCI compliance    | Yes                       | Yes                          |\r\n| Container runtime | containerd                | crun or runc                 |\r\n| Orchestration     | Docker Swarm (built-in)   | Kubernetes (via Podman pods) |\r\n| Image building    | Dockerfile                | Dockerfile (compatible)      |\r\n\r\nThis table highlights some key differences between Docker and Podman, demonstrating why it's essential to compare these technologies. For instance, an organization prioritizing security might lean towards Podman's rootless containers, while another valuing Docker's extensive ecosystem might prefer to stick with Docker.\r\n\r\nIn the rapidly evolving world of containerization, staying informed about different technologies is not just beneficial—it's necessary. As Gartner predicts, the container management market is expected to grow to $944 million by 2024, indicating the continued importance of these technologies in the software development landscape.\r\n\r\nBy understanding the nuances of different container technologies, developers and organizations can make informed decisions that align with their specific needs, workflows, and long-term goals. Whether you're a seasoned DevOps engineer or a developer just starting to explore containerization, comparing technologies like Docker and Podman is an invaluable exercise in staying current and making the most of these powerful tools.\r\n\r\nAs we delve deeper into the specifics of Docker and Podman in the following sections, keep in mind that the goal is not to declare a winner, but to provide you with the knowledge to choose the right tool for your unique circumstances. After all, in the world of technology, one size rarely fits all!\r\n\r\n## Container Basics: Docker and Podman Overview\r\n\r\nIn the world of containerization, two names stand out: Docker and Podman. Both offer powerful solutions for managing containers, but they have distinct features and philosophies. Let's take a closer look at these containerization titans and see how they compare.\r\n\r\n### What is Docker?\r\n\r\n[Docker](https://www.docker.com/) is the pioneer that popularized containerization in the software industry. Launched in 2013, it quickly became synonymous with containers, transforming how developers build, package, and deploy applications.\r\n\r\n#### Key features and architecture\r\n\r\nDocker's architecture is built around a client-server model, consisting of three main components:\r\n\r\n1. Docker Client: The command-line interface (CLI) that users interact with.\r\n2. Docker Daemon: The background service that manages Docker objects.\r\n3. Docker Registry: A repository for storing and sharing Docker images.\r\n\r\nSome of Docker's key features include:\r\n\r\n- **Easy-to-use CLI**: Docker's intuitive command-line interface makes it accessible for beginners and powerful for experts.\r\n- **Docker Hub**: A vast repository of pre-built container images, both official and community-contributed.\r\n- **Docker Compose**: A tool for defining and running multi-container applications.\r\n- **Docker Swarm**: Built-in orchestration for managing clusters of Docker containers.\r\n\r\n#### Docker's role in containerization history\r\n\r\nDocker's impact on the tech industry cannot be overstated. It popularized the concept of containerization, making it accessible to a wide range of developers and organizations. According to the [2020 Stack Overflow Developer Survey](https://insights.stackoverflow.com/survey/2020#technology-platforms-all-respondents5), Docker was used by 35.6% of professional developers, showcasing its widespread adoption.\r\n\r\nDocker's influence extends beyond its own ecosystem. It played a crucial role in the development of industry standards like the [Open Container Initiative (OCI)](https://opencontainers.org/), which aims to create open industry standards around container formats and runtimes.\r\n\r\n### What is Podman?\r\n\r\n[Podman](https://podman.io/), short for Pod Manager, is a more recent entrant in the containerization space. Developed by Red Hat, Podman offers a daemonless container engine for developing, managing, and running OCI containers.\r\n\r\n#### Key features and architecture\r\n\r\nPodman's architecture differs significantly from Docker's:\r\n\r\n- **Daemonless**: Podman doesn't require a background daemon to run, enhancing security and reducing resource usage.\r\n- **Rootless containers**: Podman can run containers without root privileges, improving security.\r\n- **Pod support**: Native support for pods, a concept borrowed from Kubernetes.\r\n\r\nSome of Podman's standout features include:\r\n\r\n- **Docker compatibility**: Podman can often be used as a drop-in replacement for Docker, with a compatible CLI.\r\n- **Systemd integration**: Containers can be managed as systemd services.\r\n- **Kubernetes integration**: Generate Kubernetes YAML from Podman containers and pods.\r\n\r\n#### Red Hat's motivation for developing Podman\r\n\r\nRed Hat developed Podman as part of its push towards more secure and flexible containerization solutions. The company wanted to address some of the limitations and potential security issues associated with Docker's architecture, particularly the reliance on a privileged daemon process.\r\n\r\nAccording to Dan Walsh, a Red Hat engineer, in a [blog post](https://developers.redhat.com/blog/2019/02/21/podman-and-buildah-for-docker-users/): \"We wanted to build a tool that could be used to run containers in HPC environments, in CI/CD systems, and even potentially as a back end for Kubernetes CRI.\"\r\n\r\n### Similarities between Docker and Podman\r\n\r\nDespite their differences, Docker and Podman share several similarities, making it easier for users to switch between the two or use them in complementary ways.\r\n\r\n#### OCI compliance\r\n\r\nBoth Docker and Podman are compliant with the Open Container Initiative (OCI) standards. This means that container images created with one tool can be run by the other, ensuring interoperability and preventing vendor lock-in.\r\n\r\nThe OCI compliance covers two main specifications:\r\n\r\n1. Runtime Specification (runtime-spec)\r\n2. Image Specification (image-spec)\r\n\r\nThis standardization has been crucial for the growth of the containerization ecosystem, allowing for a rich variety of tools and platforms that can work together seamlessly.\r\n\r\n#### Command-line interface compatibility\r\n\r\nOne of Podman's design goals was to maintain CLI compatibility with Docker. This means that in many cases, you can simply alias `docker` to `podman` and continue using your existing Docker commands and scripts.\r\n\r\nHere's a comparison of some common commands:\r\n\r\n| Action          | Docker         | Podman         |\r\n| --------------- | -------------- | -------------- |\r\n| Run a container | `docker run`   | `podman run`   |\r\n| List containers | `docker ps`    | `podman ps`    |\r\n| Build an image  | `docker build` | `podman build` |\r\n| Pull an image   | `docker pull`  | `podman pull`  |\r\n\r\nThis compatibility makes it easier for teams to transition from Docker to Podman if they choose to do so, reducing the learning curve and minimizing disruption to existing workflows.\r\n\r\nWhile Docker and Podman share these similarities, it's important to note that they are not identical. Podman introduces some unique concepts and commands, particularly around rootless containers and pod management, which don't have direct equivalents in Docker.\r\n\r\nAs containerization continues to evolve, both Docker and Podman are playing crucial roles in shaping the future of software deployment and management. Docker's pioneering work laid the foundation for the containerization revolution, while Podman represents the next generation of container engines, focusing on enhanced security and flexibility.\r\n\r\nWhether you choose Docker, Podman, or a combination of both, understanding these tools is crucial for modern software development and deployment. As the container ecosystem continues to grow and mature, we can expect to see even more innovations and improvements in how we build, ship, and run applications.\r\n\r\n## Architectural Differences\r\n\r\nWhen it comes to containerization technologies, the architectural design plays a crucial role in determining performance, security, and ease of use. Docker and Podman, while both serving similar purposes, take fundamentally different approaches to their architecture. Let's explore these differences and their implications.\r\n\r\n### Docker's client-server model\r\n\r\nDocker, the pioneer in popularizing containerization, employs a client-server architecture. This model has been the foundation of Docker's functionality since its inception.\r\n\r\n#### Docker daemon and its role\r\n\r\nAt the heart of Docker's architecture lies the Docker daemon, a background process that manages Docker objects such as images, containers, networks, and volumes. The daemon listens for Docker API requests and processes them, acting as the control center for all Docker-related operations.\r\n\r\nThe [Docker daemon](https://docs.docker.com/engine/reference/commandline/dockerd/) (dockerd) runs with root privileges, which allows it to perform system-level operations necessary for container management. Users interact with Docker through the Docker CLI (Command Line Interface), which communicates with the daemon to execute commands.\r\n\r\n#### Advantages and disadvantages of daemon-based architecture\r\n\r\n**Advantages:**\r\n\r\n- Centralized management: The daemon provides a single point of control for all Docker operations.\r\n- Performance: For certain operations, the daemon can offer improved performance due to its persistent nature.\r\n- Feature-rich: The daemon architecture allows for easy implementation of advanced features like Docker Swarm for orchestration.\r\n\r\n**Disadvantages:**\r\n\r\n- Security concerns: Running with root privileges poses potential security risks.\r\n- Single point of failure: If the daemon crashes, all Docker operations are affected.\r\n- Resource overhead: The daemon consumes system resources even when not actively managing containers.\r\n\r\n### Podman's daemonless approach\r\n\r\nIn contrast to Docker, Podman takes a radically different approach with its daemonless architecture. This design choice sets Podman apart and addresses some of the concerns associated with Docker's daemon-based model.\r\n\r\n#### Fork-exec model\r\n\r\nPodman utilizes a fork-exec model, where each Podman command runs as a separate process. When you execute a Podman command, it forks a new process to handle the operation and then exits once the task is complete. This approach is more in line with traditional Unix philosophy.\r\n\r\nThe Podman architecture doesn't require a persistent background process, which means it can run without elevated privileges for most operations.\r\n\r\n#### Benefits of daemonless architecture\r\n\r\n- **Enhanced security:** Podman can run in rootless mode, reducing the attack surface and potential vulnerabilities.\r\n- **Improved stability:** Without a central daemon, there's no single point of failure that could affect all container operations.\r\n- **Resource efficiency:** System resources are only used when Podman commands are actively running.\r\n- **Simplified debugging:** Each operation runs as a separate process, making it easier to trace and debug issues.\r\n\r\n### Impact on system resource usage\r\n\r\nThe architectural differences between Docker and Podman have significant implications for system resource usage. Let's compare their impact on memory footprint and CPU utilization.\r\n\r\n#### Memory footprint comparison\r\n\r\nDocker's daemon-based architecture results in a constant memory overhead, as the daemon runs continuously in the background. In contrast, Podman's daemonless approach means it only consumes memory when actively executing commands.\r\n\r\nHere's a simplified comparison of memory usage:\r\n\r\n| Scenario                     | Docker               | Podman                            |\r\n| ---------------------------- | -------------------- | --------------------------------- |\r\n| Idle (no containers running) | ~50-100 MB           | 0 MB                              |\r\n| Running a single container   | ~100-150 MB          | ~50 MB                            |\r\n| Multiple containers          | Incremental increase | Proportional to active containers |\r\n\r\nThese figures are approximate and can vary based on system configuration and workload. However, they illustrate the general trend of Podman's more efficient memory usage, especially in idle states.\r\n\r\n#### CPU utilization differences\r\n\r\nCPU utilization patterns also differ between Docker and Podman:\r\n\r\n- **Docker:** The daemon consumes a small amount of CPU even when idle. During container operations, CPU usage spikes as the daemon processes requests.\r\n- **Podman:** CPU usage is typically zero when no commands are running. During operations, CPU utilization is similar to Docker but limited to the duration of the command execution.\r\n\r\nA study by Red Hat compared the performance of Podman and Docker in various scenarios. They found that while both tools performed similarly for most container operations, Podman had a slight edge in CPU efficiency, particularly for operations involving multiple containers.\r\n\r\n### Key takeaways:\r\n\r\n- Docker's daemon-based architecture offers centralized management but comes with security concerns and constant resource overhead.\r\n- Podman's daemonless approach provides enhanced security and resource efficiency at the cost of some advanced features.\r\n- Memory usage is generally lower with Podman, especially when containers are not actively running.\r\n- CPU utilization is more sporadic with Podman, aligning closely with actual command execution.\r\n\r\nThe choice between Docker and Podman often comes down to specific use cases and priorities. For environments where security and resource efficiency are paramount, Podman's architecture offers clear advantages. However, Docker's rich ecosystem and advanced features may still make it the preferred choice for many developers and organizations.\r\n\r\nAs containerization continues to evolve, it's exciting to see how these architectural approaches influence the development of new features and best practices in the industry. Whether you choose Docker or Podman, understanding these architectural differences will help you make informed decisions about your containerization strategy and optimize your development workflow.\r\n\r\n## Security Considerations\r\n\r\nWhen it comes to container management, security is paramount. Both Docker and Podman offer robust security features, but their approaches differ in significant ways. Let's explore the security considerations for each platform, focusing on root privileges, user namespaces, SELinux integration, and overall vulnerability surface.\r\n\r\n### Root privileges and container management\r\n\r\n#### Docker's root-based approach\r\n\r\nDocker has traditionally relied on a root-based approach for container management. This means that the Docker daemon runs with root privileges, which can be a double-edged sword:\r\n\r\n- **Pros:**\r\n  - Simplified management and deployment\r\n  - Full access to system resources\r\n- **Cons:**\r\n  - Increased security risk if the daemon is compromised\r\n  - Potential for unintended system-wide changes\r\n\r\nAccording to a [2019 Snyk report](https://snyk.io/blog/10-docker-image-security-best-practices/), 44% of Docker images contain known vulnerabilities, highlighting the importance of careful management when using root privileges.\r\n\r\n#### Podman's rootless containers\r\n\r\nPodman takes a different approach, offering rootless containers as a default:\r\n\r\n- **Pros:**\r\n  - Reduced attack surface\r\n  - Better isolation between containers and host system\r\n  - Improved compliance with security policies\r\n- **Cons:**\r\n  - Some limitations on functionality without root access\r\n\r\nA [Red Hat survey](https://www.redhat.com/en/blog/why-red-hat-investing-cri-o-and-podman) found that 56% of respondents cited security as a primary concern when adopting container technologies, making Podman's rootless approach particularly appealing.\r\n\r\n### User namespace implementation\r\n\r\n#### How Podman leverages user namespaces\r\n\r\nPodman makes extensive use of user namespaces to enhance security:\r\n\r\n- Containers run with unprivileged user IDs\r\n- UID/GID mapping between container and host\r\n- Reduced risk of container breakout attacks\r\n\r\nThis approach aligns with the principle of least privilege, a cornerstone of modern security practices.\r\n\r\n#### Docker's rootless mode vs. Podman's native approach\r\n\r\nWhile Docker has introduced a rootless mode, it's not the default configuration:\r\n\r\n| Feature                | Docker                                 | Podman                |\r\n| ---------------------- | -------------------------------------- | --------------------- |\r\n| Default mode           | Root-based                             | Rootless              |\r\n| User namespace support | Available in rootless mode             | Native implementation |\r\n| Ease of use            | Requires additional setup for rootless | Works out of the box  |\r\n\r\nPodman's native rootless approach provides a more seamless experience for users prioritizing security.\r\n\r\n### SELinux integration\r\n\r\n#### Podman's native SELinux support\r\n\r\nPodman offers robust, native support for SELinux (Security-Enhanced Linux):\r\n\r\n- Automatic labeling of container processes and files\r\n- Fine-grained access control policies\r\n- Seamless integration with existing SELinux configurations\r\n\r\nThis integration helps organizations maintain compliance with security standards like [NIST SP 800-190](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf), which provides guidelines for application container security.\r\n\r\n#### Docker's SELinux implementation\r\n\r\nDocker also supports SELinux, but the implementation differs:\r\n\r\n- Requires additional configuration\r\n- May need custom SELinux policies for complex setups\r\n- Can be disabled, potentially leading to security gaps\r\n\r\nA StackRox survey found that 94% of respondents experienced a security incident in their Kubernetes environments in the past 12 months, underscoring the importance of robust security measures like SELinux integration.\r\n\r\n### Vulnerability surface comparison\r\n\r\n#### Attack vectors in daemon-based vs. daemonless architectures\r\n\r\nThe architectural differences between Docker and Podman have significant implications for their vulnerability surfaces:\r\n\r\n**Docker (daemon-based):**\r\n\r\n- Central daemon is a potential single point of failure\r\n- Larger attack surface due to always-running process\r\n- Vulnerabilities in the daemon could affect all containers\r\n\r\n**Podman (daemonless):**\r\n\r\n- No central daemon to exploit\r\n- Reduced attack surface\r\n- Each container runs as a separate process\r\n\r\nTo illustrate this difference, consider the following table:\r\n\r\n| Aspect                    | Docker        | Podman             |\r\n| ------------------------- | ------------- | ------------------ |\r\n| Central process           | Docker daemon | None (daemonless)  |\r\n| Process isolation         | Shared daemon | Separate processes |\r\n| Privilege escalation risk | Higher        | Lower              |\r\n\r\n#### Case studies of security incidents\r\n\r\nWhile specific security incidents are often kept confidential, we can look at some general trends and examples:\r\n\r\n1. **CVE-2019-5736:** This vulnerability in runc, which affected both Docker and other container runtimes, allowed attackers to gain root-level access on the host. Podman's rootless approach would have mitigated this risk.\r\n\r\n2. **Docker socket exposure:** Misconfigured Docker sockets have led to numerous security incidents. In 2018, Tesla's cloud infrastructure was compromised due to an exposed Docker API, resulting in cryptocurrency mining on their Kubernetes cluster.\r\n\r\n3. **Container escape vulnerabilities:** Researchers have demonstrated various container escape techniques in Docker environments. While these often require specific conditions, they highlight the importance of defense-in-depth strategies.\r\n\r\nIt's worth noting that both Docker and Podman teams actively work to address vulnerabilities and improve security. The [Docker Security](https://docs.docker.com/engine/security/) documentation and Podman Security guide provide detailed information on best practices and ongoing efforts to enhance container security.\r\n\r\nIn conclusion, while both Docker and Podman offer strong security features, Podman's rootless approach, native user namespace implementation, and seamless SELinux integration provide a more security-focused default configuration. However, with proper setup and adherence to best practices, both platforms can be used to create secure container environments.\r\n\r\nAs container adoption continues to grow—with [Gartner predicting](https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co) that by 2022, more than 75% of global organizations will be running containerized applications in production—understanding these security considerations becomes increasingly crucial for developers and operations teams alike.\r\n\r\n## Performance and Scalability\r\n\r\nWhen it comes to containerization tools, performance and scalability are crucial factors that can make or break your development and deployment processes. In this section, we'll dive deep into how Docker and Podman stack up against each other in these critical areas.\r\n\r\n### Image Building Performance\r\n\r\nThe speed at which container images can be built is a key consideration for developers and DevOps teams. Both Docker and Podman have their own approaches to image building, each with its unique advantages.\r\n\r\n#### Docker's BuildKit vs. Podman's Buildah\r\n\r\nDocker introduced [BuildKit](https://docs.docker.com/build/buildkit/) as an improved image building solution, offering faster build times and more efficient caching. On the other hand, Podman utilizes [Buildah](https://buildah.io/), a flexible and powerful tool for building OCI-compliant container images.\r\n\r\n- BuildKit advantages:\r\n\r\n  - Concurrent and distributed building\r\n  - Improved caching mechanisms\r\n  - Automatic garbage collection\r\n\r\n- Buildah advantages:\r\n  - Rootless image building\r\n  - Fine-grained control over image layers\r\n  - Integration with other tools in the container ecosystem\r\n\r\n#### Benchmark Comparisons\r\n\r\nWhile specific benchmarks can vary depending on the complexity of the image and the hardware used, some general observations can be made:\r\n\r\n| Aspect             | Docker (BuildKit)                               | Podman (Buildah)                                       |\r\n| ------------------ | ----------------------------------------------- | ------------------------------------------------------ |\r\n| Build Speed        | Generally faster for complex multi-stage builds | Competitive performance, especially for simpler images |\r\n| Caching Efficiency | Excellent, with intelligent layer caching       | Good, with improving caching mechanisms                |\r\n| Resource Usage     | Efficient use of system resources               | Lightweight, with minimal overhead                     |\r\n\r\nIt's worth noting that the Red Hat team has reported comparable or better performance with Podman and Buildah in many scenarios, particularly when building images without root privileges.\r\n\r\n### Container Startup Times\r\n\r\nThe time it takes for a container to start can significantly impact application responsiveness and scalability, especially in dynamic environments where containers are frequently spun up and down.\r\n\r\n#### Impact of Architecture on Initialization\r\n\r\nDocker's architecture involves a daemon that manages containers, while Podman uses a daemonless approach. This fundamental difference can affect startup times:\r\n\r\n- Docker:\r\n\r\n  - The daemon can provide quicker startup for subsequent containers\r\n  - Initial daemon startup may add overhead\r\n\r\n- Podman:\r\n  - No daemon overhead\r\n  - Potentially faster cold starts\r\n\r\n#### Real-world Performance Tests\r\n\r\nIn real-world scenarios, the differences in startup times between Docker and Podman are often negligible for most applications. However, in high-performance environments where milliseconds matter, these small differences can add up.\r\n\r\nA study by SUSE found that Podman showed slightly faster container creation times compared to Docker in certain scenarios, particularly when running multiple containers simultaneously.\r\n\r\n### Resource Isolation and Management\r\n\r\nEffective resource isolation and management are critical for maintaining performance and security in containerized environments.\r\n\r\n#### cgroups Implementation in Docker and Podman\r\n\r\nBoth Docker and Podman utilize Linux control groups (cgroups) for resource management, but with some differences:\r\n\r\n- Docker:\r\n\r\n  - Uses cgroups v1 by default\r\n  - Transition to cgroups v2 is ongoing\r\n\r\n- Podman:\r\n  - Native support for cgroups v2\r\n  - Better integration with systemd\r\n\r\n#### Fine-grained Control over Container Resources\r\n\r\nBoth tools offer ways to control resource allocation, but Podman's integration with cgroups v2 can provide more granular control:\r\n\r\n| Feature           | Docker  | Podman                     |\r\n| ----------------- | ------- | -------------------------- |\r\n| CPU Limits        | Yes     | Yes                        |\r\n| Memory Limits     | Yes     | Yes                        |\r\n| I/O Throttling    | Yes     | Yes                        |\r\n| Per-device Limits | Limited | Advanced (with cgroups v2) |\r\n\r\nPodman's use of cgroups v2 allows for more precise resource management, which can be particularly beneficial in multi-tenant environments or when running resource-intensive applications.\r\n\r\n### Scalability in Production Environments\r\n\r\nAs applications grow and demand increases, the ability to scale containerized workloads becomes paramount.\r\n\r\n#### Docker Swarm vs. Podman Pods\r\n\r\nFor native clustering and orchestration:\r\n\r\n- Docker Swarm:\r\n\r\n  - Integrated into Docker\r\n  - Easy to set up and use\r\n  - Good for smaller to medium-sized deployments\r\n\r\n- Podman Pods:\r\n  - Kubernetes-like pod abstraction\r\n  - Better alignment with Kubernetes concepts\r\n  - Suitable for transitioning to full Kubernetes deployments\r\n\r\nWhile Docker Swarm provides a straightforward path to clustering, Podman's approach with pods offers a more Kubernetes-friendly solution, which can be advantageous for teams planning to scale to full-fledged Kubernetes environments.\r\n\r\n#### Integration with Orchestration Tools\r\n\r\nBoth Docker and Podman integrate well with Kubernetes, the de facto standard for container orchestration at scale:\r\n\r\n- Docker:\r\n\r\n  - Long-standing integration with Kubernetes\r\n  - Widely used in Kubernetes environments\r\n\r\n- Podman:\r\n  - Native support for Kubernetes YAML\r\n  - Seamless transition between local development and Kubernetes clusters\r\n\r\nThe [Kubernetes](https://kubernetes.io/) project has been moving away from Docker-specific components, which has somewhat leveled the playing field between Docker and alternative runtimes like those used by Podman.\r\n\r\nIn conclusion, both Docker and Podman offer strong performance and scalability features. Docker's maturity and widespread adoption give it an edge in terms of ecosystem and tooling support. However, Podman's daemonless architecture, advanced resource management capabilities, and closer alignment with Kubernetes concepts make it an increasingly attractive option, especially for teams focused on security and looking for a smooth path to large-scale Kubernetes deployments.\r\n\r\nThe choice between Docker and Podman often comes down to specific use cases, existing infrastructure, and team expertise. As both tools continue to evolve, staying informed about their latest features and performance improvements is crucial for making the best decision for your containerization needs.\r\n\r\n## Developer Experience and Ecosystem\r\n\r\nWhen it comes to containerization tools, the developer experience and ecosystem play a crucial role in adoption and efficiency. Both Docker and Podman have made significant strides in this area, but they approach it in slightly different ways. Let's explore how these two platforms compare in terms of their command-line interfaces, integration with development workflows, image compatibility, and community support.\r\n\r\n### Command-line interface comparison\r\n\r\nThe command-line interface (CLI) is often the primary way developers interact with containerization tools. Both Docker and Podman have put considerable effort into making their CLIs user-friendly and powerful.\r\n\r\n#### Syntax similarities and differences\r\n\r\nDocker and Podman have intentionally similar syntaxes, which is great news for developers looking to switch between the two or use them interchangeably. This similarity is not by accident; Podman was designed to be a drop-in replacement for Docker in many scenarios. Here's a quick comparison of some common commands:\r\n\r\n| Action          | Docker         | Podman         |\r\n| --------------- | -------------- | -------------- |\r\n| Run a container | `docker run`   | `podman run`   |\r\n| List containers | `docker ps`    | `podman ps`    |\r\n| Build an image  | `docker build` | `podman build` |\r\n| Pull an image   | `docker pull`  | `podman pull`  |\r\n\r\nAs you can see, in many cases, you can simply replace \"docker\" with \"podman\" in your commands, and they'll work as expected. This similarity extends to most of the basic container operations, making the transition between the two tools relatively smooth.\r\n\r\n#### Unique features of each CLI\r\n\r\nWhile the basic syntax is similar, each CLI has its own unique features:\r\n\r\n**Docker CLI:**\r\n\r\n- Integrated Docker Compose support with `docker-compose` command\r\n- Native support for Docker Swarm commands\r\n- Built-in Docker Hub integration\r\n\r\n**Podman CLI:**\r\n\r\n- Rootless container support by default\r\n- Pod management commands (e.g., `podman pod create`)\r\n- [Systemd](https://systemd.io/) integration for managing containers as system services\r\n\r\nPodman's rootless containers are a significant advantage from a security perspective, as they allow containers to run without root privileges. This feature is particularly appealing in enterprise environments where security is a top priority.\r\n\r\n### Integration with development workflows\r\n\r\nContainerization tools need to integrate seamlessly with existing development workflows to be truly effective. Both Docker and Podman offer various integration points, but their approaches differ slightly.\r\n\r\n#### Docker Compose vs. Podman Compose\r\n\r\n[Docker Compose](https://docs.docker.com/compose/) is a popular tool for defining and running multi-container Docker applications. It uses a YAML file to configure application services, networks, and volumes, making it easy to spin up complex environments with a single command.\r\n\r\nPodman, on the other hand, doesn't have a native equivalent to Docker Compose. However, the community has developed [Podman Compose](https://github.com/containers/podman-compose), which aims to provide similar functionality. While it's not an official tool, it does allow developers to use Docker Compose files with Podman, easing the transition between the two platforms.\r\n\r\nHere's a quick comparison:\r\n\r\n| Feature              | Docker Compose    | Podman Compose                        |\r\n| -------------------- | ----------------- | ------------------------------------- |\r\n| Official tool        | Yes               | No (community-developed)              |\r\n| Syntax compatibility | N/A               | Aims for Docker Compose compatibility |\r\n| Integration with CLI | Tight integration | Separate tool                         |\r\n| Maturity             | Highly mature     | Less mature, still evolving           |\r\n\r\n#### IDE plugins and tooling support\r\n\r\nBoth Docker and Podman benefit from extensive IDE and tooling support, which is crucial for developer productivity. However, Docker has a slight edge due to its longer presence in the market.\r\n\r\n**Docker:**\r\n\r\n- Official extensions for popular IDEs like [Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker) and [JetBrains IDEs](https://www.jetbrains.com/help/idea/docker.html)\r\n- Widespread support in CI/CD tools like Jenkins, GitLab CI, and GitHub Actions\r\n- Native integration with cloud platforms like AWS, Azure, and Google Cloud\r\n\r\n**Podman:**\r\n\r\n- Growing support in IDEs, with extensions available for Visual Studio Code and others\r\n- Increasing adoption in CI/CD pipelines, especially in Red Hat OpenShift environments\r\n- Integration with Red Hat's ecosystem of tools\r\n\r\nWhile Docker still has an advantage in terms of tooling support, Podman is quickly catching up, especially in enterprise environments that favor Red Hat technologies.\r\n\r\n### Image compatibility and portability\r\n\r\nOne of the key strengths of containerization is the ability to create portable applications that can run anywhere. Both Docker and Podman excel in this area, but there are some nuances to consider.\r\n\r\n#### OCI image format support\r\n\r\nBoth Docker and Podman support the [Open Container Initiative (OCI)](https://opencontainers.org/) image format, which is a standardized format for container images. This means that images created with Docker can generally be run with Podman, and vice versa. This compatibility is a huge win for developers, as it allows for flexibility in choosing tools without sacrificing portability.\r\n\r\nAccording to a 2021 survey by the Cloud Native Computing Foundation, 84% of organizations are using containers in production, highlighting the importance of image compatibility and portability in modern development workflows.\r\n\r\n#### Challenges in migrating from Docker to Podman\r\n\r\nWhile the image format compatibility makes migration easier, there are still some challenges to consider when moving from Docker to Podman:\r\n\r\n- **Root vs. Rootless:** Podman's default rootless mode may require adjustments to existing workflows and permissions.\r\n- **Networking:** Podman's networking model differs slightly from Docker's, which may require configuration changes.\r\n- **Docker Daemon:** Podman doesn't use a daemon, which can affect how some tools and scripts interact with containers.\r\n\r\nDespite these challenges, many organizations have successfully migrated from Docker to Podman. For example, Red Hat has fully embraced Podman in its Enterprise Linux distribution, demonstrating the viability of large-scale migrations.\r\n\r\n### Community and documentation\r\n\r\nA strong community and comprehensive documentation are essential for the success of any open-source tool. Both Docker and Podman have invested heavily in these areas.\r\n\r\n#### Size and activity of user communities\r\n\r\nDocker has a massive and active community, with over 100,000 stars on [GitHub](https://github.com/docker) and millions of users worldwide. This large community translates into a wealth of resources, third-party tools, and quick problem-solving support.\r\n\r\nPodman, while smaller, has a growing and enthusiastic community, particularly in the enterprise space. Its [GitHub repository](https://github.com/containers/podman) has over 15,000 stars, and its adoption is increasing, especially among Red Hat users.\r\n\r\n#### Quality and comprehensiveness of official documentation\r\n\r\nBoth Docker and Podman offer extensive official documentation:\r\n\r\n- [Docker Documentation](https://docs.docker.com/) is comprehensive, well-organized, and includes numerous tutorials and best practices.\r\n- [Podman Documentation](https://docs.podman.io/) is also thorough and includes detailed guides for transitioning from Docker.\r\n\r\nWhile Docker's documentation benefits from years of refinement and a larger user base, Podman's documentation is rapidly improving and offers unique insights into rootless containers and Red Hat-specific integrations.\r\n\r\nIn conclusion, both Docker and Podman offer robust developer experiences and ecosystems. Docker's maturity and widespread adoption give it an edge in terms of community size and tooling support. However, Podman's focus on security (with rootless containers) and its tight integration with the Red Hat ecosystem make it an attractive option, especially for enterprise users. As containerization continues to evolve, both tools are likely to play significant roles in shaping the future of software development and deployment.\r\n\r\n## Use Cases and Adoption\r\n\r\nThe containerization landscape is constantly evolving, with Docker and Podman both vying for dominance in various sectors. Let's explore how these technologies are being adopted and used across different scenarios.\r\n\r\n### Enterprise adoption patterns\r\n\r\nIn the enterprise world, the choice between Docker and Podman often comes down to specific organizational needs, existing infrastructure, and long-term strategic goals.\r\n\r\n#### Red Hat's push for Podman in enterprise Linux distributions\r\n\r\nRed Hat, a leader in enterprise open-source solutions, has been actively promoting Podman as the container engine of choice for its enterprise Linux distributions. This push is part of a broader strategy to offer a more secure and flexible containerization solution that aligns with enterprise requirements.\r\n\r\n• [Red Hat Enterprise Linux](https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux) (RHEL) 8 and later versions come with Podman pre-installed.\r\n• Podman's daemonless architecture is particularly appealing for security-conscious enterprises.\r\n• Integration with systemd allows for better resource management and process control.\r\n\r\nAccording to a 2022 Red Hat survey, 37% of organizations using Red Hat OpenShift were already using Podman, indicating a growing adoption trend.\r\n\r\n#### Docker's established presence in DevOps pipelines\r\n\r\nDespite the push for Podman, Docker maintains a strong foothold in many enterprise DevOps pipelines due to its maturity and extensive ecosystem.\r\n\r\n• Docker's widespread adoption means it's often deeply integrated into existing workflows.\r\n• The Docker Hub registry remains a popular source for container images.\r\n• Many third-party tools and services are built around Docker, providing a rich ecosystem.\r\n\r\n76% of developers using containers were using Docker, showcasing its continued dominance.\r\n\r\n### Cloud-native development scenarios\r\n\r\nAs cloud-native development becomes increasingly prevalent, both Docker and Podman are adapting to meet the needs of modern application architectures.\r\n\r\n#### Podman's alignment with Kubernetes-style workflows\r\n\r\nPodman's design philosophy aligns closely with Kubernetes, making it an attractive option for organizations heavily invested in cloud-native technologies.\r\n\r\n• Podman supports Kubernetes YAML files out of the box, easing the transition between local development and cluster deployment.\r\n• The `podman generate kube` command can create Kubernetes manifests from running containers, facilitating DevOps practices.\r\n• Podman's rootless containers provide an additional layer of security, which is crucial in multi-tenant cloud environments.\r\n\r\n#### Docker's ecosystem advantages in cloud deployments\r\n\r\nDocker's extensive ecosystem and integrations give it an edge in many cloud deployment scenarios.\r\n\r\n• Major cloud providers offer native support for Docker containers, simplifying deployment processes.\r\n• Docker Compose remains a popular tool for defining and running multi-container applications, with cloud providers offering compatible services.\r\n• The Docker CLI is familiar to many developers, reducing the learning curve for cloud-native development.\r\n\r\n### Edge computing and IoT applications\r\n\r\nThe rise of edge computing and IoT has created new challenges and opportunities for containerization technologies.\r\n\r\n#### Podman's lightweight nature for resource-constrained environments\r\n\r\nPodman's architecture makes it particularly well-suited for edge and IoT scenarios where resources are limited.\r\n\r\n• The absence of a daemon reduces memory footprint and improves startup times.\r\n• Podman's ability to run rootless containers enhances security in distributed edge environments.\r\n• Integration with systemd allows for better resource allocation and management on small devices.\r\n\r\n#### Docker's performance in edge scenarios\r\n\r\nWhile Docker may have a larger footprint, it still performs well in many edge computing scenarios.\r\n\r\n• Docker's optimization for performance ensures efficient resource utilization.\r\n• The availability of Docker-compatible runtimes like [balena-engine](https://www.balena.io/engine/) allows for Docker-like experiences on IoT devices.\r\n• Docker's extensive documentation and community support can be valuable for troubleshooting in complex edge deployments.\r\n\r\n### Case studies\r\n\r\nExamining real-world examples can provide valuable insights into the decision-making process and outcomes of choosing between Docker and Podman.\r\n\r\n#### Organizations that have migrated from Docker to Podman\r\n\r\nSeveral organizations have made the switch from Docker to Podman, citing various reasons:\r\n\r\n1. **CERN**: The European Organization for Nuclear Research migrated to Podman for its High-Performance Computing environments, appreciating the improved security and integration with their existing tools.\r\n\r\n2. **Amadeus IT Group**: This travel technology company adopted Podman to streamline their container management and improve security across their global infrastructure.\r\n\r\n3. **US Department of Defense**: The DoD has been exploring Podman as part of its DevSecOps initiative, valuing its enhanced security features and compatibility with government systems.\r\n\r\n#### Reasons for choosing one technology over the other\r\n\r\nThe decision between Docker and Podman often comes down to specific organizational needs and priorities:\r\n\r\n| Factor            | Docker                                | Podman                                         |\r\n| ----------------- | ------------------------------------- | ---------------------------------------------- |\r\n| Ecosystem         | Extensive third-party integrations    | Growing, with focus on enterprise needs        |\r\n| Security          | Requires root access for daemon       | Supports rootless containers                   |\r\n| Performance       | Optimized for high-throughput         | Efficient in resource-constrained environments |\r\n| Learning Curve    | Widely known, extensive documentation | Steeper for those familiar with Docker         |\r\n| Cloud Integration | Native support from major providers   | Strong Kubernetes alignment                    |\r\n\r\nOrganizations prioritizing security and tight integration with Linux systems often lean towards Podman, while those valuing a rich ecosystem and widespread community support may prefer Docker.\r\n\r\nIn conclusion, both Docker and Podman have their strengths and are continually evolving to meet the changing needs of the containerization landscape. The choice between them depends on specific use cases, existing infrastructure, and organizational priorities. As the container ecosystem matures, we may see further convergence in features and capabilities, potentially blurring the lines between these two powerful technologies.\r\n\r\n## Future Trends and Implications\r\n\r\nAs we look towards the horizon of containerization technology, it's clear that both Docker and Podman are poised to play significant roles in shaping the future of software deployment and management. Let's explore the emerging trends and potential implications for these container technologies.\r\n\r\n### Containerization market evolution\r\n\r\nThe containerization market is experiencing rapid growth and evolution. According to a [report by MarketsandMarkets](https://www.marketsandmarkets.com/Market-Reports/container-technology-market-234228524.html), the global application container market size is expected to grow from $1.2 billion in 2018 to $4.98 billion by 2023, at a Compound Annual Growth Rate (CAGR) of 32.9% during the forecast period.\r\n\r\n#### Predictions for Docker's future in light of competition\r\n\r\nDocker, as the pioneer in making containers accessible to the masses, has faced increasing competition in recent years. However, its future remains promising:\r\n\r\n- **Enterprise adoption**: Docker is likely to maintain a strong presence in enterprise environments due to its mature ecosystem and extensive tooling.\r\n- **Developer mindshare**: The familiarity of Docker commands and its widespread use in development environments will continue to be a significant advantage.\r\n- **Innovation focus**: Docker is expected to focus on enhancing developer experience and integrating with cloud-native technologies to stay competitive.\r\n\r\n#### Podman's growth trajectory and potential market share\r\n\r\nPodman, with its daemonless architecture and compatibility with Docker containers, is gaining traction, especially in the Linux ecosystem:\r\n\r\n- **Red Hat backing**: As a Red Hat-sponsored project, Podman is likely to see increased adoption in enterprise Linux environments.\r\n- **Security-focused organizations**: Podman's rootless containers and enhanced security features may appeal to organizations with stringent security requirements.\r\n- **Open-source community**: The growing open-source community around Podman could accelerate its feature development and adoption.\r\n\r\n### Integration with emerging technologies\r\n\r\nThe containerization landscape is increasingly intertwining with other cutting-edge technologies, opening up new possibilities and use cases.\r\n\r\n#### AI/ML workloads in containers\r\n\r\nArtificial Intelligence and Machine Learning workloads are becoming more prevalent in containerized environments:\r\n\r\n- **GPU support**: Both Docker and Podman are improving their support for GPU-accelerated containers, essential for AI/ML tasks.\r\n- **Model serving**: Containers are becoming the de facto standard for deploying and scaling machine learning models in production.\r\n- **Distributed training**: Containerized environments facilitate distributed training of large AI models across multiple nodes.\r\n\r\nA [study by IBM](https://www.ibm.com/cloud/blog/ai-ml-model-serving-containers) found that containerized AI/ML workloads can improve resource utilization by up to 30% compared to traditional deployment methods.\r\n\r\n#### Serverless and container integration trends\r\n\r\nThe line between serverless computing and containers is blurring, with several interesting developments:\r\n\r\n- **Container-based serverless**: Platforms like [AWS Fargate](https://aws.amazon.com/fargate/) and [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) offer serverless container execution.\r\n- **Knative**: This Kubernetes-based platform aims to provide serverless capabilities for containerized applications.\r\n- **Event-driven containers**: Both Docker and Podman are likely to enhance their support for event-driven architectures, allowing containers to spin up in response to specific triggers.\r\n\r\n### Standardization efforts\r\n\r\nAs the container ecosystem matures, standardization becomes increasingly important to ensure interoperability and prevent vendor lock-in.\r\n\r\n#### OCI's role in shaping container technologies\r\n\r\nThe [Open Container Initiative (OCI)](https://opencontainers.org/) plays a crucial role in standardizing container technologies:\r\n\r\n- **Runtime specification**: OCI runtime spec ensures that containers can run consistently across different platforms.\r\n- **Image specification**: The OCI image format provides a standard way to package and distribute container images.\r\n- **Distribution specification**: This spec standardizes how container images are distributed and discovered.\r\n\r\nBoth Docker and Podman adhere to OCI standards, which helps maintain compatibility and portability across platforms.\r\n\r\n#### Potential convergence of features between Docker and Podman\r\n\r\nAs both technologies mature, we may see a convergence of features:\r\n\r\n| Feature                 | Docker             | Podman             | Convergence Potential |\r\n| ----------------------- | ------------------ | ------------------ | --------------------- |\r\n| Rootless containers     | Supported          | Native             | High                  |\r\n| Daemonless architecture | In progress        | Native             | Medium                |\r\n| Compose functionality   | Native             | Via Podman Compose | High                  |\r\n| Kubernetes integration  | Via Docker Desktop | Native             | Medium                |\r\n\r\nThis convergence could lead to a more standardized user experience across container runtimes, benefiting developers and operators alike.\r\n\r\n### Environmental impact considerations\r\n\r\nAs the world becomes more environmentally conscious, the efficiency and sustainability of container technologies are coming under scrutiny.\r\n\r\n#### Energy efficiency of different container runtimes\r\n\r\nContainer runtimes can have varying impacts on energy consumption:\r\n\r\n- **Resource utilization**: More efficient resource utilization in containerized environments can lead to reduced energy consumption.\r\n- **Startup time**: Faster container startup times, as seen in Podman's daemonless architecture, can potentially reduce idle resource consumption.\r\n- **Optimization techniques**: Both Docker and Podman are likely to focus on optimizing their runtimes for energy efficiency.\r\n\r\nA [study by the University of L'Aquila](https://ieeexplore.ieee.org/document/8914727) found that containerized applications can be up to 25% more energy-efficient compared to traditional virtual machine deployments.\r\n\r\n#### Sustainability initiatives in container technologies\r\n\r\nContainer technologies are increasingly focusing on sustainability:\r\n\r\n- **Green computing**: Initiatives like [Green Software Foundation](https://greensoftware.foundation/) are promoting sustainable software practices, including in containerization.\r\n- **Cloud provider initiatives**: Major cloud providers are offering tools to measure and reduce the carbon footprint of containerized workloads.\r\n- **Efficient scheduling**: Advanced scheduling algorithms in container orchestrators like Kubernetes are being developed to optimize for energy efficiency alongside performance.\r\n\r\nAs we look to the future, it's clear that both Docker and Podman will continue to evolve, adapting to new technological landscapes and addressing emerging challenges. The competition between these technologies will likely drive innovation, benefiting the entire container ecosystem. Organizations will need to carefully evaluate their specific needs and use cases when choosing between Docker and Podman, considering factors such as security requirements, existing infrastructure, and future scalability needs.\r\n\r\nThe integration with AI/ML workloads and serverless architectures opens up exciting possibilities for more efficient and scalable application deployment. Meanwhile, standardization efforts will ensure that containers remain a flexible and portable solution for software deployment across various environments.\r\n\r\nLastly, as environmental concerns become increasingly pressing, the container community's focus on energy efficiency and sustainability will play a crucial role in shaping the future of these technologies. By optimizing resource utilization and embracing green computing practices, container technologies like Docker and Podman can contribute to more sustainable IT practices across the industry.\r\n\r\n## Conclusion\r\n\r\nAs we wrap up our exploration of Docker and Podman, it's clear that both tools have made significant contributions to the world of containerization. Let's summarize the key differences, provide recommendations, and look towards the future of this transformative technology.\r\n\r\n### Summary of key differences between Docker and Podman\r\n\r\nWhile Docker and Podman share the common goal of simplifying container management, they differ in several important aspects:\r\n\r\n| Feature           | Docker                  | Podman                       |\r\n| ----------------- | ----------------------- | ---------------------------- |\r\n| Architecture      | Client-server           | Daemonless                   |\r\n| Root privileges   | Required for daemon     | Rootless mode available      |\r\n| Container runtime | containerd              | CRI-O                        |\r\n| Orchestration     | Docker Swarm (built-in) | Kubernetes (via Podman pods) |\r\n| Image format      | OCI-compliant           | OCI-compliant                |\r\n| User experience   | GUI and CLI             | CLI-focused                  |\r\n\r\nLet's break down these differences:\r\n\r\n1. **Architecture**: Docker's client-server model relies on a central daemon, while Podman's daemonless approach offers a more distributed and potentially more secure setup.\r\n\r\n2. **Root privileges**: Podman's rootless mode is a significant advantage for security-conscious users, allowing containers to run without elevated permissions.\r\n\r\n3. **Container runtime**: While both use OCI-compliant runtimes, Docker's containerd and Podman's CRI-O have different focuses and strengths.\r\n\r\n4. **Orchestration**: Docker Swarm provides native clustering, while Podman integrates well with Kubernetes through its pod-based approach.\r\n\r\n5. **User experience**: Docker's GUI may be more appealing to beginners, while Podman's CLI-centric approach caters to advanced users and automation scenarios.\r\n\r\n### Recommendations for choosing between Docker and Podman\r\n\r\nSelecting the right containerization tool depends on your specific needs and circumstances. Here are some guidelines to help you make an informed decision:\r\n\r\n- **Choose Docker if**:\r\n\r\n  - You're new to containerization and prefer a user-friendly interface\r\n  - You need built-in orchestration with Docker Swarm\r\n  - You're working in a Windows environment (Docker has better Windows support)\r\n  - You require extensive third-party integrations and a large ecosystem\r\n\r\n- **Choose Podman if**:\r\n  - Security is a top priority, and you need rootless containers\r\n  - You're working in a Linux environment, especially Red Hat-based systems\r\n  - You prefer a daemonless architecture for better resource management\r\n  - You're planning to use Kubernetes for orchestration\r\n\r\nRemember, these recommendations are not set in stone. Many organizations successfully use both tools in different contexts. For example, Netflix has adopted Podman for some of its workflows, citing security benefits and compatibility with their existing systems.\r\n\r\n### The future of containerization and its impact on DevOps\r\n\r\nThe containerization landscape is constantly evolving, and several trends are shaping its future:\r\n\r\n1. **Increased focus on security**: As containers become more prevalent in production environments, security will remain a top priority. We can expect continued improvements in container isolation, vulnerability scanning, and secure supply chain management.\r\n\r\n2. **Serverless containers**: The rise of [serverless container platforms](https://aws.amazon.com/fargate/) like AWS Fargate and Azure Container Instances is blurring the lines between traditional containerization and serverless computing. This trend is likely to continue, offering developers more flexibility in how they deploy and scale applications.\r\n\r\n3. **Edge computing**: As edge computing gains traction, containerization will play a crucial role in deploying and managing applications across distributed environments. Both Docker and Podman are positioning themselves to support edge use cases.\r\n\r\n4. **Standardization**: The [Open Container Initiative (OCI)](https://opencontainers.org/) has already made significant strides in standardizing container formats and runtimes. We can expect further standardization efforts, making it easier to switch between different container tools and platforms.\r\n\r\n5. **AI and ML integration**: As artificial intelligence and machine learning become more prevalent in software development, we'll likely see better integration between containerization tools and AI/ML workflows, simplifying the deployment of complex models and data pipelines.\r\n\r\nThe impact of these trends on DevOps practices will be profound:\r\n\r\n- **Shift-left security**: With improved container security features, DevOps teams will be able to incorporate security earlier in the development process, aligning with the \"shift-left\" philosophy.\r\n\r\n- **Increased automation**: As containerization tools become more sophisticated, we'll see greater automation in building, testing, and deploying containerized applications, further streamlining the CI/CD pipeline.\r\n\r\n- **Skills evolution**: DevOps professionals will need to continually update their skills to keep pace with the evolving containerization landscape, including security best practices, serverless architectures, and edge computing.\r\n\r\n- **Cross-functional collaboration**: The lines between development, operations, and security teams will continue to blur, fostering greater collaboration and shared responsibility for containerized applications.\r\n\r\nAccording to a [recent survey by the Cloud Native Computing Foundation](https://www.cncf.io/reports/cncf-annual-survey-2021/), 96% of organizations are either using or evaluating Kubernetes, indicating the growing importance of container orchestration in modern IT environments. This trend underscores the need for containerization tools that integrate seamlessly with Kubernetes and other orchestration platforms.\r\n\r\nIn conclusion, while Docker and Podman may have different strengths and target audiences, both tools are driving innovation in the containerization space. As the technology continues to mature, we can expect to see even more exciting developments that will shape the future of software development and deployment.\r\n\r\nWhether you choose Docker, Podman, or a combination of both, embracing containerization is key to staying competitive in today's fast-paced tech landscape. By understanding the unique features and capabilities of each tool, you can make informed decisions that align with your organization's goals and technical requirements.\r\n\r\nAs we look to the future, it's clear that containerization will continue to play a pivotal role in shaping DevOps practices and enabling the next generation of cloud-native applications. Stay curious, keep learning, and don't be afraid to experiment with different tools and approaches as you navigate the exciting world of containerization.","src/content/posts/podman-vs-docker.mdx",[1841],"../../assets/images/24/07/podman-vs-docker.jpeg","39fb7779eea5781d","podman-vs-docker.mdx","responsive-youtube-astrojs",{id:1844,data:1846,body:1855,filePath:1856,assetImports:1857,digest:1859,legacyId:1860,deferredRender:32},{title:1847,description:1848,date:1849,image:1850,authors:1851,categories:1852,tags:1853,canonical:1854},"Add Responsive YouTube Videos to Astro.JS MDX","Lets see how easy is to add a responsive YouTube video to Astro.js",["Date","2022-10-20T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/2210/responsive-youtube-astro-blog.jpeg",[19],[21],[23,24],"https://www.bitdoze.com/responsive-youtube-astrojs/","import { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/2210/video-speed-score.jpeg\";\r\n\r\nI am using [Astro.js](https://astro.build/) to host this blog on Cloudflare Pages and an important part of the blog is to be responsive and be able to add YouTube videos to the posts.\r\n\r\nIn case you want to deploy your blog on Astro.js and Cloudflare Pages you can check this tutorial:[How To Deploy An Astro.JS Blog On Cloudflare](https://www.bitdoze.com/deploy-astrojs-cloudflare/)\r\n\r\nWith the new MDX move done by Astro you can easily embed YouTube videos to MDX files by installing an extension, here we will see exactly how. We are going to use [astro-embed](https://www.npmjs.com/package/astro-embed) extension for this.\r\nWith it, you can not only embed YouTube videos but you will also be able to embed Vimeo or Twitter.\r\n\r\n## Add Responsive YouTube Videos to Astro.JS MDX\r\n\r\n### 1. Install astro-embed\r\n\r\nIn case you don't have it installed in your project you can install it with:\r\n\r\n```bash\r\nnpm i astro-embed\r\n```\r\n\r\n### 2. Call astro-embed in MDX\r\n\r\nWith it installed you can now call it not only in the MDX files for your blog but you can call it also on the .astro files by adding the below in the files:\r\n\r\n```bash\r\nimport { Tweet, Vimeo, YouTube } from 'astro-embed';\r\n```\r\n\r\nIf you only have YouTube in the MDX file you can call only YouTube. The Row needs to be added under the MDX header like the below example:\r\n\r\n```mdx\r\n---\r\npublishDate: \"Sep 14 2022\"\r\ntitle: \"Monitor CPU Usage and Send Email Alerts in Linux\"\r\ndescription: \"Let's see how we can monitor CPU usage on a server and receive emails.\"\r\nexcerpt: \"Let's see how we can monitor CPU usage on a server and receive emails.\"\r\nimage: \"~/assets/images/cpu_monitoring.jpeg\"\r\ncategory: \"vps\"\r\ntags: [linux, tutorials]\r\n---\r\n\r\nimport { YouTube } from \"astro-embed\";\r\n\r\n;\r\n```\r\n\r\n### 3. Add the video to the body\r\n\r\nNow you should be able to use the video where you want in your MDX file. To add it you just add the link to it below:\r\n\r\n```js\r\n<YouTube id=\"https://youtu.be/NkShQ1wwiCg\" />\r\n```\r\n\r\nAnd it will look like below where you can follow the tutorial for this article:\r\n\r\n## Conclusions\r\n\r\nThe embed is responsive and fast as it will not use the iframe for this. Usually, when you add an iframe video from youtube to the article the speed score is going down a lot because of the scripts that need to be loaded. With this embed, it will not load all the YouTube scripts and will do so only on click. Below is the score:\r\n\r\n<Picture\r\n  src={imag1}\r\n  alt=\"YouTube Speed score\"\r\n/>","src/content/posts/responsive-youtube-astrojs.mdx",[1858],"../../assets/images/2210/responsive-youtube-astro-blog.jpeg","941e7983e6fb4a40","responsive-youtube-astrojs.mdx","redirect-docker-logs-to-a-single-file",{id:1861,data:1863,body:1872,filePath:1873,assetImports:1874,digest:1876,legacyId:1877,deferredRender:32},{title:1864,description:1865,date:1866,image:1867,authors:1868,categories:1869,tags:1870,canonical:1871},"How to Redirect Docker Logs to a Single File","Learn how to Redirect Docker Logs to a Single File to make your work easier",["Date","2023-07-03T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/07/redirect_docker_logs_single_file.jpeg",[19],[98],[100],"https://www.bitdoze.com/redirect-docker-logs-to-a-single-file","Managing logs is an essential aspect of working with Docker containers. By default, Docker sends logs from different containers to different places, making it challenging to keep track of them. In this article, I will guide you on how to redirect Docker logs to a single file, simplifying the process of log management and analysis.\r\n\r\nRedirecting Docker logs to a single file has several advantages. Firstly, it allows you to consolidate all the logs in one location, making it easier to search, analyze, and troubleshoot issues. Instead of navigating through multiple log files, you can focus on a single file, saving you time and effort. Additionally, having logs in a centralized location simplifies log collection for monitoring tools and ensures that important information is not scattered.\r\n\r\nTo achieve this, we will utilize the logging driver feature in Docker. We will configure Docker to use the JSON file logging driver, which enables us to direct logs from all containers to a single file. I will provide you with step-by-step instructions to modify the Docker daemon configuration and point the logs to a specific file path. Once you have set up the configuration, you can easily access and manage your Docker logs without any hassle.\r\n\r\nSo, let's dive in and learn how to redirect Docker logs to a single file, enhancing the efficiency and convenience of managing your containers' logs. Before we begin, ensure that you have administrative access and a basic understanding of Docker. Now, let's get started with the necessary steps to simplify your log management process.\r\n\r\nSome other docker articles that can help you in your docker journey:\r\n\r\n- [Add Users to a Docker Container](https://www.bitdoze.com/add-users-to-docker-container/)\r\n- [Copy Multiple Files in One Layer Using a Dockerfile](https://www.bitdoze.com/copy-multiple-files-in-one-layer-using-a-dockerfile/)\r\n- [Install Docker & Docker-compose for Ubuntu ARM](https://www.bitdoze.com/install-docker-ubuntu-arm/)\r\n- [Environment Variables ARG and ENV in Docker](https://www.bitdoze.com/docker-env-vars/)\r\n\r\n## Understanding Docker Logging\r\n\r\nDocker logging is a crucial aspect of container management. It allows you to track and analyze the events and output generated by your Docker containers. By default, Docker logs are stored in the container's standard output/error streams, making it difficult to consolidate and manage them efficiently. In this section, we'll explore the importance of understanding Docker logging and how to redirect the logs to a single file.\r\n\r\n### Why is Docker Logging Important?\r\n\r\nEfficient logging is vital for various reasons, including:\r\n\r\n- **Troubleshooting**: Logs provide valuable insights into container behavior, helping identify issues and their root causes.\r\n- **Monitoring**: By analyzing Docker logs, you can gain visibility into resource utilization, application performance, and security incidents.\r\n- **Compliance and Auditing**: Logging plays a significant role in meeting compliance requirements and facilitating auditing processes.\r\n\r\n### Redirecting Docker Logs to a Single File\r\n\r\nTo centralize Docker logs into a single file, you can leverage the Docker logging drivers. One popular option is the `json-file` driver, which writes logs in JSON format to a file on the host machine.\r\n\r\nTo redirect Docker logs to a single file, follow these steps:\r\n\r\n1. Create a new file on the host machine that will store the consolidated logs. For example, let's call it `/var/log/docker.log`.\r\n\r\n2. Update the Docker daemon configuration file (`daemon.json`) to specify the logging driver and the log file location:\r\n\r\n   ```json\r\n   {\r\n     \"log-driver\": \"json-file\",\r\n     \"log-opts\": {\r\n       \"max-size\": \"10m\",\r\n       \"max-file\": \"3\",\r\n       \"labels\": \"production_status\",\r\n       \"env\": \"os,customer\"\r\n     }\r\n   }\r\n   ```\r\n\r\n   In the above configuration, we have set the maximum log file size to 10 megabytes (`max-size`) and the maximum number of log files to 3 (`max-file`). The `labels` and `env` options allow you to include additional metadata in the log entries.\r\n\r\n3. Restart the Docker daemon for the changes to take effect:\r\n   ```shell\r\n   $ sudo systemctl restart docker\r\n   ```\r\n\r\nNow, Docker logs will be redirected to the specified file on the host machine, making it easier to manage and analyze them.\r\n\r\n**Note**: There are several other logging drivers available in Docker, such as `syslog`, `journald`, and `gelf`. Choose the one that best fits your needs based on the logging requirements and infrastructure setup.\r\n\r\nRemember that understanding Docker logging is crucial for efficient troubleshooting, monitoring, and compliance. Redirecting Docker logs to a single file simplifies log management and enables you to gain valuable insights from your containerized applications.\r\n\r\n## Default Docker Logging Behavior\r\n\r\nWhen it comes to Docker logging, understanding the default behavior is crucial. Let's dive into what happens out of the box:\r\n\r\n- By default, Docker captures both standard output (stdout) and standard error (stderr) streams from containers.\r\n- The logs are stored on the local filesystem in JSON format.\r\n- Each container's log is saved in a separate file under the `/var/lib/docker/containers` directory.\r\n- Docker provides a command-line interface (CLI) to access these logs.\r\n\r\nNow, let's break it down further:\r\n\r\n### Log Storage Format\r\n\r\nDocker saves logs in the JSON format, which offers structured information about each log entry. This format allows for easy parsing and analysis, providing flexibility in extracting meaningful insights.\r\n\r\n### Separation by Containers\r\n\r\nOne important aspect of Docker's logging approach is its segregation of logs by containers. Each container's log is stored in a separate file. This separation ensures that logs from different containers are isolated and easily identifiable, simplifying debugging and troubleshooting.\r\n\r\n### Log Location\r\n\r\nBy default, Docker saves the log files in the `/var/lib/docker/containers` directory. However, keep in mind that this location may vary depending on your operating system and Docker configuration. It's important to reference the appropriate directory when examining or managing container logs.\r\n\r\n### Command-Line Access\r\n\r\nDocker offers a user-friendly CLI to interact with container logs. This CLI allows you to view, search, and monitor logs conveniently. With just a few commands, you can access container-specific logs or even tail logs in real-time. This accessibility proves valuable when analyzing the behavior of your containers.\r\n\r\nNow that we understand the default Docker logging behavior, let's explore how we can redirect the logs to a single file for easier management and analysis in the next section.\r\n\r\n## Redirecting Docker Logs to a Single File\r\n\r\nRedirecting Docker logs to a single file can help you centralize your logs, making it easier to analyze and troubleshoot issues. In this section, we will explore a simple approach to achieving this.\r\n\r\n### Step 1: Create a Logs Directory\r\n\r\nFirst, you need to create a directory to store your Docker logs. You can choose any location that suits your needs. Let's say we create a directory called `docker-logs` in our home directory.\r\n\r\n```bash\r\nmkdir ~/docker-logs\r\n```\r\n\r\n### Step 2: Configure Docker Logging Driver\r\n\r\nNext, we need to configure Docker to use the `json-file` logging driver and specify the log file location.\r\n\r\nOpen the Docker daemon configuration file using your preferred text editor:\r\n\r\n```bash\r\nsudo nano /etc/docker/daemon.json\r\n```\r\n\r\nAdd the following configuration:\r\n\r\n```json\r\n{\r\n  \"log-driver\": \"json-file\",\r\n  \"log-opts\": {\r\n    \"max-size\": \"10m\",\r\n    \"max-file\": \"3\",\r\n    \"path\": \"/home/<your-username>/docker-logs/docker.log\"\r\n  }\r\n}\r\n```\r\n\r\nReplace `<your-username>` with your actual username. This configuration sets a maximum log file size of 10 megabytes and keeps up to 3 rotated log files.\r\n\r\nSave the changes and restart the Docker daemon for the changes to take effect:\r\n\r\n```bash\r\nsudo systemctl restart docker\r\n```\r\n\r\n### Step 3: Verify Log Redirection\r\n\r\nTo verify if the log redirection is working correctly, you can start a Docker container and check if the logs are being written to the specified file.\r\n\r\nFor example, run the following command to start a container from an image:\r\n\r\n```bash\r\ndocker run -d --name my-container nginx\r\n```\r\n\r\nNow, check the contents of the log file:\r\n\r\n```bash\r\ntail -f ~/docker-logs/docker.log\r\n```\r\n\r\nYou should see the logs from the `my-container` container appearing in the file.\r\n\r\n### Summary\r\n\r\nRedirecting Docker logs to a single file is a practical way to consolidate and manage your container logs more efficiently. By following these simple steps, you can easily configure Docker to store logs in a centralized location, aiding in troubleshooting and analysis.\r\n\r\nRemember to adjust the log file location and rotation settings in the Docker daemon configuration file to suit your specific requirements. Happy logging!\r\n\r\n## Configuring the Docker Logging Driver\r\n\r\nWhen it comes to managing Docker logs, configuring the logging driver plays a vital role. By specifying the appropriate logging driver, you can redirect and consolidate your logs into a single file for easy monitoring and analysis. Here's a step-by-step guide to configuring the Docker logging driver:\r\n\r\n1. **Check the Available Logging Drivers**\r\n\r\nFirst, let's ensure you are familiar with the logging drivers supported by Docker. Run the following command to get a list of available logging drivers:\r\n\r\n```\r\ndocker info --format '{{.LoggingDriver}}'\r\n```\r\n\r\n2. **Update the Docker Daemon Configuration**\r\n\r\nOpen the Docker daemon configuration file, usually located at `/etc/docker/daemon.json`, and add or modify the `log-driver` key with your desired logging driver. For example, to use the `json-file` driver, your configuration file should include the following:\r\n\r\n```json\r\n{\r\n  \"log-driver\": \"json-file\"\r\n}\r\n```\r\n\r\n3. **Configure Logging Options**\r\n\r\nBased on your logging driver choice, you may want to configure additional options. For instance, the `json-file` driver offers options like `max-size` and `max-file`. These options allow you to control the size and number of log files generated. Refer to the Docker documentation for your specific logging driver to learn more about available options.\r\n\r\n4. **Restart the Docker Daemon**\r\n\r\nAfter updating the Docker daemon configuration file, you need to restart the Docker daemon for the changes to take effect. Use the following command to restart the Docker daemon:\r\n\r\n```\r\nsudo systemctl restart docker\r\n```\r\n\r\n5. **Verify the Logging Configuration**\r\n\r\nTo ensure the logging driver has been successfully configured, you can inspect a running container and check its logging driver configuration. Execute the following command to view the container's logging driver:\r\n\r\n```\r\ndocker inspect --format '{{.HostConfig.LogConfig.Type}}' <container-id>\r\n```\r\n\r\nReplace `<container-id>` with the ID or name of the container you want to inspect.\r\n\r\nThat's it! By properly configuring the Docker logging driver, you can conveniently redirect your logs to a single file, making it easier to manage and analyze your container logs. Remember to choose the logging driver that best fits your needs and utilize any additional options it provides.\r\n\r\n## Best Practices for Docker Logging\r\n\r\nWhen it comes to Docker logging, there are a few best practices that can help you manage and analyze your container logs effectively. Here are some tips to consider:\r\n\r\n1. **Keep logs inside the containers**: Storing logs within the containers themselves allows for easy access and portability. It simplifies the process of moving containers between different environments or sharing them with others.\r\n\r\n2. **Use centralized logging**: Instead of relying on individual log files in each container, it's recommended to centralize your logs. By sending logs to a single location, you can gather and analyze them more efficiently. Popular logging solutions like Elasticsearch, Logstash, and Kibana (ELK stack) or Prometheus and Grafana can help you achieve centralized logging.\r\n\r\n3. **Structure your logs**: Adding structure to your logs using a structured logging framework like JSON or key-value pairs can make them more understandable and searchable. This makes it easier to extract relevant information and perform analysis.\r\n\r\n4. **Include contextual information**: Ensure that your logs include relevant contextual information such as timestamps, container names, and unique identifiers. This information can help in troubleshooting, tracking issues, and correlating logs across different services.\r\n\r\n5. **Consider log rotation**: Log rotation is essential to manage the size and retention of your log files. Implement a log rotation mechanism that automatically rotates logs based on size or time. This prevents logs from consuming excessive storage and allows for better log management.\r\n\r\n6. **Implement log monitoring**: Set up log monitoring to get real-time alerts for critical events or errors. This enables you to identify and respond to issues promptly, improving the overall stability and performance of your Docker containers.\r\n\r\nRemember, adopting these best practices can enhance your Docker logging experience, making it easier to troubleshoot, monitor, and analyze your containerized applications.\r\n\r\n| Best Practices for Docker Logging |\r\n| --------------------------------- |\r\n| Keep logs inside the containers   |\r\n| Use centralized logging           |\r\n| Structure your logs               |\r\n| Include contextual information    |\r\n| Consider log rotation             |\r\n| Implement log monitoring          |\r\n\r\n## Conclusion\r\n\r\nIn conclusion, redirecting Docker logs to a single file can greatly simplify managing and analyzing logs in your Docker environment. By centralizing your logs, you can easily monitor and troubleshoot your containers, ensuring that everything is running smoothly. Here's a summary of the key takeaways:\r\n\r\n- **Simplicity**: Redirecting Docker logs to a single file reduces the complexity of log management. You no longer need to search through multiple log files or navigate through various directories to find the information you need.\r\n\r\n- **Efficiency**: Having all your logs in one place improves efficiency when troubleshooting issues. Instead of opening multiple terminals or SSH sessions, you can focus on analyzing the consolidated log file, saving time and effort.\r\n\r\n- **Convenience**: A single log file makes it easier to perform searches, apply filters, and extract meaningful insights from your logs. You can quickly identify patterns, detect anomalies, and gain a better understanding of your containerized applications.\r\n\r\n- **Scalability**: As your Docker environment grows, managing logs becomes more challenging. Redirecting logs to a single file ensures that log management remains scalable, regardless of the number of containers or hosts you have.\r\n\r\n- **Standardization**: By adopting a centralized log file, you can establish a standardized approach to log management across your Docker ecosystem. This consistency simplifies the overall operation and maintenance of your infrastructure.\r\n\r\nBy following the steps outlined in this article, you can redirect your Docker logs to a single file effortlessly. Remember, centralized log management is not only a good practice but also a necessity to ensure the smooth operation of your containerized applications. Happy logging!","src/content/posts/redirect-docker-logs-to-a-single-file.mdx",[1875],"../../assets/images/23/07/redirect_docker_logs_single_file.jpeg","82397b3728f8334e","redirect-docker-logs-to-a-single-file.mdx","screen-studio-review",{id:1878,data:1880,body:1890,filePath:1891,assetImports:1892,digest:1894,legacyId:1895,deferredRender:32},{title:1881,description:1882,date:1883,image:1884,authors:1885,categories:1886,tags:1887,canonical:1889},"Screen.Studio Review - Revolutionize Your Video Content"," Screen.Studio Review, see how this Mac screen recording tool can help you make more engaging videos",["Date","2023-04-04T05:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/04/screen_studio_review.jpeg",[19],[77],[1888],"screen recording","https://www.bitdoze.com/screen-studio-review/","import YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport img1 from \"../../assets/images/23/04/screen_studio_1.jpg\";\r\nimport img2 from \"../../assets/images/23/04/screen_studio_export.jpeg\";\r\n\r\n[Screen Studio](https://go.bitdoze.com/screen-studio) is a screen recorder for macOS devices that can help you create more engaging videos for your products or social media accounts. In this article, we are going to do a review of Screen Studio to see what exactly it has to offer and if it is the right one for you.\r\n\r\nI have been using [Screen Studio](https://go.bitdoze.com/screen-studio) for a few months now to record the videos on my YouTube channel and I have tested most of the features. What I can say is that I really like the product as it helps me make more engaging videos faster, Screen Studio is developed by [Adam Pietrasiak](https://twitter.com/pie6k) who has added a lot of functionalities into the product in just couple of months. At first you couldn't record sound or the camera, but now it's all possible.\r\n\r\n<Button\r\n  link=\"https://go.bitdoze.com/screen-studio\"\r\n  text=\"Check Screen Studio\"\r\n/>\r\n\r\n## Screen.Studio Screen Recorder Review\r\n\r\n### Screen.Studio Video Review\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/J9zGVtfMQi0\"\r\n  label=\"Screen.Studio Review\"\r\n/>\r\n\r\nIn this section we will see the exact features that Screen Studio has to offer as well as the pros and cons that I see after using it for a couple of months.\r\n\r\n### Features\r\n\r\nBelow are the main features of this screen recorder.\r\n\r\n- **Zoom Effects** - Screen.studio will add zoom effects when you click the mouse, you can change the animations in the editor between slow, smooth, fast and fast.\r\n- **Wallpaper & Window Frame** - You can add backgrounds to your videos, you can add wallpaper, image, color or gradient.\r\n- **Record Camera** - Screen Studio has the option to record the camera on your system, it has a nice anymation when you zoom in and you can change the position.\r\n- **Record Sound** - Screen Studio will record your microphone sound.\r\n- **Subtitle Generation** - within the editor you have the option to autogenerate the subtitles for your recording automatically, you can edit some basic things.\r\n- **Capture in Different Formats** - you can easily convert your video to different formats, you have the option for 16:9, 9:16, 1:1, 4:3, there are also nice animations for 9:16 that will help you to create an appealing video.\r\n\r\nThe Screen Studio editor will allow you to edit the video and add zooms or mouse tracking, converting the video to any size you want. Screen Studio is a complete video editor that will help you make nicer videos with little effort.\r\n\r\n### Pricing\r\n\r\nYou can download and install Screen Studio for free to see exactly what it has to offer, but you will not be able to export the video. The standard plan for Screan Studio starts at $89 for 1 device where you get 1 year of updates and you can use it forever. If you need it for more devices you have the $189 plan for 3 MacOS devices. The prices are not bad at all if you think that you can use it for as long as you want.\r\n\r\n### Support\r\n\r\nI've been using Screen.studio for a couple of months and I haven't had any problem with it, in case you have problems I've seen that Adam is quick to fix them and it's also listening to new future request. Overall Screen Studio has a nice support and is updated regularly.\r\n\r\n<Button\r\n  link=\"https://go.bitdoze.com/screen-studio\"\r\n  text=\"Check Screen Studio\"\r\n/>\r\n\r\n### UI Design\r\n\r\nScreen Studio has a nice UI and is similar to what you see in other editor tools. On the left you have the video and on the right, you have the options. Also, the recording dock it's allows you to choose from the full screen or just a window. The only downside is the fact that when you record longer videos you can only zoom out and not have a slide to go to that timeline. This makes editing larger videos more difficult as you can't see the effects as well and they are crammed together.\r\n\r\n<Picture\r\n  src={img1}\r\n  alt=\"Screen Studio Interface\"\r\n/>\r\n\r\n### Exporting a Video & Performance\r\n\r\nAfter you finish editing the video you will go to the Export section where you can export a video in GIF or mp4 format. You can choose different resolutions like 1080p or 4k and choose the quality from soclial media or studio or lower if you want.\r\n\r\nI have a MacBook AIR M1 and if you record a longer video it will take some time to have it exported, usually for me it takes about 3 times the video length, so if I have a 10 minute video the export will take about 30 minutes to finish. This is slower than other video editors, but I think this is due to the way Screen Studio captures videos. In the latest updates it looks like things have improved from a performance perspective.\r\n\r\n<Picture\r\n  src={img2}\r\n  alt=\"Screen Studio Export\"\r\n/>\r\n\r\n### What I like and what Screen Studio can do Better\r\n\r\n#### Strong points\r\n\r\n- **Beautiful Zoom Animations** - Screen Studio's animations look pretty nice and it will help you to make an engaging video easier.\r\n- **Camera Animations** - changing the camera size when zooming in and out gives a more dynamic perspective to the video.\r\n- **Creates Shorts/TikTok Videos** - with the fact that the 9:16 format has smooth animations it can help in crating quick tutorials for shorts and be engaging out of the box.\r\n- **Nice and Easy UI** - editing the video and adding various effects is easy and can be done by anyone.\r\n\r\n#### Where Screan.Studio Do Better\r\n\r\n- **Slider for Timeline** - You can only zoom in and out for the timeline, this is good if you only do small videos, if you have more than 10 minutes it can be difficult to edit the video. I wish there was a slider for the timeline.\r\n- **Slow Export** - I know Adam is working on improving this but on my MacBook AIR M1 the export can take a long time for a 30 minute video, it can take up to 2 hours which is quite high.\r\n\r\n## Conclusions\r\n\r\n[Screen Studio](https://go.bitdoze.com/screen-studio) is a wandering product that can help you make engaging videos easily without too much editing skills. I feel that Screen Studio is perfect for those who need to make small videos for their products or create tutorial YouTube shorts or TikToks. For longer content videos you should be aware that it takes some time to export the video, but if you are not in a hurry like me it is not a problem.\r\n\r\n<Button\r\n  link=\"https://go.bitdoze.com/screen-studio\"\r\n  text=\"Check Screen Studio\"\r\n/>","src/content/posts/screen-studio-review.mdx",[1893],"../../assets/images/23/04/screen_studio_review.jpeg","a8c8a6836437c439","screen-studio-review.mdx","sed-change-case",{id:1896,data:1898,body:1908,filePath:1909,assetImports:1910,digest:1912,legacyId:1913,deferredRender:32},{title:1899,description:1900,date:1901,image:1902,authors:1903,categories:1904,tags:1905,canonical:1907},"Text Case Transformations with Sed: Master Advanced Techniques","Learn how to master the power of sed Linux command to do case transformations in text.",["Date","2023-11-24T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/sed-change-case.jpeg",[19],[98],[1906,449],"sed","https://www.bitdoze.com/sed-change-case/","Ever found yourself sifting through a mountain of text files, wishing you could magically switch cases without breaking a sweat? Well, with the sed command, a staple in the Unix toolkit, I'll show you how that magic's at your fingertips. Whether you're a seasoned programmer or just starting out, mastering this nifty trick can save you hours of tedious editing.\r\n\r\n## What Is the Sed Command?\r\n\r\nThe **sed command**, an acronym for **Stream Editor**, is a potent utility in the Unix toolkit known for its stream manipulation capabilities. It's designed to filter and transform text using a simple yet comprehensive scripting language. Born in the 1970s as one of the earliest tools for text processing on Unix, it's stood the test of time, proving to be invaluable for countless scripts, automations, and daily tasks that involve text processing.\r\n\r\nAs I delve deeper into its functionalities, it's clear why **sed** is the choice tool for programmers looking to automate editing tasks. It reads from a file or a stream, performs operations as instructed by the user, and outputs the result into a new stream, making it ideal for modifying files in-place or for piping output in a shell script.\r\n\r\nOne of sed's strongest features is its ability to perform complex pattern matching. It uses regular expressions, offering a level of precision in search and replace functions that's hard to match with basic text editors. What sets it apart is the ease with which it can handle large files and its suitability for both interactive and batch processing.\r\n\r\nCommands in sed are instructed through scripts. These scripts are composed of one or more **commands** that tell sed what to do with the lines of text. Understanding how to write these scripts is crucial, especially when tackling tasks such as case conversion, because it requires knowledge of both regex patterns and sed's own syntax for the substitute command, which is integral to changing case.\r\n\r\nMoreover, the sed command works seamlessly with other Unix utilities, making it perfect for shell scripting. It can accept input from a pipe, work its magic, and then pass the results to another command, fitting neatly into the Unix philosophy of small, modular tools that do one thing well.\r\n\r\nBy leveraging sed, I've been able to automate complex editing across large text files. It is particularly efficient for batch-processing operations, where performing the equivalent actions in a standard text editor would be prohibitively time-consuming.\r\n\r\nWith sed, you've got a robust toolkit for any sort of text manipulation you might need to perform:\r\n\r\n- [Delete lines](https://www.bitdoze.com/sed-delete-lines/) using the `d` command\r\n- [Insert or append text with](https://www.bitdoze.com/sed-insert-append-text/) `i` and `a` commands\r\n- [Transform text by changing case](https://www.bitdoze.com/sed-change-case/)\r\n- [Search and replace text](https://www.bitdoze.com/sed-search-replace/)\r\n\r\n## Why Is Changing Case Important?\r\n\r\nIn countless scenarios within the realm of text processing, altering text case is of paramount importance. It can dramatically affect the way information is perceived and processed, both by humans and by computer systems. Case sensitivity plays a crucial role in programming languages, where changing the case could mean the difference between a functioning code and a syntax error.\r\n\r\n**Consistency** is key in any form of documentation or data entry. Whether it's transforming data to follow a uniform format, or adhering to coding standards that mandate a specific case usage for variables and function names, the ability to change text case swiftly is invaluable. For example, in a database of names, ensuring that all entries follow a consistent capitalization pattern improves readability and data integrity.\r\n\r\nWithin realm of SEO, text case can also **influence user experiences**. A title with proper case looks more professional and is more likely to catch the reader’s attention, thus impacting click-through rates. On websites, headings and buttons are often formatted using specific case rules to guide user interactions and maintain brand consistency across web pages.\r\n\r\nWhen processing large text files, manual case adjustments can be tedious and prone to errors. This is where the power of **sed command** comes into play, enabling these changes to be automated and applied across vast swathes of text with precision. **Sed’s** pattern matching ability combined with case transformation commands can align text data with required standards, eliminate inconsistencies, and prepare text for further processing or analysis.\r\n\r\nMaintaining accurate case is not only a matter of aesthetics but also affects the functionality in case-sensitive contexts. In configuration files and scripting, even a single mis-cased letter can result in a failure to execute commands or a misinterpretation of instructions. This emphasizes the significance of mastering tools like **sed** for effective case transformation, especially when dealing with Linux or Unix environments where such details hold great weight in system configurations and scripting tasks.\r\n\r\n## How to Use the `sed` Command to Change Case?\r\n\r\nWhen it's time to dive into changing the text case with the **sed** command, I've found it extremely powerful thanks to its versatility. sed, which stands for **stream editor**, is one of the most proficient tools available for text processing in Linux and Unix environments. It operates by processing text line by line, allowing for quick and efficient text manipulations.\r\n\r\nTo change text to lowercase using sed, you'd use the `y` command, which stands for \"translate\". Here's an example:\r\n\r\n```sh\r\necho \"HELLO WORLD\" | sed 'y/ABCDEFGHIJKLMNOPQRSTUVWXYZ/abcdefghijklmnopqrstuvwxyz/'\r\n```\r\n\r\nThis command translates all uppercase letters to lowercase. When executed, it'll output `hello world`. Simple yet effective, isn't it?\r\n\r\nFor converting text to uppercase, you reverse the translate command's letter ranges like so:\r\n\r\n```sh\r\necho \"hello world\" | sed 'y/abcdefghijklmnopqrstuvwxyz/ABCDEFGHIJKLMNOPQRSTUVWXYZ/'\r\n```\r\n\r\nExpect to see `HELLO WORLD` as the result. It's crucial to cover the whole alphabet to ensure **consistent transformations**.\r\n\r\nSometimes, you might only want to change the case of certain words or patterns. With sed's ability to support **regular expressions**, targeting specific text becomes a breeze. For example, to change the case of the word 'linux' wherever it appears in a file:\r\n\r\n```sh\r\nsed 's/linux/LINUX/g' filename.txt\r\n```\r\n\r\nThe `s` stands for \"substitute,\" and the `g` is for \"global,\" meaning the change applies to all instances of the pattern. Keep in mind though, for more complex patterns, you might need to fine-tune your regex skills.\r\n\r\nWhile sed is mighty, it doesn't have a built-in command to perform mixed case transformations like capitalizing the first letter of each word. For such tasks, awk or Perl might be better suited. That said, for straightforward case changes, sed is quick, easy to use, and extremely efficient, especially in scripts where automating text case conversion is a critical step.\r\n\r\n> Remember, mastering these commands is not just about **making the text look good**. It’s about enhancing **readability**, **maintaining consistency**, and **facilitating better user experiences**.\r\n\r\n## Changing Case in a Single File\r\n\r\nWhen working with text files in Linux, there's often a need to adjust the letter case within those files. The sed command provides a straightforward solution for case conversion tasks. I'll introduce methods to manipulate the case of text inside a single file using sed, showcasing how versatile and efficient it can be.\r\n\r\n**Using sed to Convert Text to Lowercase in a File** Let's start by transforming all the text in a file to **lowercase**. This can be particularly useful for standardizing data or preparing files for case-insensitive comparisons.\r\n\r\nHere's a simple command:\r\n\r\n```sh\r\nsed 's/\\[A-Z\\]/\\\\L&/g' filename.txt &gt; newfilename.txt\r\n```\r\n\r\nThis command searches for all uppercase letters and converts them to lowercase. The '\\\\L' is what signals sed to switch to lowercase until the end of each matched pattern—and the 'g' flag tells sed to apply this to all matches in the line.\r\n\r\n**Transforming Text to Uppercase with sed** Now suppose you have the reverse requirement—changing all text to **uppercase**. Here's how it's done:\r\n\r\n```sh\r\n sed 's/\\[a-z\\]/\\\\U&/g' filename.txt &gt; newfilename.txt\r\n```\r\n\r\nIn this case, the '\\\\U' instructs sed to turn the matching pattern to uppercase. This method seamlessly processes an entire file, ensuring consistency and uniformity throughout.\r\n\r\n**Applying Case Changes to Specific Patterns** What if you only want to change the case of certain words or patterns? No problem—sed and regular expressions make it a breeze. For instance:\r\n\r\n```sh\r\nsed '/pattern/s/\\[a-z\\]/\\\\U&/' filename.txt &gt; newfilename.txt\r\n```\r\n\r\nThis command will only capitalize patterns that match 'pattern', which is incredibly handy for targeting specific data within your text files.\r\n\r\nIt's clear that mastering sed commands is invaluable for text processing tasks. With a few simple commands, you can alter the case of your text data in files, tailor your approach to specific needs, and significantly improve your text-processing workflow. Remember that regular expressions are your friend when making more selective changes, offering precise control over the content you're adjusting.\r\n\r\n## Changing Case in Multiple Files\r\n\r\n**Batch processing** in Linux is a common task, and when you're dealing with multiple files, sed's versatility shines through. I'll walk you through how to use sed to change case across numerous files, making the task efficient and less time-consuming.\r\n\r\nFor starters, you can use a simple for loop to apply a sed command to multiple files in a directory. Here's an example that converts all text to lowercase in every .txt file in the current folder:\r\n\r\n```sh\r\nfor file in _.txt; do sed -i 's/(._)/\\\\L\\\\1/' \"$file\" done\r\n```\r\n\r\nThis loop iterates over each .txt file, invoking sed with -i to modify the file in place. The sed script uses `\\L` to convert matched patterns to lowercase. For uppercase, replace `\\L` with `\\U`.\r\n\r\nHowever, when working with a more complex directory structure or specific file types, find combined with xargs provides a powerful solution. With find, you can search for files that match certain criteria and pass them to sed through xargs:\r\n\r\n```sh\r\nfind . -type f -name '_.txt' -exec sed -i 's/(._)/\\\\U\\\\1/' {} +\r\n```\r\n\r\nThis command looks for all .txt files from the current directory downwards, applying the uppercase transformation to each file.\r\n\r\nWhen it comes to special situations, like **conditional case-changing**, where only certain lines or words need modification, combining grep with sed is particularly useful. Here's how you can selectively change the case of lines containing a specific keyword:\r\n\r\n```sh\r\ngrep -rl 'keyword' ./ | xargs sed -i '/keyword/s/(.\\*)/\\\\L\\\\1/'\r\n```\r\n\r\nThis command first identifies files with the keyword, then pipes them into sed to transform only the matching lines to lowercase.\r\n\r\n> Remember, it's vital to back up your files before running batch operations, as sed's in-place editing is irreversible.\r\n\r\nSed and shell scripting together offer a robust way to alter the case of text across multiple files efficiently. Whether you're handling a handful of documents or thousands, these methods scale to meet your needs, ensuring consistency and improving readability without a hefty time investment.\r\n\r\n## Conclusion\r\n\r\nMastering the sed command to change text case is an essential skill for anyone working in a Linux or Unix environment. I've shown you how to transform text within files, tackle batch processing, and even combine tools like grep for targeted modifications. Remember, while sed is incredibly useful for simple tasks, don't hesitate to explore awk or Perl for more complex scenarios. With these skills in your toolkit, you'll be well-equipped to ensure your text data is clean, consistent, and user-friendly. Whether you're working on a single file or managing multiple documents, the flexibility of sed makes it a go-to resource for efficient text processing.\r\n\r\n## Some Useful Tips\r\n\r\n### How can I use the sed command to change text case?\r\n\r\nTo change text case using the `sed` command, you can use the `s` command along with regular expressions. For example, to convert text to lowercase, you can use `sed 's/.*/\\L&/' filename`. To convert text to uppercase, you can use `sed 's/.*/\\U&/' filename`.\r\n\r\n### Are there any limitations to using the sed command for case changes?\r\n\r\nWhile the `sed` command is effective for simple case changes, more complex transformations may require other tools like awk or Perl. It's important to understand the limitations and consider alternative approaches for specific needs.\r\n\r\n### How can I change case within a single file using sed?\r\n\r\nTo change case within a single file using `sed`, you can use the same `s` command with regular expressions. For example, to convert text to lowercase, use `sed 's/.*/\\L&/' filename`. To convert text to uppercase, use `sed 's/.*/\\U&/' filename`.\r\n\r\n### Can I change case in multiple files using sed?\r\n\r\nYes, you can change case in multiple files using `sed`. You can combine `find` and `xargs` commands with `sed` to search for and modify files. For example, to convert text to lowercase in all .txt files in a directory, use `find . -type f -name \"*.txt\" | xargs sed -i 's/.*/\\L&/'`.\r\n\r\n### How can I selectively change the case of lines containing specific keywords?\r\n\r\nTo selectively change the case of lines containing specific keywords, you can use `grep` and `sed` together. For example, to convert text to uppercase in lines containing the word \"example\", use `grep \"example\" filename | sed 's/.*/\\U&/'`.\r\n\r\n### How can mastering sed commands benefit text processing tasks?\r\n\r\nMastering `sed` commands can enhance readability, maintain consistency, and improve user experiences. It provides a powerful tool for manipulating text and allows you to tailor the approach based on your specific needs.","src/content/posts/sed-change-case.mdx",[1911],"../../assets/images/23/11/sed-change-case.jpeg","4896ec31e929a03c","sed-change-case.mdx","sed-insert-append-text",{id:1914,data:1916,body:1925,filePath:1926,assetImports:1927,digest:1929,legacyId:1930,deferredRender:32},{title:1917,description:1918,date:1919,image:1920,authors:1921,categories:1922,tags:1923,canonical:1924},"Insert or Append Text with Sed: Master Advanced Techniques","Learn how to master the power of sed Linux command to insert or append Text.",["Date","2023-11-24T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/sed-insert-append-text.jpeg",[19],[98],[1906,449],"https://www.bitdoze.com/sed-insert-append-text/","In this article, I'll guide you through the ins and outs of inserting and appending text in files using sed. You'll learn how to effortlessly add lines of text exactly where you need them, making your scripts more powerful and your life a whole lot easier. Let's dive in and unlock the full potential of this command-line powerhouse.\r\n\r\n## What Is sed?\r\n\r\nImagine you're sitting at your desk, a dozen text files opened, each requiring similar updates. Manually editing would be a painstaking task. That's where **sed**, or the _stream editor_, swoops in to save the day. It's a powerful tool that operates right from the command line, designed specifically for parsing and transforming text in data streams and files.\r\n\r\nsed is not just any old text editor. It's non-interactive which means it doesn't require constant user input to make changes. Instead, it reads text line by line, applies specified changes, and outputs the result. This can be incredibly useful for automated editing tasks or when working with large text files where manual editing isn't practical.\r\n\r\nThe power of sed lies in its simplicity and efficiency. With a single command, you could search for a string of text, replace it, insert new lines, or append to the file without ever opening a text editor. Its syntax might seem daunting at first, but once you grasp the basic structure, you'll be adding custom text anywhere in files with a level of precision that's hard to achieve with other tools.\r\n\r\n**Commands** in sed are passed as 'scripts' that define the actions to be performed. These scripts can be as simple as substituting one word for another or as complex as executing a series of related transformations. The typical command structure looks something like this: `sed 's/original/replacement/' filename`. This example would search for the word “original” in the file and replace it with “replacement”.\r\n\r\nWith sed, you've got a robust toolkit for any sort of text manipulation you might need to perform:\r\n\r\n- [Delete lines](https://www.bitdoze.com/sed-delete-lines/) using the `d` command\r\n- [Insert or append text with](https://www.bitdoze.com/sed-insert-append-text/) `i` and `a` commands\r\n- [Transform text by changing case](https://www.bitdoze.com/sed-change-case/)\r\n- [Search and replace text](https://www.bitdoze.com/sed-search-replace/)\r\n\r\nBy harnessing these commands, I'll help simplify your text-processing tasks, allowing you to focus more on your actual work and less on the tedious aspect of editing files. Let's delve into how to accomplish specific tasks such as inserting and appending text using sed’s powerful commands.\r\n\r\n## Inserting Text With Sed\r\n\r\nWhen I'm working with text files and need to insert content at a specific line, **sed** is my go-to utility. It's a staple in my command-line toolkit because of its speed and versatility. The simplicity of inserting text with sed is appealing, especially when handling batch scripts or complicated file edits.\r\n\r\nThe basic command structure for inserting text is straightforward:\r\n\r\n```sh\r\nsed 'NUMBERiTEXT' FILENAME\r\n```\r\n\r\nLet's say you have a text file named 'example.txt' and you want to insert \"This is the inserted line.\" before line 4. The sed command would look like this:\r\n\r\n```sh\r\nsed '4iThis is the inserted line.' example.txt\r\n```\r\n\r\n> It's crucial to remember that sed operates with line numbers and you'll need to specify where the new text should go.\r\n\r\nBut what if your requirement isn't so straightforward? Perhaps you need to insert multiple lines. In that case, you'll use the newline character `\\n` to separate lines. You'll wrap the text in double quotes and use a backslash to continue the insertion:\r\n\r\n```sh\r\nsed \"4iThis is the first line.\\nAnd this is the second line.\" example.txt`\r\n```\r\n\r\nFor custom scripts or when working on a Unix-like system, you might come across the need to insert text after matching a pattern. This is where _sed_ really shines, offering functionality that other text editors struggle to match. You can combine sed with regular expressions to locate a pattern and insert the text right after it. The command modifies slightly to\r\n\r\n```sh\r\nsed '/PATTERN/iTEXT' FILENAME\r\n```\r\n\r\nIt's amazing how a single line of command can completely alter the content of large text files efficiently. Here's the beauty of sed: once you get the hang of it, these commands start to feel like second nature. Moreover, sed commands are universal across the many Unix-like OS flavors, making it a highly portable and reliable tool in your skill set. That's precisely why I always find myself going back to it for quick text manipulations.\r\n\r\nRemember that sed doesn't alter the original file by default. It displays the edited content on the screen. To save these alterations, you'll redirect the output to a new file or use the `-i` flag to edit in-place. But be cautious with the latter: always ensure that you've backed up the original file before making permanent changes.\r\n\r\n## Appending Text with sed\r\n\r\nAppending text to a file without opening it manually is one of sed's standout features. To append text after a specific line, you'll use the `a` command in sed's syntax. It’s **straightforward** and efficient, especially when dealing with large files where manual editing would be impractical.\r\n\r\nHere’s the syntax you'll use:\r\n\r\n```sh\r\n sed 'NUMa TEXT_TO_APPEND' filename\r\n```\r\n\r\nIn this command, replace `NUM` with the line number after which you want the new text to appear, and `TEXT_TO_APPEND` with the actual text you're adding. This is particularly useful for automating file modifications in scripts or makefiles.\r\n\r\nFor example, to append \"Don't forget to subscribe!\" after the second line of a file, the command looks like this:\r\n\r\n```sh\r\nsed '2a Don't forget to subscribe!' filename\r\n```\r\n\r\nBut what if you need to append text that spans multiple lines? **No problem**. You can insert a backslash (`\\`) followed by a newline character to indicate the end of a line, like so:\r\n\r\n```sh\r\nsed '4a\\\r\nFirst line to append\\\r\nSecond line to append' filename\r\n```\r\n\r\nThis sequence tells sed to append the text after line four. Each backslash indicates that there's more text to come in the subsequent line.\r\n\r\nRemember, if you're using bash or another shell, you might need to escape characters or use quotes differently. It’s important to factor in your environment. For instance, in a script, you might find earmarking variables for the text to be inserted enhances **maintainability** and clarity.\r\n\r\nAdditionally, if you need to append text after a line that matches a specific pattern, rather than after a certain line number, you’ll replace the line number with the pattern. Here’s an example:\r\n\r\n```sh\r\nsed '/pattern/a Text to append after matching line' filename\r\n```\r\n\r\nThis command searches for \"pattern\" in the file and places \"Text to append after matching line\" after each occurrence. It's an effective way to add configuration options or comments next to relevant sections in configuration files or code.\r\n\r\n> Using the `-i` flag with these commands will make the changes directly to the file. However, I'd suggest testing without `-i` first to ensure the results are as expected. Always remember to handle your files with care, and back up before making irreversible changes.\r\n\r\n## Using sed to Insert or Append Text in Specific Positions\r\n\r\nWhen working with text files, you may often need to insert or append text at specific locations rather than at the end of the file. **sed** is incredibly versatile and allows you to specify where you want to make these additions with ease.\r\n\r\nTo insert text before a line that matches a particular pattern, I use the `i` command in sed. For example, to add a comment before a line containing `export PATH`, I'd run:\r\n\r\n```sh\r\nsed '/export PATH/i # Adding to the PATH variable' filename\r\n```\r\n\r\nThis inserts `# Adding to the PATH variable` before any line containing the target phrase. If your text spans multiple lines, remember to escape each newline within the inserted text.\r\n\r\nAppending text isn't much different. The `a` command is your best friend when you need to add text after a line. Let's say I want to append a reminder after a specific configuration line:\r\n\r\n```sh\r\nsed '/^output=/a # Remember to update the path' filename\r\n```\r\n\r\nHere, after any line starting with `output=`, my inserted comment will appear.\r\n\r\nWhat if you need to target a specific line number? **sed** has got you covered. To append a warning after the third line in a file:\r\n\r\n```sh\r\nsed '3a # This is a warning' filename\r\n```\r\n\r\nBut inserting and appending aren't only about adding comments. You might need to add configuration settings or code snippets. **Sed** allows for precise targeting, giving you the capability to modify files with pinpoint accuracy.\r\n\r\nRemember the importance of **regular expressions** for pattern matching. They are the keystone of using sed effectively and allow for dynamic editing, irrespective of the complexity of the file's content.\r\n\r\nPowerful features, like in-place file editing, make it tempting to dive right in, but I can't overstate the necessity to test your sed commands before committing to the changes. Never underestimate the value of working on a backup copy of your file. This ensures you're not just editing content but doing so responsibly, maintaining the integrity of your files while you work.\r\n\r\n## Advanced sed Commands for Inserting and Appending Text\r\n\r\nWhen you've got the basics down, it's time to dive into more **sophisticated sed commands**. These can be absolute game-changers for complex editing tasks. One of sed's most potent capabilities is its ability to insert or append text conditionally based on certain patterns.\r\n\r\n**Conditionally Append with sed**: Let's say you want to add text only if a line matches a given pattern. You'd use the `/pattern/a` command sequence. Here's how it might look:\r\n\r\n```sh\r\nsed '/^MATCH_PATTERN/a\\\r\nNew line to append' filename\r\n```\r\n\r\nYou'll notice the use of the caret symbol `^`. This implies that the matching line must start with the pattern specified. Replace `MATCH_PATTERN` with the actual string you're searching for to see this in action.\r\n\r\n**Insert Text Using sed**: On the flip side, if you're looking to insert text before a matching pattern, you'll employ the `/pattern/i` command:\r\n\r\n```sh\r\nsed '/MATCH_PATTERN/i\\\r\nLine to insert' filename\r\n```\r\n\r\nIn the example above, `Line to insert` will appear before any line that contains `MATCH_PATTERN`.\r\n\r\nMoving onto repetitive tasks, **sed's loop functions** are quite the powerhouse. Want to duplicate a specific line multiple times in your file? You can accomplish this through a combination of commands:\r\n\r\n```sh\r\nsed '/pattern/{p;n;}' filename\r\n```\r\n\r\nIn this context, `p` prints the current pattern space, `n` moves to the next line, and this sequence is repeated, essentially duplicating your desired line.\r\n\r\nFor those who manage configuration files or handle **script automation**, these advanced techniques can automate what would otherwise be tedious manual processes. It's also worth mentioning that mastering regular expressions will elevate your sed prowess to a new level, enabling these nifty tricks to be executed with absolute precision. Always remember to verify your commands on a sample file before diving into the main task—better safe than sorry. Remember, sed isn't just a tool; it's _the_ secret weapon in your text-editing arsenal.\r\n\r\n## Conclusion\r\n\r\nMastering sed is like adding a Swiss Army knife to your toolkit for text manipulation. With the `i` and `a` commands at your fingertips, you're now equipped to insert or append text with precision. I've shown you how sed's power extends from simple additions to complex pattern-based modifications. Remember the golden rule: back up your files and test your commands. Embrace regular expressions and sed's advanced features to streamline your editing tasks. By applying these techniques, you'll save time and avoid errors, making your workflow more efficient. Keep practicing, and soon you'll handle sed commands with confidence, turning text file manipulation from a chore into an art.\r\n\r\n## Some Clarifications\r\n\r\n### How can I append text to a file using sed?\r\n\r\nTo append text to a file using sed, you can use the `a` command. This command allows you to add text after the current line or after a line that matches a specific pattern. For example, to append the text \"New line\" after each line in a file, you can use the command `sed 'a\\New line' file.txt`. Remember to test your changes before making them permanent.\r\n\r\n### Can sed append text that spans multiple lines?\r\n\r\nYes, sed can append text that spans multiple lines by using the newline character. To include a newline in the appended text, you can use the escape sequence `\\n`. For example, to append the text \"Line 1\\\\nLine 2\" after each line in a file, you can use the command `sed 'a\\Line 1\\nLine 2' file.txt`. Remember to test your changes before making them permanent.\r\n\r\n### Can sed append text after a line that matches a specific pattern?\r\n\r\nYes, sed can append text after a line that matches a specific pattern. This can be useful for adding configuration options or comments to files. To do this, you can use the `a` command followed by the text you want to append. For example, to append the text \"This is a match\" after each line that contains the word \"example\" in a file, you can use the command `sed '/example/a\\This is a match' file.txt`. Remember to test your changes before making them permanent.\r\n\r\n### What precautions should I take when using sed to edit files in-place?\r\n\r\nWhen using sed to edit files in-place using the `-i` flag, it is important to take some precautions. First, always back up your files before making any changes. This will help you recover the original files if something goes wrong. Second, thoroughly test your sed commands on sample files to ensure they work as expected. Finally, double-check the changes made by using the `-i` flag to avoid accidentally modifying files.","src/content/posts/sed-insert-append-text.mdx",[1928],"../../assets/images/23/11/sed-insert-append-text.jpeg","ddea46e3e477825c","sed-insert-append-text.mdx","secure-ssh-server-linux",{id:1931,data:1933,body:1942,filePath:1943,assetImports:1944,digest:1946,legacyId:1947,deferredRender:32},{title:1934,description:1935,date:1936,image:1937,authors:1938,categories:1939,tags:1940,canonical:1941},"How to Secure an SSH Server in Linux","Learn essential steps to enhance the security of your SSH server on Linux systems.",["Date","2024-07-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/07/secure-ssh-server.jpeg",[19],[98],[135],"https://www.bitdoze.com/secure-ssh-server-linux/","Secure Shell (SSH) is a cryptographic network protocol that allows users to securely access and manage remote systems over an unsecured network. It's an essential tool for system administrators, developers, and anyone who needs to interact with remote servers safely.\r\n\r\nHowever, with the increasing sophistication of cyber threats, securing your SSH server is more critical than ever. Here's why:\r\n\r\n- SSH servers are prime targets for attackers due to their direct access to systems\r\n- Unsecured SSH can lead to unauthorized access, data breaches, and system compromises\r\n- Implementing robust security measures is crucial to protect your infrastructure and data\r\n\r\n## Understanding SSH\r\n\r\n### What is SSH?\r\n\r\nSSH, or Secure Shell, is a protocol that provides a secure channel over an unsecured network. It's primarily used for:\r\n\r\n- Remote command-line login\r\n- Remote command execution\r\n- Secure file transfers\r\n\r\nSSH was created in 1995 by Tatu Ylönen as a replacement for insecure protocols like Telnet and rsh. It has since become the standard for secure remote access in Unix-like operating systems.\r\n\r\n### How SSH Works\r\n\r\nSSH operates on a client-server model, utilizing strong encryption to ensure secure communication. Here's a simplified breakdown of the process:\r\n\r\n1. The client initiates a connection to the SSH server\r\n2. The server sends its public key to the client\r\n3. The client verifies the server's identity\r\n4. A secure, encrypted channel is established\r\n5. User authentication takes place (password, public key, etc.)\r\n6. Upon successful authentication, the session begins\r\n\r\nKey components of SSH include:\r\n\r\n| Component  | Description                                                                         |\r\n| ---------- | ----------------------------------------------------------------------------------- |\r\n| SSH Server | The program running on the remote machine that listens for incoming SSH connections |\r\n| SSH Client | The program used to connect to an SSH server                                        |\r\n| Encryption | Algorithms used to secure the communication channel                                 |\r\n\r\n### Why SSH Security is Crucial\r\n\r\nWhile SSH is inherently more secure than its predecessors, it's not immune to threats. Common risks include:\r\n\r\n- Brute-force attacks\r\n- Man-in-the-middle attacks\r\n- Unauthorized key-based access\r\n- Exploitation of vulnerabilities in SSH implementations\r\n\r\nThe consequences of a compromised SSH server can be severe:\r\n\r\n- Unauthorized access to sensitive systems and data\r\n- Potential for lateral movement within a network\r\n- Data theft or manipulation\r\n- Installation of malware or backdoors\r\n- Reputational damage and financial losses\r\n\r\nGiven these risks, it's clear that implementing robust security measures for your SSH server is not just recommended—it's essential. In the following sections, we'll explore various strategies and best practices to enhance the security of your SSH server and protect your systems from potential threats.\r\n\r\n## Basic SSH Server Security Measures\r\n\r\nSecuring your SSH server is crucial for protecting your Linux system from unauthorized access. Let's explore some fundamental security measures you can implement:\r\n\r\n### Use Strong Passwords\r\n\r\nStrong passwords are your first line of defense against potential intruders. Here's why they matter and how to create them:\r\n\r\n- **Importance of complex passwords:**\r\n\r\n  - Resist brute-force attacks\r\n  - Prevent unauthorized access\r\n  - Protect sensitive data\r\n\r\n- **Tips for creating and managing strong passwords:**\r\n  - Use a mix of uppercase and lowercase letters, numbers, and special characters\r\n  - Aim for a minimum length of 12 characters\r\n  - Avoid using personal information or common words\r\n  - Use a password manager to generate and store complex passwords securely\r\n\r\n### Disable Root Login\r\n\r\nAllowing direct root login via SSH is a significant security risk. Here's why and how to disable it:\r\n\r\n- **Risks associated with root login:**\r\n\r\n  - Unlimited access to the entire system\r\n  - Increased vulnerability to brute-force attacks\r\n  - No accountability for individual user actions\r\n\r\n- **Steps to disable root login in SSH configuration:**\r\n  1. Open the SSH configuration file: `sudo nano /etc/ssh/sshd_config`\r\n  2. Find the line `PermitRootLogin yes`\r\n  3. Change it to `PermitRootLogin no`\r\n  4. Save the file and restart the SSH service: `sudo systemctl restart sshd`\r\n\r\n### Change Default SSH Port\r\n\r\nModifying the default SSH port can help reduce automated attacks. Here's why and how:\r\n\r\n- **Reasons for changing the default port:**\r\n\r\n  - Avoid automated scans targeting the default port (22)\r\n  - Reduce log clutter from random connection attempts\r\n  - Add an extra layer of security through obscurity\r\n\r\n- **How to modify the SSH port in configuration files:**\r\n  1. Open the SSH configuration file: `sudo nano /etc/ssh/sshd_config`\r\n  2. Find the line `#Port 22`\r\n  3. Uncomment it and change the number to your desired port (e.g., `Port 2222`)\r\n  4. Save the file and restart the SSH service: `sudo systemctl restart sshd`\r\n\r\n> **Note:** Remember to update your firewall rules to allow connections on the new port!\r\n\r\n### Implement Key-Based Authentication\r\n\r\nKey-based authentication offers several advantages over traditional password-based methods:\r\n\r\n- **Benefits of key-based authentication:**\r\n\r\n  - Stronger security than passwords\r\n  - Resistance to brute-force attacks\r\n  - Convenient for automated processes and scripts\r\n\r\n- **Process of generating and implementing SSH keys:**\r\n\r\n| Step | Action                                                                 |\r\n| ---- | ---------------------------------------------------------------------- |\r\n| 1    | Generate a key pair on your local machine: `ssh-keygen -t rsa -b 4096` |\r\n| 2    | Copy the public key to the server: `ssh-copy-id user@server_ip`        |\r\n| 3    | Test the key-based login: `ssh user@server_ip`                         |\r\n| 4    | (Optional) Disable password authentication in `/etc/ssh/sshd_config`   |\r\n\r\nBy implementing these basic security measures, you'll significantly enhance the security of your SSH server. Remember, security is an ongoing process, so stay informed about best practices and regularly update your system!\r\n\r\n## Advanced SSH Server Security Techniques\r\n\r\n### Limit User Access\r\n\r\nRestricting user access is a crucial step in enhancing your SSH server's security. By implementing strict access controls, you can significantly reduce the risk of unauthorized access and potential security breaches.\r\n\r\n#### Configuring SSH to allow only specific users\r\n\r\nOne of the most effective ways to limit user access is by configuring SSH to permit only specific users. This approach ensures that only authorized individuals can connect to your server via SSH. Here's how you can implement this:\r\n\r\n1. Open the SSH configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/ssh/sshd_config\r\n   ```\r\n\r\n2. Look for or add the following directives:\r\n\r\n   - `AllowUsers`: Specifies which users are allowed to connect\r\n   - `DenyUsers`: Specifies which users are explicitly denied access\r\n\r\n3. Use these directives to control access:\r\n\r\n   ```sh\r\n   AllowUsers user1 user2 user3\r\n   DenyUsers baduser suspicioususer\r\n   ```\r\n\r\n4. Save the file and restart the SSH service:\r\n   ```sh\r\n   sudo systemctl restart sshd\r\n   ```\r\n\r\n#### Using AllowUsers and DenyUsers directives\r\n\r\nThe `AllowUsers` and `DenyUsers` directives are powerful tools for managing SSH access. Here's a more detailed look at how to use them effectively:\r\n\r\n| Directive  | Purpose                                    | Example                      |\r\n| ---------- | ------------------------------------------ | ---------------------------- |\r\n| AllowUsers | Specify users allowed to connect via SSH   | `AllowUsers alice bob carol` |\r\n| DenyUsers  | Specify users explicitly denied SSH access | `DenyUsers eve mallory`      |\r\n\r\nRemember these key points when using these directives:\r\n\r\n- If `AllowUsers` is used, only listed users can connect, and all others are denied by default.\r\n- `DenyUsers` takes precedence over `AllowUsers` if a user is listed in both.\r\n- You can use wildcards, like `AllowUsers admin* support*` to allow all users whose names start with \"admin\" or \"support\".\r\n- It's generally safer to use `AllowUsers` and explicitly list allowed users rather than trying to deny specific users with `DenyUsers`.\r\n\r\nBy implementing these access control measures, you'll significantly enhance your SSH server's security posture. Remember to test your configuration after making changes to ensure legitimate users can still access the system as needed.\r\n\r\n### Implement Two-Factor Authentication (2FA)\r\n\r\nTwo-Factor Authentication (2FA) adds an extra layer of security to your SSH server, making it significantly more challenging for unauthorized users to gain access. Let's explore the benefits of 2FA and how to set it up for SSH access.\r\n\r\n#### Benefits of 2FA for SSH\r\n\r\nImplementing 2FA for your SSH server offers several advantages:\r\n\r\n- **Enhanced security**: Requires an additional verification step beyond just a password\r\n- **Protection against stolen credentials**: Even if a password is compromised, the attacker still needs the second factor\r\n- **Reduced risk of brute-force attacks**: Makes automated login attempts much less effective\r\n- **Compliance**: Helps meet security requirements for various industry standards\r\n- **User accountability**: Provides a clear audit trail of who accessed the system and when\r\n\r\n#### Steps to set up 2FA for SSH access\r\n\r\nSetting up 2FA for SSH is a straightforward process. Here's a step-by-step guide to get you started:\r\n\r\n1. **Install the necessary packages**:\r\n\r\n   ```\r\n   sudo apt-get update\r\n   sudo apt-get install libpam-google-authenticator\r\n   ```\r\n\r\n2. **Configure PAM (Pluggable Authentication Modules)**:\r\n   Edit the SSH PAM configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/pam.d/sshd\r\n   ```\r\n\r\n   Add the following line at the end of the file:\r\n\r\n   ```sh\r\n   auth required pam_google_authenticator.so\r\n   ```\r\n\r\n3. **Update SSH configuration**:\r\n   Edit the SSH daemon configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/ssh/sshd_config\r\n   ```\r\n\r\n   Modify or add the following lines:\r\n\r\n   ```sh\r\n   ChallengeResponseAuthentication yes\r\n   UsePAM yes\r\n   ```\r\n\r\n4. **Restart the SSH service**:\r\n\r\n   ```sh\r\n   sudo systemctl restart sshd\r\n   ```\r\n\r\n5. **Set up Google Authenticator for each user**:\r\n   Run the following command as the user who needs 2FA:\r\n\r\n   ```\r\n   google-authenticator\r\n   ```\r\n\r\n   Follow the prompts to configure the authenticator.\r\n\r\n6. **Test the 2FA setup**:\r\n   Try logging in via SSH. You should now be prompted for both your password and a verification code.\r\n\r\nHere's a quick reference table for the 2FA setup process:\r\n\r\n| Step | Action                                |\r\n| ---- | ------------------------------------- |\r\n| 1    | Install required packages             |\r\n| 2    | Configure PAM                         |\r\n| 3    | Update SSH configuration              |\r\n| 4    | Restart SSH service                   |\r\n| 5    | Set up Google Authenticator for users |\r\n| 6    | Test the 2FA setup                    |\r\n\r\nRemember, while 2FA significantly enhances your SSH security, it's just one part of a comprehensive security strategy. Be sure to combine it with other best practices like strong passwords, regular updates, and proper firewall configuration for optimal protection.\r\n\r\nBy following these steps, you'll add an extra layer of security to your SSH server, making it much more resilient against unauthorized access attempts. Happy securing!\r\n\r\n### Use SSH Protocol 2\r\n\r\nSSH Protocol 2 is a more secure and feature-rich version of the SSH protocol. It's crucial to use this version to ensure the highest level of security for your SSH server. Let's explore the advantages of SSH Protocol 2 and how to enforce its use.\r\n\r\n#### Advantages of SSH Protocol 2 over Protocol 1\r\n\r\nSSH Protocol 2 offers several improvements over its predecessor:\r\n\r\n• **Enhanced security**: Uses stronger encryption algorithms and improved key exchange methods.\r\n• **Improved authentication**: Supports more authentication mechanisms, including public key authentication.\r\n• **Integrity checking**: Provides better protection against data tampering during transmission.\r\n• **Performance**: Generally faster and more efficient than Protocol 1.\r\n• **Flexibility**: Allows for the use of multiple channels within a single SSH connection.\r\n\r\nHere's a quick comparison of the two protocols:\r\n\r\n| Feature         | SSH Protocol 1       | SSH Protocol 2                      |\r\n| --------------- | -------------------- | ----------------------------------- |\r\n| Encryption      | Weaker algorithms    | Stronger algorithms (AES, ChaCha20) |\r\n| Key Exchange    | Less secure          | More secure (Diffie-Hellman, ECDH)  |\r\n| Integrity       | CRC-32               | HMAC-SHA1, HMAC-SHA2                |\r\n| Authentication  | Limited options      | More options (including public key) |\r\n| Vulnerabilities | Several known issues | Fewer known vulnerabilities         |\r\n\r\n#### How to enforce the use of Protocol 2\r\n\r\nEnforcing SSH Protocol 2 is straightforward and can be done by modifying the SSH server configuration file. Follow these steps:\r\n\r\n1. Open the SSH configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/ssh/sshd_config\r\n   ```\r\n\r\n2. Look for the line that says `Protocol` or add it if it doesn't exist.\r\n\r\n3. Set the value to 2:\r\n\r\n   ```\r\n   Protocol 2\r\n   ```\r\n\r\n4. If you find any lines referencing Protocol 1, comment them out by adding a # at the beginning of the line.\r\n\r\n5. Save the file and exit the text editor.\r\n\r\n6. Restart the SSH service to apply the changes:\r\n   ```sh\r\n   sudo systemctl restart sshd\r\n   ```\r\n\r\nBy following these steps, you'll ensure that your SSH server only accepts connections using the more secure Protocol 2.\r\n\r\nRemember, most modern SSH clients and servers use Protocol 2 by default. However, it's always a good practice to explicitly set this in your configuration to prevent any potential downgrade attacks or accidental use of the less secure Protocol 1.\r\n\r\nBy enforcing the use of SSH Protocol 2, you're taking a significant step towards securing your SSH server and protecting your system from potential vulnerabilities associated with the older protocol version.\r\n\r\n### Configure Idle Timeout Interval\r\n\r\nImplementing an idle timeout interval for SSH connections is a crucial step in enhancing your server's security. This feature automatically disconnects inactive sessions, reducing the risk of unauthorized access if a user forgets to log out or leaves their computer unattended.\r\n\r\n#### Importance of session timeouts\r\n\r\n• Prevents unauthorized access to abandoned sessions\r\n• Frees up server resources by closing inactive connections\r\n• Complies with security best practices and regulations\r\n\r\n#### Setting up automatic disconnection for idle sessions\r\n\r\nTo configure the idle timeout interval, you'll need to modify the SSH server configuration file. Follow these steps:\r\n\r\n1. Open the SSH configuration file:\r\n\r\n   ```sh\r\n   sudo nano /etc/ssh/sshd_config\r\n   ```\r\n\r\n2. Add or modify the following lines:\r\n\r\n   ```\r\n   ClientAliveInterval 300\r\n   ClientAliveCountMax 0\r\n   ```\r\n\r\n   These settings mean:\r\n\r\n   - `ClientAliveInterval 300`: The server will send a null packet to the client every 300 seconds (5 minutes)\r\n   - `ClientAliveCountMax 0`: The connection will be terminated immediately if there's no response to the null packet\r\n\r\n3. Save the file and exit the editor\r\n\r\n4. Restart the SSH service to apply the changes:\r\n   ```sh\r\n   sudo systemctl restart sshd\r\n   ```\r\n\r\nYou can adjust the timeout duration by changing the `ClientAliveInterval` value. Here's a table with some common timeout settings:\r\n\r\n| Desired Timeout | ClientAliveInterval | ClientAliveCountMax |\r\n| --------------- | ------------------- | ------------------- |\r\n| 5 minutes       | 300                 | 0                   |\r\n| 10 minutes      | 600                 | 0                   |\r\n| 15 minutes      | 900                 | 0                   |\r\n| 30 minutes      | 1800                | 0                   |\r\n\r\nRemember, shorter timeout intervals enhance security but may inconvenience users who need longer idle periods. Choose a balance that suits your organization's needs and security requirements.\r\n\r\nBy implementing these timeout settings, you'll significantly improve your SSH server's security posture and protect against potential threats from unattended sessions.\r\n\r\n## Firewall Configuration for SSH\r\n\r\nConfiguring your firewall is a crucial step in securing your SSH server. Let's explore two popular methods: using iptables and configuring fail2ban.\r\n\r\n### Using iptables\r\n\r\niptables is a powerful firewall tool built into the Linux kernel. It allows you to set up rules to control incoming and outgoing network traffic.\r\n\r\n#### Basic iptables rules for SSH security\r\n\r\nHere are some basic iptables rules to enhance SSH security:\r\n\r\n```sh\r\n# Allow established connections\r\n\r\niptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\r\n\r\n# Allow SSH connections (adjust port if necessary)\r\n\r\niptables -A INPUT -p tcp --dport 22 -j ACCEPT\r\n\r\n# Drop all other incoming traffic\r\n\r\niptables -A INPUT -j DROP\r\n```\r\n\r\n#### Limiting connection attempts using iptables\r\n\r\nTo prevent brute-force attacks, you can limit the number of connection attempts:\r\n\r\n```sh\r\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set\r\niptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 -j DROP\r\n```\r\n\r\nThis rule allows only 4 new SSH connections per minute from the same IP address.\r\n\r\n### Configuring fail2ban\r\n\r\nfail2ban is an intrusion prevention software that protects your server from brute-force attacks.\r\n\r\n#### Introduction to fail2ban\r\n\r\nfail2ban works by monitoring log files for suspicious activity and automatically updating firewall rules to block malicious IP addresses. It's highly customizable and can protect various services, including SSH.\r\n\r\n#### Setting up fail2ban for SSH protection\r\n\r\nTo set up fail2ban for SSH protection:\r\n\r\n1. Install fail2ban:\r\n\r\n   ```sh\r\n   sudo apt-get install fail2ban\r\n   ```\r\n\r\n2. Create a local configuration file:\r\n\r\n   ```\r\n   sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local\r\n   ```\r\n\r\n3. Edit the SSH section in `/etc/fail2ban/jail.local`:\r\n\r\n   ```sh\r\n   [sshd]\r\n   enabled = true\r\n   port = ssh\r\n   filter = sshd\r\n   logpath = /var/log/auth.log\r\n   maxretry = 3\r\n   bantime = 3600\r\n   ```\r\n\r\n   This configuration:\r\n\r\n   - Enables SSH protection\r\n   - Sets the maximum retry attempts to 3\r\n   - Sets the ban time to 1 hour (3600 seconds)\r\n\r\n4. Restart fail2ban:\r\n   ```sh\r\n   sudo systemctl restart fail2ban\r\n   ```\r\n\r\nBy implementing these firewall configurations, you'll significantly enhance the security of your SSH server. Remember to test your settings thoroughly to ensure they don't interfere with legitimate access attempts.\r\n\r\n## Monitoring and Logging\r\n\r\nKeeping a watchful eye on your SSH server is crucial for maintaining its security. Let's explore how to set up effective monitoring and logging practices.\r\n\r\n### Enable SSH Logging\r\n\r\nLogging is your best friend when it comes to understanding what's happening on your SSH server. Here's why it's important and how to set it up:\r\n\r\n- **Importance of logging for security analysis**:\r\n\r\n  - Provides a detailed record of SSH activities\r\n  - Helps identify potential security breaches\r\n  - Assists in troubleshooting connection issues\r\n  - Supports compliance with security policies\r\n\r\n- **Configuring SSH logging options**:\r\n  1. Open the SSH configuration file:\r\n     ```sh\r\n     sudo nano /etc/ssh/sshd_config\r\n     ```\r\n  2. Add or modify the following lines:\r\n     ```sh\r\n     LogLevel VERBOSE\r\n     SyslogFacility AUTH\r\n     ```\r\n  3. Save the file and restart the SSH service:\r\n     ```sh\r\n     sudo systemctl restart sshd\r\n     ```\r\n\r\nThese settings will ensure that SSH logs are detailed and stored in the system's authentication log file, typically `/var/log/auth.log`.\r\n\r\n### Monitor SSH Access Attempts\r\n\r\nMonitoring access attempts helps you stay ahead of potential threats. Here are some tools and techniques to keep your SSH server secure:\r\n\r\n- **Tools for monitoring SSH access attempts**:\r\n\r\n| Tool     | Description                                                           |\r\n| -------- | --------------------------------------------------------------------- |\r\n| Fail2Ban | Automatically blocks IP addresses with too many failed login attempts |\r\n| OSSEC    | Host-based intrusion detection system that monitors SSH logs          |\r\n| Logwatch | Analyzes log files and sends daily reports                            |\r\n\r\n- **Setting up alerts for suspicious activities**:\r\n  - Configure email notifications for failed login attempts\r\n  - Set up real-time alerts for multiple failed logins from the same IP\r\n  - Create custom scripts to monitor and alert on specific patterns in SSH logs\r\n\r\nTo get started with Fail2Ban, which is a popular choice, follow these steps:\r\n\r\n1. Install Fail2Ban:\r\n   ```sh\r\n   sudo apt-get install fail2ban\r\n   ```\r\n2. Create a local configuration file:\r\n   ```sh\r\n   sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local\r\n   ```\r\n3. Edit the local configuration file to set up SSH protection:\r\n   ```sh\r\n   sudo nano /etc/fail2ban/jail.local\r\n   ```\r\n4. Find the `[sshd]` section and ensure it looks like this:\r\n   ```sh\r\n   [sshd]\r\n   enabled = true\r\n   port = ssh\r\n   filter = sshd\r\n   logpath = /var/log/auth.log\r\n   maxretry = 3\r\n   bantime = 3600\r\n   ```\r\n5. Save the file and restart Fail2Ban:\r\n   ```sh\r\n   sudo systemctl restart fail2ban\r\n   ```\r\n\r\nBy implementing these monitoring and logging practices, you'll significantly enhance your SSH server's security. Remember, staying informed about your server's activities is key to maintaining a robust security posture. Happy monitoring!\r\n\r\n## Keeping Your SSH Server Updated\r\n\r\nMaintaining an up-to-date SSH server is crucial for ensuring the security and stability of your Linux system. Let's explore how to keep your SSH server and the underlying system updated.\r\n\r\n### Regular System Updates\r\n\r\nKeeping your entire Linux system updated is the foundation of a secure SSH server. Here's why it's important and how to do it:\r\n\r\n- **Importance of keeping the system updated:**\r\n\r\n  - Patches security vulnerabilities\r\n  - Improves system stability\r\n  - Enhances performance\r\n  - Adds new features and functionality\r\n\r\n- **How to automate system updates:**\r\n  1. Use the built-in package manager:\r\n     - For Ubuntu/Debian: `sudo apt-get update && sudo apt-get upgrade -y`\r\n     - For CentOS/RHEL: `sudo yum update -y`\r\n  2. Set up automatic updates:\r\n     - Ubuntu/Debian: Install and configure `unattended-upgrades`\r\n     - CentOS/RHEL: Use `yum-cron` or `dnf-automatic`\r\n\r\nHere's a simple cron job to schedule daily updates:\r\n\r\n```markdown\r\n| Time         | Command                                                |\r\n| ------------ | ------------------------------------------------------ |\r\n| 0 2 \\* \\* \\* | /usr/bin/apt-get update && /usr/bin/apt-get upgrade -y |\r\n```\r\n\r\n### Updating SSH Software\r\n\r\nKeeping your SSH software up-to-date is crucial for maintaining a secure server. Here's how to do it:\r\n\r\n- **Checking for SSH software updates:**\r\n\r\n  1. Determine your current SSH version:\r\n     ```sh\r\n     ssh -V\r\n     ```\r\n  2. Compare it with the latest version available in your distribution's repositories or on the OpenSSH website.\r\n\r\n- **Safely updating SSH server software:**\r\n  1. Back up your SSH configuration:\r\n     ```sh\r\n     sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup\r\n     ```\r\n  2. Update SSH using your package manager:\r\n     - Ubuntu/Debian: `sudo apt-get install openssh-server`\r\n     - CentOS/RHEL: `sudo yum update openssh-server`\r\n  3. Restart the SSH service:\r\n     ```sh\r\n     sudo systemctl restart sshd\r\n     ```\r\n  4. Verify the new version:\r\n     ```sh\r\n     ssh -V\r\n     ```\r\n\r\nRemember, it's always a good idea to test SSH connectivity from another terminal before closing your current session after making changes.\r\n\r\nBy following these steps, you'll ensure that your SSH server remains secure and up-to-date. Regular updates are a simple yet effective way to maintain the integrity of your system and protect against potential vulnerabilities.\r\n\r\n## Best Practices for SSH Key Management\r\n\r\nProper SSH key management is crucial for maintaining a secure SSH server. Let's explore some best practices to ensure your SSH keys are generated, stored, and managed securely.\r\n\r\n### Secure Key Generation\r\n\r\nGenerating strong SSH keys is the foundation of a secure SSH setup. Here are some best practices:\r\n\r\n- Use `ssh-keygen` with strong algorithms:\r\n\r\n  ```sh\r\n  ssh-keygen -t ed25519 -a 100\r\n  ```\r\n\r\n  or\r\n\r\n  ```sh\r\n  ssh-keygen -t rsa -b 4096\r\n  ```\r\n\r\n- Choose the right key type and size:\r\n  | Key Type | Recommended Size | Notes |\r\n  |----------|------------------|-------|\r\n  | ED25519 | 256 bits (fixed) | Modern, fast, and secure |\r\n  | RSA | 4096 bits | Widely compatible |\r\n  | ECDSA | 256-521 bits | Avoid if possible due to potential issues |\r\n\r\n- Always use a strong passphrase when generating keys\r\n\r\n### Key Storage and Protection\r\n\r\nProperly storing and protecting your SSH keys is essential to prevent unauthorized access:\r\n\r\n- Store private keys securely:\r\n\r\n  - Keep private keys on your local machine, never on the server\r\n  - Use restrictive file permissions: `chmod 600 ~/.ssh/id_ed25519`\r\n\r\n- Use passphrase protection:\r\n\r\n  - Always set a strong passphrase when generating keys\r\n  - Use `ssh-agent` to manage passphrases conveniently:\r\n    ```sh\r\n    eval $(ssh-agent)\r\n    ssh-add ~/.ssh/id_ed25519\r\n    ```\r\n\r\n- Consider using a hardware security key for added protection\r\n\r\n### Regular Key Rotation\r\n\r\nPeriodically rotating your SSH keys helps maintain security over time:\r\n\r\n- Implement a key rotation policy:\r\n\r\n  - Set a schedule for key rotation (e.g., every 6-12 months)\r\n  - Create a process for generating and distributing new keys\r\n\r\n- Steps for key rotation:\r\n  1. Generate new SSH key pair\r\n  2. Add the new public key to authorized_keys on servers\r\n  3. Test the new key\r\n  4. Remove the old public key from servers\r\n  5. Delete the old private key from your local machine\r\n\r\nRemember, key rotation doesn't have to be a hassle! With proper planning and automation, you can maintain a robust SSH key management system that keeps your servers secure.\r\n\r\nBy following these best practices for SSH key management, you'll significantly enhance the security of your SSH server. Keep in mind that security is an ongoing process, so stay informed about the latest recommendations and adjust your practices accordingly.\r\n\r\n## Additional Security Measures\r\n\r\n### Use SSH Certificates\r\n\r\nWhile traditional SSH key pairs offer robust security, SSH certificates provide an even more advanced layer of protection for your Linux server. Let's explore why you might want to consider implementing SSH certificate authentication.\r\n\r\n#### Benefits of SSH certificates over traditional key pairs\r\n\r\nSSH certificates offer several advantages:\r\n\r\n- **Centralized management**: Easier to manage access for multiple users and servers\r\n- **Time-based access**: Certificates can be set to expire automatically\r\n- **Reduced key sprawl**: No need to distribute and manage public keys on each server\r\n- **Improved auditing**: Better tracking of who accessed what and when\r\n- **Simplified user revocation**: Revoke access quickly without touching individual servers\r\n\r\n#### Implementing SSH certificate authentication\r\n\r\nFollow these steps to set up SSH certificate authentication:\r\n\r\n1. **Set up a Certificate Authority (CA)**:\r\n\r\n   - Generate a CA key pair\r\n   - Secure the CA private key\r\n\r\n2. **Configure the SSH server**:\r\n\r\n   - Add the CA public key to the server's `sshd_config` file\r\n   - Restart the SSH service\r\n\r\n3. **Generate and sign user certificates**:\r\n\r\n   - Create a certificate for each user\r\n   - Sign certificates with the CA private key\r\n\r\n4. **Distribute certificates to users**:\r\n   - Securely send certificates to respective users\r\n   - Instruct users on how to use their certificates\r\n\r\nHere's a simple comparison of traditional SSH keys vs. SSH certificates:\r\n\r\n| Feature     | Traditional SSH Keys               | SSH Certificates     |\r\n| ----------- | ---------------------------------- | -------------------- |\r\n| Expiration  | Manual revocation required         | Automatic expiration |\r\n| Management  | Distributed (on each server)       | Centralized          |\r\n| Scalability | Challenging for large environments | Easily scalable      |\r\n| Auditing    | Limited                            | Comprehensive        |\r\n\r\nRemember, while SSH certificates offer enhanced security and management features, they do require additional setup and maintenance. Evaluate your specific needs and resources before making the switch.\r\n\r\nBy implementing SSH certificates, you're taking a significant step towards a more secure and manageable SSH infrastructure. It's an investment that can pay off in improved security and reduced administrative overhead in the long run.\r\n\r\n### Implement Port Knocking\r\n\r\nPort knocking is an ingenious technique that adds an extra layer of security to your SSH server. It's like having a secret handshake before anyone can even attempt to log in! Let's dive into what port knocking is and how to set it up.\r\n\r\n#### What is Port Knocking?\r\n\r\nPort knocking is a method of externally opening ports on a firewall by generating a connection attempt on a set of prespecified closed ports. Here's how it works:\r\n\r\n- The SSH port (usually 22) remains closed by default\r\n- A specific sequence of connection attempts must be made to predetermined ports\r\n- Once the correct sequence is detected, the firewall opens the SSH port\r\n- After a set time, the SSH port closes again\r\n\r\nThis method makes it much harder for potential attackers to even find your SSH port, let alone attempt to break in.\r\n\r\n#### Benefits of Port Knocking\r\n\r\n- Increased security: SSH port remains hidden from port scans\r\n- Flexible: Can be customized with various sequences and timeouts\r\n- Low overhead: Minimal impact on system resources\r\n\r\n#### Setting up a Basic Port Knocking System\r\n\r\nLet's set up a simple port knocking system using `knockd`. Here's how:\r\n\r\n1. Install knockd:\r\n\r\n   ```sh\r\n   sudo apt-get install knockd\r\n   ```\r\n\r\n2. Configure knockd by editing `/etc/knockd.conf`:\r\n\r\n   ```\r\n   [options]\r\n       UseSyslog\r\n\r\n   [openSSH]\r\n       sequence    = 7000,8000,9000\r\n       seq_timeout = 5\r\n       command     = /sbin/iptables -A INPUT -s %IP% -p tcp --dport 22 -j ACCEPT\r\n       tcpflags    = syn\r\n\r\n   [closeSSH]\r\n       sequence    = 9000,8000,7000\r\n       seq_timeout = 5\r\n       command     = /sbin/iptables -D INPUT -s %IP% -p tcp --dport 22 -j ACCEPT\r\n       tcpflags    = syn\r\n   ```\r\n\r\n3. Enable knockd:\r\n\r\n   ```sh\r\n   sudo systemctl enable knockd\r\n   sudo systemctl start knockd\r\n   ```\r\n\r\n4. Configure your firewall to drop all incoming connections to port 22 by default.\r\n\r\n5. To open the SSH port, use the `knock` command:\r\n\r\n   ```\r\n   knock your_server_ip 7000 8000 9000\r\n   ```\r\n\r\n6. To close the SSH port after use:\r\n   ```\r\n   knock your_server_ip 9000 8000 7000\r\n   ```\r\n\r\n#### Port Knocking Sequence Examples\r\n\r\nHere's a markdown table with some example port knocking sequences:\r\n\r\n| Sequence Name | Ports               | Description                           |\r\n| ------------- | ------------------- | ------------------------------------- |\r\n| Simple        | 7000,8000,9000      | Basic three-port sequence             |\r\n| Complex       | 1234,5678,9012      | More random port numbers              |\r\n| Long          | 1000,2000,3000,4000 | Four-port sequence for added security |\r\n| Time-based    | 5000:tcp,6000:udp   | Mix of TCP and UDP ports              |\r\n\r\nRemember, the more complex and unique your sequence, the more secure your system will be. However, balance this with usability – you don't want to forget your own \"secret knock\"!\r\n\r\nBy implementing port knocking, you've added a clever and effective layer of security to your SSH server. It's like having a secret passage that only you know about. Happy knocking!\r\n\r\n### Consider SSH Honeypots\r\n\r\nSSH honeypots can be a valuable addition to your server security strategy. They provide an innovative way to detect and study potential threats while keeping your actual systems safe. Let's explore what SSH honeypots are and how you can implement a simple one for threat detection.\r\n\r\n#### Introduction to SSH honeypots\r\n\r\nSSH honeypots are decoy systems designed to:\r\n\r\n- Attract potential attackers\r\n- Monitor their activities\r\n- Gather information about attack patterns and techniques\r\n\r\nThese fake systems appear vulnerable to intruders but are actually isolated and closely monitored. By deploying an SSH honeypot, you can:\r\n\r\n- Gain insights into attacker behavior\r\n- Identify new attack vectors\r\n- Improve your overall security posture\r\n\r\n#### Implementing a simple SSH honeypot for threat detection\r\n\r\nSetting up a basic SSH honeypot doesn't have to be complicated. Here's a straightforward approach using Cowrie, a popular open-source honeypot:\r\n\r\n1. **Prepare the environment:**\r\n\r\n   - Set up a separate virtual machine or container\r\n   - Ensure it's isolated from your production network\r\n\r\n2. **Install dependencies:**\r\n\r\n   ```sh\r\n   sudo apt update\r\n   sudo apt install git python3-virtualenv libssl-dev libffi-dev build-essential libpython3-dev\r\n   ```\r\n\r\n3. **Clone and set up Cowrie:**\r\n\r\n   ```sh\r\n   git clone https://github.com/cowrie/cowrie\r\n   cd cowrie\r\n   python3 -m virtualenv cowrie-env\r\n   source cowrie-env/bin/activate\r\n   pip install --upgrade pip\r\n   pip install -r requirements.txt\r\n   ```\r\n\r\n4. **Configure Cowrie:**\r\n\r\n   - Copy the example configuration:\r\n     ```sh\r\n     cp etc/cowrie.cfg.dist etc/cowrie.cfg\r\n     ```\r\n   - Edit `etc/cowrie.cfg` to customize settings like:\r\n     - Hostname\r\n     - Listening port\r\n     - Allowed/denied IP addresses\r\n\r\n5. **Start the honeypot:**\r\n\r\n   ```\r\n   bin/cowrie start\r\n   ```\r\n\r\n6. **Monitor and analyze logs:**\r\n   - Check `var/log/cowrie/cowrie.log` for activity\r\n   - Use tools like ELK stack for advanced log analysis\r\n\r\nHere's a simple markdown table summarizing the benefits and considerations of using SSH honeypots:\r\n\r\n| Benefits                     | Considerations                      |\r\n| ---------------------------- | ----------------------------------- |\r\n| Early threat detection       | Resource consumption                |\r\n| Attacker behavior insights   | Legal and ethical implications      |\r\n| Improved security posture    | Maintenance and monitoring overhead |\r\n| Minimal risk to real systems | Potential for false positives       |\r\n\r\nRemember, while SSH honeypots can be a powerful security tool, they should be used as part of a comprehensive security strategy. Always ensure you're complying with local laws and regulations when deploying honeypots.\r\n\r\nBy implementing an SSH honeypot, you're adding an extra layer of defense to your server security, giving you valuable insights into potential threats and helping you stay one step ahead of attackers. Happy securing!\r\n\r\n## Testing Your SSH Security\r\n\r\nAfter implementing security measures, it's crucial to verify their effectiveness. This section covers tools and techniques to assess your SSH server's security posture.\r\n\r\n### Using SSH Audit Tools\r\n\r\nSSH audit tools are invaluable for identifying potential vulnerabilities in your SSH configuration. Here's an overview of popular options and how to use them:\r\n\r\n#### Popular SSH Audit Tools\r\n\r\n- **ssh-audit**: A comprehensive Python-based tool\r\n- **Lynis**: An open-source security auditing tool\r\n- **Nmap**: A versatile network scanner with SSH-specific scripts\r\n\r\n#### Running and Interpreting SSH Security Audits\r\n\r\n1. Install your chosen audit tool (e.g., `sudo apt install ssh-audit`)\r\n2. Run the audit against your server:\r\n   ```sh\r\n   ssh-audit your_server_ip\r\n   ```\r\n3. Review the output, which typically includes:\r\n   - SSH version information\r\n   - Key exchange algorithms\r\n   - Encryption algorithms\r\n   - MAC algorithms\r\n   - Compression algorithms\r\n   - Any detected vulnerabilities or weak configurations\r\n\r\n| Severity | Description                      | Action Required       |\r\n| -------- | -------------------------------- | --------------------- |\r\n| High     | Critical security issues         | Address immediately   |\r\n| Medium   | Potential vulnerabilities        | Plan to fix soon      |\r\n| Low      | Minor concerns or best practices | Consider implementing |\r\n| Info     | Informational findings           | No action required    |\r\n\r\n### Penetration Testing\r\n\r\nRegular penetration testing helps identify security weaknesses that automated tools might miss.\r\n\r\n#### Importance of Regular Penetration Testing\r\n\r\n- Simulates real-world attack scenarios\r\n- Uncovers complex vulnerabilities\r\n- Validates the effectiveness of security controls\r\n- Helps maintain compliance with security standards\r\n\r\n#### Basic Penetration Testing Techniques for SSH Servers\r\n\r\n1. **Port Scanning**:\r\n\r\n   - Use Nmap to identify open ports and services\r\n   - Example: `nmap -sV -p22 your_server_ip`\r\n\r\n2. **Brute Force Attacks**:\r\n\r\n   - Test password strength using tools like Hydra\r\n   - Example: `hydra -l username -P wordlist.txt ssh://your_server_ip`\r\n\r\n3. **Version Exploitation**:\r\n\r\n   - Research known vulnerabilities for your SSH version\r\n   - Attempt to exploit using tools like Metasploit\r\n\r\n4. **Configuration Testing**:\r\n\r\n   - Manually attempt to connect using weak algorithms\r\n   - Example: `ssh -oKexAlgorithms=+diffie-hellman-group1-sha1 user@your_server_ip`\r\n\r\n5. **Social Engineering**:\r\n   - Attempt to gather user information through non-technical means\r\n   - Test user awareness of security policies\r\n\r\nRemember, always obtain proper authorization before performing penetration tests on systems you don't own or manage!\r\n\r\nBy regularly employing these testing methods, you'll gain valuable insights into your SSH server's security posture and be better equipped to defend against potential threats. Stay vigilant and keep your SSH configuration up-to-date for the best protection.\r\n\r\n## Conclusion\r\n\r\nAs we wrap up our guide on securing an SSH server in Linux, let's recap the essential points and reflect on the importance of maintaining a robust security posture.\r\n\r\n### Key Points Recap\r\n\r\nHere's a quick summary of the crucial steps we've covered:\r\n\r\n- Use strong, unique passwords and consider implementing SSH keys\r\n- Disable root login and use a non-standard port\r\n- Implement fail2ban to protect against brute-force attacks\r\n- Configure firewall rules to limit SSH access\r\n- Keep your system and SSH software up-to-date\r\n- Use SSH protocol version 2 exclusively\r\n- Limit user access and implement two-factor authentication\r\n\r\n### Ongoing Vigilance\r\n\r\nRemember, securing your SSH server is not a one-time task. It requires ongoing attention and updates:\r\n\r\n- Regularly review and update your security measures\r\n- Stay informed about new vulnerabilities and patches\r\n- Monitor SSH logs for suspicious activities\r\n- Conduct periodic security audits\r\n\r\n### Balancing Security and Usability\r\n\r\nWhile security is paramount, it's essential to strike a balance with usability:\r\n\r\n| Security Measure          | Usability Impact            | Recommended Approach                |\r\n| ------------------------- | --------------------------- | ----------------------------------- |\r\n| Complex passwords         | Can be hard to remember     | Use a password manager              |\r\n| Two-factor authentication | Adds an extra step to login | Implement for critical systems      |\r\n| Strict firewall rules     | May limit legitimate access | Carefully whitelist necessary IPs   |\r\n| Frequent key rotation     | Requires regular updates    | Automate the process where possible |\r\n\r\nRemember, the goal is to create a secure environment that doesn't overly burden legitimate users. It's about finding the right balance for your specific needs and use cases.\r\n\r\n### Final Thoughts\r\n\r\nSecuring your SSH server is a critical step in protecting your Linux system from unauthorized access. By implementing the measures we've discussed and staying vigilant, you can significantly reduce the risk of security breaches.\r\n\r\n> \"Security is a journey, not a destination.\"\r\n\r\nThis adage holds particularly true for SSH security. As threats evolve, so must our defenses. Stay informed, stay proactive, and don't hesitate to seek expert advice when needed.\r\n\r\nBy following these guidelines and maintaining a security-first mindset, you'll be well on your way to maintaining a secure and efficient SSH server. Happy (and secure) SSHing!","src/content/posts/secure-ssh-server-linux.mdx",[1945],"../../assets/images/24/07/secure-ssh-server.jpeg","2f86ff74e5691e2f","secure-ssh-server-linux.mdx","sed-search-replace",{id:1948,data:1950,body:1959,filePath:1960,assetImports:1961,digest:1963,legacyId:1964,deferredRender:32},{title:1951,description:1952,date:1953,image:1954,authors:1955,categories:1956,tags:1957,canonical:1958},"Search and Replace Lines with Sed: Master Advanced Techniques","Learn how to master the power of sed Linux command to search and replace lines.",["Date","2023-11-24T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/sed-search-replace.jpeg",[19],[98],[1906,449],"https://www.bitdoze.com/sed-search-replace/","Navigating through large text files can be a daunting task, but with the power of `sed`, a stream editor, I've managed to simplify the process. Whether you're a developer, a system administrator, or just a curious techie, mastering `sed` commands can be a game-changer for your workflow.\r\n\r\n## What is `sed` and Why is It Useful?\r\n\r\n`sed`, short for stream editor, is a powerful tool found in most Unix-like operating systems. It's designed to process text in a stream, making it incredibly efficient for editing large amounts of data. **I've found** `sed` particularly useful when dealing with log files, configuration files, or any large text datasets that require quick and repetitive modifications.\r\n\r\nWhat makes `sed` stand out is its use of a non-interactive command line interface, which means it can operate without manual inputs beyond the initial command. This is ideal when I have to perform the same action on multiple files or need to automate editing tasks in a script. For instance, incorporating `sed` within shell scripts boosts productivity by automating routine text transformations.\r\n\r\nHere are a few reasons why I consider `sed` indispensable:\r\n\r\n- **Speed**: It works swiftly through streams of text without the need to open an editor interface.\r\n- **Flexibility**: `sed` commands are versatile, allowing complex patterns and actions to be executed.\r\n- **Power**: It supports regular expressions, providing a powerful way to match and manipulate text patterns.\r\n- **Automation**: It's perfect for batch processing files, saving time on repetitive tasks.\r\n\r\nOne scenario where `sed` proves to be useful is in renaming variables across multiple codebase files. Instead of manually finding and replacing each instance, `sed` can accomplish the same with a single command. Similarly, I can extract specific sections from logs or modify file contents on the fly.\r\n\r\nThe real strength of `sed` is realized when it is combined with other shell commands like `grep`, `awk`, or `cut`. The ability to **pipe commands together** allows for intricate text processing tasks to be completed with concise one-liners. In a nutshell, `sed` is a staple in my toolkit because it simplifies text manipulation tasks that are otherwise daunting and time-consuming.\r\n\r\n## Basic Syntax of `sed` Commands\r\n\r\nWhen diving into `sed` commands, it's essential to grasp the basic syntax. **Understanding sed's syntax** unlocks its potential; it follows a simple structure that, once mastered, allows for a myriad of text processing operations. The most common form of a `sed` command is:\r\n\r\n```sh\r\nsed [options] 'command' file\r\n```\r\n\r\nIn its simplest form, a `sed` command includes an action to be taken and the target text. To perform a basic find and replace operation, `sed` uses a syntax pattern that looks like this:\r\n\r\n```sh\r\nsed 's/old_text/new_text/' filename\r\n```\r\n\r\nThis command tells `sed` to search for `old_text` and replace it with `new_text` in the file specified by `filename`.\r\n\r\nThe `s` at the beginning of the command stands for substitute, which is the fundamental operation of replacing text. The `/` characters are delimiters that separate the different parts of the command. **Delimiters** can be any character not used in the old or new text, although the forward slash is conventional.\r\n\r\n**Options** like `-i` can alter the behavior of `sed`. For example, using:\r\n\r\n```sh\r\n sed -i 's/old_text/new_text/' filename\r\n```\r\n\r\napplies the changes directly to the file, effectively saving the edited version in place.\r\n\r\nThe power of `sed` is not just in simple substitutions; it can also perform complex pattern matching using regular expressions. A command like:\r\n\r\n```sh\r\n sed 's/[0-9]{4}/YEAR/' filename\r\n```\r\n\r\nwould replace any sequence of four digits with the text YEAR. Regular expressions open up a vast world of possibilities for pattern recognition and text manipulation.\r\n\r\nBeyond simple text replacement, `sed` commands can:\r\n\r\n- [Delete lines](https://www.bitdoze.com/sed-delete-lines/) using the `d` command\r\n- [Insert or append text with](https://www.bitdoze.com/sed-insert-append-text/) `i` and `a` commands\r\n- [Transform text by changing case](https://www.bitdoze.com/sed-change-case/)\r\n- [Search and replace text](https://www.bitdoze.com/sed-search-replace/)\r\n- **Optimize workflow** by supporting multiple commands in a single run\r\n\r\nBy experimenting with these commands and options, I've found that `sed` can handle most text processing needs with ease. It's all about choosing the right combination of patterns, commands, and options to get the job done.\r\n\r\n## Finding and Replacing Text Using `sed`\r\n\r\nWhen working with text files, I often need to find and replace specific strings. `sed` makes this task effortless with its `s` command, which stands for \"substitute.\" To use it, you need to define the pattern to search for and the text to replace it with, along with any flags to control the operation.\r\n\r\nHere's the basic structure of the find and replace command in `sed`:\r\n\r\n```sh\r\nsed 's/original/replacement/' file.txt\r\n```\r\n\r\nIn this command, `s` triggers the substitution, `original` is the text you're targeting, and `replacement` is the text you want to insert in its place.\r\n\r\nTo elaborate, let's say I want to replace the word \"apple\" with \"orange\" in a file. I'd use the following command:\r\n\r\n```sh\r\nsed 's/apple/orange/' fruitlist.txt\r\n```\r\n\r\nBy default, `sed` only replaces the first occurrence of the pattern in each line. To replace all instances in a file, I’d add the `g` flag at the end of the command, which stands for \"global\":\r\n\r\n```sh\r\nsed 's/apple/orange/g' fruitlist.txt\r\n```\r\n\r\nAdditionally, `sed` supports regular expressions, allowing for more complex pattern matching. For example, to replace every word that starts with \"a\" with \"orange\", I'd use:\r\n\r\n```sh\r\nsed 's/\\\\ba\\\\w\\*/orange/g' fruitlist.txt\r\n```\r\n\r\nIn the above command, `\\b` marks a word boundary, `a` is the literal character we start with, and `\\w*` matches any word character that follows.\r\n\r\nMoreover, to make these changes directly in the file, I'd use the `-i` option, which stands for \"in-place\":\r\n\r\n```sh\r\nsed -i 's/apple/orange/g' fruitlist.txt\r\n```\r\n\r\n> Beware though, using `-i` will overwrite the original file, so it's a good practice to back up the file before making in-place changes.\r\n\r\n## Using Regular Expressions with `sed` for More Advanced Text Manipulation\r\n\r\nWhen it’s time to go beyond simple find and replace within `sed`, I often turn to **regular expressions (regex)**. These powerful sequences of characters act as patterns for matching and manipulating text at a level of complexity unreachable by basic string matching.\r\n\r\nOne of the most commonly utilized regular expressions in `sed` is the **wildcard character (**`.`), which stands in for any single character within a string. For example:\r\n\r\n```sh\r\nsed 's/te.t/test/' filename\r\n```\r\n\r\nThis command replaces any four-letter word that starts with 'te' and ends with 't' with the word 'test'. Here, the wildcard replaces the character between 'e' and 't'.\r\n\r\nAnother regex feature in `sed` is **character classes**. You can use these to match any character from a defined set:\r\n\r\n```sh\r\nsed 's/\\[aeiou\\]/?/g' filename\r\n```\r\n\r\nIn this command, every vowel in the file gets replaced with a question mark.\r\n\r\nTo further refine text manipulation, **quantifiers** such as `*`, `+`, and `{}` tell `sed` how many instances of the preceding character or group to match:\r\n\r\n- The `*` quantifier matches zero or more occurrences\r\n- The `+` quantifier matches one or more occurrences\r\n- Curly braces `{}` can specify an exact number of occurrences\r\n\r\nHere’s an example that matches strings of one or more digits:\r\n\r\n```sh\r\nsed 's/\\[0-9\\]+/\\*\\*\\*/g' filename\r\n```\r\n\r\nThis command replaces sequences of digits with asterisks.\r\n\r\nFor situations demanding precision, **anchors** `^` for the start of a line and `$` for the end are invaluable. Let’s say I want to remove lines that contain nothing but whitespace:\r\n\r\n```sh\r\nsed '/^$/d' filename\r\n```\r\n\r\nWith this command, any line matching the start-followed-immediately-by-the-end pattern—essentially empty lines—gets deleted.\r\n\r\n**Grouping** is another feature of regex in `sed` that amplifies its flexibility. By enclosing parts of the regex in parentheses `()`, you can choose to operate on that segment alone. My go-to example involves swapping the first two words of a line:\r\n\r\n```sh\r\nsed 's/^(\\[^ \\]*) (\\[^ \\]*)/\\\\2 \\\\1/' filename\r\n```\r\n\r\n## Tips and Tricks for Efficient Text Searching and Replacing With `sed`\r\n\r\nMastering `sed` requires an understanding of the nuances that can make text processing both efficient and powerful. **Careful crafting of** `sed` commands can drastically improve your productivity. Here are some valuable tips that I've learned over the years.\r\n\r\nFirstly, knowing how to limit `sed` operations to certain lines can greatly speed up your workflow. This can be done by specifying **line numbers or patterns**. For example, if you only want to replace text in the first 10 lines, you would use:\r\n\r\n```sh\r\nsed '1,10s/search/replace/' file.txt\r\n```\r\n\r\nThis avoids unnecessary scanning of the entire file.\r\n\r\nWhen dealing with large files, utilizing the `-n` flag combined with the `p` command can help you preview changes before actually applying them. Run:\r\n\r\n```sh\r\n sed -n 's/search/replace/p' file.txt`\r\n```\r\n\r\nto see what will be changed. This is particularly useful for avoiding costly mistakes.\r\n\r\nAnother trick is to **escape special characters** properly. If your search pattern includes characters like `*`, `.`, `?`, or `/`, remember to precede them with a backslash `\\` to ensure they're treated as literals. Forgetting to do so can lead to unexpected results since these are regular expression metacharacters.\r\n\r\nMoreover, when you're tasked with replacing strings across multiple files, `sed` becomes even handier with the use of **xargs or a for loop**.\r\n\r\nThe command:\r\n\r\n```sh\r\nfind . -type f -name \"*.txt\" | xargs sed -i 's/search/replace/g'`\r\n```\r\n\r\nreplaces 'search' with 'replace' in all `.txt` files within the directory.\r\n\r\nLeveraging **backreferences** is a cornerstone for complex replacements. When you group parts of the search pattern using parentheses `()`, you can refer to these groups in the replace pattern. Say you want to swap 'foo' and 'bar' in a line -\r\n\r\n```sh\r\nsed 's/\\(foo\\).*\\(bar\\)/\\2 \\1/'\r\n```\r\n\r\ndoes the trick.\r\n\r\nLastly, always perform a dry run or backup your files before running `sed` with the `-i` option. I can't stress this enough; **having a backup** can save you from the unintended consequences that might not be immediately apparent, especially when dealing with large-scale file modifications.\r\n\r\n## Conclusion\r\n\r\nMastering `sed` for searching and replacing text is a game-changer for anyone who works with text files regularly. The tricks I've shared are just the start to harnessing the full potential of this powerful stream editor. Remember to craft your commands with precision and always double-check your patterns. Whether you're tweaking a single file or tackling multiple files at once, `sed` can be your best ally—just be sure to back up your data before you dive in. With these strategies in your toolkit, you're well on your way to becoming a `sed` command wizard. Happy editing!","src/content/posts/sed-search-replace.mdx",[1962],"../../assets/images/23/11/sed-search-replace.jpeg","96d2ff27094f279a","sed-search-replace.mdx","sed-delete-lines",{id:1965,data:1967,body:1976,filePath:1977,assetImports:1978,digest:1980,legacyId:1981,deferredRender:32},{title:1968,description:1969,date:1970,image:1971,authors:1972,categories:1973,tags:1974,canonical:1975},"Deleting Lines with Sed: Master Advanced Techniques","Learn how to master the power of sed Linux command to delete lines.",["Date","2023-11-24T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/23/11/sed-delete-lines.jpeg",[19],[98],[1906,449],"https://www.bitdoze.com/sed-delete-lines/","Ever stumbled upon a bulky text file and wished you could quickly remove unnecessary lines? That's where `sed`, the stream editor, becomes your text-processing superhero. I'm going to show you how to wield `sed` to effortlessly delete lines from any text file.\r\n\r\n## What is `sed` and Why Is It Useful for Deleting Lines?\r\n\r\n`sed`, short for **stream editor**, is a powerful tool in Unix and Unix-like operating systems. It's designed for filtering and transforming text. When I'm dealing with large files, `sed` is my go-to utility because it makes text manipulation a breeze. It doesn't just delete lines; `sed` can also be utilized for inserting, updating, and finding text within files.\r\n\r\nOne of the main reasons `sed` is exceptionally useful is its **non-interactive nature**. This means I can make changes to files without opening them in a text editor. This is particularly advantageous when I'm working with server files or automating text edits in scripts.\r\n\r\nFurthermore, `sed` operates in a **streaming** fashion, reading input line by line, which means it can process large files without loading the entire file into memory. This approach is resource-efficient and ideal for systems with limited memory. For deleting lines, `sed` offers simple yet **powerful commands** that can target:\r\n\r\n- Specific lines by their number\r\n- Ranges of lines\r\n- Lines matching a particular pattern\r\n\r\nHere's why `sed` stands out for me while deleting lines:\r\n\r\n- **Speed**: `sed` executes quickly, even when sifting through vast amounts of data.\r\n- **Flexibility**: It doesn't require a graphical interface and works well both within scripts and command lines.\r\n- **Precision**: Commands can be crafted to match very specific text patterns ensuring only the desired lines are deleted.\r\n\r\nBy incorporating regular expressions with `sed` commands, I can further refine the criteria for line deletion, making it a tailored solution for the task at hand. It reminds me not just of a simple text manipulation tool, but of a surgical instrument with the finesse to address complex editing needs.\r\n\r\nHaving understood the nature of `sed` and its aptitude for line deletion, it becomes clear why many prefer its utility in text file manipulation. It's not just about deletion; `sed` embodies the essence of streamlined editing, providing a multitude of functions beyond the scope of this section.\r\n\r\nWith sed, you've got a robust toolkit for any sort of text manipulation you might need to perform:\r\n\r\n- [Delete lines](https://www.bitdoze.com/sed-delete-lines/) using the `d` command\r\n- [Insert or append text with](https://www.bitdoze.com/sed-insert-append-text/) `i` and `a` commands\r\n- [Transform text by changing case](https://www.bitdoze.com/sed-change-case/)\r\n- [Search and replace text](https://www.bitdoze.com/sed-search-replace/)\r\n\r\n## Understanding the `sed` Syntax for Deleting Lines\r\n\r\nBefore we dive into the specifics, it's crucial to grasp the syntax of `sed` that allows us to manipulate text with precision. The `sed` command uses a simple yet powerful syntax that you can quickly become familiar with. Here's a breakdown of the basic components:\r\n\r\n- `d` command: In `sed`, the `d` command is used specifically for deletion.\r\n- **Addresses**: You can specify which lines to delete by using addresses that can be line numbers, ranges, or patterns.\r\n\r\nLet's elaborate on how these components come together.\r\n\r\nWhen I want to delete a single line, the syntax is straightforward: `\r\n\r\n```shell\r\nsed 'Nd' file.txt\r\n```\r\n\r\nwhere `N` is the number of the line I wish to remove. To demonstrate the power of `sed`, let's say I need to delete a range of lines, from the second to the fifth.\r\n\r\nHere's how I'd do it:\r\n\r\n```shell\r\nsed '2,5d' file.txt\r\n```\r\n\r\nThe comma between the numbers specifies the range.\r\n\r\nPatterns offer even more control. If I'm looking to delete lines containing a particular string, let's say \"Error\", my command would look like this:\r\n\r\n```shell\r\nsed '/Error/d' file.txt\r\n```\r\n\r\nThe forward slashes `/` encapsulate the pattern, telling sed to search for the string and apply the `d` action to each matching line.\r\n\r\n**Regular expressions** elevate the capability to target lines with precision. For example:\r\n\r\n```shell\r\nsed '/^#/d' file.txt\r\n```\r\n\r\nwill delete all lines starting with a hashtag, crucial for cleaning up comments in configuration files.\r\n\r\nUnderstanding `sed` also involves its execution context:\r\n\r\n- **Default behavior**: By default, `sed` prints all lines to standard output, except those deleted.\r\n- **In-place editing**: Using the `-i` option allows changes to be written directly to the file.\r\n\r\nRemember, sed works in a cycle, reading a line into a pattern space, processing it, and then outputting it before moving onto the next line. Mastering the nuances of `sed` syntax equips you with the knowledge to perform complex text manipulations efficiently, making tasks like line deletion seem almost effortless.\r\n\r\n## How to Delete Specific Lines with `sed`\r\n\r\nWhen you've got a clear target, deleting specific lines using `sed` becomes a straightforward process. Let's say I need to delete the 2nd line from a file. I'd use:\r\n\r\n```shell\r\nsed '2d' filename\r\n```\r\n\r\nand just like that, the line disappears.\r\n\r\nBut it’s not just about single lines. Sometimes I might need to remove lines that start with a certain character or match a specific word. Here's where **pattern matching** comes in handy. For instance, to remove lines that start with `\"#\"` (commonly used for comments), the command:\r\n\r\n```shell\r\nsed '/^#/d' filename\r\n```\r\n\r\ngets the job done efficiently.\r\n\r\nImagine you're working with a massive log file and you need to extract information minus the lines containing errors. A command like `sed '/error/d' filename` becomes invaluable because it scans each line for the word \"error\" and removes such lines, leaving behind a much cleaner data set.\r\n\r\nWhat about ranges? Suppose I want to chop off a slice of a document, say from line 10 to line 20.\r\n\r\n```sh\r\nsed '10,20d' filename\r\n```\r\n\r\nmakes that happen without breaking a sweat.\r\n\r\nI always double-check before I execute these commands, especially when using the **in-place edit option** (-i), as it overwrites the original file. If I slip up with my addresses or patterns, there's no undo button. That's why I sometimes redirect the output to a new file or preview it on the terminal before committing the changes.\r\n\r\nBy leveraging the power of **regular expressions**, I can amp up my line-deleting prowess to match exact phrases, words, or even complex patterns. This level of precision is what makes `sed` an indispensable tool in my text processing arsenal. It’s not just about removing what’s unnecessary; it’s about reshaping the content to fit the task at hand.\r\n\r\nSo remember, whether it’s cleaning up data, refining configurations, or formatting content – mastering the use of `sed` to delete specific lines is a skill that pays dividends.\r\n\r\n## Using `sed` to Delete Lines Based on Patterns or Regular Expressions\r\n\r\nWhen you're juggling text files, knowing how to **delete lines based on patterns** with `sed` can be a lifesaver. I've found that using regular expressions elevates `sed` from a simple editing tool to a powerful text-processing ally.\r\n\r\nTo delete lines that match a specific pattern, the `sed` command can be paired with regular expressions. For example, if I want to delete lines containing the word \"error\", I'd use the following command:\r\n\r\n```sh\r\nsed '/error/d' filename\r\n```\r\n\r\nThis tells `sed` to search for the word \"error\" in the file and delete any line that contains it. But what if the pattern is more complex? Regular expressions come into play here. Say I need to delete lines that end with a number, I'd use `sed` with a pattern that matches any line ending with a digit:\r\n\r\n```sh\r\nsed '/\\[0-9\\]$/d' filename\r\n```\r\n\r\n> Remember, regular expressions are potent and require precision. A minor mistake could lead to unintended deletions.\r\n\r\nIn some cases, you might need to match patterns across multiple lines. While `sed` is predominantly a line-oriented tool, it's possible to weave commands that work on patterns spanning multiple lines. This requires using `sed`'s hold and pattern space effectively—a topic I'll dive into in more detail shortly.\r\n\r\n**Examples and practice** make perfect when it comes to mastering `sed` with patterns. Here's how you'd delete every line except those containing the word \"success\":\r\n\r\n```sh\r\nsed '/success/!d' filename\r\n```\r\n\r\nThe exclamation mark `!` is the negation operator in `sed`, which makes this command an inverse match—delete all except a match.\r\n\r\nAdditionally, if you're dealing with complex patterns or special characters, be sure to escape them properly or use extended regular expressions with `sed -E`. This can help avoid common pitfalls and ensure the patterns are interpreted as intended.\r\n\r\nAs with any command-line tool, I always recommend running `sed` commands without the `-i` flag first. This gives you a preview of the changes before they're permanently applied to the file. Once you're confident the command does exactly what you want, you can execute it with `-i` to make the changes in place.\r\n\r\n## Advanced Techniques for Deleting Lines with `sed`\r\n\r\nSometimes, it's necessary to get more sophisticated with `sed` to handle more complex file editing tasks. Deleting lines from a file isn't always as straightforward as removing single unneeded entries; sometimes lines are part of multi-line patterns or require a more nuanced approach.\r\n\r\nOne advanced technique involves **deleting lines within a range**. To do this in `sed`, I can specify the start and end of the pattern range using their line numbers or pattern matches. For example, to delete lines 10 to 20 in a file, the command would be:\r\n\r\n```sh\r\nsed '10,20d' file.txt\r\n```\r\n\r\nIf I need to delete lines that are between two patterns, say a start marker and an end marker, I might use a command like:\r\n\r\n```sh\r\nsed '/start/,/end/d' file.txt\r\n```\r\n\r\nAnother powerful feature of `sed` is its ability to work with back-references. By using parentheses to capture patterns and then referencing them with a backslash and number, I can search for a pattern and then delete the line associated with that pattern. It's vital to escape the parentheses with a backslash to use them for capturing groups.\r\n\r\nThe example below deletes lines containing a duplicated word:\r\n\r\n```sh\r\nsed '/(&lt;\\[\\[:alpha:\\]\\]+&gt;).\\*\\\\1/d' file.txt\r\n```\r\n\r\nLastly, I'll address the sed in-situ editing with caution. The `-i` option allows `sed` to edit files in place. Nevertheless, if the patterns are complex, it's better to test the command without `-i` first. Always back up important files before running such commands, to avoid data loss.\r\n\r\nWorking with **hold and pattern buffers** also showcases 'sed's prowess. By using `h` to copy a line into the hold buffer, and `g` to replace the current line with the contents of the hold buffer, complex multi-line patterns can be manipulated with relative ease. Multi-pass editing, where `sed` runs through the input file multiple times, can be useful when the situation calls for it but requires careful crafting of the command sequence.\r\n\r\nLearning these advanced techniques elevates the control and precision I have over text processing. With practice and attention to detail, 'sed' becomes an indispensable tool in any text-processing or system administration toolkit.\r\n\r\n## Conclusion\r\n\r\nMastering `sed` for deleting lines means you're well on your way to becoming a text processing wizard. It's crucial to remember the power of `sed` commands, especially when paired with options like `-i` for direct file editing. As I've shared, using hold and pattern buffers can elevate your command line skills to new heights, allowing for intricate manipulation of text data. Always test your `sed` commands to ensure accuracy—after all, precision is key. With these advanced techniques under your belt, you're now equipped to tackle complex text processing tasks with confidence. Keep experimenting and refining your skills; there's always more to learn with `sed`.\r\n\r\n## Frequently Asked Questions\r\n\r\n### 1\\. How can I delete lines within a specific range using `sed`?\r\n\r\nTo delete lines within a specific range using `sed`, you can specify the start and end of the range using line numbers or pattern matches. For example, to delete lines 3 to 7, you can use the command `sed '3,7d' filename`. Similarly, you can use pattern matches like `sed '/start/,/end/d' filename` to delete lines between two patterns.\r\n\r\n### 2\\. Can I use back-references to search for a pattern and delete associated lines?\r\n\r\nYes, you can use back-references in `sed` to search for a pattern and delete associated lines. For example, to delete lines that contain a specific word, you can use `sed '/\\bword\\b/d' filename`. The `\\b` represents a word boundary, and the `d` command deletes the lines that match the pattern.\r\n\r\n### 3\\. Should I be cautious when using the `-i` option for in-situ editing?\r\n\r\nYes, it is advisable to be cautious when using the `-i` option for in-situ editing with `sed`. The `-i` option allows you to edit files in place, but it can overwrite the original file without creating a backup. It's recommended to test your commands without the `-i` option first, and once you're confident, add it to edit files directly.\r\n\r\n### 4\\. Are hold and pattern buffers useful for manipulating complex multi-line patterns?\r\n\r\nYes, hold and pattern buffers are useful for manipulating complex multi-line patterns in `sed`. The hold buffer allows you to save lines and retrieve them later for processing, while the pattern buffer holds the current line and can be used to perform actions like deletion, rearrangement, or substitution. Using these buffers, you can achieve better control and precision when working with complex patterns.\r\n\r\n### 5\\. Why is it important to learn advanced techniques for text processing?\r\n\r\nLearning advanced techniques for text processing, such as those discussed in the article, can provide you with better control and precision when manipulating text. These techniques allow you to perform complex operations like deleting specific lines, manipulating multi-line patterns, and more. By mastering these techniques, you can enhance your text processing skills and efficiently work with large amounts of data, saving time and effort.","src/content/posts/sed-delete-lines.mdx",[1979],"../../assets/images/23/11/sed-delete-lines.jpeg","ce0854f4e7b8f192","sed-delete-lines.mdx","seo-gets-tool",{id:1982,data:1984,body:1994,filePath:1995,assetImports:1996,digest:1998,legacyId:1999,deferredRender:32},{title:1985,description:1986,date:1987,image:1988,authors:1989,categories:1990,tags:1991,canonical:1993},"SEO Gets Tool - Google Search Console on Steroids","Keep track of your website stats in Google Search Console with SEO Gets.",["Date","2024-02-12T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/02/seo-gets-blog.jpeg",[19],[77],[1992],"seo","https://www.bitdoze.com/seo-gets-tool/","import Button from \"../../layouts/components/widgets/Button.astro\";\r\nimport { Picture } from \"astro:assets\";\r\nimport YouTubeEmbed from \"../../layouts/components/widgets/YouTubeEmbed.astro\";\r\nimport imag1 from \"../../assets/images/24/02/seo-gets-overview.jpeg\";\r\nimport imag2 from \"../../assets/images/24/02/seo-gets-website-overview.jpeg\";\r\n\r\n[SEOgets](https://seogets.com/) is a tool designed to provide an alternative to Google Search Console for search performance analytics. It's created in NextJS. It organizes search performance data in an easy-to-understand format, allowing users to make better-informed decisions about their SEO strategies. Here are some of the features and capabilities of SEOgets:\r\n\r\n- **Analytics**: SEOgets offers analytics for search data, similar to what users would find in Google Search Console, but with a focus on ease of use and readability.\r\n- **Content Groups**: This feature allows users to group content and analyze its performance collectively, which can be beneficial for SEO analytics.\r\n- **Keyword Tracking**: SEOgets tracks the performance of keywords with real data, providing insights into clicks, pages, and queries related to the most important keywords. This helps users understand if their SEO strategies are effective or if changes are needed.\r\n- **SEO Insights**: The tool is designed to deliver SEO insights quickly, potentially saving users hours of analysis time. It presents all sites at a glance, showing clicks and impressions for all sites on a single page.\r\n- **Integration with Google Search Console**: SEOgets integrates with Google Search Console, pulling in data for analysis. It only requires read access to the user's Google Search Console data.\r\n\r\nIt's important to note that SEOgets is currently free to use but will become a paid service in 2024. Early adopters are promised a special discount. SEOgets does not completely replace Google Search Console, as there are other features in Google Search Console that SEOgets does not support.\r\n\r\nIn summary, SEOgets is a tool that simplifies the analysis of search performance data, offering an alternative to Google Search Console with additional features like content grouping and keyword tracking to aid in SEO decision-making.\r\n\r\n**Google Indexing Script**\r\n\r\nAlthough not a direct feature of SEOgets, the creator, Guilherme Oenning, has also developed a Google Indexing Script that can be used in conjunction with SEOgets. This script helps with faster [Google indexing](https://seogets.com/blog/google-indexing-script) by allowing users to input a list of URLs for Googlebot to crawl and index within 48 hours. It also checks indexing status daily and allows for re-submission of URLs that return validation errors.\r\n\r\n## SEO Gets Overview\r\n\r\n<YouTubeEmbed\r\n  url=\"https://www.youtube.com/embed/Gff9Qfpx5v4\"\r\n  label=\"SEO Gets Tool\"\r\n/>\r\n\r\nOnce you sign in with your Google account all the websites that are under your Google Search Console it will be pulled and data will be displayed for all, you will be able to see an overview with all your websites and choose the period.\r\n\r\n<Picture src={imag1} alt=\"Seo Gets Overview\" />\r\n\r\nIf you click on a website you will be brought in a dashboard where you will see all the details about the website with visitors, keywords and countries. You have the option to filter them by **Popular, Growing, Decaying** and choose the exact period you want.\r\n\r\nAnother useful feature is the `Tracked Keywords` that will group stats after the keyword you add. In this way you have an overview with keywords and what is happening with them. Once you choose one will present the stats for it.\r\n\r\n_Content Groups_ will allow you to add content clusters that you can track you can add:\r\n\r\n```\r\n/blog\r\n/blog/*\r\n/subject*\r\n```\r\n\r\nI am using them to track the clusters I have and see what is happening. Below is a picture with the overview:\r\n\r\n<Picture src={imag2} alt=\"Seo Gets Website Overview\" />\r\n\r\n## Conclusions\r\n\r\nSeo Gets it's free currently and can be used by anyone, just give it a try and see if it helps you. I am playing with it for around a week and I can say I am happy with what it has. It helps me see exactly what is happening with my website.","src/content/posts/seo-gets-tool.mdx",[1997],"../../assets/images/24/02/seo-gets-blog.jpeg","8ff0de189cd48621","seo-gets-tool.mdx","self-hosted-airtable-alternatives",{id:2000,data:2002,body:2011,filePath:2012,assetImports:2013,digest:2015,legacyId:2016,deferredRender:32},{title:2003,description:2004,date:2005,image:2006,authors:2007,categories:2008,tags:2009,canonical:2010},"Best Open Source Self-hosted Airtable Alternatives","Check out this list with the best open source self hosted apps with docker containers that you can use on your as an airtable alternative.",["Date","2024-11-19T00:00:00.000Z"],"__ASTRO_IMAGE_../../assets/images/24/11/self-hosted-airtable-alternatives.jpeg",[19],[98],[100],"https://www.bitdoze.com/self-hosted-airtable-alternatives/","Looking for the best self-hosted Airtable alternatives? You've come to the right place! While Airtable is a fantastic tool for organizing and managing data with its spreadsheet-database hybrid capabilities, some users prefer self-hosted solutions that offer more control, customization, and privacy.\r\n\r\nWhether you're concerned about data sovereignty, need more features, or simply want to avoid subscription fees, there are several impressive options that can fit your needs. In this article, we'll explore some of the top contenders that provide the flexibility and power you're looking for in a self-hosted environment. In case you are interested to check the best self hosted apps for your business you can check: [Best Self Hosted Apps for Business](https://www.bitdoze.com/docker-containers-business/)\r\n\r\n## Overview of Airtable and its Popularity\r\n\r\nAirtable has revolutionized the way we think about databases and spreadsheets. Its intuitive interface combines the familiarity of a spreadsheet with the power of a relational database, making it accessible to both technical and non-technical users. The platform's popularity stems from its versatility, allowing users to create everything from simple to-do lists to complex project management systems.\r\n\r\nKey features that have contributed to Airtable's success include:\r\n\r\n- Customizable views (Grid, Calendar, Kanban, Gallery)\r\n- Rich field types (including attachments, long text, and formulas)\r\n- Automation capabilities\r\n- Integrations with popular tools and services\r\n- Collaboration features\r\n\r\nDespite its advantages, Airtable's cloud-based nature raises concerns about data privacy, control, and long-term costs for some users. This has led to a growing interest in self-hosted alternatives that offer similar functionality while addressing these concerns.\r\n\r\n## Understanding Self-Hosted Database Solutions\r\n\r\nSelf-hosted database solutions provide an alternative to cloud-based services like Airtable by allowing users to run the software on their own infrastructure. This approach offers several benefits:\r\n\r\n1. **Data Control**: You have complete ownership and control over your data, ensuring compliance with data protection regulations.\r\n2. **Customization**: Self-hosted solutions often allow for deeper customization to fit specific needs.\r\n3. **Cost-Effectiveness**: While there may be initial setup costs, self-hosting can be more economical in the long run, especially for larger organizations.\r\n4. **Privacy**: Your data remains on your servers, reducing the risk of external breaches.\r\n5. **Performance**: You can optimize hardware and network configurations for your specific use case.\r\n\r\nHowever, self-hosting also comes with responsibilities:\r\n\r\n- Server management and maintenance\r\n- Security implementation and updates\r\n- Backup and disaster recovery planning\r\n\r\nFor organizations with the technical expertise or willingness to learn, these challenges often outweigh the benefits of having full control over their data infrastructure.\r\n\r\n## Where You Can Host Airtable Alternatives\r\n\r\nWhen considering self-hosted Airtable alternatives, you have several options for where to deploy your chosen solution. Each option has its own set of advantages and considerations.\r\n\r\n### VPS Server With Hetzner, DigitalOcean, etc.\r\n\r\nVirtual Private Servers (VPS) offer a flexible and cost-effective way to host your Airtable alternative. Providers like Hetzner and DigitalOcean offer scalable resources and easy deployment options.\r\n\r\n**Advantages:**\r\n- Affordable starting prices\r\n- Scalable resources\r\n- High availability and uptime\r\n- Managed services options\r\n\r\n**Considerations:**\r\n- Requires some technical knowledge for setup and maintenance\r\n- Network bandwidth limitations may apply\r\n- Shared hardware resources (unless using dedicated instances)\r\n\r\nTo get started, you can visit [Hetzner](https://go.bitdoze.com/hetzner) or [DigitalOcean](https://go.bitdoze.com/do) to explore their VPS offerings.\r\n\r\n\r\n> In case you are interested to monitor server resources like CPU, memory, disk space you can check: [How To Monitor Server and Docker Resources](https://www.bitdoze.com/sever-monitoring/)\r\n\r\n### Home Server\r\n\r\nFor those who prefer complete control over their hardware and network, a home server can be an excellent option for hosting Airtable alternatives.\r\n\r\n#### Mini PC\r\n\r\nMini PCs offer a compact and energy-efficient solution for home servers. They're powerful enough to run most self-hosted applications while taking up minimal space.\r\n\r\n**Advantages:**\r\n- Complete hardware control\r\n- One-time hardware cost\r\n- Low power consumption\r\n\r\n**Considerations:**\r\n- Requires a stable home internet connection\r\n- May need additional setup for remote access\r\n- Limited upgradeability compared to full-sized servers\r\n\r\nFor more information on setting up a mini PC as a home server, check out this guide on [the best mini PCs for home servers](https://www.bitdoze.com/best-mini-pc-home-server/).\r\n\r\n#### NAS (Network Attached Storage)\r\n\r\nNAS devices are purpose-built for file storage and sharing, but many modern NAS systems can also run Docker containers, making them suitable for hosting Airtable alternatives.\r\n\r\n**Advantages:**\r\n- Built-in storage redundancy\r\n- Low power consumption\r\n- Often includes easy-to-use management interfaces\r\n\r\n**Considerations:**\r\n- Higher initial cost compared to mini PCs\r\n- May have limited CPU power for resource-intensive applications\r\n- Some technical knowledge required for advanced setups\r\n\r\nPopular NAS brands include Synology, QNAP, and Asustor, which offer various models suitable for home and small business use.\r\n\r\n## Top Self-Hosted Airtable Alternatives\r\n\r\n\r\n> If you are interested to see some free cool open source self hosted apps you can check [toolhunt.net self hosted section](https://toolhunt.net/sh/).\r\n\r\n\r\nWhen considering self-hosted Airtable alternatives, it's essential to compare their features, complexity, resource usage, and key benefits. The following table provides an overview of the top options we'll be discussing in detail:\r\n\r\n| Application | Complexity | Resource Usage | Key Benefits |\r\n|-------------|------------|----------------|--------------|\r\n| NocoDB      | Low        | Low to Medium  | Easy setup, MySQL/PostgreSQL support |\r\n| Baserow     | Medium     | Medium         | User-friendly interface, extensible |\r\n| AITable     | Medium     | Medium to High | AI-powered features, scalable |\r\n| Grist       | Low        | Low            | Spreadsheet-like interface, Python scripting |\r\n| undb        | Low        | Low            | Lightweight, fast performance |\r\n\r\nNow, let's dive into each of these alternatives in more detail.\r\n\r\n### NocoDB\r\n---\r\n\r\n![nocodb ui](../../assets/images/24/11/nocodb.png)\r\n\r\n[NocoDB](https://nocodb.com/) is an open-source, self-hosted alternative to Airtable that transforms any MySQL, PostgreSQL, Microsoft SQL Server, SQLite, or MariaDB into a smart spreadsheet. It offers a user-friendly interface that combines the power of a database with the simplicity of a spreadsheet.\r\n\r\nKey features include:\r\n- Multiple views (Grid, Gallery, Form, Kanban)\r\n- Role-based access control\r\n- REST and GraphQL APIs\r\n- Webhooks and integrations\r\n- Automations and workflows\r\n\r\n#### Why Choose NocoDB?\r\n\r\nNocoDB stands out for its ease of use and compatibility with existing databases. It's an excellent choice for teams that want to leverage their current database infrastructure while gaining the benefits of a spreadsheet-like interface.\r\n\r\n#### Getting Started with NocoDB\r\n\r\nTo set up NocoDB, you can use Docker with the following command:\r\n\r\n```bash\r\ndocker run -d --name nocodb -p 8080:8080 nocodb/nocodb:latest\r\n```\r\n\r\nFor more detailed installation instructions and configuration options, visit the [NocoDB GitHub repository](https://github.com/nocodb/nocodb).\r\n\r\n#### Use Cases\r\n\r\n- Project management\r\n- Customer relationship management (CRM)\r\n- Inventory tracking\r\n- Event planning and management\r\n\r\n#### Final Thoughts\r\n\r\nNocoDB offers a compelling balance of features and simplicity, making it an excellent choice for teams looking for an easy-to-deploy Airtable alternative that can work with their existing databases.\r\n\r\n### Baserow\r\n---\r\n![baserow ui](../../assets/images/24/11/baserow.webp)\r\n\r\n[Baserow](https://baserow.io/) is an open-source no-code database tool and Airtable alternative. It provides a user-friendly interface for creating and managing relational databases without requiring extensive technical knowledge.\r\n\r\nKey features include:\r\n- Intuitive drag-and-drop interface\r\n- Multiple field types (text, number, date, file, etc.)\r\n- Views (Grid, Gallery, Form)\r\n- User management and permissions\r\n- REST API\r\n- Extensible plugin system\r\n\r\n#### Why Choose nocodb?\r\n\r\nBaserow is ideal for teams that need a flexible, user-friendly database solution with the potential for customization through its plugin system. Its open-source nature allows for community contributions and extensions.\r\n\r\n#### Getting Started with Baserow\r\n\r\nTo deploy Baserow using Docker, use the following command:\r\n\r\n```bash\r\ndocker run -d --name baserow -p 80:80 -p 443:443 baserow/baserow:latest\r\n```\r\n\r\nFor more detailed setup instructions and configuration options, visit the [Baserow documentation](https://baserow.io/docs/installation/install-with-docker).\r\n\r\n#### Use Cases\r\n\r\n- Content management systems\r\n- Product catalogs\r\n- Research data management\r\n- Collaborative team workflows\r\n\r\n#### Final Thoughts\r\n\r\nBaserow offers a balance of ease of use and extensibility, making it a strong contender for teams that need a customizable, self-hosted database solution with a low barrier to entry.\r\n\r\nThank you for providing the search results. I'll rewrite the AITable section to focus on AITable.ai, following the same format as the other alternatives:\r\n\r\n### AITable.ai\r\n---\r\n\r\n![aitable ui](../../assets/images/24/11/aitableai.png)\r\n\r\nAITable.ai is a revolutionary no-code platform that combines the power of a database with the flexibility of a spreadsheet, enhanced by AI capabilities. It offers a comprehensive suite of tools for data management, analysis, and AI-driven automation.\r\n\r\nKey features include:\r\n- AI-powered data analysis and insights\r\n- Multiple views (Table, Kanban, Calendar, Gantt)\r\n- No-code app builder for CRM, ERP, and project management\r\n- Custom ChatGPT & AI Agents Builder\r\n- AI Copilot for website integration\r\n- Workflow automation with AI assistance\r\n- Integration with over 6,000 apps\r\n- Unlimited dashboards and records (up to 50,000 per datasheet)\r\n- AI sales leads collection\r\n- API access and embedding options\r\n\r\n#### Why Choose AITable.ai?\r\n\r\n[AITable.ai](https://aitable.ai/) is an excellent choice for organizations that want to leverage AI capabilities in their data management processes while maintaining control through self-hosting. Its advanced features, open-source nature, and no-code approach make it suitable for complex data handling, analysis tasks, and custom application development without coding expertise.\r\n\r\n#### Getting Started with AITable.ai\r\n\r\nTo deploy AITable.ai, you can use Docker. You can check [AITable.ai docker compose file](https://github.com/apitable/apitable/blob/develop/docker-compose.yaml) for more details.\r\n\r\n#### Use Cases\r\n\r\n- AI-driven data analysis and visualization\r\n- Custom CRM, ERP, and project management systems without coding\r\n- AI-powered customer service and sales chatbots\r\n- Marketing automation and campaign management\r\n- Business intelligence dashboards with real-time insights\r\n- Knowledge management and team collaboration\r\n- AI-assisted data entry and cleaning\r\n\r\n#### Final Thoughts\r\n\r\nAITable.ai offers a unique blend of traditional database functionality, AI-powered features, and no-code application development. It's a powerful choice for organizations looking to leverage advanced data management capabilities in a self-hosted environment while benefiting from AI-driven insights and automation. The platform's scalability, extensive integration options, and open-source flexibility make it suitable for both small businesses and large enterprises seeking a customizable, intelligent data management solution. With its AI-native architecture and API-first design, AITable.ai positions itself as a forward-thinking alternative to traditional database and spreadsheet tools.\r\n\r\n### Grist\r\n---\r\n\r\n![grist ui](../../assets/images/24/11/grist.png)\r\n\r\n[Grist](https://www.getgrist.com/) is an open-source, self-hosted spreadsheet-database hybrid that emphasizes data organization and analysis. It provides a familiar spreadsheet interface while offering the power of a relational database underneath.\r\n\r\nKey features include:\r\n- Spreadsheet-like interface with database capabilities\r\n- Custom views and widgets\r\n- Python-based formulas and data transformations\r\n- Data visualization tools\r\n- Access controls and sharing options\r\n- API access\r\n\r\n#### Why Choose Grist?\r\n\r\nGrist is particularly appealing to users who are comfortable with spreadsheets but need more powerful data manipulation and analysis tools. Its Python-based formula system allows for complex calculations and data transformations.\r\n\r\n#### Getting Started with Grist\r\n\r\nTo deploy Grist using Docker, you can use the following command:\r\n\r\n```bash\r\ndocker run -d --name grist -p 8484:8484 -v grist-data:/persist gristlabs/grist\r\n```\r\n\r\nFor more detailed setup instructions and configuration options, visit the [Grist documentation](https://support.getgrist.com/self-managed/).\r\n\r\n#### Use Cases\r\n\r\n- Financial modeling and analysis\r\n- Scientific data management\r\n- Custom business applications\r\n- Educational tools and gradebooks\r\n\r\n#### Final Thoughts\r\n\r\nGrist offers a unique approach to data management, blending the familiarity of spreadsheets with the power of databases and Python scripting. It's an excellent choice for users who need advanced data manipulation capabilities in a familiar interface.\r\n\r\n\r\n\r\n### undb\r\n---\r\n\r\n![undb ui](../../assets/images/24/11/undb.png)\r\n\r\n[undb](https://undb.io/) is a lightweight, open-source database solution that focuses on simplicity and performance. It offers a clean, minimalist interface while providing essential database functionality.\r\n\r\nKey features include:\r\n- Simple, intuitive interface\r\n- Basic views (Table, Kanban)\r\n- RESTful API\r\n- Lightweight and fast performance\r\n- Easy deployment and minimal resource usage\r\n\r\n#### Why Choose undb?\r\n\r\nundb is an excellent choice for users who need a straightforward, no-frills database solution. Its lightweight nature makes it ideal for deployment on low-resource systems or for simple data management tasks.\r\n\r\n#### Getting Started with undb\r\n\r\nTo deploy undb using Docker, you can use the following command:\r\n\r\n```bash\r\ndocker pull ghcr.io/undb-io/undb\r\ndocker run -d -p 3721:3721 ghcr.io/undb-io/undb\r\n```\r\n\r\nFor the most up-to-date installation instructions, refer to the [undb GitHub repository](https://github.com/undb-io/undb).\r\n\r\n#### Use Cases\r\n\r\n- Personal task management\r\n- Small team collaboration\r\n- Simple inventory tracking\r\n- Lightweight data collection and organization\r\n\r\n#### Final Thoughts\r\n\r\nundb offers a streamlined approach to database management, making it an excellent choice for users who prioritize simplicity and performance over extensive features. Its lightweight nature makes it particularly suitable for personal use or small team environments.\r\n\r\n## Factors to Consider When Choosing an Alternative\r\n\r\nWhen selecting a self-hosted Airtable alternative, several factors should be taken into account to ensure the chosen solution meets your specific needs:\r\n\r\n1. **Ease of use and user interface**\r\n   - Intuitiveness of the interface\r\n   - Learning curve for new users\r\n   - Availability of documentation and tutorials\r\n\r\n2. **Scalability and performance**\r\n   - Ability to handle large datasets\r\n   - Performance under concurrent user access\r\n   - Resource requirements (CPU, RAM, storage)\r\n\r\n3. **Integration capabilities**\r\n   - Available APIs (REST, GraphQL)\r\n   - Webhook support\r\n   - Pre-built integrations with other tools\r\n\r\n4. **Community support and development activity**\r\n   - Size and engagement of the user community\r\n   - Frequency of updates and bug fixes\r\n   - Availability of third-party plugins or extensions\r\n\r\n5. **Customization options**\r\n   - Ability to create custom views and fields\r\n   - Support for scripting or formulas\r\n   - Extensibility through plugins or modules\r\n\r\nConsider these factors in relation to your specific use case and organizational requirements to make an informed decision on the best self-hosted Airtable alternative for your needs.\r\n\r\n## Security Considerations for Self-Hosted Containers\r\n\r\nWhen deploying these containers, consider implementing:\r\n- Reverse proxy with SSL (like [Traefik](https://www.bitdoze.com/traefik-proxy-docker/) or Nginx Proxy Manager)\r\n- Regular backup solutions\r\n- Container update automation\r\n- Network segregation\r\n- Access control and MFA\r\n- Monitoring and logging solutions\r\n\r\n## Conclusion\r\n\r\nSelf-hosted Airtable alternatives offer a compelling solution for organizations and individuals seeking greater control over their data, customization options, and potentially reduced long-term costs. Throughout this article, we've explored several robust options, each with its unique strengths:\r\n\r\n   1. **NocoDB**: Ideal for easy setup and compatibility with existing databases.\r\n   2. **Baserow**: Great for teams needing a flexible, user-friendly solution with extensibility.\r\n   3. **AITable**: Powerful for organizations wanting AI-assisted data management.\r\n   4. **Grist**: Perfect for those comfortable with spreadsheets but needing more advanced capabilities.\r\n   5. **undb**: Lightweight option for simple data management tasks.\r\n\r\n When choosing a self-hosted Airtable alternative, consider factors such as ease of use, scalability, integration capabilities, community support, and customization options. Your specific needs, technical expertise, and resources will guide you towards the most suitable solution.\r\n\r\n Remember that self-hosting comes with responsibilities, particularly in terms of security. Implementing robust security measures, including proper network configuration, regular updates, backups, and monitoring, is crucial to protecting your data and ensuring the reliability of your chosen solution.\r\n\r\nUltimately, the transition to a self-hosted Airtable alternative can provide greater control over your data infrastructure, potentially leading to improved workflows, enhanced data privacy, and a solution tailored to your specific needs. As with any significant change in data management strategy, careful planning, testing, and gradual implementation will help ensure a smooth transition and successful adoption of your chosen self-hosted solution.\r\n\r\nBy leveraging the power of these self-hosted alternatives, you can create a data management environment that aligns perfectly with your organization's goals and requirements, fostering improved collaboration, efficiency, and data-driven decision-making.","src/content/posts/self-hosted-airtable-alternatives.mdx",[2014],"../../assets/images/24/11/self-hosted-airtable-alternatives.jpeg","32d9be3dc0817d3d","self-hosted-airtable-alternatives.mdx","pages",["Map",2019,2020,2038,2039,2054,2055,2136,2137,2197,2198],"404",{id:2019,data:2021,body:2023,filePath:2024,digest:2025,rendered:2026,legacyId:2037},{title:2022},"Error 404","## Page Not Found","src/content/pages/404.md","8b11c9dfdf7ccd03",{html:2027,metadata:2028},"<h2 id=\"page-not-found\">Page Not Found</h2>",{headings:2029,localImagePaths:2033,remoteImagePaths:2034,frontmatter:2035,imagePaths:2036},[2030],{depth:620,slug:2031,text:2032},"page-not-found","Page Not Found",[],[],{title:2022},[],"404.md","contact",{id:2038,data:2040,filePath:2043,digest:2044,rendered:2045,legacyId:2053},{title:2041,draft:2042},"Contact",false,"src/content/pages/contact.md","bc5742e58ee4058e",{html:2046,metadata:2047},"",{headings:2048,localImagePaths:2049,remoteImagePaths:2050,frontmatter:2051,imagePaths:2052},[],[],[],{title:2041,draft:2042},[],"contact.md","privacy",{id:2054,data:2056,body:2059,filePath:2060,digest:2061,rendered:2062,legacyId:2135},{title:2057,description:2058,draft:2042},"Privacy Policy","bitdoze.com privacy policy page","Last updated\\_: March 14, 2023\r\n\r\nThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\r\n\r\nWe use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy is just a Demo.\r\n\r\n## Interpretation and Definitions\r\n\r\n### Interpretation\r\n\r\nThe words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\r\n\r\n### Definitions\r\n\r\nFor the purposes of this Privacy Policy:\r\n\r\n- **Account** means a unique account created for You to access our Service or parts of our Service.\r\n- **Company** (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to bitdoze.com.\r\n- **Cookies** are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.\r\n- **Country** refers to: California, United States\r\n- **Device** means any device that can access the Service such as a computer, a cellphone or a digital tablet.\r\n- **Personal Data** is any information that relates to an identified or identifiable individual.\r\n- **Service** refers to the Website.\r\n- **Service Provider** means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.\r\n- **Usage Data** refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).\r\n- **Website** refers to Bit Doze, accessible from [https://www.bitdoze.com](https://www.bitdoze.com)\r\n- **You** means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.\r\n\r\n## Collecting and Using Your Personal Data\r\n\r\n### Types of Data Collected\r\n\r\n#### Personal Data\r\n\r\nWhile using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\r\n\r\n- Usage Data\r\n\r\n#### Usage Data\r\n\r\nUsage Data is collected automatically when using the Service.\r\n\r\nUsage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\r\n\r\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\r\n\r\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\r\n\r\n#### Tracking Technologies and Cookies\r\n\r\nWe use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\r\n\r\n- **Cookies or Browser Cookies.** A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.\r\n- **Web Beacons.** Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).\r\n\r\nCookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser.\r\n\r\nWe use both Session and Persistent Cookies for the purposes set out below:\r\n\r\n- **Necessary / Essential Cookies**\r\n\r\n  Type: Session Cookies\r\n\r\n  Administered by: Us\r\n\r\n  Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\r\n\r\n- **Cookies Policy / Notice Acceptance Cookies**\r\n\r\n  Type: Persistent Cookies\r\n\r\n  Administered by: Us\r\n\r\n  Purpose: These Cookies identify if users have accepted the use of cookies on the Website.\r\n\r\n- **Functionality Cookies**\r\n\r\n  Type: Persistent Cookies\r\n\r\n  Administered by: Us\r\n\r\n  Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\r\n\r\nFor more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\r\n\r\n## Use of Your Personal Data\r\n\r\nThe Company may use Personal Data for the following purposes:\r\n\r\n- **To provide and maintain our Service**, including to monitor the usage of our Service.\r\n- **To manage Your Account:** to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.\r\n- **For the performance of a contract:** the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.\r\n- **To contact You:** To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.\r\n- **To provide You** with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.\r\n- **To manage Your requests:** To attend and manage Your requests to Us.\r\n- **For business transfers:** We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.\r\n- **For other purposes**: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.\r\n\r\nWe may share Your personal information in the following situations:\r\n\r\n- **With Service Providers:** We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.\r\n- **For business transfers:** We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.\r\n- **With Affiliates:** We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.\r\n- **With business partners:** We may share Your information with Our business partners to offer You certain products, services or promotions.\r\n- **With other users:** when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.\r\n- **With Your consent**: We may disclose Your personal information for any other purpose with Your consent.\r\n\r\n## Retention of Your Personal Data\r\n\r\nThe Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\r\n\r\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\r\n\r\n## Transfer of Your Personal Data\r\n\r\nYour information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\r\n\r\nYour consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\r\n\r\nThe Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\r\n\r\n## Delete Your Personal Data\r\n\r\nYou have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.\r\n\r\nOur Service may give You the ability to delete certain information about You from within the Service.\r\n\r\nYou may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.\r\n\r\nPlease note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.\r\n\r\n## Disclosure of Your Personal Data\r\n\r\n### Business Transactions\r\n\r\nIf the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\r\n\r\n#### Law enforcement\r\n\r\nUnder certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\r\n\r\n#### Other legal requirements\r\n\r\nThe Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\r\n\r\n- Comply with a legal obligation\r\n- Protect and defend the rights or property of the Company\r\n- Prevent or investigate possible wrongdoing in connection with the Service\r\n- Protect the personal safety of Users of the Service or the public\r\n- Protect against legal liability\r\n\r\n## Security of Your Personal Data\r\n\r\nThe security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.\r\n\r\n## Children's Privacy\r\n\r\nOur Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.\r\n\r\nIf We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.\r\n\r\n## Links to Other Websites\r\n\r\nOur Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.\r\n\r\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\r\n\r\n## Changes to this Privacy Policy\r\n\r\nWe may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\r\n\r\nWe will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \"Last updated\" date at the top of this Privacy Policy.\r\n\r\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\r\n\r\n## Contact Us\r\n\r\nIf you have any questions about this Privacy Policy, You can contact us on our Contact Page.","src/content/pages/privacy.md","f6387d193b59f40e",{html:2063,metadata:2064},"<p>Last updated_: March 14, 2023</p>\n<p>This Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.</p>\n<p>We use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy is just a Demo.</p>\n<h2 id=\"interpretation-and-definitions\">Interpretation and Definitions</h2>\n<h3 id=\"interpretation\">Interpretation</h3>\n<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>\n<h3 id=\"definitions\">Definitions</h3>\n<p>For the purposes of this Privacy Policy:</p>\n<ul>\n<li><strong>Account</strong> means a unique account created for You to access our Service or parts of our Service.</li>\n<li><strong>Company</strong> (referred to as either “the Company”, “We”, “Us” or “Our” in this Agreement) refers to bitdoze.com.</li>\n<li><strong>Cookies</strong> are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.</li>\n<li><strong>Country</strong> refers to: California, United States</li>\n<li><strong>Device</strong> means any device that can access the Service such as a computer, a cellphone or a digital tablet.</li>\n<li><strong>Personal Data</strong> is any information that relates to an identified or identifiable individual.</li>\n<li><strong>Service</strong> refers to the Website.</li>\n<li><strong>Service Provider</strong> means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.</li>\n<li><strong>Usage Data</strong> refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).</li>\n<li><strong>Website</strong> refers to Bit Doze, accessible from <a href=\"https://www.bitdoze.com\">https://www.bitdoze.com</a></li>\n<li><strong>You</strong> means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</li>\n</ul>\n<h2 id=\"collecting-and-using-your-personal-data\">Collecting and Using Your Personal Data</h2>\n<h3 id=\"types-of-data-collected\">Types of Data Collected</h3>\n<h4 id=\"personal-data\">Personal Data</h4>\n<p>While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:</p>\n<ul>\n<li>Usage Data</li>\n</ul>\n<h4 id=\"usage-data\">Usage Data</h4>\n<p>Usage Data is collected automatically when using the Service.</p>\n<p>Usage Data may include information such as Your Device’s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.</p>\n<p>When You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.</p>\n<p>We may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.</p>\n<h4 id=\"tracking-technologies-and-cookies\">Tracking Technologies and Cookies</h4>\n<p>We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:</p>\n<ul>\n<li><strong>Cookies or Browser Cookies.</strong> A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.</li>\n<li><strong>Web Beacons.</strong> Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).</li>\n</ul>\n<p>Cookies can be “Persistent” or “Session” Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser.</p>\n<p>We use both Session and Persistent Cookies for the purposes set out below:</p>\n<ul>\n<li>\n<p><strong>Necessary / Essential Cookies</strong></p>\n<p>Type: Session Cookies</p>\n<p>Administered by: Us</p>\n<p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.</p>\n</li>\n<li>\n<p><strong>Cookies Policy / Notice Acceptance Cookies</strong></p>\n<p>Type: Persistent Cookies</p>\n<p>Administered by: Us</p>\n<p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website.</p>\n</li>\n<li>\n<p><strong>Functionality Cookies</strong></p>\n<p>Type: Persistent Cookies</p>\n<p>Administered by: Us</p>\n<p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p>\n</li>\n</ul>\n<p>For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.</p>\n<h2 id=\"use-of-your-personal-data\">Use of Your Personal Data</h2>\n<p>The Company may use Personal Data for the following purposes:</p>\n<ul>\n<li><strong>To provide and maintain our Service</strong>, including to monitor the usage of our Service.</li>\n<li><strong>To manage Your Account:</strong> to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.</li>\n<li><strong>For the performance of a contract:</strong> the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.</li>\n<li><strong>To contact You:</strong> To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application’s push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.</li>\n<li><strong>To provide You</strong> with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.</li>\n<li><strong>To manage Your requests:</strong> To attend and manage Your requests to Us.</li>\n<li><strong>For business transfers:</strong> We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.</li>\n<li><strong>For other purposes</strong>: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.</li>\n</ul>\n<p>We may share Your personal information in the following situations:</p>\n<ul>\n<li><strong>With Service Providers:</strong> We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.</li>\n<li><strong>For business transfers:</strong> We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.</li>\n<li><strong>With Affiliates:</strong> We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.</li>\n<li><strong>With business partners:</strong> We may share Your information with Our business partners to offer You certain products, services or promotions.</li>\n<li><strong>With other users:</strong> when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.</li>\n<li><strong>With Your consent</strong>: We may disclose Your personal information for any other purpose with Your consent.</li>\n</ul>\n<h2 id=\"retention-of-your-personal-data\">Retention of Your Personal Data</h2>\n<p>The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.</p>\n<p>The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.</p>\n<h2 id=\"transfer-of-your-personal-data\">Transfer of Your Personal Data</h2>\n<p>Your information, including Personal Data, is processed at the Company’s operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.</p>\n<p>Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.</p>\n<p>The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.</p>\n<h2 id=\"delete-your-personal-data\">Delete Your Personal Data</h2>\n<p>You have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.</p>\n<p>Our Service may give You the ability to delete certain information about You from within the Service.</p>\n<p>You may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.</p>\n<p>Please note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.</p>\n<h2 id=\"disclosure-of-your-personal-data\">Disclosure of Your Personal Data</h2>\n<h3 id=\"business-transactions\">Business Transactions</h3>\n<p>If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.</p>\n<h4 id=\"law-enforcement\">Law enforcement</h4>\n<p>Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).</p>\n<h4 id=\"other-legal-requirements\">Other legal requirements</h4>\n<p>The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:</p>\n<ul>\n<li>Comply with a legal obligation</li>\n<li>Protect and defend the rights or property of the Company</li>\n<li>Prevent or investigate possible wrongdoing in connection with the Service</li>\n<li>Protect the personal safety of Users of the Service or the public</li>\n<li>Protect against legal liability</li>\n</ul>\n<h2 id=\"security-of-your-personal-data\">Security of Your Personal Data</h2>\n<p>The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.</p>\n<h2 id=\"childrens-privacy\">Children’s Privacy</h2>\n<p>Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.</p>\n<p>If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent’s consent before We collect and use that information.</p>\n<h2 id=\"links-to-other-websites\">Links to Other Websites</h2>\n<p>Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party’s site. We strongly advise You to review the Privacy Policy of every site You visit.</p>\n<p>We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.</p>\n<h2 id=\"changes-to-this-privacy-policy\">Changes to this Privacy Policy</h2>\n<p>We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.</p>\n<p>We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the “Last updated” date at the top of this Privacy Policy.</p>\n<p>You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.</p>\n<h2 id=\"contact-us\">Contact Us</h2>\n<p>If you have any questions about this Privacy Policy, You can contact us on our Contact Page.</p>",{headings:2065,localImagePaths:2131,remoteImagePaths:2132,frontmatter:2133,imagePaths:2134},[2066,2069,2073,2076,2079,2082,2086,2089,2092,2095,2098,2101,2104,2107,2110,2113,2116,2119,2122,2125,2128],{depth:620,slug:2067,text:2068},"interpretation-and-definitions","Interpretation and Definitions",{depth:2070,slug:2071,text:2072},3,"interpretation","Interpretation",{depth:2070,slug:2074,text:2075},"definitions","Definitions",{depth:620,slug:2077,text:2078},"collecting-and-using-your-personal-data","Collecting and Using Your Personal Data",{depth:2070,slug:2080,text:2081},"types-of-data-collected","Types of Data Collected",{depth:2083,slug:2084,text:2085},4,"personal-data","Personal Data",{depth:2083,slug:2087,text:2088},"usage-data","Usage Data",{depth:2083,slug:2090,text:2091},"tracking-technologies-and-cookies","Tracking Technologies and Cookies",{depth:620,slug:2093,text:2094},"use-of-your-personal-data","Use of Your Personal Data",{depth:620,slug:2096,text:2097},"retention-of-your-personal-data","Retention of Your Personal Data",{depth:620,slug:2099,text:2100},"transfer-of-your-personal-data","Transfer of Your Personal Data",{depth:620,slug:2102,text:2103},"delete-your-personal-data","Delete Your Personal Data",{depth:620,slug:2105,text:2106},"disclosure-of-your-personal-data","Disclosure of Your Personal Data",{depth:2070,slug:2108,text:2109},"business-transactions","Business Transactions",{depth:2083,slug:2111,text:2112},"law-enforcement","Law enforcement",{depth:2083,slug:2114,text:2115},"other-legal-requirements","Other legal requirements",{depth:620,slug:2117,text:2118},"security-of-your-personal-data","Security of Your Personal Data",{depth:620,slug:2120,text:2121},"childrens-privacy","Children’s Privacy",{depth:620,slug:2123,text:2124},"links-to-other-websites","Links to Other Websites",{depth:620,slug:2126,text:2127},"changes-to-this-privacy-policy","Changes to this Privacy Policy",{depth:620,slug:2129,text:2130},"contact-us","Contact Us",[],[],{title:2057,description:2058,draft:2042},[],"privacy.md","terms",{id:2136,data:2138,body:2141,filePath:2142,digest:2143,rendered:2144,legacyId:2196},{title:2139,description:2140,draft:2042},"Terms and Conditions","bitdoze.com Terms and Conditions page","Last updated\\_: March 14, 2023\r\n\r\nPlease read these terms and conditions carefully before using Our Service.\r\n\r\n## Interpretation and Definitions\r\n\r\n### Interpretation\r\n\r\nThe words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\r\n\r\n### Definitions\r\n\r\nFor the purposes of these Terms and Conditions:\r\n\r\n- **Affiliate** means an entity that controls, is controlled by or is under common control with a party, where \"control\" means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.\r\n\r\n- **Country** refers to: California, United States\r\n\r\n- **Company** (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to bitdoze.com\r\n\r\n- **Device** means any device that can access the Service such as a computer, a cellphone or a digital tablet.\r\n\r\n- **Service** refers to the Website.\r\n\r\n- **Terms and Conditions** (also referred as \"Terms\") mean these Terms and Conditions that form the entire agreement between You and the Company regarding the use of the Service. This Terms and Conditions agreement is a Demo.\r\n\r\n- **Third-party Social Media Service** means any services or content (including data, information, products or services) provided by a third-party that may be displayed, included or made available by the Service.\r\n\r\n- **Website** refers to Bit Doze, accessible from [https://www.bitdoze.com](https://www.bitdoze.com)\r\n\r\n- **You** means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.\r\n\r\n## Acknowledgment\r\n\r\nThese are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.\r\n\r\nYour access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.\r\n\r\nBy accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.\r\n\r\nYou represent that you are over the age of 18\\. The Company does not permit those under 18 to use the Service.\r\n\r\nYour access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.\r\n\r\n## Links to Other Websites\r\n\r\nOur Service may contain links to third-party web sites or services that are not owned or controlled by the Company.\r\n\r\nThe Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such web sites or services.\r\n\r\nWe strongly advise You to read the terms and conditions and privacy policies of any third-party web sites or services that You visit.\r\n\r\n## Termination\r\n\r\nWe may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.\r\n\r\nUpon termination, Your right to use the Service will cease immediately.\r\n\r\n## Limitation of Liability\r\n\r\nNotwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of this Terms and Your exclusive remedy for all of the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven't purchased anything through the Service.\r\n\r\nTo the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of this Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.\r\n\r\nSome states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party's liability will be limited to the greatest extent permitted by law.\r\n\r\n## \"AS IS\" and \"AS AVAILABLE\" Disclaimer\r\n\r\nThe Service is provided to You \"AS IS\" and \"AS AVAILABLE\" and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice. Without limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error free or that any errors or defects can or will be corrected.\r\n\r\nWithout limiting the foregoing, neither the Company nor any of the company's provider makes any representation or warranty of any kind, express or implied: (i) as to the operation or availability of the Service, or the information, content, and materials or products included thereon; (ii) that the Service will be uninterrupted or error-free; (iii) as to the accuracy, reliability, or currency of any information or content provided through the Service; or (iv) that the Service, its servers, the content, or e-mails sent from or on behalf of the Company are free of viruses, scripts, trojan horses, worms, malware, timebombs or other harmful components.\r\n\r\nSome jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.\r\n\r\n## Governing Law\r\n\r\nThe laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.\r\n\r\n## Disputes Resolution\r\n\r\nIf You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.\r\n\r\n## For European Union (EU) Users\r\n\r\nIf You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which you are resident in.\r\n\r\n## United States Legal Compliance\r\n\r\nYou represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a \"terrorist supporting\" country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.\r\n\r\n## Severability and Waiver\r\n\r\n### Severability\r\n\r\nIf any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.\r\n\r\n### Waiver\r\n\r\nExcept as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not effect a party's ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.\r\n\r\n## Translation Interpretation\r\n\r\nThese Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.\r\n\r\n## Changes to These Terms and Conditions\r\n\r\nWe reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.\r\n\r\nBy continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.\r\n\r\n## Contact Us\r\n\r\nIf you have any questions about these Terms and Conditions, You can contact us by using Contact page.","src/content/pages/terms.md","53f17b1151d7105a",{html:2145,metadata:2146},"<p>Last updated_: March 14, 2023</p>\n<p>Please read these terms and conditions carefully before using Our Service.</p>\n<h2 id=\"interpretation-and-definitions\">Interpretation and Definitions</h2>\n<h3 id=\"interpretation\">Interpretation</h3>\n<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>\n<h3 id=\"definitions\">Definitions</h3>\n<p>For the purposes of these Terms and Conditions:</p>\n<ul>\n<li>\n<p><strong>Affiliate</strong> means an entity that controls, is controlled by or is under common control with a party, where “control” means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.</p>\n</li>\n<li>\n<p><strong>Country</strong> refers to: California, United States</p>\n</li>\n<li>\n<p><strong>Company</strong> (referred to as either “the Company”, “We”, “Us” or “Our” in this Agreement) refers to bitdoze.com</p>\n</li>\n<li>\n<p><strong>Device</strong> means any device that can access the Service such as a computer, a cellphone or a digital tablet.</p>\n</li>\n<li>\n<p><strong>Service</strong> refers to the Website.</p>\n</li>\n<li>\n<p><strong>Terms and Conditions</strong> (also referred as “Terms”) mean these Terms and Conditions that form the entire agreement between You and the Company regarding the use of the Service. This Terms and Conditions agreement is a Demo.</p>\n</li>\n<li>\n<p><strong>Third-party Social Media Service</strong> means any services or content (including data, information, products or services) provided by a third-party that may be displayed, included or made available by the Service.</p>\n</li>\n<li>\n<p><strong>Website</strong> refers to Bit Doze, accessible from <a href=\"https://www.bitdoze.com\">https://www.bitdoze.com</a></p>\n</li>\n<li>\n<p><strong>You</strong> means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</p>\n</li>\n</ul>\n<h2 id=\"acknowledgment\">Acknowledgment</h2>\n<p>These are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.</p>\n<p>Your access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.</p>\n<p>By accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.</p>\n<p>You represent that you are over the age of 18. The Company does not permit those under 18 to use the Service.</p>\n<p>Your access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.</p>\n<h2 id=\"links-to-other-websites\">Links to Other Websites</h2>\n<p>Our Service may contain links to third-party web sites or services that are not owned or controlled by the Company.</p>\n<p>The Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such web sites or services.</p>\n<p>We strongly advise You to read the terms and conditions and privacy policies of any third-party web sites or services that You visit.</p>\n<h2 id=\"termination\">Termination</h2>\n<p>We may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.</p>\n<p>Upon termination, Your right to use the Service will cease immediately.</p>\n<h2 id=\"limitation-of-liability\">Limitation of Liability</h2>\n<p>Notwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of this Terms and Your exclusive remedy for all of the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven’t purchased anything through the Service.</p>\n<p>To the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of this Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.</p>\n<p>Some states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party’s liability will be limited to the greatest extent permitted by law.</p>\n<h2 id=\"as-is-and-as-available-disclaimer\">”AS IS” and “AS AVAILABLE” Disclaimer</h2>\n<p>The Service is provided to You “AS IS” and “AS AVAILABLE” and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice. Without limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error free or that any errors or defects can or will be corrected.</p>\n<p>Without limiting the foregoing, neither the Company nor any of the company’s provider makes any representation or warranty of any kind, express or implied: (i) as to the operation or availability of the Service, or the information, content, and materials or products included thereon; (ii) that the Service will be uninterrupted or error-free; (iii) as to the accuracy, reliability, or currency of any information or content provided through the Service; or (iv) that the Service, its servers, the content, or e-mails sent from or on behalf of the Company are free of viruses, scripts, trojan horses, worms, malware, timebombs or other harmful components.</p>\n<p>Some jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.</p>\n<h2 id=\"governing-law\">Governing Law</h2>\n<p>The laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.</p>\n<h2 id=\"disputes-resolution\">Disputes Resolution</h2>\n<p>If You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.</p>\n<h2 id=\"for-european-union-eu-users\">For European Union (EU) Users</h2>\n<p>If You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which you are resident in.</p>\n<h2 id=\"united-states-legal-compliance\">United States Legal Compliance</h2>\n<p>You represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a “terrorist supporting” country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.</p>\n<h2 id=\"severability-and-waiver\">Severability and Waiver</h2>\n<h3 id=\"severability\">Severability</h3>\n<p>If any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.</p>\n<h3 id=\"waiver\">Waiver</h3>\n<p>Except as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not effect a party’s ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.</p>\n<h2 id=\"translation-interpretation\">Translation Interpretation</h2>\n<p>These Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.</p>\n<h2 id=\"changes-to-these-terms-and-conditions\">Changes to These Terms and Conditions</h2>\n<p>We reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days’ notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.</p>\n<p>By continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.</p>\n<h2 id=\"contact-us\">Contact Us</h2>\n<p>If you have any questions about these Terms and Conditions, You can contact us by using Contact page.</p>",{headings:2147,localImagePaths:2192,remoteImagePaths:2193,frontmatter:2194,imagePaths:2195},[2148,2149,2150,2151,2154,2155,2158,2161,2164,2167,2170,2173,2176,2179,2182,2185,2188,2191],{depth:620,slug:2067,text:2068},{depth:2070,slug:2071,text:2072},{depth:2070,slug:2074,text:2075},{depth:620,slug:2152,text:2153},"acknowledgment","Acknowledgment",{depth:620,slug:2123,text:2124},{depth:620,slug:2156,text:2157},"termination","Termination",{depth:620,slug:2159,text:2160},"limitation-of-liability","Limitation of Liability",{depth:620,slug:2162,text:2163},"as-is-and-as-available-disclaimer","”AS IS” and “AS AVAILABLE” Disclaimer",{depth:620,slug:2165,text:2166},"governing-law","Governing Law",{depth:620,slug:2168,text:2169},"disputes-resolution","Disputes Resolution",{depth:620,slug:2171,text:2172},"for-european-union-eu-users","For European Union (EU) Users",{depth:620,slug:2174,text:2175},"united-states-legal-compliance","United States Legal Compliance",{depth:620,slug:2177,text:2178},"severability-and-waiver","Severability and Waiver",{depth:2070,slug:2180,text:2181},"severability","Severability",{depth:2070,slug:2183,text:2184},"waiver","Waiver",{depth:620,slug:2186,text:2187},"translation-interpretation","Translation Interpretation",{depth:620,slug:2189,text:2190},"changes-to-these-terms-and-conditions","Changes to These Terms and Conditions",{depth:620,slug:2129,text:2130},[],[],{title:2139,description:2140,draft:2042},[],"terms.md","elements",{id:2197,data:2199,body:2201,filePath:2202,digest:2203,rendered:2204,legacyId:2263},{title:2200,draft:2042},"Elements","#### Heading example\r\n\r\nHere is an example of headings. You can use this heading by the following markdown rules. For example: use `#` for heading 1 and use `######` for heading 6.\r\n\r\n# Heading 1\r\n\r\n## Heading 2\r\n\r\n### Heading 3\r\n\r\n#### Heading 4\r\n\r\n##### Heading 5\r\n\r\n###### Heading 6\r\n\r\n---\r\n\r\n### Emphasis\r\n\r\nThe emphasis, aka italics, with _asterisks_ or _underscores_.\r\n\r\nStrong emphasis, aka bold, with **asterisks** or **underscores**.\r\n\r\nThe combined emphasis with **asterisks and _underscores_**.\r\n\r\nStrikethrough uses two tildes. ~~Scratch this.~~\r\n\r\n---\r\n\r\n### Link\r\n\r\n[I'm an inline-style link](https://www.google.com)\r\n\r\n[I'm an inline-style link with title](https://www.google.com \"Google's Homepage\")\r\n\r\n[I'm a reference-style link][arbitrary case-insensitive reference text]\r\n\r\n[I'm a relative reference to a repository file](../blob/master/LICENSE)\r\n\r\n[You can use numbers for reference-style link definitions][1]\r\n\r\nOr leave it empty and use the [link text itself].\r\n\r\nexample.com (but not on Github, for example).\r\n\r\nSome text to show that the reference links can follow later.\r\n\r\n[arbitrary case-insensitive reference text]: https://www.themefisher.com\r\n[1]: https://gethugothemes.com\r\n[link text itself]: https://www.getjekyllthemes.com\r\n\r\n---\r\n\r\n### Paragraph\r\n\r\nLorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\r\n\r\n---\r\n\r\n### Ordered List\r\n\r\n1. List item\r\n2. List item\r\n3. List item\r\n4. List item\r\n5. List item\r\n\r\n---\r\n\r\n### Unordered List\r\n\r\n- List item\r\n- List item\r\n- List item\r\n- List item\r\n- List item\r\n\r\n---\r\n\r\n### Code and Syntax Highlighting\r\n\r\nThis is an `Inline code` sample.\r\n\r\n```javascript\r\nvar s = \"JavaScript syntax highlighting\";\r\nalert(s);\r\n```\r\n\r\n```python\r\ns = \"Python syntax highlighting\"\r\nprint s\r\n```\r\n\r\n---\r\n\r\n### Blockquote\r\n\r\n> This is a blockquote example.\r\n\r\n---\r\n\r\n### Inline HTML\r\n\r\nYou can also use raw HTML in your Markdown, and it'll mostly work pretty well.\r\n\r\n<dl>\r\n  <dt>Definition list</dt>\r\n  <dd>Is something people use sometimes.</dd>\r\n\r\n  <dt>Markdown in HTML</dt>\r\n  <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd>\r\n</dl>\r\n\r\n---\r\n\r\n### Tables\r\n\r\n| Tables        |      Are      |  Cool |\r\n| ------------- | :-----------: | ----: |\r\n| col 3 is      | right-aligned | $1600 |\r\n| col 2 is      |   centered    |   $12 |\r\n| zebra stripes |   are neat    |    $1 |\r\n\r\nThere must be at least 3 dashes separating each header cell.\r\nThe outer pipes (|) are optional, and you don't need to make the\r\nraw Markdown line up prettily. You can also use inline Markdown.\r\n\r\n| Markdown | Less      | Pretty     |\r\n| -------- | --------- | ---------- |\r\n| _Still_  | `renders` | **nicely** |\r\n| 1        | 2         | 3          |","src/content/pages/elements.md","dbc1974b448ba3ae",{html:2205,metadata:2206},"<h4 id=\"heading-example\">Heading example</h4>\n<p>Here is an example of headings. You can use this heading by the following markdown rules. For example: use <code>#</code> for heading 1 and use <code>######</code> for heading 6.</p>\n<h1 id=\"heading-1\">Heading 1</h1>\n<h2 id=\"heading-2\">Heading 2</h2>\n<h3 id=\"heading-3\">Heading 3</h3>\n<h4 id=\"heading-4\">Heading 4</h4>\n<h5 id=\"heading-5\">Heading 5</h5>\n<h6 id=\"heading-6\">Heading 6</h6>\n<hr>\n<h3 id=\"emphasis\">Emphasis</h3>\n<p>The emphasis, aka italics, with <em>asterisks</em> or <em>underscores</em>.</p>\n<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>\n<p>The combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>\n<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>\n<hr>\n<h3 id=\"link\">Link</h3>\n<p><a href=\"https://www.google.com\">I’m an inline-style link</a></p>\n<p><a href=\"https://www.google.com\" title=\"Google&#x27;s Homepage\">I’m an inline-style link with title</a></p>\n<p><a href=\"https://www.themefisher.com\">I’m a reference-style link</a></p>\n<p><a href=\"../blob/master/LICENSE\">I’m a relative reference to a repository file</a></p>\n<p><a href=\"https://gethugothemes.com\">You can use numbers for reference-style link definitions</a></p>\n<p>Or leave it empty and use the <a href=\"https://www.getjekyllthemes.com\">link text itself</a>.</p>\n<p>example.com (but not on Github, for example).</p>\n<p>Some text to show that the reference links can follow later.</p>\n<hr>\n<h3 id=\"paragraph\">Paragraph</h3>\n<p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.</p>\n<hr>\n<h3 id=\"ordered-list\">Ordered List</h3>\n<ol>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n</ol>\n<hr>\n<h3 id=\"unordered-list\">Unordered List</h3>\n<ul>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n<li>List item</li>\n</ul>\n<hr>\n<h3 id=\"code-and-syntax-highlighting\">Code and Syntax Highlighting</h3>\n<p>This is an <code>Inline code</code> sample.</p>\n<pre class=\"astro-code one-dark-pro\" style=\"background-color:#282c34;color:#abb2bf; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"javascript\"><code><span class=\"line\"><span style=\"color:#C678DD\">var</span><span style=\"color:#E06C75\"> s</span><span style=\"color:#56B6C2\"> =</span><span style=\"color:#98C379\"> \"JavaScript syntax highlighting\"</span><span style=\"color:#ABB2BF\">;</span></span>\n<span class=\"line\"><span style=\"color:#61AFEF\">alert</span><span style=\"color:#ABB2BF\">(</span><span style=\"color:#E06C75\">s</span><span style=\"color:#ABB2BF\">);</span></span></code></pre>\n<pre class=\"astro-code one-dark-pro\" style=\"background-color:#282c34;color:#abb2bf; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\"><code><span class=\"line\"><span style=\"color:#ABB2BF\">s </span><span style=\"color:#56B6C2\">=</span><span style=\"color:#98C379\"> \"Python syntax highlighting\"</span></span>\n<span class=\"line\"><span style=\"color:#56B6C2\">print</span><span style=\"color:#ABB2BF\"> s</span></span></code></pre>\n<hr>\n<h3 id=\"blockquote\">Blockquote</h3>\n<blockquote>\n<p>This is a blockquote example.</p>\n</blockquote>\n<hr>\n<h3 id=\"inline-html\">Inline HTML</h3>\n<p>You can also use raw HTML in your Markdown, and it’ll mostly work pretty well.</p>\n<dl>\n  <dt>Definition list</dt>\n  <dd>Is something people use sometimes.</dd>\n  <dt>Markdown in HTML</dt>\n  <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd>\n</dl>\n<hr>\n<h3 id=\"tables\">Tables</h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Tables</th><th align=\"center\">Are</th><th align=\"right\">Cool</th></tr></thead><tbody><tr><td>col 3 is</td><td align=\"center\">right-aligned</td><td align=\"right\">$1600</td></tr><tr><td>col 2 is</td><td align=\"center\">centered</td><td align=\"right\">$12</td></tr><tr><td>zebra stripes</td><td align=\"center\">are neat</td><td align=\"right\">$1</td></tr></tbody></table>\n<p>There must be at least 3 dashes separating each header cell.\r\nThe outer pipes (|) are optional, and you don’t need to make the\r\nraw Markdown line up prettily. You can also use inline Markdown.</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th>Markdown</th><th>Less</th><th>Pretty</th></tr></thead><tbody><tr><td><em>Still</em></td><td><code>renders</code></td><td><strong>nicely</strong></td></tr><tr><td>1</td><td>2</td><td>3</td></tr></tbody></table>",{headings:2207,localImagePaths:2259,remoteImagePaths:2260,frontmatter:2261,imagePaths:2262},[2208,2211,2215,2218,2221,2224,2228,2232,2235,2238,2241,2244,2247,2250,2253,2256],{depth:2083,slug:2209,text:2210},"heading-example","Heading example",{depth:2212,slug:2213,text:2214},1,"heading-1","Heading 1",{depth:620,slug:2216,text:2217},"heading-2","Heading 2",{depth:2070,slug:2219,text:2220},"heading-3","Heading 3",{depth:2083,slug:2222,text:2223},"heading-4","Heading 4",{depth:2225,slug:2226,text:2227},5,"heading-5","Heading 5",{depth:2229,slug:2230,text:2231},6,"heading-6","Heading 6",{depth:2070,slug:2233,text:2234},"emphasis","Emphasis",{depth:2070,slug:2236,text:2237},"link","Link",{depth:2070,slug:2239,text:2240},"paragraph","Paragraph",{depth:2070,slug:2242,text:2243},"ordered-list","Ordered List",{depth:2070,slug:2245,text:2246},"unordered-list","Unordered List",{depth:2070,slug:2248,text:2249},"code-and-syntax-highlighting","Code and Syntax Highlighting",{depth:2070,slug:2251,text:2252},"blockquote","Blockquote",{depth:2070,slug:2254,text:2255},"inline-html","Inline HTML",{depth:2070,slug:2257,text:2258},"tables","Tables",[],[],{title:2200,draft:2042},[],"elements.md","authors",["Map",2266,2267,2288,2289],"dragos",{id:2266,data:2268,body:2275,filePath:2276,digest:2277,rendered:2278,legacyId:2287},{title:19,image:2269,description:2270,social:2271},"/images/authors/dragos.webp","Dragos the author of the website",{facebook:2272,twitter:2273,instagram:2274},"https://www.facebook.com/dragos.balota","https://www.twitter.com/bitdoze","https://www.instagram.com/","Hey there! Allow me to introduce myself. My name is Dragos, and I'm a seasoned IT professional with over a decade of experience in the field. For the past four years, I have been deeply immersed in the fascinating world of DevOps, constantly honing my skills and staying up to date with the latest industry trends.\r\n\r\nOne of my passions lies in writing about WordPress, and I have a dedicated platform called [wpdoze.com ](https://www.wpdoze.com/)where I share my insights and expertise on this incredibly popular content management system. From practical tips to in-depth tutorials, I strive to provide valuable resources that empower individuals and businesses to make the most out of their WordPress websites.\r\n\r\nIn addition to my love for WordPress, I am equally enthusiastic about Linux, static sites, CMS, VPS, and all things related to DevOps. These subjects have captivated my attention, prompting me to launch another platform called [bitdoze.com](https://www.bitdoze.com/). On this website, I delve into the intricate workings of Linux, explore the realm of static sites, and shed light on the ever-evolving world of content management systems. Moreover, I offer expert guidance on VPS setups and share my insights into the fascinating realm of DevOps.\r\n\r\nBeyond my professional pursuits, I find solace in two enjoyable hobbies: traveling and indulging in movies. Exploring new destinations and immersing myself in diverse cultures rejuvenates my spirit and broadens my perspectives. It fuels my creativity and inspires me to think outside the box, which undoubtedly reflects in my work.\r\n\r\nFurthermore, I have an extensive network of more than ten affiliate sites that I actively promote. These platforms cover a range of topics and products, allowing me to connect with a diverse audience and share valuable recommendations. Collaborating with like-minded individuals and fostering partnerships is something I genuinely cherish, as it enables me to contribute to the growth and success of others while expanding my own horizons.\r\n\r\nIn a nutshell, I'm Dragos—an IT professional with a profound passion for WordPress, Linux, static sites, CMS, VPS, and DevOps. Through my websites, wpdoze.com and bitdoze.com, I aim to share my expertise, empower others, and foster a sense of community within the tech realm. When I'm not immersed in the digital landscape, you can find me embarking on exciting journeys or unwinding with a captivating movie. I'm thrilled to be on this ever-evolving professional journey, and I look forward to sharing my knowledge and experiences with you.","src/content/authors/dragos.md","4525f41b8c75f20f",{html:2279,metadata:2280},"<p>Hey there! Allow me to introduce myself. My name is Dragos, and I’m a seasoned IT professional with over a decade of experience in the field. For the past four years, I have been deeply immersed in the fascinating world of DevOps, constantly honing my skills and staying up to date with the latest industry trends.</p>\n<p>One of my passions lies in writing about WordPress, and I have a dedicated platform called <a href=\"https://www.wpdoze.com/\">wpdoze.com </a>where I share my insights and expertise on this incredibly popular content management system. From practical tips to in-depth tutorials, I strive to provide valuable resources that empower individuals and businesses to make the most out of their WordPress websites.</p>\n<p>In addition to my love for WordPress, I am equally enthusiastic about Linux, static sites, CMS, VPS, and all things related to DevOps. These subjects have captivated my attention, prompting me to launch another platform called <a href=\"https://www.bitdoze.com/\">bitdoze.com</a>. On this website, I delve into the intricate workings of Linux, explore the realm of static sites, and shed light on the ever-evolving world of content management systems. Moreover, I offer expert guidance on VPS setups and share my insights into the fascinating realm of DevOps.</p>\n<p>Beyond my professional pursuits, I find solace in two enjoyable hobbies: traveling and indulging in movies. Exploring new destinations and immersing myself in diverse cultures rejuvenates my spirit and broadens my perspectives. It fuels my creativity and inspires me to think outside the box, which undoubtedly reflects in my work.</p>\n<p>Furthermore, I have an extensive network of more than ten affiliate sites that I actively promote. These platforms cover a range of topics and products, allowing me to connect with a diverse audience and share valuable recommendations. Collaborating with like-minded individuals and fostering partnerships is something I genuinely cherish, as it enables me to contribute to the growth and success of others while expanding my own horizons.</p>\n<p>In a nutshell, I’m Dragos—an IT professional with a profound passion for WordPress, Linux, static sites, CMS, VPS, and DevOps. Through my websites, wpdoze.com and bitdoze.com, I aim to share my expertise, empower others, and foster a sense of community within the tech realm. When I’m not immersed in the digital landscape, you can find me embarking on exciting journeys or unwinding with a captivating movie. I’m thrilled to be on this ever-evolving professional journey, and I look forward to sharing my knowledge and experiences with you.</p>",{headings:2281,localImagePaths:2282,remoteImagePaths:2283,frontmatter:2284,imagePaths:2286},[],[],[],{title:19,image:2269,description:2270,social:2285},{facebook:2272,twitter:2273,instagram:2274},[],"dragos.md","-index",{id:2288,data:2290,filePath:2292,digest:2293,rendered:2294,legacyId:2301},{title:2291},"Authors","src/content/authors/-index.md","0af5d121d4a9f4a7",{html:2046,metadata:2295},{headings:2296,localImagePaths:2297,remoteImagePaths:2298,frontmatter:2299,imagePaths:2300},[],[],[],{title:2291},[],"-index.md","about",["Map",2304,2305],"index",{id:2304,data:2306,body:2321,filePath:2322,digest:2323,rendered:2324,legacyId:2334},{title:2307,meta_title:2307,image:2308,draft:2042,what_i_do:2309},"About","/dragos.jpeg",{title:2310,items:2311},"What I Do",[2312,2315,2318],{title:2313,description:2314},"Content Writing","I write content on my blogs.",{title:2316,description:2317},"DevOps","DevOps on my bussiness.",{title:2319,description:2320},"Web Design","Web Design on WordPress and Astro","Hey there! Allow me to introduce myself. My name is Dragos, and I'm a seasoned IT professional with over a decade of experience in the field. For the past four years, I have been deeply immersed in the fascinating world of DevOps, constantly honing my skills and staying up to date with the latest industry trends.\r\n\r\nOne of my passions lies in writing about WordPress, and I have a dedicated platform called [wpdoze.com ](https://www.wpdoze.com/)where I share my insights and expertise on this incredibly popular content management system. From practical tips to in-depth tutorials, I strive to provide valuable resources that empower individuals and businesses to make the most out of their WordPress websites.\r\n\r\nIn addition to my love for WordPress, I am equally enthusiastic about Linux, static sites, CMS, VPS, and all things related to DevOps. These subjects have captivated my attention, prompting me to launch another platform called [bitdoze.com](https://www.bitdoze.com/). On this website, I delve into the intricate workings of Linux, explore the realm of static sites, and shed light on the ever-evolving world of content management systems. Moreover, I offer expert guidance on VPS setups and share my insights into the fascinating realm of DevOps.\r\n\r\nBeyond my professional pursuits, I find solace in two enjoyable hobbies: traveling and indulging in movies. Exploring new destinations and immersing myself in diverse cultures rejuvenates my spirit and broadens my perspectives. It fuels my creativity and inspires me to think outside the box, which undoubtedly reflects in my work.\r\n\r\nFurthermore, I have an extensive network of more than ten affiliate sites that I actively promote. These platforms cover a range of topics and products, allowing me to connect with a diverse audience and share valuable recommendations. Collaborating with like-minded individuals and fostering partnerships is something I genuinely cherish, as it enables me to contribute to the growth and success of others while expanding my own horizons.\r\n\r\nIn a nutshell, I'm Dragos—an IT professional with a profound passion for WordPress, Linux, static sites, CMS, VPS, and DevOps. Through my websites, wpdoze.com and bitdoze.com, I aim to share my expertise, empower others, and foster a sense of community within the tech realm. When I'm not immersed in the digital landscape, you can find me embarking on exciting journeys or unwinding with a captivating movie. I'm thrilled to be on this ever-evolving professional journey, and I look forward to sharing my knowledge and experiences with you.\r\n\r\n## Get In Touch With Me\r\n\r\n- [Use Contact Page](https://www.bitdoze.com/contact/)\r\n- [Twitter](https://twitter.com/bitdoze)\r\n- [YouTube](https://www.youtube.com/@webdoze)\r\n- [FaceBook](https://www.facebook.com/dragos.balota)","src/content/about/index.md","10c427927e8e98db",{html:2325,metadata:2326},"<p>Hey there! Allow me to introduce myself. My name is Dragos, and I’m a seasoned IT professional with over a decade of experience in the field. For the past four years, I have been deeply immersed in the fascinating world of DevOps, constantly honing my skills and staying up to date with the latest industry trends.</p>\n<p>One of my passions lies in writing about WordPress, and I have a dedicated platform called <a href=\"https://www.wpdoze.com/\">wpdoze.com </a>where I share my insights and expertise on this incredibly popular content management system. From practical tips to in-depth tutorials, I strive to provide valuable resources that empower individuals and businesses to make the most out of their WordPress websites.</p>\n<p>In addition to my love for WordPress, I am equally enthusiastic about Linux, static sites, CMS, VPS, and all things related to DevOps. These subjects have captivated my attention, prompting me to launch another platform called <a href=\"https://www.bitdoze.com/\">bitdoze.com</a>. On this website, I delve into the intricate workings of Linux, explore the realm of static sites, and shed light on the ever-evolving world of content management systems. Moreover, I offer expert guidance on VPS setups and share my insights into the fascinating realm of DevOps.</p>\n<p>Beyond my professional pursuits, I find solace in two enjoyable hobbies: traveling and indulging in movies. Exploring new destinations and immersing myself in diverse cultures rejuvenates my spirit and broadens my perspectives. It fuels my creativity and inspires me to think outside the box, which undoubtedly reflects in my work.</p>\n<p>Furthermore, I have an extensive network of more than ten affiliate sites that I actively promote. These platforms cover a range of topics and products, allowing me to connect with a diverse audience and share valuable recommendations. Collaborating with like-minded individuals and fostering partnerships is something I genuinely cherish, as it enables me to contribute to the growth and success of others while expanding my own horizons.</p>\n<p>In a nutshell, I’m Dragos—an IT professional with a profound passion for WordPress, Linux, static sites, CMS, VPS, and DevOps. Through my websites, wpdoze.com and bitdoze.com, I aim to share my expertise, empower others, and foster a sense of community within the tech realm. When I’m not immersed in the digital landscape, you can find me embarking on exciting journeys or unwinding with a captivating movie. I’m thrilled to be on this ever-evolving professional journey, and I look forward to sharing my knowledge and experiences with you.</p>\n<h2 id=\"get-in-touch-with-me\">Get In Touch With Me</h2>\n<ul>\n<li><a href=\"https://www.bitdoze.com/contact/\">Use Contact Page</a></li>\n<li><a href=\"https://twitter.com/bitdoze\">Twitter</a></li>\n<li><a href=\"https://www.youtube.com/@webdoze\">YouTube</a></li>\n<li><a href=\"https://www.facebook.com/dragos.balota\">FaceBook</a></li>\n</ul>",{headings:2327,localImagePaths:2331,remoteImagePaths:2332,frontmatter:2306,imagePaths:2333},[2328],{depth:620,slug:2329,text:2330},"get-in-touch-with-me","Get In Touch With Me",[],[],[],"index.md"];

export { _astro_dataLayerContent as default };
